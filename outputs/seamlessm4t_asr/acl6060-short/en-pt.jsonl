{"dataset_id": "acl_6060", "sample_id": 0, "src_lang": "en", "tgt_lang": "en", "output": "hi everyone today i am going to present our research work learning to reason deductively material problem solving as complex reason extraction"}
{"dataset_id": "acl_6060", "sample_id": 1, "src_lang": "en", "tgt_lang": "en", "output": "i'm alan from bython's air lab and this is a joint work with cheri from the university of texas at austin and weido from sdd"}
{"dataset_id": "acl_6060", "sample_id": 2, "src_lang": "en", "tgt_lang": "en", "output": "first i'd like to talk about our motivation for reasoning"}
{"dataset_id": "acl_6060", "sample_id": 3, "src_lang": "en", "tgt_lang": "en", "output": "We'll show you examples where the staple is healthy."}
{"dataset_id": "acl_6060", "sample_id": 4, "src_lang": "en", "tgt_lang": "en", "output": "This figure is taken from the paper where they perform prompting to solve the math problem in a future learning scenario."}
{"dataset_id": "acl_6060", "sample_id": 5, "src_lang": "en", "tgt_lang": "en", "output": "so on the net pen site we can see if we give some samples with just correct and in answers we may not be able to obtain the correct answers"}
{"dataset_id": "acl_6060", "sample_id": 6, "src_lang": "en", "tgt_lang": "en", "output": "but if we give some more reasoning description the model is able to predict the reasoning description and also make the correct prediction here"}
{"dataset_id": "acl_6060", "sample_id": 7, "src_lang": "en", "tgt_lang": "en", "output": "So it's good to have an interchangeable multi-step reasoning out."}
{"dataset_id": "acl_6060", "sample_id": 8, "src_lang": "en", "tgt_lang": "en", "output": "And we also think method problem is a straightforward application to evaluate such reasoning abilities."}
{"dataset_id": "acl_6060", "sample_id": 9, "src_lang": "en", "tgt_lang": "en", "output": "So here in our problem setup, given the questions, we need to solve this question and obtain the numerical answers."}
{"dataset_id": "acl_6060", "sample_id": 10, "src_lang": "en", "tgt_lang": "en", "output": "so in our datasets we are also given the mathematical expression which leads to the to this particular answer as well"}
{"dataset_id": "acl_6060", "sample_id": 11, "src_lang": "en", "tgt_lang": "en", "output": "So certain assumptions also apply as in previous work."}
{"dataset_id": "acl_6060", "sample_id": 12, "src_lang": "en", "tgt_lang": "en", "output": "we assume the precision of quantities known"}
{"dataset_id": "acl_6060", "sample_id": 13, "src_lang": "en", "tgt_lang": "en", "output": "And we only consider basic operators such as addition, subtraction, multiplication, division and exponential."}
{"dataset_id": "acl_6060", "sample_id": 14, "src_lang": "en", "tgt_lang": "en", "output": "Furthermore, complicated operators can be actually decomposed into these basic operators."}
{"dataset_id": "acl_6060", "sample_id": 15, "src_lang": "en", "tgt_lang": "en", "output": "So previous work in mathematical problem solving can actually categorize into sequence-to-sequence and sequence-to-tree models."}
{"dataset_id": "acl_6060", "sample_id": 16, "src_lang": "en", "tgt_lang": "en", "output": "so traditional sequence to sequence model convert the expression to a specific sequence for generation"}
{"dataset_id": "acl_6060", "sample_id": 17, "src_lang": "en", "tgt_lang": "en", "output": "and it is pretty easy to implement and it can generalize to many different complicated problems"}
{"dataset_id": "acl_6060", "sample_id": 18, "src_lang": "en", "tgt_lang": "en", "output": "but the drawbacks of the performance is actually generally not better than the structural model and it is lack of the interpretability for prediction"}
{"dataset_id": "acl_6060", "sample_id": 19, "src_lang": "en", "tgt_lang": "en", "output": "But actually this direction is still quite popular because of the transformer model."}
{"dataset_id": "acl_6060", "sample_id": 20, "src_lang": "en", "tgt_lang": "en", "output": "So in tree-based models, we actually structure these expressions in a tree form and follow a preorder traverse in tree generations."}
{"dataset_id": "acl_6060", "sample_id": 21, "src_lang": "en", "tgt_lang": "en", "output": "so here we keep generating the operators until we reach the lefts which are the quantities"}
{"dataset_id": "acl_6060", "sample_id": 22, "src_lang": "en", "tgt_lang": "en", "output": "So here the good thing is that it actually gives us this binary tree structure and it is actually quite contingency because we generate the operator first and then at the end we generate the quantities"}
{"dataset_id": "acl_6060", "sample_id": 23, "src_lang": "en", "tgt_lang": "en", "output": "and the second thing is that it also contains some repetitive commutations"}
{"dataset_id": "acl_6060", "sample_id": 24, "src_lang": "en", "tgt_lang": "en", "output": "so here if we look at this expression atoms three plus three is actually generated twice but in facts we should we use the results"}
{"dataset_id": "acl_6060", "sample_id": 25, "src_lang": "en", "tgt_lang": "en", "output": "So in our proposed approach we want to solve those problems in a step by step and interpretable manner."}
{"dataset_id": "acl_6060", "sample_id": 26, "src_lang": "en", "tgt_lang": "en", "output": "So for example here in the second step, we can obtain these divisors, which is 27."}
{"dataset_id": "acl_6060", "sample_id": 27, "src_lang": "en", "tgt_lang": "en", "output": "We can also refer back to the original questions to find the relevant contents."}
{"dataset_id": "acl_6060", "sample_id": 28, "src_lang": "en", "tgt_lang": "en", "output": "And in these steps, we obtain the divisors."}
{"dataset_id": "acl_6060", "sample_id": 29, "src_lang": "en", "tgt_lang": "en", "output": "So, and then at this third step, we actually get the quotient"}
{"dataset_id": "acl_6060", "sample_id": 30, "src_lang": "en", "tgt_lang": "en", "output": "And after these three steps, we can actually reduce the results from the second step and then get the results of the fourth step and then finally we can get the dividends."}
{"dataset_id": "acl_6060", "sample_id": 31, "src_lang": "en", "tgt_lang": "en", "output": "So here we actually generate the whole expression directly, rather than generating a single operators or quantities."}
{"dataset_id": "acl_6060", "sample_id": 32, "src_lang": "en", "tgt_lang": "en", "output": "so this makes the process more accurate"}
{"dataset_id": "acl_6060", "sample_id": 33, "src_lang": "en", "tgt_lang": "en", "output": "So in our didactic system, we first start with a bunch of quantities presented in the questions, and also including some constants as our initialisms."}
{"dataset_id": "acl_6060", "sample_id": 34, "src_lang": "en", "tgt_lang": "en", "output": "So the expression is represented by E.I.J.O.P."}
{"dataset_id": "acl_6060", "sample_id": 35, "src_lang": "en", "tgt_lang": "en", "output": "where we perform operators from QI to QJ, and such expression is actually directed."}
{"dataset_id": "acl_6060", "sample_id": 36, "src_lang": "en", "tgt_lang": "en", "output": "So we also have subtraction with words here to represent the opposite direction."}
{"dataset_id": "acl_6060", "sample_id": 37, "src_lang": "en", "tgt_lang": "en", "output": "This is quite similar to Rhodesian extraction."}
{"dataset_id": "acl_6060", "sample_id": 38, "src_lang": "en", "tgt_lang": "en", "output": "so in a formal deductive system at the time step t we apply the operator between the q and qjp here and then we obtain this new expression"}
{"dataset_id": "acl_6060", "sample_id": 39, "src_lang": "en", "tgt_lang": "en", "output": "we add it to the uh to the next state to become a new quantity"}
{"dataset_id": "acl_6060", "sample_id": 40, "src_lang": "en", "tgt_lang": "en", "output": "So this slide actually visualizes the evolution of the states, where we keep adding expression to the current states."}
{"dataset_id": "acl_6060", "sample_id": 41, "src_lang": "en", "tgt_lang": "en", "output": "So in our model implementations, we first use a pre-trained language model which can be birds or robots, and then we encode sentences, and then we obtain these quantitative representations."}
{"dataset_id": "acl_6060", "sample_id": 42, "src_lang": "en", "tgt_lang": "en", "output": "So once we get the quantity representations, we can start to do inference."}
{"dataset_id": "acl_6060", "sample_id": 43, "src_lang": "en", "tgt_lang": "en", "output": "Here we show an example of Q1 to obtain the representation for Q1 divided by Q2 and then times Q4"}
{"dataset_id": "acl_6060", "sample_id": 44, "src_lang": "en", "tgt_lang": "en", "output": "First we get the pair representation, which is basically just the concatenation between Q1 and Q2, and then we apply a feed-forward network, which is parameterized by the operator."}
{"dataset_id": "acl_6060", "sample_id": 45, "src_lang": "en", "tgt_lang": "en", "output": "And then finally we get the expression representation Q1 divided by Q2"}
{"dataset_id": "acl_6060", "sample_id": 46, "src_lang": "en", "tgt_lang": "en", "output": "but in practice in the infancy stage we might be able to get the incorrect expression as well"}
{"dataset_id": "acl_6060", "sample_id": 47, "src_lang": "en", "tgt_lang": "en", "output": "So here all the possible expressions is equal to three times the number of operators."}
{"dataset_id": "acl_6060", "sample_id": 48, "src_lang": "en", "tgt_lang": "en", "output": "so the nice thing here is that we can easily add constraints to control this search this search"}
{"dataset_id": "acl_6060", "sample_id": 49, "src_lang": "en", "tgt_lang": "en", "output": "For example, if this expression is not allowed, we can simply remove this expression in our search space."}
{"dataset_id": "acl_6060", "sample_id": 50, "src_lang": "en", "tgt_lang": "en", "output": "So in the second step, we do the same thing, but the only difference is that we, the only difference is one more quantities."}
{"dataset_id": "acl_6060", "sample_id": 51, "src_lang": "en", "tgt_lang": "en", "output": "This quantity comes from the previous calculated expression."}
{"dataset_id": "acl_6060", "sample_id": 52, "src_lang": "en", "tgt_lang": "en", "output": "So finally we can obtain this final expression,"}
{"dataset_id": "acl_6060", "sample_id": 53, "src_lang": "en", "tgt_lang": "en", "output": "times Q4 and we can also see the number of all the possible expressions is different from the previous step"}
{"dataset_id": "acl_6060", "sample_id": 54, "src_lang": "en", "tgt_lang": "en", "output": "so such differences make it hard to apply beam search because the probability distribution between these two steps is unbalanced"}
{"dataset_id": "acl_6060", "sample_id": 55, "src_lang": "en", "tgt_lang": "en", "output": "so the training procedure is similar to training a sequence to sequence model where we optimize the laws at each time step"}
{"dataset_id": "acl_6060", "sample_id": 56, "src_lang": "en", "tgt_lang": "en", "output": "And here we also use this to represent when we should terminate this generation process."}
{"dataset_id": "acl_6060", "sample_id": 57, "src_lang": "en", "tgt_lang": "en", "output": "And here the space is different from sequence to sequence, because the space is different at each time step, while in traditional sequence to sequence model, it's the number of vocabulary."}
{"dataset_id": "acl_6060", "sample_id": 58, "src_lang": "en", "tgt_lang": "en", "output": "and it also allows us to impose certain constraints from prior knowledge"}
{"dataset_id": "acl_6060", "sample_id": 59, "src_lang": "en", "tgt_lang": "en", "output": "so we conducted experiments on the commonly used metropolis problem datasets, MAWPS, Math23k, MathQA and swamp"}
{"dataset_id": "acl_6060", "sample_id": 60, "src_lang": "en", "tgt_lang": "en", "output": "and here we briefly shows the results compared with the previous best approaches"}
{"dataset_id": "acl_6060", "sample_id": 61, "src_lang": "en", "tgt_lang": "en", "output": "So our best performing weapon is Robert's detective reasoning."}
{"dataset_id": "acl_6060", "sample_id": 62, "src_lang": "en", "tgt_lang": "en", "output": "And in fact we don't use beam search, in contrast, obvious approaches are using beam search."}
{"dataset_id": "acl_6060", "sample_id": 63, "src_lang": "en", "tgt_lang": "en", "output": "Alright. So the best approaches are often a tree-based model"}
{"dataset_id": "acl_6060", "sample_id": 64, "src_lang": "en", "tgt_lang": "en", "output": "So overall our reasoner is able to significantly outperform this three-base model."}
{"dataset_id": "acl_6060", "sample_id": 65, "src_lang": "en", "tgt_lang": "en", "output": "but we can see the absolute number on maths quays or swam are not really high"}
{"dataset_id": "acl_6060", "sample_id": 66, "src_lang": "en", "tgt_lang": "en", "output": "So we further investigate the results on"}
{"dataset_id": "acl_6060", "sample_id": 67, "src_lang": "en", "tgt_lang": "en", "output": "And this dataset is challenging because the author tried to manually add something to confuse the NLB model, such as adding environmental information and extra quantities."}
{"dataset_id": "acl_6060", "sample_id": 68, "src_lang": "en", "tgt_lang": "en", "output": "So in our prediction we find some of the intermediate values are actually negative."}
{"dataset_id": "acl_6060", "sample_id": 69, "src_lang": "en", "tgt_lang": "en", "output": "For example, in these questions, we are asking how many apples does Jake have?"}
{"dataset_id": "acl_6060", "sample_id": 70, "src_lang": "en", "tgt_lang": "en", "output": "but we have some extra information like seventeen field pitches and steven has eight pitches which is totally irrelevant"}
{"dataset_id": "acl_6060", "sample_id": 71, "src_lang": "en", "tgt_lang": "en", "output": "So our model makes some predictions like these, which is producing negative values."}
{"dataset_id": "acl_6060", "sample_id": 72, "src_lang": "en", "tgt_lang": "en", "output": "And we observe these two expressions"}
{"dataset_id": "acl_6060", "sample_id": 73, "src_lang": "en", "tgt_lang": "en", "output": "So we can actually limit this search space by removing like those results are negative, so that we can make the uh make the answer correct."}
{"dataset_id": "acl_6060", "sample_id": 74, "src_lang": "en", "tgt_lang": "en", "output": "So we further find such constraint actually improves quite a lot for some models"}
{"dataset_id": "acl_6060", "sample_id": 75, "src_lang": "en", "tgt_lang": "en", "output": "For example, for birds, we improve seven points, and then for the robot base model, we actually improve two points."}
{"dataset_id": "acl_6060", "sample_id": 76, "src_lang": "en", "tgt_lang": "en", "output": "So better language model has a better language understanding ability so that the number here is higher for robot and lower for robot"}
{"dataset_id": "acl_6060", "sample_id": 77, "src_lang": "en", "tgt_lang": "en", "output": "And we also try to analyze the difficulty behind this #ahB."}
{"dataset_id": "acl_6060", "sample_id": 78, "src_lang": "en", "tgt_lang": "en", "output": "we assume the number of unused quantity can be regarded as irrelevant information here"}
{"dataset_id": "acl_6060", "sample_id": 79, "src_lang": "en", "tgt_lang": "en", "output": "So here we can see that we have the percentage of samples with unused quantities and the swamp dataset has the largest portion"}
{"dataset_id": "acl_6060", "sample_id": 80, "src_lang": "en", "tgt_lang": "en", "output": "And here we also show the overall performance"}
{"dataset_id": "acl_6060", "sample_id": 81, "src_lang": "en", "tgt_lang": "en", "output": "for those samples with no use quantities so the overall performance is actually higher than the overall performance."}
{"dataset_id": "acl_6060", "sample_id": 82, "src_lang": "en", "tgt_lang": "en", "output": "but with those samples that with unused quality is actually way worse than the come uh way worse than"}
{"dataset_id": "acl_6060", "sample_id": 83, "src_lang": "en", "tgt_lang": "en", "output": "for M.W.P.S. we don't really have how many cases so I just can't figure this out."}
{"dataset_id": "acl_6060", "sample_id": 84, "src_lang": "en", "tgt_lang": "en", "output": "So finally we want to show the interpretability through a crash and participation example"}
{"dataset_id": "acl_6060", "sample_id": 85, "src_lang": "en", "tgt_lang": "en", "output": "So here our model actually makes the wrong prediction at the first step."}
{"dataset_id": "acl_6060", "sample_id": 86, "src_lang": "en", "tgt_lang": "en", "output": "So we can actually correlate this expression with the sentence here, right?"}
{"dataset_id": "acl_6060", "sample_id": 87, "src_lang": "en", "tgt_lang": "en", "output": "So we think these indicators might be misleading the model to an incorrect prediction."}
{"dataset_id": "acl_6060", "sample_id": 88, "src_lang": "en", "tgt_lang": "en", "output": "so here planting another thirty five makes the model think it should be an addition operator"}
{"dataset_id": "acl_6060", "sample_id": 89, "src_lang": "en", "tgt_lang": "en", "output": "so we tried to revise the sentence to be something like the number of pear trees is thirty five fewer than the apple trees"}
{"dataset_id": "acl_6060", "sample_id": 90, "src_lang": "en", "tgt_lang": "en", "output": "So we make it to come with more accurate semantics, such that the model is able to make the prediction correct."}
{"dataset_id": "acl_6060", "sample_id": 91, "src_lang": "en", "tgt_lang": "en", "output": "so this study shows how the interpretable predictions help us understand the model behavior"}
{"dataset_id": "acl_6060", "sample_id": 92, "src_lang": "en", "tgt_lang": "en", "output": "So to conclude our work, so first our model is actually pretty efficient."}
{"dataset_id": "acl_6060", "sample_id": 93, "src_lang": "en", "tgt_lang": "en", "output": "and we are able to provide interpretable savings procedures"}
{"dataset_id": "acl_6060", "sample_id": 94, "src_lang": "en", "tgt_lang": "en", "output": "and we can easily incorporate some prior knowledge as constraint which can help improve the performance"}
{"dataset_id": "acl_6060", "sample_id": 95, "src_lang": "en", "tgt_lang": "en", "output": "And the last thing is that the underlying mechanism does not only apply to network problem solving tasks, but also other tasks that involve multi-step reasoning."}
{"dataset_id": "acl_6060", "sample_id": 96, "src_lang": "en", "tgt_lang": "en", "output": "but we also have certain limitations"}
{"dataset_id": "acl_6060", "sample_id": 97, "src_lang": "en", "tgt_lang": "en", "output": "if we have a large number of operators or constants, the memory consumption could be pretty high"}
{"dataset_id": "acl_6060", "sample_id": 98, "src_lang": "en", "tgt_lang": "en", "output": "And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time stages, it's also quite challenging to apply beam searches."}
{"dataset_id": "acl_6060", "sample_id": 99, "src_lang": "en", "tgt_lang": "en", "output": "So this is the end of the talk and questions are welcome. Thank you."}
{"dataset_id": "acl_6060", "sample_id": 100, "src_lang": "en", "tgt_lang": "en", "output": "Hi, my name is Antoine and I'm from Maastricht University."}
{"dataset_id": "acl_6060", "sample_id": 101, "src_lang": "en", "tgt_lang": "en", "output": "i will be presenting my john work with jerry which is about a new dataset for statutory article retrieval"}
{"dataset_id": "acl_6060", "sample_id": 102, "src_lang": "en", "tgt_lang": "en", "output": "Legal issues are an integral part of many people's lives."}
{"dataset_id": "acl_6060", "sample_id": 103, "src_lang": "en", "tgt_lang": "en", "output": "but the majority of citizens have little to no knowledge about their rights and fundamental legal processes"}
{"dataset_id": "acl_6060", "sample_id": 104, "src_lang": "en", "tgt_lang": "en", "output": "as a result many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or worse exploited"}
{"dataset_id": "acl_6060", "sample_id": 105, "src_lang": "en", "tgt_lang": "en", "output": "our work aims to bridge the gap between people and the law by developing effective retrieval systems for statutory articles"}
{"dataset_id": "acl_6060", "sample_id": 106, "src_lang": "en", "tgt_lang": "en", "output": "such a system could provide a free professional legal help service for unskilled humans"}
{"dataset_id": "acl_6060", "sample_id": 107, "src_lang": "en", "tgt_lang": "en", "output": "before diving into the main contribution of this work let's first describe the problem of statutory article retrieval"}
{"dataset_id": "acl_6060", "sample_id": 108, "src_lang": "en", "tgt_lang": "en", "output": "given a simple question on a real matter such as what do i risk if i violate professional confidentiality"}
{"dataset_id": "acl_6060", "sample_id": 109, "src_lang": "en", "tgt_lang": "en", "output": "a model is required to retrieve all relevant statutory articles from a large body of legislation"}
{"dataset_id": "acl_6060", "sample_id": 110, "src_lang": "en", "tgt_lang": "en", "output": "this information retrieval task comes with its own set of challenges"}
{"dataset_id": "acl_6060", "sample_id": 111, "src_lang": "en", "tgt_lang": "en", "output": "first it deals with two types of language"}
{"dataset_id": "acl_6060", "sample_id": 112, "src_lang": "en", "tgt_lang": "en", "output": "common natural language for the questions and complex legal language for the statutes"}
{"dataset_id": "acl_6060", "sample_id": 113, "src_lang": "en", "tgt_lang": "en", "output": "this difference in language distributions makes it harder for a system to retrieve relevant candidates as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes"}
{"dataset_id": "acl_6060", "sample_id": 114, "src_lang": "en", "tgt_lang": "en", "output": "besides statutory law is not a stack of independent article that can be treated as a complete source of information on their own unlike news or recipes for example"}
{"dataset_id": "acl_6060", "sample_id": 115, "src_lang": "en", "tgt_lang": "en", "output": "instead it's a structure collection of legal provision that have a whole meaning only when considered in their overall context that is together with the supplementary information from their neighbouring articles the fields and subfields they belong to and their place in the structure of the law"}
{"dataset_id": "acl_6060", "sample_id": 116, "src_lang": "en", "tgt_lang": "en", "output": "lastly, statutory articles are in small paragraph which usually is the typical retrieval unit in most retrieval works"}
{"dataset_id": "acl_6060", "sample_id": 117, "src_lang": "en", "tgt_lang": "en", "output": "here they are long documents that may be up to six"}
{"dataset_id": "acl_6060", "sample_id": 118, "src_lang": "en", "tgt_lang": "en", "output": "the recent advances in nlp have sparked huge interest in many legal tasks such as legal judgment prediction or automated contact contract review"}
{"dataset_id": "acl_6060", "sample_id": 119, "src_lang": "en", "tgt_lang": "en", "output": "but statutory article retrieval has remained mainly in touch due to the lack of large and high quality labelled datasets"}
{"dataset_id": "acl_6060", "sample_id": 120, "src_lang": "en", "tgt_lang": "en", "output": "in this work we present a new french native citizen centric dataset to study whether retrieval model can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval"}
{"dataset_id": "acl_6060", "sample_id": 121, "src_lang": "en", "tgt_lang": "en", "output": "or Belgian statutory article retrieval dat set z. consist of more than one thousand one hundred"}
{"dataset_id": "acl_6060", "sample_id": 122, "src_lang": "en", "tgt_lang": "en", "output": "these questions cover a wide range of topics from family housing money to work and social security"}
{"dataset_id": "acl_6060", "sample_id": 123, "src_lang": "en", "tgt_lang": "en", "output": "each of them has been labelled by experienced jurists with references to relevant articles from a corpus of more than twenty two thousand six hundred"}
{"dataset_id": "acl_6060", "sample_id": 124, "src_lang": "en", "tgt_lang": "en", "output": "Belgian codes of law. Let's not talk about how we collected these datasets."}
{"dataset_id": "acl_6060", "sample_id": 125, "src_lang": "en", "tgt_lang": "en", "output": "first we started by compiling a large corpus of legal articles"}
{"dataset_id": "acl_6060", "sample_id": 126, "src_lang": "en", "tgt_lang": "en", "output": "we considered thirty two publicly available belgian codes and extracted all their articles as well as the corresponding section headings"}
{"dataset_id": "acl_6060", "sample_id": 127, "src_lang": "en", "tgt_lang": "en", "output": "then we gathered legal questions with references to relevant statutes"}
{"dataset_id": "acl_6060", "sample_id": 128, "src_lang": "en", "tgt_lang": "en", "output": "to do so we partner with a belgian law firm that receives each year around four thousand's emails from belgian citizens who ask for advice on a personal legal issue"}
{"dataset_id": "acl_6060", "sample_id": 129, "src_lang": "en", "tgt_lang": "en", "output": "we were lucky enough to get access to their websites where their team of experienced jurists addresses belgian most common legal issues"}
{"dataset_id": "acl_6060", "sample_id": 130, "src_lang": "en", "tgt_lang": "en", "output": "we collected thousands of questions annotated with categories subcategories and legal references to relevant statutes"}
{"dataset_id": "acl_6060", "sample_id": 131, "src_lang": "en", "tgt_lang": "en", "output": "lastly we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered"}
{"dataset_id": "acl_6060", "sample_id": 132, "src_lang": "en", "tgt_lang": "en", "output": "the remaining references were matched and converted to the corresponding article ids from o corpus"}
{"dataset_id": "acl_6060", "sample_id": 133, "src_lang": "en", "tgt_lang": "en", "output": "we eventually ended up with one thousand one hundred and eight questions each carefully labeled with the ideas of the relevant articles from"}
{"dataset_id": "acl_6060", "sample_id": 134, "src_lang": "en", "tgt_lang": "en", "output": "in addition each question comes with a main category and a concatenation of subcategories"}
{"dataset_id": "acl_6060", "sample_id": 135, "src_lang": "en", "tgt_lang": "en", "output": "and each articles comes with a concatenation of their subsequence heading in the structure of the law"}
{"dataset_id": "acl_6060", "sample_id": 136, "src_lang": "en", "tgt_lang": "en", "output": "this extra information is not used in the present work but might be of interest for future research on legal information retrieval or legal text classification"}
{"dataset_id": "acl_6060", "sample_id": 137, "src_lang": "en", "tgt_lang": "en", "output": "let's look at some characteristic of all datasets"}
{"dataset_id": "acl_6060", "sample_id": 138, "src_lang": "en", "tgt_lang": "en", "output": "the questionnaire between five and forty four words long with a median of forty"}
{"dataset_id": "acl_6060", "sample_id": 139, "src_lang": "en", "tgt_lang": "en", "output": "the articles are much longer with a median length of&nbsp; seventy seven words with one hundred and forty"}
{"dataset_id": "acl_6060", "sample_id": 140, "src_lang": "en", "tgt_lang": "en", "output": "two of them, exceeding one thumb."}
{"dataset_id": "acl_6060", "sample_id": 141, "src_lang": "en", "tgt_lang": "en", "output": "as previously mentioned the question covered a wide range of topics with around eighty five percent of them being either about family housing money or justice"}
{"dataset_id": "acl_6060", "sample_id": 142, "src_lang": "en", "tgt_lang": "en", "output": "while the remaining fifteen percent concern either social security, foreigners or work"}
{"dataset_id": "acl_6060", "sample_id": 143, "src_lang": "en", "tgt_lang": "en", "output": "the articles are also very diverse as they come from 32 different belgian codes that cover a large number of legal topics"}
{"dataset_id": "acl_6060", "sample_id": 144, "src_lang": "en", "tgt_lang": "en", "output": "here is the total number of articles collected from each of these belgian codes"}
{"dataset_id": "acl_6060", "sample_id": 145, "src_lang": "en", "tgt_lang": "en", "output": "out of the twenty two thousand six hundred and thirty three articles only one thousand six hundred and twelve are referred to as relevant to at least"}
{"dataset_id": "acl_6060", "sample_id": 146, "src_lang": "en", "tgt_lang": "en", "output": "One question in the data set, and about eighty percent of these quoted articles come from either the civil courts, judicial courts, criminal investigative courts or criminal courts."}
{"dataset_id": "acl_6060", "sample_id": 147, "src_lang": "en", "tgt_lang": "en", "output": "meanwhile eighteen out of thirty two codes have less than five article mentioned as relevant to at least one question"}
{"dataset_id": "acl_6060", "sample_id": 148, "src_lang": "en", "tgt_lang": "en", "output": "which can be explained by the fact that does code focus less on individuals and their concerns"}
{"dataset_id": "acl_6060", "sample_id": 149, "src_lang": "en", "tgt_lang": "en", "output": "overall, the median number of citations for these&nbsp; cited articles is two and less than 25 percent of them are&nbsp;&nbsp;"}
{"dataset_id": "acl_6060", "sample_id": 150, "src_lang": "en", "tgt_lang": "en", "output": "using our datasets, we benchmark several retrieval approaches including lexical and dense architecture"}
{"dataset_id": "acl_6060", "sample_id": 151, "src_lang": "en", "tgt_lang": "en", "output": "given a query in an article a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article"}
{"dataset_id": "acl_6060", "sample_id": 152, "src_lang": "en", "tgt_lang": "en", "output": "we experiment with the standard t f i d f and b m twenty five ranking functions"}
{"dataset_id": "acl_6060", "sample_id": 153, "src_lang": "en", "tgt_lang": "en", "output": "the main problem with these approaches is that they can only retrieve article that contain key words present in the query"}
{"dataset_id": "acl_6060", "sample_id": 154, "src_lang": "en", "tgt_lang": "en", "output": "to overcome this limitation we experiment with a neural based architecture that can capture semantic relationship between queries and articles"}
{"dataset_id": "acl_6060", "sample_id": 155, "src_lang": "en", "tgt_lang": "en", "output": "we use a biancode model that maps queries and articles into dense vector representations and calculates a relevant score between a query article pair by the similarity of their embeddings"}
{"dataset_id": "acl_6060", "sample_id": 156, "src_lang": "en", "tgt_lang": "en", "output": "these embeddings typically result from a pooling operation on the output of a word embedding model"}
{"dataset_id": "acl_6060", "sample_id": 157, "src_lang": "en", "tgt_lang": "en", "output": "first we study the effectiveness of siamese bian coders in a zero shot evaluation setup meaning that pre trained wood embedding models are applied out of the box without any additional fine tuning"}
{"dataset_id": "acl_6060", "sample_id": 158, "src_lang": "en", "tgt_lang": "en", "output": "we experiment with context independent text encoder namely word to veque and fast text and context dependent embedding models namely roberta and more specifically camembert which is a french roberta model"}
{"dataset_id": "acl_6060", "sample_id": 159, "src_lang": "en", "tgt_lang": "en", "output": "Additionally, we train our own camembert-based model beyond coders."}
{"dataset_id": "acl_6060", "sample_id": 160, "src_lang": "en", "tgt_lang": "en", "output": "on all datasets note that for training we experiment with the two flavors of the biancoro architecture"}
{"dataset_id": "acl_6060", "sample_id": 161, "src_lang": "en", "tgt_lang": "en", "output": "siamese which uses a unique word embedding model that maps the query and article together in a shared dense vector space and two tower which uses two independent word embedding models that encode the query and article separately into different embedding spaces"}
{"dataset_id": "acl_6060", "sample_id": 162, "src_lang": "en", "tgt_lang": "en", "output": "we experiment with mean, max and cls pooling as well as dot product and cosine for computing similarities"}
{"dataset_id": "acl_6060", "sample_id": 163, "src_lang": "en", "tgt_lang": "en", "output": "Here are the results of our baseline on the test set."}
{"dataset_id": "acl_6060", "sample_id": 164, "src_lang": "en", "tgt_lang": "en", "output": "with the lexical methods above the siamese beacon coders evaluated in a zero shot set up in the middle and the fine tuned beacon coders below"}
{"dataset_id": "acl_6060", "sample_id": 165, "src_lang": "en", "tgt_lang": "en", "output": "Overall, the fine tuned bianchors significantly outperform all the other bass lines."}
{"dataset_id": "acl_6060", "sample_id": 166, "src_lang": "en", "tgt_lang": "en", "output": "the two tower model improves over its siamese variant on recall at one hundred but performs similarly on the other metrics"}
{"dataset_id": "acl_6060", "sample_id": 167, "src_lang": "en", "tgt_lang": "en", "output": "Although B M twenty five underperformed the train significantly, its performance indicates that it is still a strong baseline for domain specific retrieval."}
{"dataset_id": "acl_6060", "sample_id": 168, "src_lang": "en", "tgt_lang": "en", "output": "regarding the zero shot evaluation of siamese bian coder we find that directly using the embeddings of a pre trained camambert model without optimizing for the information retrieval task gives poor results which is consistent with previous findings"}
{"dataset_id": "acl_6060", "sample_id": 169, "src_lang": "en", "tgt_lang": "en", "output": "furthermore we observe that the word to verb based biancoder significantly outperformed the fast text and verb based model suggesting that maybe pre trained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box"}
{"dataset_id": "acl_6060", "sample_id": 170, "src_lang": "en", "tgt_lang": "en", "output": "although promising these results suggest ample opportunity for improvement compared to a skilled expert who can eventually retrieve all relevant article to any question and thus get perfect scores"}
{"dataset_id": "acl_6060", "sample_id": 171, "src_lang": "en", "tgt_lang": "en", "output": "let's conclude by discussing two limitations of all datasets"}
{"dataset_id": "acl_6060", "sample_id": 172, "src_lang": "en", "tgt_lang": "en", "output": "first the corpus of article is limited to those collected from the thirty two considered belgian codes which does not cover the entire belgian law as articles from decrees directives and ordinances are missing"}
{"dataset_id": "acl_6060", "sample_id": 173, "src_lang": "en", "tgt_lang": "en", "output": "during the dataset construction all references to these uncollected articles are ignored which causes some question to end up with only a fraction of their initial number of relevant articles"}
{"dataset_id": "acl_6060", "sample_id": 174, "src_lang": "en", "tgt_lang": "en", "output": "this information loss implies that the answer contained in the remaining relevant articles might be incomplete although it's still completely appropriate"}
{"dataset_id": "acl_6060", "sample_id": 175, "src_lang": "en", "tgt_lang": "en", "output": "second we should note that not all legal question can be answered with statutes alone"}
{"dataset_id": "acl_6060", "sample_id": 176, "src_lang": "en", "tgt_lang": "en", "output": "for instance the question can i evict my tenants if they make too much noise"}
{"dataset_id": "acl_6060", "sample_id": 177, "src_lang": "en", "tgt_lang": "en", "output": "might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed"}
{"dataset_id": "acl_6060", "sample_id": 178, "src_lang": "en", "tgt_lang": "en", "output": "instead the landlord should probably rely more on case law and find precedents similar to their current situation"}
{"dataset_id": "acl_6060", "sample_id": 179, "src_lang": "en", "tgt_lang": "en", "output": "For example, the tenant makes two parties a week until 2 a.m."}
{"dataset_id": "acl_6060", "sample_id": 180, "src_lang": "en", "tgt_lang": "en", "output": "hence some questions are better suited than others to the statutory article retrieval task and the domain of the less suitable ones remains to be determined"}
{"dataset_id": "acl_6060", "sample_id": 181, "src_lang": "en", "tgt_lang": "en", "output": "we hope that all work sparks interest in developing practical and reliable statutory article retrieval models"}
{"dataset_id": "acl_6060", "sample_id": 182, "src_lang": "en", "tgt_lang": "en", "output": "that can help improve access to justice fold"}
{"dataset_id": "acl_6060", "sample_id": 183, "src_lang": "en", "tgt_lang": "en", "output": "you can check out our paper that is set in code at the following links thank you"}
{"dataset_id": "acl_6060", "sample_id": 184, "src_lang": "en", "tgt_lang": "en", "output": "hello we are happy to present our work on vowels a task independent benchmark meant for testing vision and language models with specific linguistic phenomena"}
{"dataset_id": "acl_6060", "sample_id": 185, "src_lang": "en", "tgt_lang": "en", "output": "why did we do the trouble in setting up this benchmark"}
{"dataset_id": "acl_6060", "sample_id": 186, "src_lang": "en", "tgt_lang": "en", "output": "well during the last years we have seen an explosion of transformer based vision and language models pre trained on large amounts of image text pairs"}
{"dataset_id": "acl_6060", "sample_id": 187, "src_lang": "en", "tgt_lang": "en", "output": "each one of these models pushes state of the art on vision and language tasks such as visual question answering visual common sense reasoning image retrieval phrase grounding"}
{"dataset_id": "acl_6060", "sample_id": 188, "src_lang": "en", "tgt_lang": "en", "output": "so we got a message the accuracies on these task specific benchmarks are increasing steadily"}
{"dataset_id": "acl_6060", "sample_id": 189, "src_lang": "en", "tgt_lang": "en", "output": "but do we know what the models have actually learned"}
{"dataset_id": "acl_6060", "sample_id": 190, "src_lang": "en", "tgt_lang": "en", "output": "what is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match"}
{"dataset_id": "acl_6060", "sample_id": 191, "src_lang": "en", "tgt_lang": "en", "output": "and a low score for this one"}
{"dataset_id": "acl_6060", "sample_id": 192, "src_lang": "en", "tgt_lang": "en", "output": "Do vision and language models focus on the right thing?"}
{"dataset_id": "acl_6060", "sample_id": 193, "src_lang": "en", "tgt_lang": "en", "output": "or do they focus on biases as shown by previous work"}
{"dataset_id": "acl_6060", "sample_id": 194, "src_lang": "en", "tgt_lang": "en", "output": "To shed more light on this aspect, we propose a more task-agnostic direction and introduce vowels that test the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities."}
{"dataset_id": "acl_6060", "sample_id": 195, "src_lang": "en", "tgt_lang": "en", "output": "We target existence, plurality, counting, spatial relations, actions and entity coreference."}
{"dataset_id": "acl_6060", "sample_id": 196, "src_lang": "en", "tgt_lang": "en", "output": "but how do we test whether the vision and language models have captured these phenomena"}
{"dataset_id": "acl_6060", "sample_id": 197, "src_lang": "en", "tgt_lang": "en", "output": "by foiling a method previously applied for vision and language models only for noun phrases by ravi shekar and collaborators and on counting by us in previous work"}
{"dataset_id": "acl_6060", "sample_id": 198, "src_lang": "en", "tgt_lang": "en", "output": "foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image any more"}
{"dataset_id": "acl_6060", "sample_id": 199, "src_lang": "en", "tgt_lang": "en", "output": "And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity co-reference, where each piece can consist of one or more instruments in case we found more than one interesting way to create foil instances."}
{"dataset_id": "acl_6060", "sample_id": 200, "src_lang": "en", "tgt_lang": "en", "output": "for example in the case of the action piece we have two instruments one in which the action verb is changed with a different action and one in which actants are swapped"}
{"dataset_id": "acl_6060", "sample_id": 201, "src_lang": "en", "tgt_lang": "en", "output": "counting and coreference also are&nbsp; pieces that have more than one instrument"}
{"dataset_id": "acl_6060", "sample_id": 202, "src_lang": "en", "tgt_lang": "en", "output": "and we create these foils by making sure that they fail to describe the image that they are grammatical and otherwise valid sentences"}
{"dataset_id": "acl_6060", "sample_id": 203, "src_lang": "en", "tgt_lang": "en", "output": "this is not easy to do because a foiled caption may be less likely than the original caption"}
{"dataset_id": "acl_6060", "sample_id": 204, "src_lang": "en", "tgt_lang": "en", "output": "For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants and large vision and language models could pick up on this."}
{"dataset_id": "acl_6060", "sample_id": 205, "src_lang": "en", "tgt_lang": "en", "output": "therefore to obtain valid foils we must take action"}
{"dataset_id": "acl_6060", "sample_id": 206, "src_lang": "en", "tgt_lang": "en", "output": "First we make use of strong language models to propose foils"}
{"dataset_id": "acl_6060", "sample_id": 207, "src_lang": "en", "tgt_lang": "en", "output": "Second, we use natural language inference or short NLI to filter out folios that could still be describing the image, since when constructing folios we need to ensure that they fail to describe the image."}
{"dataset_id": "acl_6060", "sample_id": 208, "src_lang": "en", "tgt_lang": "en", "output": "to test this automatically we apply natural language inference with the following rational"}
{"dataset_id": "acl_6060", "sample_id": 209, "src_lang": "en", "tgt_lang": "en", "output": "we consider an image to be the premise and its caption its&nbsp;involved hypothesis"}
{"dataset_id": "acl_6060", "sample_id": 210, "src_lang": "en", "tgt_lang": "en", "output": "in addition we consider the caption to be the premise and the foil is its hypothesis"}
{"dataset_id": "acl_6060", "sample_id": 211, "src_lang": "en", "tgt_lang": "en", "output": "If an NLI model predicts a foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil."}
{"dataset_id": "acl_6060", "sample_id": 212, "src_lang": "en", "tgt_lang": "en", "output": "If an NLI predicts the foil to be entailed by the caption, it can't be a good foil since by transitivity it will give a truthful description of the image and we filter these foils out."}
{"dataset_id": "acl_6060", "sample_id": 213, "src_lang": "en", "tgt_lang": "en", "output": "but this procedure is not perfect, it is just an indicator for valid foils"}
{"dataset_id": "acl_6060", "sample_id": 214, "src_lang": "en", "tgt_lang": "en", "output": "Therefore, as a third measure for generating valid foils, we employ human&nbsp; annotators to validate the data used in vowels."}
{"dataset_id": "acl_6060", "sample_id": 215, "src_lang": "en", "tgt_lang": "en", "output": "So after filtering and human evaluation we have as many test instances as described in this table"}
{"dataset_id": "acl_6060", "sample_id": 216, "src_lang": "en", "tgt_lang": "en", "output": "Note that Valve does not deliver any training data but only test data"}
{"dataset_id": "acl_6060", "sample_id": 217, "src_lang": "en", "tgt_lang": "en", "output": "since it is a zero shot testing benchmark only it is designed to leverage the existing capabilities of vision and language models after pre training"}
{"dataset_id": "acl_6060", "sample_id": 218, "src_lang": "en", "tgt_lang": "en", "output": "fine tuning would only enable models to exploit&nbsp; artefacts or statistical biases in the data"}
{"dataset_id": "acl_6060", "sample_id": 219, "src_lang": "en", "tgt_lang": "en", "output": "And we all know that these models like to cheat and take shortcuts."}
{"dataset_id": "acl_6060", "sample_id": 220, "src_lang": "en", "tgt_lang": "en", "output": "And as we said, we are interested in assessing what capabilities the vision and language models have after pre-training."}
{"dataset_id": "acl_6060", "sample_id": 221, "src_lang": "en", "tgt_lang": "en", "output": "We experiment with five vision and language models on vowels namely with clip, alexmert, wilbert, wilbert twelve in one and visual bird"}
{"dataset_id": "acl_6060", "sample_id": 222, "src_lang": "en", "tgt_lang": "en", "output": "two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils"}
{"dataset_id": "acl_6060", "sample_id": 223, "src_lang": "en", "tgt_lang": "en", "output": "perhaps more relevant for this video we will showcase our more primitive metric the pairwise accuracy which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair"}
{"dataset_id": "acl_6060", "sample_id": 224, "src_lang": "en", "tgt_lang": "en", "output": "for more metrics and results on them do&nbsp; check out our paper"}
{"dataset_id": "acl_6060", "sample_id": 225, "src_lang": "en", "tgt_lang": "en", "output": "the results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by wilbert twelve in one followed by wilbert alexmer clip and finally visual bird"}
{"dataset_id": "acl_6060", "sample_id": 226, "src_lang": "en", "tgt_lang": "en", "output": "It's notable how instruments centred on the individual objects like existence and noun phrases are almost solved by Wilbert twelve in one, highlighting that models are capable of identifying named objects and their presence in images."}
{"dataset_id": "acl_6060", "sample_id": 227, "src_lang": "en", "tgt_lang": "en", "output": "however none of the remaining pieces can be reliably solved in our adversarial foiling settings"}
{"dataset_id": "acl_6060", "sample_id": 228, "src_lang": "en", "tgt_lang": "en", "output": "we see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects or counting them in an image"}
{"dataset_id": "acl_6060", "sample_id": 229, "src_lang": "en", "tgt_lang": "en", "output": "the relation p's shows that they have difficulties in correctly&nbsp; classifying a named spatial relation between objects in an image"}
{"dataset_id": "acl_6060", "sample_id": 230, "src_lang": "en", "tgt_lang": "en", "output": "they also have trouble distinguishing actions and identifying their participants even if supported by plausibility biases as we see in the action piece"}
{"dataset_id": "acl_6060", "sample_id": 231, "src_lang": "en", "tgt_lang": "en", "output": "from the co reference piece we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models"}
{"dataset_id": "acl_6060", "sample_id": 232, "src_lang": "en", "tgt_lang": "en", "output": "As a sanity check and because it's an interesting experiment, we also benchmark two text-only models, GPT1 and GPT2, to assess whether the valve is solvable by these unimodels by computing the perplexity of the correct and the wrong captions."}
{"dataset_id": "acl_6060", "sample_id": 233, "src_lang": "en", "tgt_lang": "en", "output": "if the perplexity is higher for the foil we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases"}
{"dataset_id": "acl_6060", "sample_id": 234, "src_lang": "en", "tgt_lang": "en", "output": "And it's interesting to see that in some cases the text only gpt models have captured the plausibility of the world better than the vision and language models."}
{"dataset_id": "acl_6060", "sample_id": 235, "src_lang": "en", "tgt_lang": "en", "output": "so to sum up waltz is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities"}
{"dataset_id": "acl_6060", "sample_id": 236, "src_lang": "en", "tgt_lang": "en", "output": "Our experiments show that language models identify named objects and their presence in images well as shown by the existence of the spaces, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators."}
{"dataset_id": "acl_6060", "sample_id": 237, "src_lang": "en", "tgt_lang": "en", "output": "We would really like to encourage the community to use vows for measuring progress towards language grounding with vision and language models."}
{"dataset_id": "acl_6060", "sample_id": 238, "src_lang": "en", "tgt_lang": "en", "output": "and even more valves could be used as an indirect assessment of datasets as models could be evaluated before and after training or fine tuning to see whether a dataset helps models improve on any of the aspects tested by valves"}
{"dataset_id": "acl_6060", "sample_id": 239, "src_lang": "en", "tgt_lang": "en", "output": "If you are interested, check out the fake data on GitHub and if you have any questions, do not hesitate to contact us."}
{"dataset_id": "acl_6060", "sample_id": 240, "src_lang": "en", "tgt_lang": "en", "output": "hello, my name is kamizera from the university of tokyo"}
{"dataset_id": "acl_6060", "sample_id": 241, "src_lang": "en", "tgt_lang": "en", "output": "i will be presenting a paper entitled l n sum a large scale dissertation for automatic renaissance via committal summization"}
{"dataset_id": "acl_6060", "sample_id": 242, "src_lang": "en", "tgt_lang": "en", "output": "I will explain in this order."}
{"dataset_id": "acl_6060", "sample_id": 243, "src_lang": "en", "tgt_lang": "en", "output": "First, I will introduce the automatic whistle-blowing generation that we are working on in this research."}
{"dataset_id": "acl_6060", "sample_id": 244, "src_lang": "en", "tgt_lang": "en", "output": "ReleaseNote is a technical document that sums up the changes distributed with each release of a software product."}
{"dataset_id": "acl_6060", "sample_id": 245, "src_lang": "en", "tgt_lang": "en", "output": "The email shows a release note for budget 2.6"}
{"dataset_id": "acl_6060", "sample_id": 246, "src_lang": "en", "tgt_lang": "en", "output": "These notes play an important role in open source development, but they are time-consuming to prepare manually."}
{"dataset_id": "acl_6060", "sample_id": 247, "src_lang": "en", "tgt_lang": "en", "output": "Therefore, it would be very useful to be able to automatically generate high quality lease notes."}
{"dataset_id": "acl_6060", "sample_id": 248, "src_lang": "en", "tgt_lang": "en", "output": "I have referred to two previous researches on automatic risk-free generation."}
{"dataset_id": "acl_6060", "sample_id": 249, "src_lang": "en", "tgt_lang": "en", "output": "The first is a system called Array, released in 2014."}
{"dataset_id": "acl_6060", "sample_id": 250, "src_lang": "en", "tgt_lang": "en", "output": "It takes a rule-based approach, for example, using the change extract to extract core differences, library changes and document changes from the differences between releases, and finally combining them."}
{"dataset_id": "acl_6060", "sample_id": 251, "src_lang": "en", "tgt_lang": "en", "output": "the most notable feature of this system is the issue of the structure in the upper right corner"}
{"dataset_id": "acl_6060", "sample_id": 252, "src_lang": "en", "tgt_lang": "en", "output": "which must be linked to zero, the issue cycle, and can only be applied to products that use zero."}
{"dataset_id": "acl_6060", "sample_id": 253, "src_lang": "en", "tgt_lang": "en", "output": "In other words, it cannot be used for many projects on guitar."}
{"dataset_id": "acl_6060", "sample_id": 254, "src_lang": "en", "tgt_lang": "en", "output": "the second is grief recently announced in twenty"}
{"dataset_id": "acl_6060", "sample_id": 255, "src_lang": "en", "tgt_lang": "en", "output": "It is available on the internet, and can be stored via pip."}
{"dataset_id": "acl_6060", "sample_id": 256, "src_lang": "en", "tgt_lang": "en", "output": "This system has a simple running-based classification model, and outputs one of five labels, such as features or bug fixes, for each input commit message."}
{"dataset_id": "acl_6060", "sample_id": 257, "src_lang": "en", "tgt_lang": "en", "output": "The image is a sample usage that returns a corrective or bugfixes label."}
{"dataset_id": "acl_6060", "sample_id": 258, "src_lang": "en", "tgt_lang": "en", "output": "Griffith's training data is fairly small, about five thousand, and will be shown in the experiments described below."}
{"dataset_id": "acl_6060", "sample_id": 259, "src_lang": "en", "tgt_lang": "en", "output": "the performance of the text classification model is not high"}
{"dataset_id": "acl_6060", "sample_id": 260, "src_lang": "en", "tgt_lang": "en", "output": "I present two related researches, but there were problems of limited applicability and scarce data resources."}
{"dataset_id": "acl_6060", "sample_id": 261, "src_lang": "en", "tgt_lang": "en", "output": "Our paper solves these two problems and automatically generates high quality listeners."}
{"dataset_id": "acl_6060", "sample_id": 262, "src_lang": "en", "tgt_lang": "en", "output": "For the limited applicability problem, we propose a high quality classification summation method, using only committee message as input."}
{"dataset_id": "acl_6060", "sample_id": 263, "src_lang": "en", "tgt_lang": "en", "output": "This proposed method can be used for all English speakers."}
{"dataset_id": "acl_6060", "sample_id": 264, "src_lang": "en", "tgt_lang": "en", "output": "For the second problem of scarce data resources, we built our own enzyme set consisting of about 82,000 pieces of data by collecting data from public GitHub repositories using the GitHub API."}
{"dataset_id": "acl_6060", "sample_id": 265, "src_lang": "en", "tgt_lang": "en", "output": "next, i describe our desert."}
{"dataset_id": "acl_6060", "sample_id": 266, "src_lang": "en", "tgt_lang": "en", "output": "Here is an example of data."}
{"dataset_id": "acl_6060", "sample_id": 267, "src_lang": "en", "tgt_lang": "en", "output": "The left side is the commit message and the right side is the read note."}
{"dataset_id": "acl_6060", "sample_id": 268, "src_lang": "en", "tgt_lang": "en", "output": "The Risonnes are rated as improvements of physicists, etc."}
{"dataset_id": "acl_6060", "sample_id": 269, "src_lang": "en", "tgt_lang": "en", "output": "We have set up a task that takes the commit messages as input and output that is not allowed."}
{"dataset_id": "acl_6060", "sample_id": 270, "src_lang": "en", "tgt_lang": "en", "output": "This can be regarded as a summation task."}
{"dataset_id": "acl_6060", "sample_id": 271, "src_lang": "en", "tgt_lang": "en", "output": "We have predefined four levels: Features, Improvements, Bugfixes, Depreciations, Removals, and Braking Changes."}
{"dataset_id": "acl_6060", "sample_id": 272, "src_lang": "en", "tgt_lang": "en", "output": "These bills are based on previous research and other factors."}
{"dataset_id": "acl_6060", "sample_id": 273, "src_lang": "en", "tgt_lang": "en", "output": "there is no on the bottom right and extracted when there is no shown on the bottom left"}
{"dataset_id": "acl_6060", "sample_id": 274, "src_lang": "en", "tgt_lang": "en", "output": "at this time, it is necessary to detect the four ruins that have been set up in advance"}
{"dataset_id": "acl_6060", "sample_id": 275, "src_lang": "en", "tgt_lang": "en", "output": "but the rates are not always consistent with each liposuction"}
{"dataset_id": "acl_6060", "sample_id": 276, "src_lang": "en", "tgt_lang": "en", "output": "For example, the improvement level includes improvements, enhancements, optimizations and so on."}
{"dataset_id": "acl_6060", "sample_id": 277, "src_lang": "en", "tgt_lang": "en", "output": "We prepared a vocabulary list of thirty words for each of these notational variations."}
{"dataset_id": "acl_6060", "sample_id": 278, "src_lang": "en", "tgt_lang": "en", "output": "Use it to detect the rational clause and correct the text of the remainder that follows as the rational sentence for the clause."}
{"dataset_id": "acl_6060", "sample_id": 279, "src_lang": "en", "tgt_lang": "en", "output": "Next is a commit message."}
{"dataset_id": "acl_6060", "sample_id": 280, "src_lang": "en", "tgt_lang": "en", "output": "Committed messages are not tied to each piece."}
{"dataset_id": "acl_6060", "sample_id": 281, "src_lang": "en", "tgt_lang": "en", "output": "as shown in the image below if the current release is 2.5 to 19 we need to identify"}
{"dataset_id": "acl_6060", "sample_id": 282, "src_lang": "en", "tgt_lang": "en", "output": "This is a bit tedious and it is not enough to just get a list of releases and look at the before and after."}
{"dataset_id": "acl_6060", "sample_id": 283, "src_lang": "en", "tgt_lang": "en", "output": "He created a heuristic matching rule to get the previous and next versions."}
{"dataset_id": "acl_6060", "sample_id": 284, "src_lang": "en", "tgt_lang": "en", "output": "It's called a \"death sentence\"."}
{"dataset_id": "acl_6060", "sample_id": 285, "src_lang": "en", "tgt_lang": "en", "output": "In the end, 7,200 repositories"}
{"dataset_id": "acl_6060", "sample_id": 286, "src_lang": "en", "tgt_lang": "en", "output": "Also, the average number of released tokens is sixty-three, which is quite high for simulation tasks."}
{"dataset_id": "acl_6060", "sample_id": 287, "src_lang": "en", "tgt_lang": "en", "output": "Also, the number of unique tokens is quite large at 8 830,000."}
{"dataset_id": "acl_6060", "sample_id": 288, "src_lang": "en", "tgt_lang": "en", "output": "Due to the large number of unique class and method names found in the laboratory."}
{"dataset_id": "acl_6060", "sample_id": 289, "src_lang": "en", "tgt_lang": "en", "output": "Next I will explain the proposed method."}
{"dataset_id": "acl_6060", "sample_id": 290, "src_lang": "en", "tgt_lang": "en", "output": "The crosswise extractive then abstract summation model consists of two neutral modules."}
{"dataset_id": "acl_6060", "sample_id": 291, "src_lang": "en", "tgt_lang": "en", "output": "a classifier using butt or code butt and a generator using butt"}
{"dataset_id": "acl_6060", "sample_id": 292, "src_lang": "en", "tgt_lang": "en", "output": "First, CAS uses a classifier to classify each committed message into five discrete classes: features, improvements, bug fixes, applications plus and others."}
{"dataset_id": "acl_6060", "sample_id": 293, "src_lang": "en", "tgt_lang": "en", "output": "the commit messages classified as others are discarded"}
{"dataset_id": "acl_6060", "sample_id": 294, "src_lang": "en", "tgt_lang": "en", "output": "Then, GAS applies the generator to the four-row documents independently and generates riddles for each class."}
{"dataset_id": "acl_6060", "sample_id": 295, "src_lang": "en", "tgt_lang": "en", "output": "In this task the direct correspondences between committee messages and reasoning are not known."}
{"dataset_id": "acl_6060", "sample_id": 296, "src_lang": "en", "tgt_lang": "en", "output": "therefore, to train the classifier we assign sublevels to each input comment message using the first ten characters of each comment message"}
{"dataset_id": "acl_6060", "sample_id": 297, "src_lang": "en", "tgt_lang": "en", "output": "We model the classifiers obstructive summary approach by two different methods."}
{"dataset_id": "acl_6060", "sample_id": 298, "src_lang": "en", "tgt_lang": "en", "output": "The first model, which we call GAS single, consists of a single six to six network and generates a single room with no text given a concatenation of input commit messages."}
{"dataset_id": "acl_6060", "sample_id": 299, "src_lang": "en", "tgt_lang": "en", "output": "The output tags can be divided into cross-by segments based on special cross-specific endpoint symbols."}
{"dataset_id": "acl_6060", "sample_id": 300, "src_lang": "en", "tgt_lang": "en", "output": "The second method, which we call GSmart, consists of four different sec-to-sec networks, each of which corresponds to one of the three not-classes."}
{"dataset_id": "acl_6060", "sample_id": 301, "src_lang": "en", "tgt_lang": "en", "output": "Okay, let me explain the experiment."}
{"dataset_id": "acl_6060", "sample_id": 302, "src_lang": "en", "tgt_lang": "en", "output": "Five methods were compared: cheers, cheers single, cheers march, wrestling, and previous study griefs."}
{"dataset_id": "acl_6060", "sample_id": 303, "src_lang": "en", "tgt_lang": "en", "output": "Regarding aberration, in some cases, these notes are output in multiple sentences."}
{"dataset_id": "acl_6060", "sample_id": 304, "src_lang": "en", "tgt_lang": "en", "output": "Since it is difficult to correct the number of sentences as zero, they are combined with spaces and treated as one long sentence."}
{"dataset_id": "acl_6060", "sample_id": 305, "src_lang": "en", "tgt_lang": "en", "output": "the bureau is penalized when the system outputs a short sentence"}
{"dataset_id": "acl_6060", "sample_id": 306, "src_lang": "en", "tgt_lang": "en", "output": "This penalty results in a lower real value in the experiment's results described next."}
{"dataset_id": "acl_6060", "sample_id": 307, "src_lang": "en", "tgt_lang": "en", "output": "finally we also calculate the specificity because blue and blue cannot be calculated if the reels are not empty"}
{"dataset_id": "acl_6060", "sample_id": 308, "src_lang": "en", "tgt_lang": "en", "output": "A high specificity means that the model correctly outputs empty text, in cases where the leads do not assume empty."}
{"dataset_id": "acl_6060", "sample_id": 309, "src_lang": "en", "tgt_lang": "en", "output": "Here's the third one."}
{"dataset_id": "acl_6060", "sample_id": 310, "src_lang": "en", "tgt_lang": "en", "output": "Since the dataset contains email addresses, hash values, etc., we also operate a print dataset, which excludes them."}
{"dataset_id": "acl_6060", "sample_id": 311, "src_lang": "en", "tgt_lang": "en", "output": "C.E.A.S. and C.E.A.S. achieved R.U.S. scores more than ten points higher than the baselines."}
{"dataset_id": "acl_6060", "sample_id": 312, "src_lang": "en", "tgt_lang": "en", "output": "In particular, on the clearing test set, the score gap between the proposed method and the baseline jumped to more than twenty points."}
{"dataset_id": "acl_6060", "sample_id": 313, "src_lang": "en", "tgt_lang": "en", "output": "These results indicate that she is and she is are significantly effective."}
{"dataset_id": "acl_6060", "sample_id": 314, "src_lang": "en", "tgt_lang": "en", "output": "GAS got a better root pass score than GAS, suggesting that combining a classifier and a generator is effective and training the classifier using subroutines."}
{"dataset_id": "acl_6060", "sample_id": 315, "src_lang": "en", "tgt_lang": "en", "output": "High coverage of CS can be achieved properly because the classifier can focus on selecting relevant commit messages for each class."}
{"dataset_id": "acl_6060", "sample_id": 316, "src_lang": "en", "tgt_lang": "en", "output": "She's much tended to be higher-paid than she is single."}
{"dataset_id": "acl_6060", "sample_id": 317, "src_lang": "en", "tgt_lang": "en", "output": "suggesting that it is also effective to independently develop differently abstract summation models for each piece of note class"}
{"dataset_id": "acl_6060", "sample_id": 318, "src_lang": "en", "tgt_lang": "en", "output": "hero and eronasis"}
{"dataset_id": "acl_6060", "sample_id": 319, "src_lang": "en", "tgt_lang": "en", "output": "Shear's methods tend to output shorter sentences than human-reference sentences."}
{"dataset_id": "acl_6060", "sample_id": 320, "src_lang": "en", "tgt_lang": "en", "output": "In the figure on the right, the reference sentence has three or four sentences while the other has only one."}
{"dataset_id": "acl_6060", "sample_id": 321, "src_lang": "en", "tgt_lang": "en", "output": "the reason for this smaller reluctance is that in the training data only thirty three percent of the sentences are present in the feature level and forty percent in the implementation level"}
{"dataset_id": "acl_6060", "sample_id": 322, "src_lang": "en", "tgt_lang": "en", "output": "Furthermore, Cia's methods cannot generate accurate read notes without additional information."}
{"dataset_id": "acl_6060", "sample_id": 323, "src_lang": "en", "tgt_lang": "en", "output": "The top example on the right is an example of a very messy commutative message, and the complete sentence cannot be generated without reference to the corresponding prologue or issue."}
{"dataset_id": "acl_6060", "sample_id": 324, "src_lang": "en", "tgt_lang": "en", "output": "the example below shows that the two committed messages in the input are related and should be combined into one sentence, but it fails to do so"}
{"dataset_id": "acl_6060", "sample_id": 325, "src_lang": "en", "tgt_lang": "en", "output": "Finally a conclusion."}
{"dataset_id": "acl_6060", "sample_id": 326, "src_lang": "en", "tgt_lang": "en", "output": "we have built a new dash set for automatic generation"}
{"dataset_id": "acl_6060", "sample_id": 327, "src_lang": "en", "tgt_lang": "en", "output": "We have also for me the task of entering commit messages and summarising them so that it is applicable to all projects written in English."}
{"dataset_id": "acl_6060", "sample_id": 328, "src_lang": "en", "tgt_lang": "en", "output": "Our experiments show that the proposed method generated less noise and not at higher coverage than the baselines."}
{"dataset_id": "acl_6060", "sample_id": 329, "src_lang": "en", "tgt_lang": "en", "output": "Please check out for the set on GitHub!"}
{"dataset_id": "acl_6060", "sample_id": 330, "src_lang": "en", "tgt_lang": "en", "output": "Thank you."}
{"dataset_id": "acl_6060", "sample_id": 331, "src_lang": "en", "tgt_lang": "en", "output": "Hello, this is Mizzou Ferrari."}
{"dataset_id": "acl_6060", "sample_id": 332, "src_lang": "en", "tgt_lang": "en", "output": "And I will present our paper, Future Tabular Data Enrichment using FineTuner Transformers architectures."}
{"dataset_id": "acl_6060", "sample_id": 333, "src_lang": "en", "tgt_lang": "en", "output": "Does a scientist analyze data and mainly focus on de manipulating the data existing features?"}
{"dataset_id": "acl_6060", "sample_id": 334, "src_lang": "en", "tgt_lang": "en", "output": "But sometimes these features are limited."}
{"dataset_id": "acl_6060", "sample_id": 335, "src_lang": "en", "tgt_lang": "en", "output": "future generation using another data source may add substantial information"}
{"dataset_id": "acl_6060", "sample_id": 336, "src_lang": "en", "tgt_lang": "en", "output": "Our research goal is automatic tabular data enrichment using external sources free text."}
{"dataset_id": "acl_6060", "sample_id": 337, "src_lang": "en", "tgt_lang": "en", "output": "As a sum, we have a tabular dataset, and a knowledge base."}
{"dataset_id": "acl_6060", "sample_id": 338, "src_lang": "en", "tgt_lang": "en", "output": "We need an automatic process, which involves entity linking and text analysis, to extract new features from the knowledge base free text."}
{"dataset_id": "acl_6060", "sample_id": 339, "src_lang": "en", "tgt_lang": "en", "output": "Our framework, first, is exactly this automatic process."}
{"dataset_id": "acl_6060", "sample_id": 340, "src_lang": "en", "tgt_lang": "en", "output": "So let's see an example. In a dataset fed into first."}
{"dataset_id": "acl_6060", "sample_id": 341, "src_lang": "en", "tgt_lang": "en", "output": "In this example, the dataset is university dataset"}
{"dataset_id": "acl_6060", "sample_id": 342, "src_lang": "en", "tgt_lang": "en", "output": "When its goal is to classify universities into low-ranked universities and high-ranked universities."}
{"dataset_id": "acl_6060", "sample_id": 343, "src_lang": "en", "tgt_lang": "en", "output": "As a knowledge base, we use Wikipedia."}
{"dataset_id": "acl_6060", "sample_id": 344, "src_lang": "en", "tgt_lang": "en", "output": "The first phase of Fest is entity linking."}
{"dataset_id": "acl_6060", "sample_id": 345, "src_lang": "en", "tgt_lang": "en", "output": "when each entity, in this example the university name, is linked to an entity within the knowledge base"}
{"dataset_id": "acl_6060", "sample_id": 346, "src_lang": "en", "tgt_lang": "en", "output": "and the text of the entities of the knowledge base is extracted and added to the dataset."}
{"dataset_id": "acl_6060", "sample_id": 347, "src_lang": "en", "tgt_lang": "en", "output": "In this example, the text is the Wikipedia page abstract."}
{"dataset_id": "acl_6060", "sample_id": 348, "src_lang": "en", "tgt_lang": "en", "output": "Now we need to generate or extract features from the retrieved text."}
{"dataset_id": "acl_6060", "sample_id": 349, "src_lang": "en", "tgt_lang": "en", "output": "So we need a feature extraction phase which includes text analysis."}
{"dataset_id": "acl_6060", "sample_id": 350, "src_lang": "en", "tgt_lang": "en", "output": "and this is the main novelty of this paper and i will deep dive into it in the next slide"}
{"dataset_id": "acl_6060", "sample_id": 351, "src_lang": "en", "tgt_lang": "en", "output": "After the feature extraction phase, there's a feature generation phase, where we use the extracted features to generate a small number of new features."}
{"dataset_id": "acl_6060", "sample_id": 352, "src_lang": "en", "tgt_lang": "en", "output": "First, generate features in the number of classes of the original dataset."}
{"dataset_id": "acl_6060", "sample_id": 353, "src_lang": "en", "tgt_lang": "en", "output": "in this example the original dataset has two classes"}
{"dataset_id": "acl_6060", "sample_id": 354, "src_lang": "en", "tgt_lang": "en", "output": "So first generate two new features"}
{"dataset_id": "acl_6060", "sample_id": 355, "src_lang": "en", "tgt_lang": "en", "output": "but if the dataset has five classes, first generate five new features"}
{"dataset_id": "acl_6060", "sample_id": 356, "src_lang": "en", "tgt_lang": "en", "output": "each feature represents the likelihood for each class"}
{"dataset_id": "acl_6060", "sample_id": 357, "src_lang": "en", "tgt_lang": "en", "output": "To analyse the text, we use the current state of the art of text analysis, which are transformer based language models, S B G P T accent lettering, and so on."}
{"dataset_id": "acl_6060", "sample_id": 358, "src_lang": "en", "tgt_lang": "en", "output": "But it's not likely that we can train language model using the input data sets."}
{"dataset_id": "acl_6060", "sample_id": 359, "src_lang": "en", "tgt_lang": "en", "output": "so a naive approach will be a target task fine tuning"}
{"dataset_id": "acl_6060", "sample_id": 360, "src_lang": "en", "tgt_lang": "en", "output": "So in the future extraction phase we can download per-trend language model, fine-tune the language model over the target dataset"}
{"dataset_id": "acl_6060", "sample_id": 361, "src_lang": "en", "tgt_lang": "en", "output": "In this example to fine tune the language model to classify text into classes, abstract into classes low or high"}
{"dataset_id": "acl_6060", "sample_id": 362, "src_lang": "en", "tgt_lang": "en", "output": "receive the language model output, which is the likelihood for each class, and use as new features"}
{"dataset_id": "acl_6060", "sample_id": 363, "src_lang": "en", "tgt_lang": "en", "output": "The problem with this approach is datasets may have few distinct entity tags."}
{"dataset_id": "acl_6060", "sample_id": 364, "src_lang": "en", "tgt_lang": "en", "output": "In our experiment, almost half of the data sets contain less than 400 samples and the smallest data set contains 35 samples in its training set."}
{"dataset_id": "acl_6060", "sample_id": 365, "src_lang": "en", "tgt_lang": "en", "output": "So to fine-tune a language model over this dataset will be ineffective."}
{"dataset_id": "acl_6060", "sample_id": 366, "src_lang": "en", "tgt_lang": "en", "output": "But we can use prior knowledge about pre-analyzed data."}
{"dataset_id": "acl_6060", "sample_id": 367, "src_lang": "en", "tgt_lang": "en", "output": "Because we can use multiple data sets, we can use the N-minus-one data sets to gather information about the N-minus-one data sets and use this information when we analyze the N-data set."}
{"dataset_id": "acl_6060", "sample_id": 368, "src_lang": "en", "tgt_lang": "en", "output": "what we suggest is to add another fine tuning phase"}
{"dataset_id": "acl_6060", "sample_id": 369, "src_lang": "en", "tgt_lang": "en", "output": "and preliminary multitasking fine tuning phase."}
{"dataset_id": "acl_6060", "sample_id": 370, "src_lang": "en", "tgt_lang": "en", "output": "When we find the language model over the NMS1 datasets,"}
{"dataset_id": "acl_6060", "sample_id": 371, "src_lang": "en", "tgt_lang": "en", "output": "And then we execute another fine-tuning phase, which is a targeted fine-tuning, when we find the language model over the end-target data set."}
{"dataset_id": "acl_6060", "sample_id": 372, "src_lang": "en", "tgt_lang": "en", "output": "The state of the art in multitasking fine tuning called MDDN."}
{"dataset_id": "acl_6060", "sample_id": 373, "src_lang": "en", "tgt_lang": "en", "output": "In MTDN, MTDN maintains heads in the number of tasks in the training set."}
{"dataset_id": "acl_6060", "sample_id": 374, "src_lang": "en", "tgt_lang": "en", "output": "So in this example there are four tasks in the training set so empty&nbsp; DNA maintain four heads as you can see in the image"}
{"dataset_id": "acl_6060", "sample_id": 375, "src_lang": "en", "tgt_lang": "en", "output": "and it samples a random badge from the training set."}
{"dataset_id": "acl_6060", "sample_id": 376, "src_lang": "en", "tgt_lang": "en", "output": "and if the random badge belongs to for example singing sentences classification tasks its execute forward and backward passes through the first head"}
{"dataset_id": "acl_6060", "sample_id": 377, "src_lang": "en", "tgt_lang": "en", "output": "And if the random batch is going to be able to rank, the task is to go back and forth through the last head."}
{"dataset_id": "acl_6060", "sample_id": 378, "src_lang": "en", "tgt_lang": "en", "output": "In our scenario, table, dataset, and row are the number of classes."}
{"dataset_id": "acl_6060", "sample_id": 379, "src_lang": "en", "tgt_lang": "en", "output": "So there are many tasks."}
{"dataset_id": "acl_6060", "sample_id": 380, "src_lang": "en", "tgt_lang": "en", "output": "MTDN maintains a number of classes of heads, output layers."}
{"dataset_id": "acl_6060", "sample_id": 381, "src_lang": "en", "tgt_lang": "en", "output": "And additionally, MTDN needs to initiate new heads for a new data set with a new task."}
{"dataset_id": "acl_6060", "sample_id": 382, "src_lang": "en", "tgt_lang": "en", "output": "Our approach is called task reformulation fine-tuning. Instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes of tasks."}
{"dataset_id": "acl_6060", "sample_id": 383, "src_lang": "en", "tgt_lang": "en", "output": "So let's see an example."}
{"dataset_id": "acl_6060", "sample_id": 384, "src_lang": "en", "tgt_lang": "en", "output": "This is our input data set, which consists of entities, features, text and classes."}
{"dataset_id": "acl_6060", "sample_id": 385, "src_lang": "en", "tgt_lang": "en", "output": "And we reformulate the task from classifying the text into low and high to classifying the text, the abstract and the class into true or false."}
{"dataset_id": "acl_6060", "sample_id": 386, "src_lang": "en", "tgt_lang": "en", "output": "In other words, we train the language model to classify abstract and class, whether the abstract belongs to the class or not."}
{"dataset_id": "acl_6060", "sample_id": 387, "src_lang": "en", "tgt_lang": "en", "output": "So the label vector in this case is always a which always consists of two classes"}
{"dataset_id": "acl_6060", "sample_id": 388, "src_lang": "en", "tgt_lang": "en", "output": "And this is the algorithm for our refined fine tuning approach."}
{"dataset_id": "acl_6060", "sample_id": 389, "src_lang": "en", "tgt_lang": "en", "output": "so let's see the full framework"}
{"dataset_id": "acl_6060", "sample_id": 390, "src_lang": "en", "tgt_lang": "en", "output": "The data set is really fast."}
{"dataset_id": "acl_6060", "sample_id": 391, "src_lang": "en", "tgt_lang": "en", "output": "and then first execute the linking phase."}
{"dataset_id": "acl_6060", "sample_id": 392, "src_lang": "en", "tgt_lang": "en", "output": "extract the text from the knowledge base which in this example is the abstract of the Wikipedia page."}
{"dataset_id": "acl_6060", "sample_id": 393, "src_lang": "en", "tgt_lang": "en", "output": "Then it reformulates the task into a per sentence per classification task"}
{"dataset_id": "acl_6060", "sample_id": 394, "src_lang": "en", "tgt_lang": "en", "output": "applied the language model to the new task and output likelihood for each class"}
{"dataset_id": "acl_6060", "sample_id": 395, "src_lang": "en", "tgt_lang": "en", "output": "Note that the language model is already fine-tuned over N minus 1 dataset using preliminary multitask fine-tuning."}
{"dataset_id": "acl_6060", "sample_id": 396, "src_lang": "en", "tgt_lang": "en", "output": "Then we use the output vector of the language model as a newly generated feature in the number of classes."}
{"dataset_id": "acl_6060", "sample_id": 397, "src_lang": "en", "tgt_lang": "en", "output": "To evaluate our framework, we use a 17 tabular classification dataset, which varies in size, features, balance, domain and initial performance."}
{"dataset_id": "acl_6060", "sample_id": 398, "src_lang": "en", "tgt_lang": "en", "output": "and as knowledge bases we use Wikipedia"}
{"dataset_id": "acl_6060", "sample_id": 399, "src_lang": "en", "tgt_lang": "en", "output": "We design our experiment as a live one out evaluation when we train fast over 16 datasets and apply it to the 17th dataset."}
{"dataset_id": "acl_6060", "sample_id": 400, "src_lang": "en", "tgt_lang": "en", "output": "We also split this data into four folds and apply four folds cross validation."}
{"dataset_id": "acl_6060", "sample_id": 401, "src_lang": "en", "tgt_lang": "en", "output": "then we generate the new feature and evaluate them using five evaluation classifiers"}
{"dataset_id": "acl_6060", "sample_id": 402, "src_lang": "en", "tgt_lang": "en", "output": "we use in our experiment based birth based architecture"}
{"dataset_id": "acl_6060", "sample_id": 403, "src_lang": "en", "tgt_lang": "en", "output": "Here are the results for our experiment."}
{"dataset_id": "acl_6060", "sample_id": 404, "src_lang": "en", "tgt_lang": "en", "output": "You can see that we compare our framework to target dataset fine-tuning and MTDN preliminary fine-tuning."}
{"dataset_id": "acl_6060", "sample_id": 405, "src_lang": "en", "tgt_lang": "en", "output": "And our reformulated fine tuning achieves the best result, the best performance."}
{"dataset_id": "acl_6060", "sample_id": 406, "src_lang": "en", "tgt_lang": "en", "output": "While MTDNN achieved a 2% improvement over the target dataset fine tuning,"}
{"dataset_id": "acl_6060", "sample_id": 407, "src_lang": "en", "tgt_lang": "en", "output": "our poach achieved six percent improvement"}
{"dataset_id": "acl_6060", "sample_id": 408, "src_lang": "en", "tgt_lang": "en", "output": "When we look at the small dataset, we can see that the performance of the MTDN decreases and the improvement of the preliminary multitasking fine tuning phase decreases to 1.5%."}
{"dataset_id": "acl_6060", "sample_id": 409, "src_lang": "en", "tgt_lang": "en", "output": "but our performance increased to 11% compared to the target task fine tuning alone"}
{"dataset_id": "acl_6060", "sample_id": 410, "src_lang": "en", "tgt_lang": "en", "output": "for summing fast enables few shot enrichment from thirty five samples in our experiment"}
{"dataset_id": "acl_6060", "sample_id": 411, "src_lang": "en", "tgt_lang": "en", "output": "It uses one architecture for all task datasets."}
{"dataset_id": "acl_6060", "sample_id": 412, "src_lang": "en", "tgt_lang": "en", "output": "And it keeps the head of the model."}
{"dataset_id": "acl_6060", "sample_id": 413, "src_lang": "en", "tgt_lang": "en", "output": "But it adds reformulation phase"}
{"dataset_id": "acl_6060", "sample_id": 414, "src_lang": "en", "tgt_lang": "en", "output": "It's called the \"Train Set\" and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence per classification problem."}
{"dataset_id": "acl_6060", "sample_id": 415, "src_lang": "en", "tgt_lang": "en", "output": "Thank you."}
