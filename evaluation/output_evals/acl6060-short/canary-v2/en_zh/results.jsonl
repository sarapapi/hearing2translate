{"dataset_id": "acl_6060", "sample_id": 0, "src_lang": "en", "tgt_lang": "zh", "output": "Hello everyone. Today I am going to present our research work Learning to Raison Deductible Metwork Problem Solving as Complex Raison Extraction.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7706462740898132, "xcomet_qe_score": 0.8048941493034363, "metricx_score": 17.035236358642578, "metricx_qe_score": 17.525583267211914, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 1, "src_lang": "en", "tgt_lang": "zh", "output": "I am Alan from Biden's AI Lab and this is a joint work with Thierry from the University of Texas at Austin and Wayloo from SUDD.", "metrics": {"bleu_score": 0.0, "chrf_score": 8.12019258203651, "xcomet_score": 0.3641108274459839, "xcomet_qe_score": 0.46046918630599976, "metricx_score": 19.307069778442383, "metricx_qe_score": 16.712892532348633, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 2, "src_lang": "en", "tgt_lang": "zh", "output": "First, I'd like to talk about our motivation for reasoning.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9925901889801025, "xcomet_qe_score": 1.0, "metricx_score": 24.46194839477539, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 3, "src_lang": "en", "tgt_lang": "zh", "output": "Here we show examples where multi-step reasoning is helpful.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9959603548049927, "xcomet_qe_score": 1.0, "metricx_score": 14.498343467712402, "metricx_qe_score": 20.903114318847656, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 4, "src_lang": "en", "tgt_lang": "zh", "output": "This figure is taken from the PALM paper, where they perform promptings to solve the mathwork problem in a fusion learning scenario.", "metrics": {"bleu_score": 0.0, "chrf_score": 1.4368605450797869, "xcomet_score": 0.718303918838501, "xcomet_qe_score": 0.7735165357589722, "metricx_score": 10.18935489654541, "metricx_qe_score": 7.643226146697998, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 5, "src_lang": "en", "tgt_lang": "zh", "output": "So on the net hand side, we can see if we give some examples with just questions and answers, we may not be able to obtain the correct answers.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9200109839439392, "xcomet_qe_score": 0.8970243334770203, "metricx_score": 23.31963539123535, "metricx_qe_score": 22.460851669311523, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 6, "src_lang": "en", "tgt_lang": "zh", "output": "But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9726340770721436, "xcomet_qe_score": 0.9704749584197998, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 7, "src_lang": "en", "tgt_lang": "zh", "output": "Therefore, it is good to have interpretable multi-step reasoning as output.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9996922016143799, "xcomet_qe_score": 1.0, "metricx_score": 8.140820503234863, "metricx_qe_score": 11.144943237304688, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 8, "src_lang": "en", "tgt_lang": "zh", "output": "We also believe that the mathwork problem is a straightforward application to evaluate such reasoning abilities.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9260294437408447, "xcomet_qe_score": 0.989971399307251, "metricx_score": 10.756255149841309, "metricx_qe_score": 17.708942413330078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 9, "src_lang": "en", "tgt_lang": "zh", "output": "So here in our problem setup, given the questions, we need to solve this question and obtain the numerical answers.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9681559801101685, "xcomet_qe_score": 0.981998085975647, "metricx_score": 17.673046112060547, "metricx_qe_score": 24.447513580322266, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 10, "src_lang": "en", "tgt_lang": "zh", "output": "So in our datasets we also get the mathematical expression which leads to this particular answer as well.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9532638788223267, "xcomet_qe_score": 0.977789044380188, "metricx_score": 11.457730293273926, "metricx_qe_score": 13.581287384033203, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 11, "src_lang": "en", "tgt_lang": "zh", "output": "So certain assumptions also apply as in previous work.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9887440204620361, "xcomet_qe_score": 1.0, "metricx_score": 6.870549201965332, "metricx_qe_score": 15.043588638305664, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 12, "src_lang": "en", "tgt_lang": "zh", "output": "We assume the precision of quantities are known,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9974874258041382, "xcomet_qe_score": 1.0, "metricx_score": 22.498504638671875, "metricx_qe_score": 23.431303024291992, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 13, "src_lang": "en", "tgt_lang": "zh", "output": "And we only consider basic operators such as addition, subtraction, multiplication, division, and exponential.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9834758043289185, "xcomet_qe_score": 1.0, "metricx_score": 19.374589920043945, "metricx_qe_score": 22.95928955078125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 14, "src_lang": "en", "tgt_lang": "zh", "output": "Furthermore, complicated operators can be actually decomposed into these basic operators.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 24.563905715942383, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 15, "src_lang": "en", "tgt_lang": "zh", "output": "So previous work in methodological problem solving can actually categorize into sequence to sequence and sequence to tree model.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7900973558425903, "xcomet_qe_score": 0.8457448482513428, "metricx_score": 13.828315734863281, "metricx_qe_score": 10.884852409362793, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 16, "src_lang": "en", "tgt_lang": "zh", "output": "So traditional sequence to sequence model converts the expression to a specific sequence for generation.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9479944109916687, "xcomet_qe_score": 0.9247918128967285, "metricx_score": 22.04604721069336, "metricx_qe_score": 23.953399658203125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 17, "src_lang": "en", "tgt_lang": "zh", "output": "And it is quite easy to implement and it can generalize to many different complicated problems.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9850649833679199, "xcomet_qe_score": 1.0, "metricx_score": 16.783323287963867, "metricx_qe_score": 23.61210823059082, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 18, "src_lang": "en", "tgt_lang": "zh", "output": "But the drawbacks are that the performance is actually generally not better than the structured model and it is lack of interpretability for prediction.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9209890365600586, "xcomet_qe_score": 0.9369809627532959, "metricx_score": 16.798025131225586, "metricx_qe_score": 24.12628936767578, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 19, "src_lang": "en", "tgt_lang": "zh", "output": "But actually, this direction is still quite popular because of the transformer model.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.988990068435669, "xcomet_qe_score": 0.9868515729904175, "metricx_score": 9.837638854980469, "metricx_qe_score": 16.21417999267578, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 20, "src_lang": "en", "tgt_lang": "zh", "output": "In tree-based models, we actually structure these expressions in a tree form and follow a pre-order traversal in tree generations.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8828630447387695, "xcomet_qe_score": 0.898513674736023, "metricx_score": 20.340648651123047, "metricx_qe_score": 23.081043243408203, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 21, "src_lang": "en", "tgt_lang": "zh", "output": "So here we keep generating the operators until we reach the leaves which are the quantities.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9522260427474976, "xcomet_qe_score": 0.8735766410827637, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 22, "src_lang": "en", "tgt_lang": "zh", "output": "So, the good thing is that it actually gives us this binary tree structure. But in fact, it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9650626182556152, "xcomet_qe_score": 0.9580494165420532, "metricx_score": 6.076565742492676, "metricx_qe_score": 5.626248359680176, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 23, "src_lang": "en", "tgt_lang": "zh", "output": "And the second thing is that it also contains some repetitive computations.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9975053071975708, "xcomet_qe_score": 1.0, "metricx_score": 23.65230941772461, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 24, "src_lang": "en", "tgt_lang": "zh", "output": "So here, if we look at this expression, eight times three plus three is actually generated twice. But in fact, we should reuse the results", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9500764608383179, "xcomet_qe_score": 0.9739632606506348, "metricx_score": 10.16425895690918, "metricx_qe_score": 10.766345024108887, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 25, "src_lang": "en", "tgt_lang": "zh", "output": "Therefore, in our proposed approach, we want to solve those problems in a step by step and interpretable manners.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9984220266342163, "xcomet_qe_score": 1.0, "metricx_score": 10.472973823547363, "metricx_qe_score": 19.036895751953125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 26, "src_lang": "en", "tgt_lang": "zh", "output": "For example, here in the second step, we can obtain this divisor, which is twenty seven.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9618192911148071, "xcomet_qe_score": 0.9625301361083984, "metricx_score": 3.5515825748443604, "metricx_qe_score": 4.012419700622559, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 27, "src_lang": "en", "tgt_lang": "zh", "output": "We can also refer back to the original questions to find the relevant contents.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9927129745483398, "xcomet_qe_score": 0.9928059577941895, "metricx_score": 10.157617568969727, "metricx_qe_score": 20.826019287109375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 28, "src_lang": "en", "tgt_lang": "zh", "output": "And in these steps we obtain the divisors.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9824161529541016, "xcomet_qe_score": 1.0, "metricx_score": 21.120159149169922, "metricx_qe_score": 24.19206428527832, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 29, "src_lang": "en", "tgt_lang": "zh", "output": "So, and then at this third step, we actually get the quotient.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9710906744003296, "xcomet_qe_score": 0.9918110370635986, "metricx_score": 8.974353790283203, "metricx_qe_score": 10.098901748657227, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 30, "src_lang": "en", "tgt_lang": "zh", "output": "Okay. And after these three steps, we can actually reuse the results from the second step and then get the results of the fourth step. And then finally, we can obtain the dividends.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8576209545135498, "xcomet_qe_score": 0.9524476528167725, "metricx_score": 8.496502876281738, "metricx_qe_score": 7.0395188331604, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 31, "src_lang": "en", "tgt_lang": "zh", "output": "So here we actually generate the whole expression directly rather than generating single operators or quantities.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9838807582855225, "xcomet_qe_score": 0.9998210668563843, "metricx_score": 19.763011932373047, "metricx_qe_score": 23.705677032470703, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 32, "src_lang": "en", "tgt_lang": "zh", "output": "This makes the process more accurate.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9996356964111328, "xcomet_qe_score": 1.0, "metricx_score": 5.016537189483643, "metricx_qe_score": 9.74727725982666, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 33, "src_lang": "en", "tgt_lang": "zh", "output": "So in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constants as our initial state.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9612497091293335, "xcomet_qe_score": 0.9069106578826904, "metricx_score": 22.224594116210938, "metricx_qe_score": 24.02285385131836, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 34, "src_lang": "en", "tgt_lang": "zh", "output": "So the expression is represented by EIJOP.", "metrics": {"bleu_score": 0.0, "chrf_score": 3.968253968253968, "xcomet_score": 0.9679457545280457, "xcomet_qe_score": 0.981087863445282, "metricx_score": 5.084350109100342, "metricx_qe_score": 7.330475807189941, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 35, "src_lang": "en", "tgt_lang": "zh", "output": "Where we perform operators from Qi to Qj, and such expression is actually directed.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.8591065292096219, "xcomet_score": 0.8611978888511658, "xcomet_qe_score": 0.9457181096076965, "metricx_score": 19.78164291381836, "metricx_qe_score": 18.8267822265625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 36, "src_lang": "en", "tgt_lang": "zh", "output": "So, we also have subtraction words here to represent the opposite direction.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8881214261054993, "xcomet_qe_score": 0.916271448135376, "metricx_score": 13.402886390686035, "metricx_qe_score": 13.322848320007324, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 37, "src_lang": "en", "tgt_lang": "zh", "output": "This is quite similar to radiation extraction.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8034764528274536, "xcomet_qe_score": 0.820365846157074, "metricx_score": 13.66975212097168, "metricx_qe_score": 10.544365882873535, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 38, "src_lang": "en", "tgt_lang": "zh", "output": "So in a formal deductive system, at the time step t, we apply the operator between the Qi and Qj pair, and then we obtain these new expressions.", "metrics": {"bleu_score": 0.7035582084801814, "chrf_score": 0.8090614886731389, "xcomet_score": 0.866591215133667, "xcomet_qe_score": 0.9092796444892883, "metricx_score": 20.892288208007812, "metricx_qe_score": 19.991987228393555, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 39, "src_lang": "en", "tgt_lang": "zh", "output": "We add it to the next state to become a new quantity.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9890700578689575, "xcomet_qe_score": 0.9828684329986572, "metricx_score": 20.11712074279785, "metricx_qe_score": 22.78107452392578, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 40, "src_lang": "en", "tgt_lang": "zh", "output": "So these slides actually visualize the evolution of the state where we keep adding expression to the current state.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9619845151901245, "xcomet_qe_score": 1.0, "metricx_score": 24.153221130371094, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 41, "src_lang": "en", "tgt_lang": "zh", "output": "So in our model implementations, we first use a pre-trained language model which can be birds or robots, and then we encode a sentence and then we obtain these quantity representations.", "metrics": {"bleu_score": 0.0, "chrf_score": 2.4194441443570898, "xcomet_score": 0.7535191178321838, "xcomet_qe_score": 0.835195004940033, "metricx_score": 22.718141555786133, "metricx_qe_score": 21.672027587890625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 42, "src_lang": "en", "tgt_lang": "zh", "output": "So once we get the quantity representations, we can start to do inference.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9946925640106201, "xcomet_qe_score": 0.9996826648712158, "metricx_score": 23.541303634643555, "metricx_qe_score": 24.017343521118164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 43, "src_lang": "en", "tgt_lang": "zh", "output": "Here we show an example of Q1. To obtain the representation for Q1, they will be divided by Q2 and then multiplied by Q4.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.6561679790026246, "xcomet_score": 0.6783182621002197, "xcomet_qe_score": 0.8261867761611938, "metricx_score": 17.20003890991211, "metricx_qe_score": 12.363167762756348, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 44, "src_lang": "en", "tgt_lang": "zh", "output": "First, we get the pair representation, which is basically just the concatenation between Q1 and Q2. And then we apply a feedforward network, which is parameterized by the operator.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.4629629629629629, "xcomet_score": 0.9286011457443237, "xcomet_qe_score": 0.9558662176132202, "metricx_score": 10.582924842834473, "metricx_qe_score": 7.5657830238342285, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 45, "src_lang": "en", "tgt_lang": "zh", "output": "And finally, we obtain the expression representation q1 divided by q2.", "metrics": {"bleu_score": 0.0, "chrf_score": 2.314814814814815, "xcomet_score": 0.9517452716827393, "xcomet_qe_score": 0.9866584539413452, "metricx_score": 7.547996997833252, "metricx_qe_score": 7.9337029457092285, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 46, "src_lang": "en", "tgt_lang": "zh", "output": "But in fact, in practice, in the inference stage, we might be able to get the incorrect expression as well.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9864141941070557, "xcomet_qe_score": 0.985496997833252, "metricx_score": 24.305519104003906, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 47, "src_lang": "en", "tgt_lang": "zh", "output": "So here, all possible expressions are equal to three times the number of operators.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9895375967025757, "xcomet_qe_score": 1.0, "metricx_score": 6.473905563354492, "metricx_qe_score": 9.161357879638672, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 48, "src_lang": "en", "tgt_lang": "zh", "output": "So the nice thing here is that we can easily add constraints to control this search space.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9058668613433838, "xcomet_qe_score": 0.9648828506469727, "metricx_score": 24.83228302001953, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 49, "src_lang": "en", "tgt_lang": "zh", "output": "For example, if this expression is not allowed, we can simply remove this expression in our search space.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9848891496658325, "xcomet_qe_score": 0.9898691177368164, "metricx_score": 23.708969116210938, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 50, "src_lang": "en", "tgt_lang": "zh", "output": "So in the second step, we do the same thing, but the only difference is that we the only difference is one more quantities.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7435784935951233, "xcomet_qe_score": 0.8366759419441223, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 51, "src_lang": "en", "tgt_lang": "zh", "output": "This quantity comes from the previous calculated expression.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9892408847808838, "xcomet_qe_score": 1.0, "metricx_score": 5.3420257568359375, "metricx_qe_score": 5.327550411224365, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 52, "src_lang": "en", "tgt_lang": "zh", "output": "So finally we can obtain this final expression Q3.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.6038647342995169, "xcomet_score": 0.8044122457504272, "xcomet_qe_score": 0.8134092092514038, "metricx_score": 21.502973556518555, "metricx_qe_score": 20.584035873413086, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 53, "src_lang": "en", "tgt_lang": "zh", "output": "Times Q four and we can also see the number of all the possible expression is different from the previous step.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7446668744087219, "xcomet_qe_score": 0.7632541060447693, "metricx_score": 18.39958953857422, "metricx_qe_score": 16.459274291992188, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 54, "src_lang": "en", "tgt_lang": "zh", "output": "Such differences make it hard to apply beam search because the probability distribution between these two steps is unbalanced.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9718927145004272, "xcomet_qe_score": 0.9669569730758667, "metricx_score": 14.038931846618652, "metricx_qe_score": 22.389638900756836, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 55, "src_lang": "en", "tgt_lang": "zh", "output": "The training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.886177659034729, "xcomet_qe_score": 0.8810088634490967, "metricx_score": 23.654401779174805, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 56, "src_lang": "en", "tgt_lang": "zh", "output": "And here we also use this tau to represent when we should terminate this generation process.", "metrics": {"bleu_score": 1.0645116331223183, "chrf_score": 2.4803147106205454, "xcomet_score": 0.9846441745758057, "xcomet_qe_score": 0.994234561920166, "metricx_score": 22.24704933166504, "metricx_qe_score": 23.569669723510742, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 57, "src_lang": "en", "tgt_lang": "zh", "output": "And here the space is different from sequence to sequence because the space is different at each time that while in traditional sequence to sequence model it is the number of vocabulary.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6616077423095703, "xcomet_qe_score": 0.6887407898902893, "metricx_score": 22.481534957885742, "metricx_qe_score": 23.327348709106445, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 58, "src_lang": "en", "tgt_lang": "zh", "output": "And it also allows us to impose certain constraints from prior knowledge.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9789882898330688, "xcomet_qe_score": 0.9840245246887207, "metricx_score": 24.292133331298828, "metricx_qe_score": 24.107093811035156, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 59, "src_lang": "en", "tgt_lang": "zh", "output": "Therefore, we conduct experiments on the commonly used methodological problem data sets MAWPS, Math twenty three K, Math QA and SWAM.", "metrics": {"bleu_score": 1.335870092217132, "chrf_score": 14.438406208901597, "xcomet_score": 0.7417181134223938, "xcomet_qe_score": 0.7878133058547974, "metricx_score": 7.745582580566406, "metricx_qe_score": 6.183465957641602, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 60, "src_lang": "en", "tgt_lang": "zh", "output": "And here we briefly show the results compared with the previous best approaches.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9788966178894043, "xcomet_qe_score": 0.9943420886993408, "metricx_score": 22.892601013183594, "metricx_qe_score": 24.32426643371582, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 61, "src_lang": "en", "tgt_lang": "zh", "output": "So our best performing variant is Roberta Dedative Reasoner.", "metrics": {"bleu_score": 0.0, "chrf_score": 40.818699816184775, "xcomet_score": 0.9672900438308716, "xcomet_qe_score": 0.9822712540626526, "metricx_score": 11.087382316589355, "metricx_qe_score": 12.023717880249023, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 62, "src_lang": "en", "tgt_lang": "zh", "output": "And in fact, we do not use BeamSearch, on the contrary, obvious approaches using BeamSearch.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.864842414855957, "xcomet_qe_score": 0.8943363428115845, "metricx_score": 9.083404541015625, "metricx_qe_score": 6.441227912902832, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 63, "src_lang": "en", "tgt_lang": "zh", "output": "All right. So the best approaches are often a tree-based model.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 7.625439643859863, "metricx_qe_score": 15.911699295043945, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 64, "src_lang": "en", "tgt_lang": "zh", "output": "So, in general, our reasoner is able to significantly outperform this tree-based model.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9759938716888428, "xcomet_qe_score": 0.9839122295379639, "metricx_score": 7.404326438903809, "metricx_qe_score": 9.121699333190918, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 65, "src_lang": "en", "tgt_lang": "zh", "output": "But we can see the absolute number on mathQA or swam are not really high.", "metrics": {"bleu_score": 0.0, "chrf_score": 7.263943672903743, "xcomet_score": 0.8561683893203735, "xcomet_qe_score": 0.8375938534736633, "metricx_score": 19.07914924621582, "metricx_qe_score": 22.94822883605957, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 66, "src_lang": "en", "tgt_lang": "zh", "output": "So we further investigate the results on site.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.8090614886731389, "xcomet_score": 0.8358107805252075, "xcomet_qe_score": 0.8583472967147827, "metricx_score": 11.138876914978027, "metricx_qe_score": 13.664750099182129, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 67, "src_lang": "en", "tgt_lang": "zh", "output": "Swamp. And this data set is a challenge because the author tried to manually add something to confuse the NLP model, such as adding irrelevant information and extra quantities.", "metrics": {"bleu_score": 0.6819119980476283, "chrf_score": 1.4342333139625139, "xcomet_score": 0.653191864490509, "xcomet_qe_score": 0.7271159887313843, "metricx_score": 12.008380889892578, "metricx_qe_score": 10.554959297180176, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 68, "src_lang": "en", "tgt_lang": "zh", "output": "So, in our prediction, we find some of the intermediate values are actually negative.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9824434518814087, "xcomet_qe_score": 0.9984627962112427, "metricx_score": 17.331920623779297, "metricx_qe_score": 21.904321670532227, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 69, "src_lang": "en", "tgt_lang": "zh", "output": "For example, in these questions we are asking how many apples does Jake have?", "metrics": {"bleu_score": 1.789234746542202, "chrf_score": 5.383112966146135, "xcomet_score": 0.9933526515960693, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 15.453292846679688, "metricx_qe_score": 20.8984375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 70, "src_lang": "en", "tgt_lang": "zh", "output": "But we have some extra information like 17 fewer pitches and Stephen has 8 pitches, which is totally irrelevant.", "metrics": {"bleu_score": 0.8738976140726407, "chrf_score": 5.4950872354949185, "xcomet_score": 0.6433314085006714, "xcomet_qe_score": 0.7219223380088806, "metricx_score": 15.1372652053833, "metricx_qe_score": 6.957475185394287, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 71, "src_lang": "en", "tgt_lang": "zh", "output": "So our model makes some predictions like this, which is producing negative values.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.969541072845459, "xcomet_qe_score": 0.9660704135894775, "metricx_score": 7.194301605224609, "metricx_qe_score": 15.234479904174805, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 72, "src_lang": "en", "tgt_lang": "zh", "output": "And we observe these two expressions.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7971609830856323, "xcomet_qe_score": 0.8808423280715942, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 73, "src_lang": "en", "tgt_lang": "zh", "output": "So we can actually limit this search space by removing like those results are negative so that we can make the make the answer correct.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.645977795124054, "xcomet_qe_score": 0.794758677482605, "metricx_score": 13.924846649169922, "metricx_qe_score": 16.30267906188965, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 74, "src_lang": "en", "tgt_lang": "zh", "output": "Therefore, we further find that such constraint actually improves quite a lot for some models.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9652844667434692, "xcomet_qe_score": 0.9704136848449707, "metricx_score": 8.839587211608887, "metricx_qe_score": 10.047356605529785, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 75, "src_lang": "en", "tgt_lang": "zh", "output": "For example, we improved seven points for Birds, and then for the Roberta Base model, we actually improved two points.", "metrics": {"bleu_score": 0.9153072425533536, "chrf_score": 8.521656485414823, "xcomet_score": 0.7493616342544556, "xcomet_qe_score": 0.7521862983703613, "metricx_score": 13.70331859588623, "metricx_qe_score": 14.22436809539795, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 76, "src_lang": "en", "tgt_lang": "zh", "output": "So a better language model has better language understanding abilities so that the number here is higher for Roberta and lower for Bertha.", "metrics": {"bleu_score": 0.6290808546066479, "chrf_score": 7.138715075967548, "xcomet_score": 0.84991854429245, "xcomet_qe_score": 0.8393122553825378, "metricx_score": 24.604522705078125, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 77, "src_lang": "en", "tgt_lang": "zh", "output": "We also tried to analyze the difficulty behind this BPP.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.75840163230896, "xcomet_qe_score": 0.7784525156021118, "metricx_score": 9.605749130249023, "metricx_qe_score": 11.605393409729004, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 78, "src_lang": "en", "tgt_lang": "zh", "output": "We assume that the number of unused quantities can be regarded as irrelevant information here.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9997564554214478, "xcomet_qe_score": 1.0, "metricx_score": 17.001285552978516, "metricx_qe_score": 23.204273223876953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 79, "src_lang": "en", "tgt_lang": "zh", "output": "So here we can see that we have the percentage of samples with unused quantities and the swamp data set has the largest portion.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.29655990510083036, "xcomet_score": 0.8509857654571533, "xcomet_qe_score": 0.8250189423561096, "metricx_score": 9.102568626403809, "metricx_qe_score": 9.864274024963379, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 80, "src_lang": "en", "tgt_lang": "zh", "output": "And here we also show the overall performance.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9731849431991577, "xcomet_qe_score": 0.9899024963378906, "metricx_score": 12.858589172363281, "metricx_qe_score": 14.731225967407227, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 81, "src_lang": "en", "tgt_lang": "zh", "output": "For those samples without unused quantities, so the overall performance is actually higher than the performance is actually higher than the overall performance.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.24885715544223785, "xcomet_qe_score": 0.34036046266555786, "metricx_score": 19.65530776977539, "metricx_qe_score": 22.44283103942871, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 82, "src_lang": "en", "tgt_lang": "zh", "output": "But with those samples that with unused quantity, it is actually much worse than the, uh, much worse than.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7675555944442749, "xcomet_qe_score": 0.7526404857635498, "metricx_score": 16.431385040283203, "metricx_qe_score": 15.14046573638916, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 83, "src_lang": "en", "tgt_lang": "zh", "output": "Poor performance. For MAWPS, we don't really have too many desk cases, so I just ignore this part.", "metrics": {"bleu_score": 1.2006691370115936, "chrf_score": 5.949024160887909, "xcomet_score": 0.7130285501480103, "xcomet_qe_score": 0.6935619115829468, "metricx_score": 12.27579116821289, "metricx_qe_score": 10.754623413085938, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 84, "src_lang": "en", "tgt_lang": "zh", "output": "So, finally, we want to show the interpretability through a question participation example.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8461724519729614, "xcomet_qe_score": 0.8735700845718384, "metricx_score": 18.104793548583984, "metricx_qe_score": 19.0692081451416, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 85, "src_lang": "en", "tgt_lang": "zh", "output": "So, our model actually makes a wrong prediction at the first step.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9950982332229614, "xcomet_qe_score": 1.0, "metricx_score": 18.706342697143555, "metricx_qe_score": 23.710636138916016, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 86, "src_lang": "en", "tgt_lang": "zh", "output": "So we can actually correlate this expression with the sentence here, right?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9901900291442871, "xcomet_qe_score": 1.0, "metricx_score": 7.234060287475586, "metricx_qe_score": 5.488640785217285, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 87, "src_lang": "en", "tgt_lang": "zh", "output": "So we think this sentence might be misleading the model to an incorrect prediction.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9957265853881836, "xcomet_qe_score": 1.0, "metricx_score": 21.933990478515625, "metricx_qe_score": 24.860246658325195, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 88, "src_lang": "en", "tgt_lang": "zh", "output": "So here planting another 35 makes the model think it should be an addition operator.", "metrics": {"bleu_score": 1.229721858538738, "chrf_score": 1.386596322267878, "xcomet_score": 0.6603856086730957, "xcomet_qe_score": 0.8671082258224487, "metricx_score": 13.169360160827637, "metricx_qe_score": 18.174957275390625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 89, "src_lang": "en", "tgt_lang": "zh", "output": "So we try to revise the sentence to be something like the number of pear trees are fifty five fewer than the apple trees.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8868792057037354, "xcomet_qe_score": 0.8926577568054199, "metricx_score": 23.783292770385742, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 90, "src_lang": "en", "tgt_lang": "zh", "output": "Therefore, we make it so that it conveys more accurate semantics, so that the model is able to make the prediction correct.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9694794416427612, "xcomet_qe_score": 0.9644346237182617, "metricx_score": 2.754818916320801, "metricx_qe_score": 2.6932992935180664, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 91, "src_lang": "en", "tgt_lang": "zh", "output": "This study shows how interpretable predictions help us understand the model behavior.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9957221746444702, "xcomet_qe_score": 0.994318962097168, "metricx_score": 18.119163513183594, "metricx_qe_score": 20.531551361083984, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 92, "src_lang": "en", "tgt_lang": "zh", "output": "So to conclude our work. So first our model is actually pretty efficient.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8579465746879578, "xcomet_qe_score": 0.8905684351921082, "metricx_score": 20.70159149169922, "metricx_qe_score": 23.818923950195312, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 93, "src_lang": "en", "tgt_lang": "zh", "output": "and we are able to provide interpretable solving procedure.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 14.988175392150879, "metricx_qe_score": 21.255823135375977, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 94, "src_lang": "en", "tgt_lang": "zh", "output": "And we can easily incorporate some prior knowledge as constraints which can help improve the performance.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 23.699710845947266, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 95, "src_lang": "en", "tgt_lang": "zh", "output": "And the last thing is that the underlying mechanism does not only apply to tasks of solving problems in the network, but also to other tasks that involve multi-step reasoning.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9941365718841553, "xcomet_qe_score": 1.0, "metricx_score": 7.002996444702148, "metricx_qe_score": 12.269800186157227, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 96, "src_lang": "en", "tgt_lang": "zh", "output": "But we also have certain limitations.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 1.2405693531036377, "metricx_qe_score": 2.835557460784912, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 97, "src_lang": "en", "tgt_lang": "zh", "output": "If we have a large number of operators or constants, the memory consumption could be quite high.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9947634935379028, "xcomet_qe_score": 1.0, "metricx_score": 12.641768455505371, "metricx_qe_score": 18.769914627075195, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 98, "src_lang": "en", "tgt_lang": "zh", "output": "And the second thing is that, as mentioned, because the probability distribution is unbalanced between at different time steps, so it's also pretty challenging to apply beam searches.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9462546110153198, "xcomet_qe_score": 0.9499963521957397, "metricx_score": 23.24781608581543, "metricx_qe_score": 24.825857162475586, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 99, "src_lang": "en", "tgt_lang": "zh", "output": "So this is the end of the talk and questions are welcome. Thank you.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9706733226776123, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 16.052629470825195, "metricx_qe_score": 23.88555908203125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 100, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, my name is Antoine and I'm from Maastricht University.", "metrics": {"bleu_score": 1.8897852222361986, "chrf_score": 15.818056866752281, "xcomet_score": 0.9570515155792236, "xcomet_qe_score": 1.0, "metricx_score": 8.974778175354004, "metricx_qe_score": 21.448993682861328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 101, "src_lang": "en", "tgt_lang": "zh", "output": "I will present my John Work with Jerry, which is about a new dataset for statutory article retrieval.", "metrics": {"bleu_score": 1.220804815982381, "chrf_score": 5.977632474446089, "xcomet_score": 0.8050938844680786, "xcomet_qe_score": 0.7919946908950806, "metricx_score": 12.372358322143555, "metricx_qe_score": 12.93947982788086, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 102, "src_lang": "en", "tgt_lang": "zh", "output": "Legal issues are an integral part of many people's lives.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 24.647174835205078, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 103, "src_lang": "en", "tgt_lang": "zh", "output": "But the majority of citizens have little to no knowledge about their rights and fundamental legal processes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 104, "src_lang": "en", "tgt_lang": "zh", "output": "As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or worse exploited.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.983279824256897, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 105, "src_lang": "en", "tgt_lang": "zh", "output": "Our work aims to bridge the gap between people and the law by developing effective retrieval systems for statutory articles.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9794843196868896, "xcomet_qe_score": 0.9896780848503113, "metricx_score": 15.714083671569824, "metricx_qe_score": 19.51784896850586, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 106, "src_lang": "en", "tgt_lang": "zh", "output": "Such a system could provide a free professional legal help service for unskilled humans.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9236090183258057, "xcomet_qe_score": 0.9935920238494873, "metricx_score": 24.996997833251953, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 107, "src_lang": "en", "tgt_lang": "zh", "output": "Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9021722078323364, "xcomet_qe_score": 0.8788645267486572, "metricx_score": 24.549938201904297, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 108, "src_lang": "en", "tgt_lang": "zh", "output": "given a simple question on a legal matter such as what do I risk if I violate professional confidentiality?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9719746112823486, "xcomet_qe_score": 0.9890373349189758, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 109, "src_lang": "en", "tgt_lang": "zh", "output": "A model is required to retrieve all relevant statutory articles from a large body of legislation.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9997987747192383, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 110, "src_lang": "en", "tgt_lang": "zh", "output": "This task for information retrieval comes with its own set of challenges.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9808257818222046, "xcomet_qe_score": 0.9857171773910522, "metricx_score": 17.778661727905273, "metricx_qe_score": 23.99557113647461, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 111, "src_lang": "en", "tgt_lang": "zh", "output": "First, it deals with two types of language.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9935343265533447, "xcomet_qe_score": 1.0, "metricx_score": 20.913578033447266, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 112, "src_lang": "en", "tgt_lang": "zh", "output": "common natural language for the questions and complex legal language for the statutes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9650925397872925, "xcomet_qe_score": 0.974053144454956, "metricx_score": 23.232120513916016, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 113, "src_lang": "en", "tgt_lang": "zh", "output": "This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8981918096542358, "xcomet_qe_score": 0.9564414024353027, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 114, "src_lang": "en", "tgt_lang": "zh", "output": "Moreover, statutory law is not a stack of independent articles that can be treated as a complete source of information on its own, like news or recipes, for example.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8756210803985596, "xcomet_qe_score": 1.0, "metricx_score": 19.438949584960938, "metricx_qe_score": 22.865097045898438, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 115, "src_lang": "en", "tgt_lang": "zh", "output": "Instead, it's a structure collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighbouring articles, the fields and subfields they belong to, and their place in the structure of the law.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8710541129112244, "xcomet_qe_score": 0.9406022429466248, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 116, "src_lang": "en", "tgt_lang": "zh", "output": "Lastly, statutory articles are in small paragraph, which usually is the typical retrieval unit in most retrieval works.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8701720237731934, "xcomet_qe_score": 0.8807176947593689, "metricx_score": 23.56937599182129, "metricx_qe_score": 24.620521545410156, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 117, "src_lang": "en", "tgt_lang": "zh", "output": "Here, there are long documents that may be up to sixty", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7518526315689087, "xcomet_qe_score": 0.7669395208358765, "metricx_score": 21.3930606842041, "metricx_qe_score": 24.396717071533203, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 118, "src_lang": "en", "tgt_lang": "zh", "output": "The recent progress in NLP has sparked huge interest in many legal tasks, such as the prediction of legal judgments or automated contract review.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9600963592529297, "xcomet_qe_score": 0.981494665145874, "metricx_score": 9.222058296203613, "metricx_qe_score": 15.584371566772461, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 119, "src_lang": "en", "tgt_lang": "zh", "output": "However, the statutory article retrieval has remained mainly untouched due to the lack of large and high-quality labeled data sets.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9409068822860718, "xcomet_qe_score": 0.8417669534683228, "metricx_score": 6.711694717407227, "metricx_qe_score": 5.13593864440918, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 120, "src_lang": "en", "tgt_lang": "zh", "output": "In this work, we present a new French native citizen centric data set to study whether a retrieval model can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9099023342132568, "xcomet_qe_score": 0.9167613983154297, "metricx_score": 25.0, "metricx_qe_score": 24.46809196472168, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 121, "src_lang": "en", "tgt_lang": "zh", "output": "Our Belgian statutory article retrieval datasets consists of more than one thousand one hundred liters.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.3242542153047989, "xcomet_score": 0.5774150490760803, "xcomet_qe_score": 0.6753051280975342, "metricx_score": 22.072124481201172, "metricx_qe_score": 23.77834701538086, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 122, "src_lang": "en", "tgt_lang": "zh", "output": "These questions cover a wide range of topics, from family, housing, money, to work and social security.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 123, "src_lang": "en", "tgt_lang": "zh", "output": "Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty two thousand six hundred thousand.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6979116797447205, "xcomet_qe_score": 0.7449820041656494, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 124, "src_lang": "en", "tgt_lang": "zh", "output": "Belgian codes of law. Let's now talk about how we collected these data sets.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3226027488708496, "xcomet_qe_score": 0.31997713446617126, "metricx_score": 17.35209846496582, "metricx_qe_score": 22.038185119628906, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 125, "src_lang": "en", "tgt_lang": "zh", "output": "First, we started by compiling a large corpus of legal articles.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9724109172821045, "xcomet_qe_score": 1.0, "metricx_score": 23.04967498779297, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 126, "src_lang": "en", "tgt_lang": "zh", "output": "We considered thirty two publicly available Belgian codes and extracted all their articles as well as the corresponding section headings.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9570508599281311, "xcomet_qe_score": 1.0, "metricx_score": 17.393877029418945, "metricx_qe_score": 22.80816078186035, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 127, "src_lang": "en", "tgt_lang": "zh", "output": "Then we gathered legal questions with references to relevant statutes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9916418790817261, "xcomet_qe_score": 1.0, "metricx_score": 24.396305084228516, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 128, "src_lang": "en", "tgt_lang": "zh", "output": "To do this, we partner with a Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9986120462417603, "xcomet_qe_score": 1.0, "metricx_score": 22.897472381591797, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 129, "src_lang": "en", "tgt_lang": "zh", "output": "We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgian most common legal issues.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9767496585845947, "xcomet_qe_score": 0.9905999898910522, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 130, "src_lang": "en", "tgt_lang": "zh", "output": "We collected thousands of questions annotated with categories, subcategories, and legal references to relevant statutes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9992095232009888, "xcomet_qe_score": 1.0, "metricx_score": 24.6804256439209, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 131, "src_lang": "en", "tgt_lang": "zh", "output": "Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8452504873275757, "xcomet_qe_score": 0.9722477197647095, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 132, "src_lang": "en", "tgt_lang": "zh", "output": "The remaining references were matched and converted to the corresponding article IDs from O Corpus.", "metrics": {"bleu_score": 0.0, "chrf_score": 1.2543946100871408, "xcomet_score": 0.9304412603378296, "xcomet_qe_score": 0.9481940865516663, "metricx_score": 12.708693504333496, "metricx_qe_score": 14.939692497253418, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 133, "src_lang": "en", "tgt_lang": "zh", "output": "We eventually ended up with one thousand one hundred eight questions, each carefully labeled with the IDs of the relevant articles from the book.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.7346132134893072, "xcomet_score": 0.7590665221214294, "xcomet_qe_score": 0.7978571653366089, "metricx_score": 18.967763900756836, "metricx_qe_score": 23.073833465576172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 134, "src_lang": "en", "tgt_lang": "zh", "output": "In addition, each question comes with a main category and a concatenation of subcategories.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.983391284942627, "xcomet_qe_score": 1.0, "metricx_score": 18.5040283203125, "metricx_qe_score": 24.11787986755371, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 135, "src_lang": "en", "tgt_lang": "zh", "output": "and each article comes with a concatenation of their subsequent heading in the structure of the law.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9259305000305176, "xcomet_qe_score": 0.9533981680870056, "metricx_score": 12.308541297912598, "metricx_qe_score": 19.329547882080078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 136, "src_lang": "en", "tgt_lang": "zh", "output": "This extra information is not used in the present work but might be of interest for future research on legal information retrieval or legal text classification.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9958162307739258, "xcomet_qe_score": 0.9995161294937134, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 137, "src_lang": "en", "tgt_lang": "zh", "output": "Let's look at some characteristics of our dataset.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9969571828842163, "xcomet_qe_score": 0.9984924793243408, "metricx_score": 15.07298469543457, "metricx_qe_score": 22.85808563232422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 138, "src_lang": "en", "tgt_lang": "zh", "output": "The questions are between five and forty four words long with a median of forty words.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7942086458206177, "xcomet_qe_score": 0.9033284783363342, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 139, "src_lang": "en", "tgt_lang": "zh", "output": "The article are much longer, with a median length of seventy seven words, with one hundred forty grams.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5761176347732544, "xcomet_qe_score": 0.6319115161895752, "metricx_score": 24.32790184020996, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 140, "src_lang": "en", "tgt_lang": "zh", "output": "two of them exceeding one thousand.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1882385015487671, "xcomet_qe_score": 0.1865082085132599, "metricx_score": 8.97647476196289, "metricx_qe_score": 9.49603271484375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 141, "src_lang": "en", "tgt_lang": "zh", "output": "As previously mentioned, the question covered a wide range of topics, with around eighty five percent of them being either about family, housing, money, or justice, or", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9004744291305542, "xcomet_qe_score": 0.8762468099594116, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 142, "src_lang": "en", "tgt_lang": "zh", "output": "while the remaining 15% concern either social security, foreigners or work.", "metrics": {"bleu_score": 3.444898507753047, "chrf_score": 3.3433325155666647, "xcomet_score": 0.9566702246665955, "xcomet_qe_score": 0.9616316556930542, "metricx_score": 20.269014358520508, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 143, "src_lang": "en", "tgt_lang": "zh", "output": "The articles are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8629878759384155, "xcomet_qe_score": 0.9966293573379517, "metricx_score": 24.736841201782227, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 144, "src_lang": "en", "tgt_lang": "zh", "output": "Here's the total number of articles collected from each of these Belgian codes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9734033346176147, "xcomet_qe_score": 1.0, "metricx_score": 24.628326416015625, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 145, "src_lang": "en", "tgt_lang": "zh", "output": "Out of the twenty two thousand six hundred thirty three articles, only one thousand six hundred twelve are referred to as relevant to at least one of the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5870530605316162, "xcomet_qe_score": 0.7560817003250122, "metricx_score": 19.03945541381836, "metricx_qe_score": 21.121919631958008, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 146, "src_lang": "en", "tgt_lang": "zh", "output": "at least one question in the data sets. And around eighty percent of these cited articles come from either the civil code, judicial code, criminal investigation code or penal code.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5777419805526733, "xcomet_qe_score": 0.7063043117523193, "metricx_score": 17.423377990722656, "metricx_qe_score": 19.556297302246094, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 147, "src_lang": "en", "tgt_lang": "zh", "output": "Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9517099261283875, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 148, "src_lang": "en", "tgt_lang": "zh", "output": "Which can be explained by the fact that those codes focus less on individuals and their concerns.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9880475997924805, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 149, "src_lang": "en", "tgt_lang": "zh", "output": "Overall, the median number of citation for these cited articles is two, and less than twenty five percent of them are cited.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7442975044250488, "xcomet_qe_score": 0.7744380235671997, "metricx_score": 24.649858474731445, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 150, "src_lang": "en", "tgt_lang": "zh", "output": "Using our data sets, we benchmark several retrieval approaches, including lexical and dense architecture.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7563602328300476, "xcomet_qe_score": 0.8899133205413818, "metricx_score": 12.7796630859375, "metricx_qe_score": 18.9008731842041, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 151, "src_lang": "en", "tgt_lang": "zh", "output": "Given a query in an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9067046642303467, "xcomet_qe_score": 0.9407706260681152, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 152, "src_lang": "en", "tgt_lang": "zh", "output": "We experiment with the standard TFIDF and BM twenty five ranking functions.", "metrics": {"bleu_score": 0.0, "chrf_score": 5.902177703975765, "xcomet_score": 0.8660111427307129, "xcomet_qe_score": 0.8826728463172913, "metricx_score": 11.64867115020752, "metricx_qe_score": 17.58673858642578, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 153, "src_lang": "en", "tgt_lang": "zh", "output": "The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9920210838317871, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 154, "src_lang": "en", "tgt_lang": "zh", "output": "To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationship between queries and articles.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9556413888931274, "xcomet_qe_score": 0.9859662055969238, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 155, "src_lang": "en", "tgt_lang": "zh", "output": "We use a B-encoder model that maps queries and articles into dense vector representations and calculates a relevant score between a query article pair by the similarity of their embeddings.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8124831914901733, "xcomet_qe_score": 0.8632498979568481, "metricx_score": 19.74155616760254, "metricx_qe_score": 20.698524475097656, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 156, "src_lang": "en", "tgt_lang": "zh", "output": "These embeddings are typically the result of a pooling operation on the output of a word embedding model.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9958209991455078, "xcomet_qe_score": 1.0, "metricx_score": 19.180700302124023, "metricx_qe_score": 21.609312057495117, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 157, "src_lang": "en", "tgt_lang": "zh", "output": "First, we study the effectiveness of Siamese Biancoders in a zero shot evaluation setup, meaning that pre-trained word embedding models are applied out of the box without any additional fine tuning.", "metrics": {"bleu_score": 0.47772128851934803, "chrf_score": 5.2255961183564335, "xcomet_score": 0.73804771900177, "xcomet_qe_score": 0.8367854356765747, "metricx_score": 18.275943756103516, "metricx_qe_score": 22.139179229736328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 158, "src_lang": "en", "tgt_lang": "zh", "output": "We experiment with context-independent text encoder, namely Word2Vec and FastText, and context-dependent embedding models, namely Roberta, and more specifically Camembert, which is a French Roberta model.", "metrics": {"bleu_score": 0.6516042567922691, "chrf_score": 17.42888171594834, "xcomet_score": 0.918556809425354, "xcomet_qe_score": 0.9582432508468628, "metricx_score": 6.566202640533447, "metricx_qe_score": 5.286703109741211, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 159, "src_lang": "en", "tgt_lang": "zh", "output": "Additionally, we train our own camembert based model. Beyond quoters,", "metrics": {"bleu_score": 0.0, "chrf_score": 4.598923358566558, "xcomet_score": 0.6620916724205017, "xcomet_qe_score": 0.7402422428131104, "metricx_score": 18.9390926361084, "metricx_qe_score": 18.81562614440918, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 160, "src_lang": "en", "tgt_lang": "zh", "output": "uses on all data sets. Note that for training we experiment with the two flavors of the Bianco architecture.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.39704710245132446, "xcomet_qe_score": 0.6206835508346558, "metricx_score": 20.388704299926758, "metricx_qe_score": 19.30509376525879, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 161, "src_lang": "en", "tgt_lang": "zh", "output": "Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and Tutor, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.", "metrics": {"bleu_score": 0.0, "chrf_score": 3.183236524289095, "xcomet_score": 0.6864355802536011, "xcomet_qe_score": 0.6624739170074463, "metricx_score": 23.537748336791992, "metricx_qe_score": 22.431156158447266, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 162, "src_lang": "en", "tgt_lang": "zh", "output": "We experimentate with mean, max, and CLS pooling, as well as dot product and cosine for computing similarities.", "metrics": {"bleu_score": 1.2006691370115936, "chrf_score": 2.287401341069678, "xcomet_score": 0.8174411058425903, "xcomet_qe_score": 0.9050110578536987, "metricx_score": 9.977287292480469, "metricx_qe_score": 9.431306838989258, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 163, "src_lang": "en", "tgt_lang": "zh", "output": "Here are the results of our baseline on the test sets.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9499307870864868, "xcomet_qe_score": 0.9676105976104736, "metricx_score": 8.444252967834473, "metricx_qe_score": 20.98896598815918, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 164, "src_lang": "en", "tgt_lang": "zh", "output": "With the lexical methods above, the Siamese B encoders evaluated in a zero shot setup in the middle, and the finest tune B encoders below.", "metrics": {"bleu_score": 0.0, "chrf_score": 6.5055936893260045, "xcomet_score": 0.5955122113227844, "xcomet_qe_score": 0.7592103481292725, "metricx_score": 20.973976135253906, "metricx_qe_score": 20.638010025024414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 165, "src_lang": "en", "tgt_lang": "zh", "output": "Overall, the fine-tuned Biancore significantly outperforms all the other bass lines.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7336224317550659, "xcomet_qe_score": 0.7820703983306885, "metricx_score": 19.189250946044922, "metricx_qe_score": 17.83257484436035, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 166, "src_lang": "en", "tgt_lang": "zh", "output": "The two tower model improves over its Siamese variant on Recall at one hundred, but performs similarly on the other metrics.", "metrics": {"bleu_score": 0.0, "chrf_score": 7.608383300771034, "xcomet_score": 0.8045989274978638, "xcomet_qe_score": 0.8397613763809204, "metricx_score": 24.91500473022461, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 167, "src_lang": "en", "tgt_lang": "zh", "output": "Although BM twenty five underperformed the trained Biancode significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.7478022012475095, "xcomet_score": 0.7584266662597656, "xcomet_qe_score": 0.7627054452896118, "metricx_score": 24.49001121520996, "metricx_qe_score": 24.563621520996094, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 168, "src_lang": "en", "tgt_lang": "zh", "output": "Regarding the zero shot evaluation of Siamese Biancoder, we find that directly using the embeddings of a pre trained Kamembert model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.", "metrics": {"bleu_score": 0.0, "chrf_score": 5.704215999437018, "xcomet_score": 0.7140986919403076, "xcomet_qe_score": 0.809081494808197, "metricx_score": 23.304676055908203, "metricx_qe_score": 23.835590362548828, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 169, "src_lang": "en", "tgt_lang": "zh", "output": "Furthermore, we observe that the word-to-vec-based Biancoder significantly outperforms the fast text and bird-based model, suggesting that maybe pre-train word level embeddings are more appropriate for the task than character level or sub-word level embeddings when used out of the box.", "metrics": {"bleu_score": 0.0, "chrf_score": 4.6008555218414635, "xcomet_score": 0.507652223110199, "xcomet_qe_score": 0.5819869041442871, "metricx_score": 17.399272918701172, "metricx_qe_score": 14.59615707397461, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 170, "src_lang": "en", "tgt_lang": "zh", "output": "Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9690454006195068, "xcomet_qe_score": 0.9892449378967285, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 171, "src_lang": "en", "tgt_lang": "zh", "output": "Let's conclude by discussing two limitations of all datasets.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9907854795455933, "xcomet_qe_score": 0.9961296319961548, "metricx_score": 10.115772247314453, "metricx_qe_score": 13.354596138000488, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 172, "src_lang": "en", "tgt_lang": "zh", "output": "First, the corpus of articles is limited to those collected from the 32 considered Belgian codes, which does not cover the entire Belgian law, as articles from decrees, directives and ordinances are missing.", "metrics": {"bleu_score": 0.5725191224772704, "chrf_score": 0.6107738247103347, "xcomet_score": 0.7791643142700195, "xcomet_qe_score": 0.9620044231414795, "metricx_score": 17.312288284301758, "metricx_qe_score": 19.075166702270508, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 173, "src_lang": "en", "tgt_lang": "zh", "output": "During the dataset construction, all references to these uncollected articles are ignored, which causes some question to end up with only a fraction of the initial number of relevant articles.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8688021302223206, "xcomet_qe_score": 0.8810131549835205, "metricx_score": 24.275911331176758, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 174, "src_lang": "en", "tgt_lang": "zh", "output": "This information loss implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9011154174804688, "xcomet_qe_score": 0.9263384342193604, "metricx_score": 24.35335922241211, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 175, "src_lang": "en", "tgt_lang": "zh", "output": "Second, we should note that not all legal questions can be answered with statutes alone.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 176, "src_lang": "en", "tgt_lang": "zh", "output": "For instance, the question Can I evict my tenants if they make too much noise?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9738367795944214, "xcomet_qe_score": 0.9677332043647766, "metricx_score": 22.985637664794922, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 177, "src_lang": "en", "tgt_lang": "zh", "output": "Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9299091100692749, "xcomet_qe_score": 0.9681105613708496, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 178, "src_lang": "en", "tgt_lang": "zh", "output": "Instead, the landlord should probably rely more on case law and find precedents similar to their current situation.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 179, "src_lang": "en", "tgt_lang": "zh", "output": "For example, the tenant makes two parties a week until two o'clock.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9733309149742126, "xcomet_qe_score": 0.9763040542602539, "metricx_score": 20.539037704467773, "metricx_qe_score": 23.68607521057129, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 180, "src_lang": "en", "tgt_lang": "zh", "output": "Therefore, some questions are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.897780179977417, "xcomet_qe_score": 0.8475546836853027, "metricx_score": 24.2918701171875, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 181, "src_lang": "en", "tgt_lang": "zh", "output": "We hope that all work sparks interest in the development of practical and reliable statutory article retrieval models,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9277328252792358, "xcomet_qe_score": 0.967008113861084, "metricx_score": 17.367778778076172, "metricx_qe_score": 17.455760955810547, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 182, "src_lang": "en", "tgt_lang": "zh", "output": "This can help improve access to justice for all.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9952396154403687, "xcomet_qe_score": 1.0, "metricx_score": 9.903719902038574, "metricx_qe_score": 12.38088321685791, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 183, "src_lang": "en", "tgt_lang": "zh", "output": "You can check out our paper, that set and code at the following links. Thank you.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9085267782211304, "xcomet_qe_score": 0.9257940053939819, "metricx_score": 15.124894142150879, "metricx_qe_score": 18.1261043548584, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 184, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, we are happy to present our work on Vowls, a task independent benchmark meant for testing vision and language models with specific linguistic phenomena.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.22222222222222227, "xcomet_score": 0.7623271942138672, "xcomet_qe_score": 0.7925093173980713, "metricx_score": 17.589744567871094, "metricx_qe_score": 17.031879425048828, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 185, "src_lang": "en", "tgt_lang": "zh", "output": "Why did we do the trouble in setting up this benchmark?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8524470329284668, "xcomet_qe_score": 0.9971965551376343, "metricx_score": 22.692073822021484, "metricx_qe_score": 24.371423721313477, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 186, "src_lang": "en", "tgt_lang": "zh", "output": "Well, during the last years we have seen an explosion of transformer-based vision and language models pre-trained on large amounts of image text pairs.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.984227180480957, "xcomet_qe_score": 0.9961551427841187, "metricx_score": 17.68745994567871, "metricx_qe_score": 24.861221313476562, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 187, "src_lang": "en", "tgt_lang": "zh", "output": "Each of these models pushes state of the art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7545181512832642, "xcomet_qe_score": 0.838408350944519, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 188, "src_lang": "en", "tgt_lang": "zh", "output": "So we got the message. The accuracies on these task specific benchmarks are steadily increasing.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9181071519851685, "xcomet_qe_score": 0.8906804323196411, "metricx_score": 11.417426109313965, "metricx_qe_score": 11.570944786071777, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 189, "src_lang": "en", "tgt_lang": "zh", "output": "But do we know what the models have actually learned?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9978679418563843, "xcomet_qe_score": 1.0, "metricx_score": 20.984336853027344, "metricx_qe_score": 24.301639556884766, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 190, "src_lang": "en", "tgt_lang": "zh", "output": "What is it that a Vision and Language Transformer understood when assigning a high score for this image and this sentence to match?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8932533264160156, "xcomet_qe_score": 0.9568833112716675, "metricx_score": 22.361251831054688, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 191, "src_lang": "en", "tgt_lang": "zh", "output": "and the low score for this one.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9659461975097656, "xcomet_qe_score": 0.9770287871360779, "metricx_score": 7.053254127502441, "metricx_qe_score": 11.804882049560547, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 192, "src_lang": "en", "tgt_lang": "zh", "output": "Do vision and language models focus on the right thing?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.957260012626648, "xcomet_qe_score": 0.9926931858062744, "metricx_score": 21.795440673828125, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 193, "src_lang": "en", "tgt_lang": "zh", "output": "Or do they focus on biases as shown by previous work?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9728238582611084, "xcomet_qe_score": 1.0, "metricx_score": 23.898130416870117, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 194, "src_lang": "en", "tgt_lang": "zh", "output": "To shed more light on this aspect, we propose a more task agnostic direction and introduce valves that test the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7633082866668701, "xcomet_qe_score": 0.8073660731315613, "metricx_score": 23.69077491760254, "metricx_qe_score": 23.364988327026367, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 195, "src_lang": "en", "tgt_lang": "zh", "output": "We aim existence, plurality, counting, spatial relations, actions and entity co-reference.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7461680769920349, "xcomet_qe_score": 0.8312443494796753, "metricx_score": 15.673873901367188, "metricx_qe_score": 20.51972770690918, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 196, "src_lang": "en", "tgt_lang": "zh", "output": "But how do we test whether the vision and language models have captured these phenomena?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9980562925338745, "xcomet_qe_score": 0.9964276552200317, "metricx_score": 23.111358642578125, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 197, "src_lang": "en", "tgt_lang": "zh", "output": "By foiling, a method previously applied for vision and language models only for noun phrases by Ravi Shakar and collaborators and on counting by us in previous work.", "metrics": {"bleu_score": 0.6590093442496134, "chrf_score": 6.423891146579488, "xcomet_score": 0.8122447729110718, "xcomet_qe_score": 0.7799372673034668, "metricx_score": 23.964305877685547, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 198, "src_lang": "en", "tgt_lang": "zh", "output": "Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8383947610855103, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 199, "src_lang": "en", "tgt_lang": "zh", "output": "We make these phrase alterations by focusing on six specific pieces, such as existence, plurality, counting, spatial relations, actions and entity co-reference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create FOIL instances.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5817241668701172, "xcomet_qe_score": 0.6464594602584839, "metricx_score": 14.515023231506348, "metricx_qe_score": 13.309370994567871, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 200, "src_lang": "en", "tgt_lang": "zh", "output": "For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6952926516532898, "xcomet_qe_score": 0.8123655319213867, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 201, "src_lang": "en", "tgt_lang": "zh", "output": "Counting and coreference are also pieces that have more than one instrument.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8088037967681885, "xcomet_qe_score": 0.8569647073745728, "metricx_score": 16.405595779418945, "metricx_qe_score": 23.304729461669922, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 202, "src_lang": "en", "tgt_lang": "zh", "output": "And we create these foils by making sure that they fail to describe the image, that they are grammatical and otherwise valid sentences.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9769084453582764, "xcomet_qe_score": 0.9866379499435425, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 203, "src_lang": "en", "tgt_lang": "zh", "output": "This is not easy to do because a foiled caption may be less likely than the original caption.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9835644960403442, "xcomet_qe_score": 0.9985752105712891, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 204, "src_lang": "en", "tgt_lang": "zh", "output": "For example, although it is not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7883784770965576, "xcomet_qe_score": 0.6836444139480591, "metricx_score": 24.533248901367188, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 205, "src_lang": "en", "tgt_lang": "zh", "output": "Therefore, to obtain valid foils, we must take action.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9227454662322998, "xcomet_qe_score": 0.9856106042861938, "metricx_score": 24.988754272460938, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 206, "src_lang": "en", "tgt_lang": "zh", "output": "First, we make use of strong language models to propose foils.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.960938572883606, "xcomet_qe_score": 0.969338059425354, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 207, "src_lang": "en", "tgt_lang": "zh", "output": "Second, we use natural language inference or short NLI to filter out foils that could still be describing the image since when constructing foils we need to ensure that they fail to describe the image.", "metrics": {"bleu_score": 0.5648729097007756, "chrf_score": 1.2387718856226093, "xcomet_score": 0.8782339096069336, "xcomet_qe_score": 0.9701697826385498, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 208, "src_lang": "en", "tgt_lang": "zh", "output": "To test this automatically, we apply natural language inference with the following rationale.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9823644161224365, "xcomet_qe_score": 1.0, "metricx_score": 22.562660217285156, "metricx_qe_score": 24.150941848754883, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 209, "src_lang": "en", "tgt_lang": "zh", "output": "We consider an image to be the premise and its caption its entailed hypothesis.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9534472227096558, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 210, "src_lang": "en", "tgt_lang": "zh", "output": "In addition, we consider the caption to be the premise and the foil is its hypothesis.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9660574197769165, "xcomet_qe_score": 0.9914631843566895, "metricx_score": 22.78455352783203, "metricx_qe_score": 24.68431282043457, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 211, "src_lang": "en", "tgt_lang": "zh", "output": "If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.", "metrics": {"bleu_score": 0.9831264157279513, "chrf_score": 1.8896236418060288, "xcomet_score": 0.900964617729187, "xcomet_qe_score": 0.9762262105941772, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 212, "src_lang": "en", "tgt_lang": "zh", "output": "If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil since by transitivity it will give a truthful description of the image and we filter these foils out.", "metrics": {"bleu_score": 0.47619889341203103, "chrf_score": 1.205896756869373, "xcomet_score": 0.6175916194915771, "xcomet_qe_score": 0.7431660294532776, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 213, "src_lang": "en", "tgt_lang": "zh", "output": "But this procedure is not perfect, it is just an indicator for valid foil.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9031415581703186, "xcomet_qe_score": 0.9826416969299316, "metricx_score": 23.050668716430664, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 214, "src_lang": "en", "tgt_lang": "zh", "output": "Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in valves.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.812946081161499, "xcomet_qe_score": 0.8656632900238037, "metricx_score": 22.453706741333008, "metricx_qe_score": 22.529760360717773, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 215, "src_lang": "en", "tgt_lang": "zh", "output": "So after filtering and human evaluation, we have as many test instances as described in this table.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9750458002090454, "xcomet_qe_score": 1.0, "metricx_score": 19.875215530395508, "metricx_qe_score": 24.488203048706055, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 216, "src_lang": "en", "tgt_lang": "zh", "output": "Note that VALS does not deliver any training data but only test data.", "metrics": {"bleu_score": 0.0, "chrf_score": 5.086565849117969, "xcomet_score": 0.9207503199577332, "xcomet_qe_score": 0.9175747036933899, "metricx_score": 12.878519058227539, "metricx_qe_score": 20.581085205078125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 217, "src_lang": "en", "tgt_lang": "zh", "output": "Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pre-training.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8907678127288818, "xcomet_qe_score": 0.9846575260162354, "metricx_score": 24.611499786376953, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 218, "src_lang": "en", "tgt_lang": "zh", "output": "Fine tuning would only enable models to exploit artefacts or statistical biases in the data.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9784120321273804, "xcomet_qe_score": 0.9479517340660095, "metricx_score": 22.818193435668945, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 219, "src_lang": "en", "tgt_lang": "zh", "output": "And we all know that these models like to cheat and take shortcuts.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.999495267868042, "xcomet_qe_score": 1.0, "metricx_score": 23.581077575683594, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 220, "src_lang": "en", "tgt_lang": "zh", "output": "And as we said, we are interested in assessing what capabilities the Vision and Language Models have after pre-training.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.964184045791626, "xcomet_qe_score": 0.9747000336647034, "metricx_score": 21.22954559326172, "metricx_qe_score": 24.943571090698242, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 221, "src_lang": "en", "tgt_lang": "zh", "output": "We experiment with five vision and language models on vowels, namely with CLIP, AlexMert, Wilbert, Wilbert Kelvin one and VisualBERT.", "metrics": {"bleu_score": 1.200657867104886, "chrf_score": 15.016290372228635, "xcomet_score": 0.45812100172042847, "xcomet_qe_score": 0.5405716896057129, "metricx_score": 15.085578918457031, "metricx_qe_score": 10.218033790588379, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 222, "src_lang": "en", "tgt_lang": "zh", "output": "Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.89361572265625, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 223, "src_lang": "en", "tgt_lang": "zh", "output": "Perhaps more relevant for this video is that we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8320227861404419, "xcomet_qe_score": 0.9597727060317993, "metricx_score": 24.48722267150879, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 224, "src_lang": "en", "tgt_lang": "zh", "output": "For more metrics and results on them, do check out our paper.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9672584533691406, "xcomet_qe_score": 0.9875798225402832, "metricx_score": 21.3964900970459, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 225, "src_lang": "en", "tgt_lang": "zh", "output": "The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics. The best zero shot performance is achieved by Wilbert twelve in one, followed by Wilbert, LXMerd, Clip and finally VisualBird.", "metrics": {"bleu_score": 0.0, "chrf_score": 7.12742218409287, "xcomet_score": 0.44815540313720703, "xcomet_qe_score": 0.48079341650009155, "metricx_score": 20.64979362487793, "metricx_qe_score": 17.281681060791016, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 226, "src_lang": "en", "tgt_lang": "zh", "output": "It's notable how instruments centered on individual objects like existence and noun phrases are almost solved by Wilbert Twelve in One, highlighting that models are capable of identifying named objects and their presence in images.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.332667997338656, "xcomet_score": 0.6755338311195374, "xcomet_qe_score": 0.7846137285232544, "metricx_score": 21.20455551147461, "metricx_qe_score": 22.276294708251953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 227, "src_lang": "en", "tgt_lang": "zh", "output": "However, none of the remaining pieces can be reliably solved in our adversarial foiling settings,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.957491397857666, "xcomet_qe_score": 0.9621268510818481, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 228, "src_lang": "en", "tgt_lang": "zh", "output": "We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects or counting them in an image.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9409617781639099, "xcomet_qe_score": 0.9779086112976074, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 229, "src_lang": "en", "tgt_lang": "zh", "output": "The relation Ps shows that they have difficulties in correctly classifying a named spatial relation between objects in an image,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8239279985427856, "xcomet_qe_score": 0.8637343049049377, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 230, "src_lang": "en", "tgt_lang": "zh", "output": "They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases, as we see in the actions piece.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8954020738601685, "xcomet_qe_score": 0.9814682006835938, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 231, "src_lang": "en", "tgt_lang": "zh", "output": "From the co-reference piece we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8533434867858887, "xcomet_qe_score": 0.9180147647857666, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 232, "src_lang": "en", "tgt_lang": "zh", "output": "As a sanity check and because it is an interesting experiment, we also benchmark two text only models GPT one and GPT two to assess whether Vault is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption no image here and predicting the entry with the lowest perplexity.", "metrics": {"bleu_score": 0.7365731670806237, "chrf_score": 5.0025438463890195, "xcomet_score": 0.5821586847305298, "xcomet_qe_score": 0.7203240394592285, "metricx_score": 21.56654167175293, "metricx_qe_score": 20.468608856201172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 233, "src_lang": "en", "tgt_lang": "zh", "output": "If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7920447587966919, "xcomet_qe_score": 0.9316962361335754, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 234, "src_lang": "en", "tgt_lang": "zh", "output": "And it's interesting to see that in some cases the text only GPT models have captured the plausibility of the world better than the vision and language models.", "metrics": {"bleu_score": 0.8805404910048886, "chrf_score": 1.685726432299426, "xcomet_score": 0.9822126626968384, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 235, "src_lang": "en", "tgt_lang": "zh", "output": "In summary, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.", "metrics": {"bleu_score": 0.5354362577465922, "chrf_score": 3.195166596347955, "xcomet_score": 0.8604164123535156, "xcomet_qe_score": 0.9714624881744385, "metricx_score": 19.823698043823242, "metricx_qe_score": 22.433835983276367, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 236, "src_lang": "en", "tgt_lang": "zh", "output": "Our experiments show that vision and language models identify well named objects and their presence in images, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7687749266624451, "xcomet_qe_score": 0.8289618492126465, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 237, "src_lang": "en", "tgt_lang": "zh", "output": "We would really like to encourage the community to use Vals for measuring progress towards language grounding with vision and language models.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.3052503052503053, "xcomet_score": 0.8362635970115662, "xcomet_qe_score": 0.8308665752410889, "metricx_score": 23.54808807373047, "metricx_qe_score": 24.506351470947266, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 238, "src_lang": "en", "tgt_lang": "zh", "output": "And even more, valves could be used as an indirect assessment of datasets, as models could be evaluated before and after training or fine tuning to see whether a dataset helps models improve on any of the aspects tested by valves.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.17006802721088432, "xcomet_score": 0.7298380136489868, "xcomet_qe_score": 0.78118896484375, "metricx_score": 25.0, "metricx_qe_score": 21.68629264831543, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 239, "src_lang": "en", "tgt_lang": "zh", "output": "If you're interested, check out the valse data on GitHub, and if you have any questions, don't hesitate to contact us.", "metrics": {"bleu_score": 1.0096288963946694, "chrf_score": 6.593106419347913, "xcomet_score": 0.9696340560913086, "xcomet_qe_score": 0.9699967503547668, "metricx_score": 6.315456867218018, "metricx_qe_score": 11.90217113494873, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 240, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, my name is Kamisara, from the University of Tokyo.", "metrics": {"bleu_score": 0.0, "chrf_score": 7.352227212679412, "xcomet_score": 0.7668853998184204, "xcomet_qe_score": 0.7994793057441711, "metricx_score": 11.17902660369873, "metricx_qe_score": 11.921820640563965, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 241, "src_lang": "en", "tgt_lang": "zh", "output": "I will be presenting a paper entitled RNSAM a large scale data set for automatic risk not duration by a committee log summization.", "metrics": {"bleu_score": 0.0, "chrf_score": 2.667386961644874, "xcomet_score": 0.5868344306945801, "xcomet_qe_score": 0.5972913503646851, "metricx_score": 14.062111854553223, "metricx_qe_score": 14.009814262390137, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 242, "src_lang": "en", "tgt_lang": "zh", "output": "I will explain in this order", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9973217248916626, "xcomet_qe_score": 1.0, "metricx_score": 1.7473177909851074, "metricx_qe_score": 2.6663570404052734, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 243, "src_lang": "en", "tgt_lang": "zh", "output": "First, I will introduce the automatic risk neutralization that we are working on in this research.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7708486318588257, "xcomet_qe_score": 0.7978443503379822, "metricx_score": 14.924484252929688, "metricx_qe_score": 13.025789260864258, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 244, "src_lang": "en", "tgt_lang": "zh", "output": "ReleaseNote is a technical document that summarizes the changes distributed with each release of a software product.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9671370983123779, "xcomet_qe_score": 0.9819262623786926, "metricx_score": 22.978498458862305, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 245, "src_lang": "en", "tgt_lang": "zh", "output": "The image shows a research note for Bodge two point six.", "metrics": {"bleu_score": 0.0, "chrf_score": 1.582278481012658, "xcomet_score": 0.3092018961906433, "xcomet_qe_score": 0.4246007800102234, "metricx_score": 17.72467613220215, "metricx_qe_score": 20.519222259521484, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 246, "src_lang": "en", "tgt_lang": "zh", "output": "User's library. These nodes play an important role in open source development, but they are time consuming to prepare manually.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7733500003814697, "xcomet_qe_score": 0.7718199491500854, "metricx_score": 20.513877868652344, "metricx_qe_score": 16.098983764648438, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 247, "src_lang": "en", "tgt_lang": "zh", "output": "Therefore, it would be very useful to be able to automatically generate high quality release notes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9941304922103882, "xcomet_qe_score": 1.0, "metricx_score": 22.33217430114746, "metricx_qe_score": 23.504287719726562, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 248, "src_lang": "en", "tgt_lang": "zh", "output": "I will refer to two previous researies on automatic freeze not generation.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6966532468795776, "xcomet_qe_score": 0.7382373809814453, "metricx_score": 14.647563934326172, "metricx_qe_score": 13.743148803710938, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 249, "src_lang": "en", "tgt_lang": "zh", "output": "The first is a system called Arena, released in twenty fourteen. It is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.5274261603375527, "xcomet_score": 0.7940237522125244, "xcomet_qe_score": 0.796453595161438, "metricx_score": 17.051563262939453, "metricx_qe_score": 21.58889389038086, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 250, "src_lang": "en", "tgt_lang": "zh", "output": "It takes a rule based approach, for example, using the change extractor to extract core differences, library changes and document changes from the differences between releases, and finally combining them.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8861186504364014, "xcomet_qe_score": 0.9610575437545776, "metricx_score": 22.714832305908203, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 251, "src_lang": "en", "tgt_lang": "zh", "output": "The most notable feature of this system is the issue extractor in the upper right corner,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9517685174942017, "xcomet_qe_score": 0.9697624444961548, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 252, "src_lang": "en", "tgt_lang": "zh", "output": "Which must be linked to zero the issue to ecosystem and can only be applied to projects that use zero.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5326699018478394, "xcomet_qe_score": 0.7479536533355713, "metricx_score": 17.95067024230957, "metricx_qe_score": 14.244195938110352, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 253, "src_lang": "en", "tgt_lang": "zh", "output": "in other words it cannot be used for many projects on GitHub.", "metrics": {"bleu_score": 2.1027582541760537, "chrf_score": 13.13979349125064, "xcomet_score": 0.9826121926307678, "xcomet_qe_score": 0.9823999404907227, "metricx_score": 11.885055541992188, "metricx_qe_score": 20.934600830078125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 254, "src_lang": "en", "tgt_lang": "zh", "output": "The second is grief. This entry announced in twenty four.", "metrics": {"bleu_score": 0.0, "chrf_score": 1.2626262626262625, "xcomet_score": 0.5955361127853394, "xcomet_qe_score": 0.6272672414779663, "metricx_score": 23.061328887939453, "metricx_qe_score": 19.83319854736328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 255, "src_lang": "en", "tgt_lang": "zh", "output": "twenty twenty. It is available on the internet and can be stored via PIP.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.5630630630630631, "xcomet_score": 0.7701519727706909, "xcomet_qe_score": 0.7963505983352661, "metricx_score": 14.055456161499023, "metricx_qe_score": 13.286821365356445, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 256, "src_lang": "en", "tgt_lang": "zh", "output": "This system has a simple running-based text classification model and outputs form of five labels, such as features or bug fixes, for each input commit message.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7584391832351685, "xcomet_qe_score": 0.7986060380935669, "metricx_score": 15.929374694824219, "metricx_qe_score": 13.948700904846191, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 257, "src_lang": "en", "tgt_lang": "zh", "output": "The image is a sample usage that returns a corrective or bug fixes rebel.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.751305103302002, "xcomet_qe_score": 0.7654050588607788, "metricx_score": 15.166253089904785, "metricx_qe_score": 16.429540634155273, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 258, "src_lang": "en", "tgt_lang": "zh", "output": "Graves' training data is fairly small, about five thousand, and will be shown in the experiment described below.", "metrics": {"bleu_score": 0.0, "chrf_score": 2.2109422837516175, "xcomet_score": 0.7042993307113647, "xcomet_qe_score": 0.7687971591949463, "metricx_score": 23.13290023803711, "metricx_qe_score": 22.67732810974121, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 259, "src_lang": "en", "tgt_lang": "zh", "output": "The performance of the statistic graphic schedule model is not high.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8579708337783813, "xcomet_qe_score": 0.9250850677490234, "metricx_score": 11.451752662658691, "metricx_qe_score": 12.079597473144531, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 260, "src_lang": "en", "tgt_lang": "zh", "output": "I present two related researches, but there are problems of limited applicability and scarce data resources.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.970605731010437, "xcomet_qe_score": 0.9882595539093018, "metricx_score": 12.178937911987305, "metricx_qe_score": 22.751209259033203, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 261, "src_lang": "en", "tgt_lang": "zh", "output": "Our paper solves these two problems and automatically generates high-quality release notes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9963724613189697, "xcomet_qe_score": 1.0, "metricx_score": 22.471784591674805, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 262, "src_lang": "en", "tgt_lang": "zh", "output": "For the limited applicability program, we propose a high quality classifier summarization method using only committee message as input.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6930601596832275, "xcomet_qe_score": 0.703155517578125, "metricx_score": 9.522750854492188, "metricx_qe_score": 9.826638221740723, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 263, "src_lang": "en", "tgt_lang": "zh", "output": "This proposed method can be used for all English repositories.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.992303729057312, "xcomet_qe_score": 1.0, "metricx_score": 17.97479248046875, "metricx_qe_score": 21.217655181884766, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 264, "src_lang": "en", "tgt_lang": "zh", "output": "For the second problem of scarce data resources, we built a RNSAM data set consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.", "metrics": {"bleu_score": 1.3020937498308554, "chrf_score": 12.646321732171636, "xcomet_score": 0.8923291563987732, "xcomet_qe_score": 0.9097940325737, "metricx_score": 20.530731201171875, "metricx_qe_score": 20.775142669677734, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 265, "src_lang": "en", "tgt_lang": "zh", "output": "Next, I describe our desert.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7145416736602783, "xcomet_qe_score": 0.7472309470176697, "metricx_score": 13.124587059020996, "metricx_qe_score": 16.594383239746094, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 266, "src_lang": "en", "tgt_lang": "zh", "output": "Here is our example of data.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9187477827072144, "xcomet_qe_score": 0.9728333950042725, "metricx_score": 2.1019301414489746, "metricx_qe_score": 2.237287759780884, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 267, "src_lang": "en", "tgt_lang": "zh", "output": "The left side is the committee message, and the right side is the release note.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8423927426338196, "xcomet_qe_score": 0.8635815978050232, "metricx_score": 5.394161701202393, "metricx_qe_score": 6.679632663726807, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 268, "src_lang": "en", "tgt_lang": "zh", "output": "The reason notes are loved as improvements, offices, etc.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.44349896907806396, "xcomet_qe_score": 0.7167853713035583, "metricx_score": 21.27727508544922, "metricx_qe_score": 21.3994083404541, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 269, "src_lang": "en", "tgt_lang": "zh", "output": "We have set up a task that takes the commit messages as input and outputs the raw-welded please notes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.762149453163147, "xcomet_qe_score": 0.7966156601905823, "metricx_score": 21.74003791809082, "metricx_qe_score": 22.475542068481445, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 270, "src_lang": "en", "tgt_lang": "zh", "output": "This can be regarded as a summarization task.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9805371761322021, "xcomet_qe_score": 1.0, "metricx_score": 14.528165817260742, "metricx_qe_score": 21.963693618774414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 271, "src_lang": "en", "tgt_lang": "zh", "output": "We have predefined four rubbers features, improvements, bug fixes, deprecations, removables and breaking changes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6585437059402466, "xcomet_qe_score": 0.7347387075424194, "metricx_score": 22.622865676879883, "metricx_qe_score": 19.717714309692383, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 272, "src_lang": "en", "tgt_lang": "zh", "output": "These were set based on previous research and other factors,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9945281744003296, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 23.722042083740234, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 273, "src_lang": "en", "tgt_lang": "zh", "output": "The release notes on the bottom right are extracted from the release notes shown on the bottom left.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9828526973724365, "xcomet_qe_score": 0.9896291494369507, "metricx_score": 7.664166450500488, "metricx_qe_score": 9.83785629272461, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 274, "src_lang": "en", "tgt_lang": "zh", "output": "At this time it is necessary to detect the four rubbish that have been set up in advance.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7942781448364258, "xcomet_qe_score": 0.8010177612304688, "metricx_score": 16.739290237426758, "metricx_qe_score": 18.202272415161133, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 275, "src_lang": "en", "tgt_lang": "zh", "output": "but the laughter is not always consistent with each liberty", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.30700528621673584, "xcomet_qe_score": 0.5107927322387695, "metricx_score": 22.775856018066406, "metricx_qe_score": 16.872699737548828, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 276, "src_lang": "en", "tgt_lang": "zh", "output": "For example, the driver of improvements increases improvements, enhancements, optimizations, and so on.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8094003796577454, "xcomet_qe_score": 0.8251983523368835, "metricx_score": 17.00608253479004, "metricx_qe_score": 15.203737258911133, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 277, "src_lang": "en", "tgt_lang": "zh", "output": "We have prepared a vocabulary list of about thirty numbers for each of these notational variations.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8593937754631042, "xcomet_qe_score": 0.87320476770401, "metricx_score": 20.239442825317383, "metricx_qe_score": 18.26564598083496, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 278, "src_lang": "en", "tgt_lang": "zh", "output": "Use it to detect the RIS not crusts and correct the text of the rest that follows as the RIS not sentence for the crust.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.39599162340164185, "xcomet_qe_score": 0.41660648584365845, "metricx_score": 19.1069393157959, "metricx_qe_score": 18.193592071533203, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 279, "src_lang": "en", "tgt_lang": "zh", "output": "Next is a committee message.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8260142803192139, "xcomet_qe_score": 0.8401266932487488, "metricx_score": 5.742269039154053, "metricx_qe_score": 6.739920139312744, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 280, "src_lang": "en", "tgt_lang": "zh", "output": "Committee messages are not tied to each vase.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5884916186332703, "xcomet_qe_score": 0.6301432251930237, "metricx_score": 15.946956634521484, "metricx_qe_score": 20.873031616210938, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 281, "src_lang": "en", "tgt_lang": "zh", "output": "As shown in the image below, if the current risk is one thousand two point five two nineteen, we need to identify", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.37325796484947205, "xcomet_qe_score": 0.42134931683540344, "metricx_score": 23.826725006103516, "metricx_qe_score": 21.99908447265625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 282, "src_lang": "en", "tgt_lang": "zh", "output": "modifies the previous release version two point five two eighteen and gets its diff. This is a bit tedious and it is not enough to just get a list of releases and look at the before and after.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.21352261304855347, "xcomet_qe_score": 0.1418398916721344, "metricx_score": 22.387229919433594, "metricx_qe_score": 22.684986114501953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 283, "src_lang": "en", "tgt_lang": "zh", "output": "He created a heuristic matching blue to get the previous and next pageant.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5425454378128052, "xcomet_qe_score": 0.6569112539291382, "metricx_score": 23.638137817382812, "metricx_qe_score": 22.318559646606445, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 284, "src_lang": "en", "tgt_lang": "zh", "output": "There's it's a horses.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.16004915535449982, "xcomet_qe_score": 0.12298910319805145, "metricx_score": 17.86426544189453, "metricx_qe_score": 22.967041015625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 285, "src_lang": "en", "tgt_lang": "zh", "output": "In the end, seven thousand two hundred repositories", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7587505578994751, "xcomet_qe_score": 0.7904128432273865, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 286, "src_lang": "en", "tgt_lang": "zh", "output": "Also, the average number of released node tokens is sixty three, which is quite high for a summarization task.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.810063362121582, "xcomet_qe_score": 0.8500559329986572, "metricx_score": 19.867605209350586, "metricx_qe_score": 23.67995262145996, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 287, "src_lang": "en", "tgt_lang": "zh", "output": "Also the number of unique tokens is quite large at eight thousand eight hundred thirty thousand. This is one hundred.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7143787145614624, "xcomet_qe_score": 0.7769857048988342, "metricx_score": 18.7974853515625, "metricx_qe_score": 22.718917846679688, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 288, "src_lang": "en", "tgt_lang": "zh", "output": "due to the large number of unique classes and method nerves found in the repository.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8447707891464233, "xcomet_qe_score": 0.9123813509941101, "metricx_score": 18.130348205566406, "metricx_qe_score": 19.172407150268555, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 289, "src_lang": "en", "tgt_lang": "zh", "output": "Next I will explain the proposed method.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 6.302163124084473, "metricx_qe_score": 10.99738883972168, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 290, "src_lang": "en", "tgt_lang": "zh", "output": "The crosswise extractive and abstractive summarization model consists of two newer modules", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6898977756500244, "xcomet_qe_score": 0.743896484375, "metricx_score": 18.808671951293945, "metricx_qe_score": 11.467986106872559, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 291, "src_lang": "en", "tgt_lang": "zh", "output": "A classifier using bot or code bot and a generator using bot.", "metrics": {"bleu_score": 0.0, "chrf_score": 2.9302292769551945, "xcomet_score": 0.607496976852417, "xcomet_qe_score": 0.6633042097091675, "metricx_score": 15.932405471801758, "metricx_qe_score": 10.960541725158691, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 292, "src_lang": "en", "tgt_lang": "zh", "output": "First, GEAS uses a classifier to classify each committee message into five disnode classes, features, improvements, bug fixes, duplications plus and others.", "metrics": {"bleu_score": 0.0, "chrf_score": 1.3600848978638482, "xcomet_score": 0.3924100399017334, "xcomet_qe_score": 0.41732436418533325, "metricx_score": 21.829151153564453, "metricx_qe_score": 18.72334861755371, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 293, "src_lang": "en", "tgt_lang": "zh", "output": "The committee messages classified as Other are discarded.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8656401634216309, "xcomet_qe_score": 0.8495007753372192, "metricx_score": 6.252999782562256, "metricx_qe_score": 8.637539863586426, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 294, "src_lang": "en", "tgt_lang": "zh", "output": "Then GAS applies the generator to the four router documents independently and generates risk notes for each class.", "metrics": {"bleu_score": 0.0, "chrf_score": 1.0275103073952299, "xcomet_score": 0.48651406168937683, "xcomet_qe_score": 0.602171003818512, "metricx_score": 19.545433044433594, "metricx_qe_score": 13.414758682250977, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 295, "src_lang": "en", "tgt_lang": "zh", "output": "In this task, the direct correspondences between commit messages and read notes are not known.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8251081705093384, "xcomet_qe_score": 0.8712300062179565, "metricx_score": 18.592147827148438, "metricx_qe_score": 12.001214981079102, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 296, "src_lang": "en", "tgt_lang": "zh", "output": "Therefore, to train the classifier, we assign pseudorubers to each input commit message using the first ten characters of each commit message.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8433473110198975, "xcomet_qe_score": 0.8762947916984558, "metricx_score": 11.662105560302734, "metricx_qe_score": 10.00277328491211, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 297, "src_lang": "en", "tgt_lang": "zh", "output": "We modulate the crosswise obstructive summary approach by two different methods.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.753309965133667, "xcomet_qe_score": 0.7561167478561401, "metricx_score": 15.118513107299805, "metricx_qe_score": 11.027475357055664, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 298, "src_lang": "en", "tgt_lang": "zh", "output": "The first model, which we call GIS Single, consists of a single sect to sect network and generates a single long isNode text, giving a concretency of input committement messages.", "metrics": {"bleu_score": 0.0, "chrf_score": 4.52799264867552, "xcomet_score": 0.38017427921295166, "xcomet_qe_score": 0.41314050555229187, "metricx_score": 22.533788681030273, "metricx_qe_score": 19.930503845214844, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 299, "src_lang": "en", "tgt_lang": "zh", "output": "The output text can be divided into crosswise segments based on special cross-specific endpoint symbols.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8099594116210938, "xcomet_qe_score": 0.8023959398269653, "metricx_score": 10.889094352722168, "metricx_qe_score": 6.960971832275391, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 300, "src_lang": "en", "tgt_lang": "zh", "output": "The second method, which we call CSMarch, consists of four different sec-to-sec networks, each of which corresponds to one of the list-node classes.", "metrics": {"bleu_score": 0.0, "chrf_score": 3.125473912836451, "xcomet_score": 0.47707927227020264, "xcomet_qe_score": 0.5214143395423889, "metricx_score": 23.13846206665039, "metricx_qe_score": 18.03091812133789, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 301, "src_lang": "en", "tgt_lang": "zh", "output": "Okay, that makes Fan's experiment.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.692927360534668, "xcomet_qe_score": 0.7517367601394653, "metricx_score": 13.513331413269043, "metricx_qe_score": 17.926908493041992, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 302, "src_lang": "en", "tgt_lang": "zh", "output": "Five methods were compared: CS, CS single, CS mouth, prasseling and previous study brief.", "metrics": {"bleu_score": 0.0, "chrf_score": 11.174660544326201, "xcomet_score": 0.5484380722045898, "xcomet_qe_score": 0.5889143943786621, "metricx_score": 23.06561279296875, "metricx_qe_score": 19.722814559936523, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 303, "src_lang": "en", "tgt_lang": "zh", "output": "Regarding ablation, in some cases these notes are output in multiple sentences.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7853649258613586, "xcomet_qe_score": 0.7441585063934326, "metricx_score": 17.404924392700195, "metricx_qe_score": 12.9108304977417, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 304, "src_lang": "en", "tgt_lang": "zh", "output": "Since it is difficult to calculate the number of sentences at zero, they are combined with spaces and treated as one long sentence.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8829234838485718, "xcomet_qe_score": 0.9011039137840271, "metricx_score": 20.182756423950195, "metricx_qe_score": 23.524988174438477, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 305, "src_lang": "en", "tgt_lang": "zh", "output": "The Bureau is penetrated when the system output a short sentence", "metrics": {"bleu_score": 0.0, "chrf_score": 0.6038647342995169, "xcomet_score": 0.7470401525497437, "xcomet_qe_score": 0.7580759525299072, "metricx_score": 17.324487686157227, "metricx_qe_score": 16.851398468017578, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 306, "src_lang": "en", "tgt_lang": "zh", "output": "This penalty results in a lower brew volume in the experimental results described next", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7766015529632568, "xcomet_qe_score": 0.8237258195877075, "metricx_score": 16.786169052124023, "metricx_qe_score": 9.400744438171387, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 307, "src_lang": "en", "tgt_lang": "zh", "output": "Finally, we also corrugate a specificity because rouge and blue cannot be corrugated if the release nodes are empty.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5516381859779358, "xcomet_qe_score": 0.5958787202835083, "metricx_score": 17.931915283203125, "metricx_qe_score": 15.845561981201172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 308, "src_lang": "en", "tgt_lang": "zh", "output": "A high specificity means that the model correctly outputs are empty text in cases where the reason notes assume empty.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8265407085418701, "xcomet_qe_score": 0.8664897680282593, "metricx_score": 21.94234275817871, "metricx_qe_score": 23.609477996826172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 309, "src_lang": "en", "tgt_lang": "zh", "output": "Here are the results.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9745558500289917, "xcomet_qe_score": 1.0, "metricx_score": 2.1450202465057373, "metricx_qe_score": 4.641719818115234, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 310, "src_lang": "en", "tgt_lang": "zh", "output": "Since the data set contains email addresses, hash values, etc, we also eradicate the print data set, which excludes them.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7943981289863586, "xcomet_qe_score": 0.7986107468605042, "metricx_score": 12.34206485748291, "metricx_qe_score": 9.231319427490234, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 311, "src_lang": "en", "tgt_lang": "zh", "output": "She has and She has achieved large air scores more than ten points higher than the baseline.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.8865248226950355, "xcomet_score": 0.10657557100057602, "xcomet_qe_score": 0.1953064650297165, "metricx_score": 20.85674476623535, "metricx_qe_score": 20.399627685546875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 312, "src_lang": "en", "tgt_lang": "zh", "output": "In particular, on the green test set, the square gap between the proposed method and the base end jumped to more than twenty points.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5973793864250183, "xcomet_qe_score": 0.6386516690254211, "metricx_score": 20.44799041748047, "metricx_qe_score": 18.292394638061523, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 313, "src_lang": "en", "tgt_lang": "zh", "output": "These results indicate that she is and she is are significantly effective.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.19910304248332977, "xcomet_qe_score": 0.2092183232307434, "metricx_score": 22.17466926574707, "metricx_qe_score": 21.23675537109375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 314, "src_lang": "en", "tgt_lang": "zh", "output": "GAS got a better root score than GAS, suggesting that combining a crossfire and a generator is effective on training the crossfire using pseudobus.", "metrics": {"bleu_score": 0.0, "chrf_score": 1.6832344664101235, "xcomet_score": 0.20624902844429016, "xcomet_qe_score": 0.3200078308582306, "metricx_score": 23.986583709716797, "metricx_qe_score": 20.955860137939453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 315, "src_lang": "en", "tgt_lang": "zh", "output": "AICAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.", "metrics": {"bleu_score": 0.0, "chrf_score": 1.195895469045489, "xcomet_score": 0.7297898530960083, "xcomet_qe_score": 0.8106820583343506, "metricx_score": 25.0, "metricx_qe_score": 23.29106903076172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 316, "src_lang": "en", "tgt_lang": "zh", "output": "She is much tended to eat higher this year than she is single,", "metrics": {"bleu_score": 0.0, "chrf_score": 8.398549899971988, "xcomet_score": 0.15790964663028717, "xcomet_qe_score": 0.19647285342216492, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 317, "src_lang": "en", "tgt_lang": "zh", "output": "suggesting that it is also effective to independently develop different two year perspective summarization models for each view's note grass.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5153539180755615, "xcomet_qe_score": 0.5404621362686157, "metricx_score": 22.838903427124023, "metricx_qe_score": 20.34310531616211, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 318, "src_lang": "en", "tgt_lang": "zh", "output": "Hero and Eronasus", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.13329114019870758, "xcomet_qe_score": 0.0795399397611618, "metricx_score": 17.214000701904297, "metricx_qe_score": 22.899822235107422, "linguapy_score": [1, "LITHUANIAN"]}}
{"dataset_id": "acl_6060", "sample_id": 319, "src_lang": "en", "tgt_lang": "zh", "output": "Xia's methods tend to output shorter sentences than human reference sentences.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6280919313430786, "xcomet_qe_score": 0.7267454266548157, "metricx_score": 23.608430862426758, "metricx_qe_score": 23.22526741027832, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 320, "src_lang": "en", "tgt_lang": "zh", "output": "In the figure on the right, the reference sentence has three or four sentences, while she has only one.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7968859672546387, "xcomet_qe_score": 0.7772432565689087, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 321, "src_lang": "en", "tgt_lang": "zh", "output": "The reason for this model reluctance is that in the training data only 33% of the sentences are present in the features rubric and 40% in the improvements rubric.", "metrics": {"bleu_score": 2.0531313309518464, "chrf_score": 3.061698871427366, "xcomet_score": 0.8713895082473755, "xcomet_qe_score": 0.9346963763237, "metricx_score": 9.069704055786133, "metricx_qe_score": 8.610536575317383, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 322, "src_lang": "en", "tgt_lang": "zh", "output": "Furthermore, GES methods cannot generate accurate risk notes without additional information.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.40453074433656966, "xcomet_score": 0.7174838185310364, "xcomet_qe_score": 0.8598259091377258, "metricx_score": 15.909832000732422, "metricx_qe_score": 11.310776710510254, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 323, "src_lang": "en", "tgt_lang": "zh", "output": "The top example on the right is an example of a very messy committee message and the complete sentence cannot be generated without reference to the corresponding peru request or issue.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7050794363021851, "xcomet_qe_score": 0.7295483350753784, "metricx_score": 18.982358932495117, "metricx_qe_score": 20.890111923217773, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 324, "src_lang": "en", "tgt_lang": "zh", "output": "The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9734575748443604, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 325, "src_lang": "en", "tgt_lang": "zh", "output": "Slutligen, en conclusion.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9884397983551025, "xcomet_qe_score": 0.9822278022766113, "metricx_score": 1.4869105815887451, "metricx_qe_score": 2.709137201309204, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 326, "src_lang": "en", "tgt_lang": "zh", "output": "We have built a new data set for automatic personal generation,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7659030556678772, "xcomet_qe_score": 0.7690122723579407, "metricx_score": 16.766576766967773, "metricx_qe_score": 13.799038887023926, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 327, "src_lang": "en", "tgt_lang": "zh", "output": "We have also formulated the task of entering committee messages and summarizing them so that it is applicable to all projects written in English.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8641452193260193, "xcomet_qe_score": 0.8945795297622681, "metricx_score": 21.157779693603516, "metricx_qe_score": 24.966758728027344, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 328, "src_lang": "en", "tgt_lang": "zh", "output": "Our experiments show that the proposed method generates less noisy reason not at higher coverage than the base rise.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.633336067199707, "xcomet_qe_score": 0.6713894605636597, "metricx_score": 22.703371047973633, "metricx_qe_score": 23.12870216369629, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 329, "src_lang": "en", "tgt_lang": "zh", "output": "Ple check out our desert audit app.", "metrics": {"bleu_score": 0.0, "chrf_score": 3.343367294432438, "xcomet_score": 0.24960848689079285, "xcomet_qe_score": 0.31667986512184143, "metricx_score": 15.102442741394043, "metricx_qe_score": 19.771087646484375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 330, "src_lang": "en", "tgt_lang": "zh", "output": "Thank you.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4112658202648163, "metricx_qe_score": 0.48872482776641846, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 331, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, my name is a safarari,", "metrics": {"bleu_score": 0.0, "chrf_score": 18.67332583148526, "xcomet_score": 0.6610270738601685, "xcomet_qe_score": 0.724624514579773, "metricx_score": 13.076451301574707, "metricx_qe_score": 15.100526809692383, "linguapy_score": [1, "TSWANA"]}}
{"dataset_id": "acl_6060", "sample_id": 332, "src_lang": "en", "tgt_lang": "zh", "output": "And I will present our paper Few Short Tabular Data Enrichment using fine tuning transformers architectures.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7586417198181152, "xcomet_qe_score": 0.80966717004776, "metricx_score": 15.232962608337402, "metricx_qe_score": 16.783817291259766, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 333, "src_lang": "en", "tgt_lang": "zh", "output": "Data scientists analyze data and focus mainly on manipulating existing data features.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9898301362991333, "xcomet_qe_score": 0.9876905679702759, "metricx_score": 11.73897933959961, "metricx_qe_score": 20.555618286132812, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 334, "src_lang": "en", "tgt_lang": "zh", "output": "But sometimes its features are limited.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9852420091629028, "xcomet_qe_score": 0.9896385669708252, "metricx_score": 6.719892501831055, "metricx_qe_score": 9.624013900756836, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 335, "src_lang": "en", "tgt_lang": "zh", "output": "Future generation using another data source may add substantial information.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.832729160785675, "xcomet_qe_score": 0.834688663482666, "metricx_score": 21.599746704101562, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 336, "src_lang": "en", "tgt_lang": "zh", "output": "Our research goal is automatic tabular data enrichment using external sources free text.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9418507814407349, "xcomet_qe_score": 0.9403562545776367, "metricx_score": 22.81277084350586, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 337, "src_lang": "en", "tgt_lang": "zh", "output": "Assume we have a tabular dataset and a knowledge base.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9919666051864624, "xcomet_qe_score": 1.0, "metricx_score": 18.208459854125977, "metricx_qe_score": 24.35892677307129, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 338, "src_lang": "en", "tgt_lang": "zh", "output": "We need an automatic process, which involves linking of entities and text analysis to extract new features from the knowledge base free text.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9382832050323486, "xcomet_qe_score": 0.9547634124755859, "metricx_score": 11.395269393920898, "metricx_qe_score": 20.00147819519043, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 339, "src_lang": "en", "tgt_lang": "zh", "output": "Our framework first is exactly this automatic process.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.6775067750677506, "xcomet_score": 0.810126781463623, "xcomet_qe_score": 0.8076381683349609, "metricx_score": 11.695890426635742, "metricx_qe_score": 14.16227912902832, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 340, "src_lang": "en", "tgt_lang": "zh", "output": "So let's see an example in a data set that's fed into FAST.", "metrics": {"bleu_score": 0.0, "chrf_score": 3.020030359308351, "xcomet_score": 0.844284176826477, "xcomet_qe_score": 0.8343127965927124, "metricx_score": 13.045032501220703, "metricx_qe_score": 11.990516662597656, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 341, "src_lang": "en", "tgt_lang": "zh", "output": "In this example, the data set is a university data set.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9922816753387451, "xcomet_qe_score": 0.9884041547775269, "metricx_score": 0.6011859178543091, "metricx_qe_score": 0.8418247699737549, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 342, "src_lang": "en", "tgt_lang": "zh", "output": "When its goal is to classify universities into low ranked universities and high ranked universities.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9870502948760986, "xcomet_qe_score": 0.9559069275856018, "metricx_score": 22.574846267700195, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 343, "src_lang": "en", "tgt_lang": "zh", "output": "As a knowledge base, we use Wikipedia.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 6.557746887207031, "metricx_qe_score": 13.929158210754395, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 344, "src_lang": "en", "tgt_lang": "zh", "output": "The first phase of FEST is entity linking.", "metrics": {"bleu_score": 0.0, "chrf_score": 5.095490948026265, "xcomet_score": 0.9000904560089111, "xcomet_qe_score": 0.873663067817688, "metricx_score": 15.42374038696289, "metricx_qe_score": 16.606063842773438, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 345, "src_lang": "en", "tgt_lang": "zh", "output": "when each entity in this example the university name is linked to an entity within the knowledge base.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9424930214881897, "xcomet_qe_score": 0.982330322265625, "metricx_score": 23.590272903442383, "metricx_qe_score": 24.40215492248535, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 346, "src_lang": "en", "tgt_lang": "zh", "output": "and the text of the entities of the knowledge base is extracted and add to the data set.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9267385005950928, "xcomet_qe_score": 0.943268895149231, "metricx_score": 11.343087196350098, "metricx_qe_score": 15.241178512573242, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 347, "src_lang": "en", "tgt_lang": "zh", "output": "In this example, the text is the Wikipedia page abstract.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9776065349578857, "xcomet_qe_score": 1.0, "metricx_score": 10.427809715270996, "metricx_qe_score": 15.902045249938965, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 348, "src_lang": "en", "tgt_lang": "zh", "output": "Now we need to generate or extract features from the retriever text.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9082953929901123, "xcomet_qe_score": 0.9315522313117981, "metricx_score": 15.228096008300781, "metricx_qe_score": 18.394811630249023, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 349, "src_lang": "en", "tgt_lang": "zh", "output": "So we need to we need in a feature extraction phase which includes text analysis.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2863813042640686, "xcomet_qe_score": 0.2342987060546875, "metricx_score": 10.521468162536621, "metricx_qe_score": 12.913956642150879, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 350, "src_lang": "en", "tgt_lang": "zh", "output": "And this is the main novelty of this paper, and I will deepen into it in the next slides.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9807440042495728, "xcomet_qe_score": 0.9864468574523926, "metricx_score": 20.026338577270508, "metricx_qe_score": 23.583993911743164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 351, "src_lang": "en", "tgt_lang": "zh", "output": "After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9874038696289062, "xcomet_qe_score": 0.9862300157546997, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 352, "src_lang": "en", "tgt_lang": "zh", "output": "First, generate features in the number of classes of the original dataset.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9417574405670166, "xcomet_qe_score": 0.9496772289276123, "metricx_score": 6.481925010681152, "metricx_qe_score": 6.347663402557373, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 353, "src_lang": "en", "tgt_lang": "zh", "output": "In this example, the original dataset has two classes", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9865169525146484, "xcomet_qe_score": 1.0, "metricx_score": 18.314586639404297, "metricx_qe_score": 23.076900482177734, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 354, "src_lang": "en", "tgt_lang": "zh", "output": "So first generate two new features.", "metrics": {"bleu_score": 0.0, "chrf_score": 1.851851851851852, "xcomet_score": 0.8121817111968994, "xcomet_qe_score": 0.8193319439888, "metricx_score": 8.694124221801758, "metricx_qe_score": 12.545434951782227, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 355, "src_lang": "en", "tgt_lang": "zh", "output": "But if the dataset has five classes, first generate five new features.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.499001996007984, "xcomet_score": 0.771119236946106, "xcomet_qe_score": 0.8261669278144836, "metricx_score": 21.820308685302734, "metricx_qe_score": 21.322607040405273, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 356, "src_lang": "en", "tgt_lang": "zh", "output": "Each feature represents the likelihood for each class.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9996727705001831, "xcomet_qe_score": 1.0, "metricx_score": 18.256467819213867, "metricx_qe_score": 23.814733505249023, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 357, "src_lang": "en", "tgt_lang": "zh", "output": "To analyze the text, we use the current state of the art of text analysis, which are transformator-based language models as BERT, GPT, XLEDs, etc.", "metrics": {"bleu_score": 0.907966382708614, "chrf_score": 5.484084753777045, "xcomet_score": 0.8419569730758667, "xcomet_qe_score": 0.8109389543533325, "metricx_score": 13.224916458129883, "metricx_qe_score": 16.530969619750977, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 358, "src_lang": "en", "tgt_lang": "zh", "output": "But it is not likely that we can train language model using the input datasets.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.988860011100769, "xcomet_qe_score": 0.9791598916053772, "metricx_score": 9.448162078857422, "metricx_qe_score": 16.250688552856445, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 359, "src_lang": "en", "tgt_lang": "zh", "output": "So a naive approach will be a target task fine tuning.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9098484516143799, "xcomet_qe_score": 0.9905097484588623, "metricx_score": 17.229761123657227, "metricx_qe_score": 22.037683486938477, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 360, "src_lang": "en", "tgt_lang": "zh", "output": "So in the future extraction phase, we can download the peretrained language model, fine-tune the language model over the target dataset.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7814209461212158, "xcomet_qe_score": 0.8025063276290894, "metricx_score": 10.403276443481445, "metricx_qe_score": 9.559436798095703, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 361, "src_lang": "en", "tgt_lang": "zh", "output": "In this example, to fine-tune the language model, to classify, to classify text into classes, abstract into classes, low or high.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7470858097076416, "xcomet_qe_score": 0.742637038230896, "metricx_score": 16.522090911865234, "metricx_qe_score": 16.640722274780273, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 362, "src_lang": "en", "tgt_lang": "zh", "output": "Recefire the language model output, which is the likelihood for each class, and use as new features.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.881547212600708, "xcomet_qe_score": 0.9079331159591675, "metricx_score": 19.355133056640625, "metricx_qe_score": 19.23780059814453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 363, "src_lang": "en", "tgt_lang": "zh", "output": "The problem with this approach is that a set of data may have few distinct entity stacks.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8734848499298096, "xcomet_qe_score": 0.8787447810173035, "metricx_score": 8.295949935913086, "metricx_qe_score": 8.733627319335938, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 364, "src_lang": "en", "tgt_lang": "zh", "output": "In our experiment, almost half of the datasets contain less than four hundred sample, and the smallest dataset contained thirty five sample in its training set.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9121931195259094, "xcomet_qe_score": 0.9486434459686279, "metricx_score": 21.260494232177734, "metricx_qe_score": 20.131977081298828, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 365, "src_lang": "en", "tgt_lang": "zh", "output": "Therefore, fine-tuning a language model over this dataset will be ineffective.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9857172966003418, "xcomet_qe_score": 0.9864475727081299, "metricx_score": 2.015204429626465, "metricx_qe_score": 3.0028076171875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 366, "src_lang": "en", "tgt_lang": "zh", "output": "But we can use prior knowledge about pre-analyzed datasets.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 20.978322982788086, "metricx_qe_score": 23.2044677734375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 367, "src_lang": "en", "tgt_lang": "zh", "output": "Because FAST is applied over multiple datasets, we can use the N-1 datasets to gather information about the N-1 datasets and use this information when we analyze the nth dataset.", "metrics": {"bleu_score": 0.0, "chrf_score": 2.139045194249437, "xcomet_score": 0.6475052833557129, "xcomet_qe_score": 0.6642277836799622, "metricx_score": 7.8061017990112305, "metricx_qe_score": 5.3036956787109375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 368, "src_lang": "en", "tgt_lang": "zh", "output": "What we suggest is to add another fine-tuning phase.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8572061061859131, "xcomet_qe_score": 0.8595061302185059, "metricx_score": 6.830289840698242, "metricx_qe_score": 9.828969955444336, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 369, "src_lang": "en", "tgt_lang": "zh", "output": "A preliminary multitask fine tuning phase.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9729776382446289, "xcomet_qe_score": 0.9815179109573364, "metricx_score": 10.353214263916016, "metricx_qe_score": 17.584819793701172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 370, "src_lang": "en", "tgt_lang": "zh", "output": "When we find you in the language model over n minus one data sets", "metrics": {"bleu_score": 2.1340743160056204, "chrf_score": 0.6720430107526881, "xcomet_score": 0.6128970384597778, "xcomet_qe_score": 0.7333304286003113, "metricx_score": 17.14956283569336, "metricx_qe_score": 17.30443572998047, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 371, "src_lang": "en", "tgt_lang": "zh", "output": "And then we execute another fine tuning phase, which is a target task fine tuning when we fine tuning the language model over the nth target dataset.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.2787068004459309, "xcomet_score": 0.8560763597488403, "xcomet_qe_score": 0.8269174695014954, "metricx_score": 16.715747833251953, "metricx_qe_score": 20.816997528076172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 372, "src_lang": "en", "tgt_lang": "zh", "output": "The state of the art in multitask, multitask fine tuning called MTDNN.", "metrics": {"bleu_score": 2.1340743160056204, "chrf_score": 8.929036854547165, "xcomet_score": 0.8941460847854614, "xcomet_qe_score": 0.9051835536956787, "metricx_score": 5.955831527709961, "metricx_qe_score": 9.587898254394531, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 373, "src_lang": "en", "tgt_lang": "zh", "output": "In MTDNN, MTDNN maintains heads in the number of tasks in the training set.", "metrics": {"bleu_score": 2.1277706909980765, "chrf_score": 14.613398194949792, "xcomet_score": 0.8435653448104858, "xcomet_qe_score": 0.8611192107200623, "metricx_score": 22.312894821166992, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 374, "src_lang": "en", "tgt_lang": "zh", "output": "So in this example there are four tasks in the training set. So empty DNN maintain four heads as you can see in the image.", "metrics": {"bleu_score": 0.0, "chrf_score": 1.8207622479172376, "xcomet_score": 0.8336399793624878, "xcomet_qe_score": 0.8544659614562988, "metricx_score": 14.689184188842773, "metricx_qe_score": 13.865865707397461, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 375, "src_lang": "en", "tgt_lang": "zh", "output": "and it samples a random batch from from the training set.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9095203876495361, "xcomet_qe_score": 0.9245066046714783, "metricx_score": 13.416980743408203, "metricx_qe_score": 13.135748863220215, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 376, "src_lang": "en", "tgt_lang": "zh", "output": "And if the random batch belongs to, for example, Sing and Selton's classification tasks, it executes forward and backward pass through the first head.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7470101118087769, "xcomet_qe_score": 0.7720592021942139, "metricx_score": 15.118847846984863, "metricx_qe_score": 14.45650863647461, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 377, "src_lang": "en", "tgt_lang": "zh", "output": "And if the random batch belongs to pairwise ranking task, it is added to forward and backward path through the last head.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.809871256351471, "xcomet_qe_score": 0.7798738479614258, "metricx_score": 21.874666213989258, "metricx_qe_score": 17.837848663330078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 378, "src_lang": "en", "tgt_lang": "zh", "output": "In our scenario, a table of datasets varies the number of classes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9305547475814819, "xcomet_qe_score": 0.8870574235916138, "metricx_score": 9.335827827453613, "metricx_qe_score": 10.817584991455078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 379, "src_lang": "en", "tgt_lang": "zh", "output": "So there are many tasks.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.957852840423584, "xcomet_qe_score": 1.0, "metricx_score": 6.0360260009765625, "metricx_qe_score": 18.6107177734375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 380, "src_lang": "en", "tgt_lang": "zh", "output": "MTDNN maintains number of classes heads output layers.", "metrics": {"bleu_score": 2.447822402834545, "chrf_score": 10.782428041542522, "xcomet_score": 0.9586200714111328, "xcomet_qe_score": 0.9579192399978638, "metricx_score": 15.158699035644531, "metricx_qe_score": 23.381969451904297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 381, "src_lang": "en", "tgt_lang": "zh", "output": "And additionally, MTDNN needs to initialize new heads for a new data set with a new task.", "metrics": {"bleu_score": 1.2606305140562377, "chrf_score": 6.43987349831572, "xcomet_score": 0.9736677408218384, "xcomet_qe_score": 0.9889193773269653, "metricx_score": 7.577099323272705, "metricx_qe_score": 8.789032936096191, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 382, "src_lang": "en", "tgt_lang": "zh", "output": "Our approach, called Task Reformulation Fine Tuning, is in our approach Task Reformulation Fine Tuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes tasks.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5264744758605957, "xcomet_qe_score": 0.5265905857086182, "metricx_score": 18.07027816772461, "metricx_qe_score": 23.149057388305664, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 383, "src_lang": "en", "tgt_lang": "zh", "output": "So let's see an example.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.964895486831665, "xcomet_qe_score": 1.0, "metricx_score": 5.776617527008057, "metricx_qe_score": 13.044326782226562, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 384, "src_lang": "en", "tgt_lang": "zh", "output": "Here is our input data set, which consists of entities, features, text and classes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.976855993270874, "xcomet_qe_score": 1.0, "metricx_score": 5.3306884765625, "metricx_qe_score": 11.067325592041016, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 385, "src_lang": "en", "tgt_lang": "zh", "output": "And we will formulate the task from classifying the text into low and high to classify the text, the abstract and the class into true or false.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8032291531562805, "xcomet_qe_score": 0.8246132731437683, "metricx_score": 16.211124420166016, "metricx_qe_score": 11.44909954071045, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 386, "src_lang": "en", "tgt_lang": "zh", "output": "In other words, we train the language model to classify abstract and class to abstract and class if the abstract belongs to the class or not.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5911668539047241, "xcomet_qe_score": 0.7469456791877747, "metricx_score": 12.451531410217285, "metricx_qe_score": 13.250701904296875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 387, "src_lang": "en", "tgt_lang": "zh", "output": "So the label vector in this case always stays which consists always with two classes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8715569972991943, "xcomet_qe_score": 0.89436936378479, "metricx_score": 15.370590209960938, "metricx_qe_score": 17.520401000976562, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 388, "src_lang": "en", "tgt_lang": "zh", "output": "And this is the algorithm for our formulated fine tuning approach.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8617284297943115, "xcomet_qe_score": 0.8653576374053955, "metricx_score": 7.090026378631592, "metricx_qe_score": 7.778250694274902, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 389, "src_lang": "en", "tgt_lang": "zh", "output": "So let's see the full framework.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9845939874649048, "xcomet_qe_score": 1.0, "metricx_score": 7.440143585205078, "metricx_qe_score": 15.2227783203125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 390, "src_lang": "en", "tgt_lang": "zh", "output": "and dataset fade into fast.", "metrics": {"bleu_score": 0.0, "chrf_score": 1.243781094527363, "xcomet_score": 0.8038917779922485, "xcomet_qe_score": 0.8560512065887451, "metricx_score": 11.06760311126709, "metricx_qe_score": 14.387494087219238, "linguapy_score": [1, "NYNORSK"]}}
{"dataset_id": "acl_6060", "sample_id": 391, "src_lang": "en", "tgt_lang": "zh", "output": "And then a fast execute entity linking phase.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.7861635220125787, "xcomet_score": 0.8268241286277771, "xcomet_qe_score": 0.784534215927124, "metricx_score": 14.633893013000488, "metricx_qe_score": 14.921003341674805, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 392, "src_lang": "en", "tgt_lang": "zh", "output": "It extract the text from the knowledge base, which in this example is the abstract of the Wikipedia page.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9544849395751953, "xcomet_qe_score": 0.9607034921646118, "metricx_score": 12.32930850982666, "metricx_qe_score": 17.442447662353516, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 393, "src_lang": "en", "tgt_lang": "zh", "output": "Then it reformulates the task into a sentence per classification task.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8383939266204834, "xcomet_qe_score": 0.8102976083755493, "metricx_score": 12.80601692199707, "metricx_qe_score": 13.570816993713379, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 394, "src_lang": "en", "tgt_lang": "zh", "output": "apply the language model to the new task and the output likelihood for each class.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9343870878219604, "xcomet_qe_score": 0.9562808275222778, "metricx_score": 15.536383628845215, "metricx_qe_score": 20.84455108642578, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 395, "src_lang": "en", "tgt_lang": "zh", "output": "Note that the language model is already fine-tuned over n-1 dataset using a preliminary multitask fine-tuning.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.7342143906020557, "xcomet_score": 0.8676614761352539, "xcomet_qe_score": 0.9077779054641724, "metricx_score": 9.068567276000977, "metricx_qe_score": 7.040529727935791, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 396, "src_lang": "en", "tgt_lang": "zh", "output": "Then we use the output vector of the language model as a newly generated feature in the number of classes.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8885036706924438, "xcomet_qe_score": 0.8957700729370117, "metricx_score": 24.765047073364258, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 397, "src_lang": "en", "tgt_lang": "zh", "output": "To evaluate our framework, we use a seventeen tabular classification dataset, which varies size, features, balance, domain and initial performance.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8422215580940247, "xcomet_qe_score": 0.8724451065063477, "metricx_score": 22.821624755859375, "metricx_qe_score": 23.420331954956055, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 398, "src_lang": "en", "tgt_lang": "zh", "output": "And as a knowledge base we use Wikipedia.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 8.063361167907715, "metricx_qe_score": 15.15066146850586, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 399, "src_lang": "en", "tgt_lang": "zh", "output": "We design our experiment as a live one out evaluation when we train fast over sixteen datasets and apply it to the seventeenth dataset.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.5707762557077625, "xcomet_score": 0.7250174283981323, "xcomet_qe_score": 0.7831423282623291, "metricx_score": 17.044321060180664, "metricx_qe_score": 13.396465301513672, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 400, "src_lang": "en", "tgt_lang": "zh", "output": "We also split each data set into four faults and apply four faults cross validation.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7466515302658081, "xcomet_qe_score": 0.7465330362319946, "metricx_score": 15.6591796875, "metricx_qe_score": 12.228315353393555, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 401, "src_lang": "en", "tgt_lang": "zh", "output": "Then we generate the new feature and evaluate them using five evaluation classifiers.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9741644859313965, "xcomet_qe_score": 0.979765772819519, "metricx_score": 23.70929527282715, "metricx_qe_score": 24.45237922668457, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 402, "src_lang": "en", "tgt_lang": "zh", "output": "We use in our experiment-based BERT-based architecture.", "metrics": {"bleu_score": 0.0, "chrf_score": 6.136853274645176, "xcomet_score": 0.8355133533477783, "xcomet_qe_score": 0.8667906522750854, "metricx_score": 9.41002082824707, "metricx_qe_score": 7.506394386291504, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 403, "src_lang": "en", "tgt_lang": "zh", "output": "Here are the results of our experiment.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9955570697784424, "xcomet_qe_score": 0.9935439825057983, "metricx_score": 2.4470932483673096, "metricx_qe_score": 10.651443481445312, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 404, "src_lang": "en", "tgt_lang": "zh", "output": "You can see that we compare our framework to target dataset fine tuning, target task fine tuning and MTDNN preliminary fine tuning.", "metrics": {"bleu_score": 0.7128419972514801, "chrf_score": 4.292872194053738, "xcomet_score": 0.9041717052459717, "xcomet_qe_score": 0.9468773007392883, "metricx_score": 15.714634895324707, "metricx_qe_score": 19.38660430908203, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 405, "src_lang": "en", "tgt_lang": "zh", "output": "And our reformulated fine tuning achieved the best result, the best performance.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9510488510131836, "xcomet_qe_score": 0.9445697665214539, "metricx_score": 14.361400604248047, "metricx_qe_score": 18.956676483154297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 406, "src_lang": "en", "tgt_lang": "zh", "output": "while MTDNN achieved 2% improvement over the target data set fine tuning.", "metrics": {"bleu_score": 5.448844552389355, "chrf_score": 11.044764932970336, "xcomet_score": 0.8715531826019287, "xcomet_qe_score": 0.9560351371765137, "metricx_score": 9.024614334106445, "metricx_qe_score": 15.697964668273926, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 407, "src_lang": "en", "tgt_lang": "zh", "output": "Our poach achieved six percent improvement.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8089160919189453, "xcomet_qe_score": 0.8074094653129578, "metricx_score": 17.48135757446289, "metricx_qe_score": 23.202058792114258, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 408, "src_lang": "en", "tgt_lang": "zh", "output": "When we look at the small data set, we can see that the performance of MTDNN decreases and the improvement of the preliminary multitask fine tuning phase decreases to 1.5 percent.", "metrics": {"bleu_score": 0.9284329018073406, "chrf_score": 5.0408286360980625, "xcomet_score": 0.9354060888290405, "xcomet_qe_score": 0.8895311951637268, "metricx_score": 5.714644908905029, "metricx_qe_score": 12.27670955657959, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 409, "src_lang": "en", "tgt_lang": "zh", "output": "But our performance increased to 11% compared to the target task fine tuning alone.", "metrics": {"bleu_score": 2.618064208849317, "chrf_score": 2.864137212559397, "xcomet_score": 0.9562841653823853, "xcomet_qe_score": 0.9602059125900269, "metricx_score": 17.89322853088379, "metricx_qe_score": 23.696481704711914, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 410, "src_lang": "en", "tgt_lang": "zh", "output": "For summing, FAST enables view shot enrichment from thirty five samples in our experiment.", "metrics": {"bleu_score": 0.0, "chrf_score": 1.929198238110873, "xcomet_score": 0.6052093505859375, "xcomet_qe_score": 0.7349330186843872, "metricx_score": 21.58875846862793, "metricx_qe_score": 19.000164031982422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 411, "src_lang": "en", "tgt_lang": "zh", "output": "It uses one architecture for all tasks and datasets.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.999873161315918, "xcomet_qe_score": 1.0, "metricx_score": 21.2529296875, "metricx_qe_score": 23.59760856628418, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 412, "src_lang": "en", "tgt_lang": "zh", "output": "And he keeps the head of the model.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8132379055023193, "xcomet_qe_score": 0.787628173828125, "metricx_score": 14.568405151367188, "metricx_qe_score": 17.949615478515625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 413, "src_lang": "en", "tgt_lang": "zh", "output": "But it adds three formulation phases.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8608323335647583, "xcomet_qe_score": 0.8778830170631409, "metricx_score": 14.484085083007812, "metricx_qe_score": 10.11747932434082, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 414, "src_lang": "en", "tgt_lang": "zh", "output": "It is augmented train set and it needs a target value with semantic meaning so we can fed it into the language model and use it in the sentence per classification problem.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7424728870391846, "xcomet_qe_score": 0.7395585775375366, "metricx_score": 15.295550346374512, "metricx_qe_score": 17.497867584228516, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "acl_6060", "sample_id": 415, "src_lang": "en", "tgt_lang": "zh", "output": "Thank you.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4112658202648163, "metricx_qe_score": 0.48872482776641846, "linguapy_score": [1, "ENGLISH"]}}
