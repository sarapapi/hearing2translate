{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Assa Farari und ich werde unsere Arbeit FUSHOT Tabellendatenanbau mit fein abgestimmten Transformers-Architekturen vorstellen. Datenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der bestehenden Datenmerkmale, aber manchmal sind diese Merkmale begrenzt. Die Generierung von Merkmalen mit einer anderen Datenquelle kann erhebliche Informationen hinzufügen. Unser Forschungsziel ist die automatische Tabellendatenanbau mit externen Quellen, freiem Text. Nehmen wir an, wir haben eine Tabellendatenmenge und eine Wissensbasis. Wir benötigen einen automatischen Prozess, der die Verknüpfung von Intensität und die Textanalyse beinhaltet, um neue Merkmale aus dem kostenlosen Text der Wissensbasis zu extrahieren. Unser Framework FEST ist genau dieser automatische Prozess. Lassen Sie uns ein Beispiel betrachten. In einem Datensatz, der in FEST eingeführt wird. In diesem Beispiel ist der Datensatz ein Universitätsdatensatz, wenn sein Ziel es ist, Universitäten in niedrig rankende Universitäten und hochrangige Universitäten. Als Wissensbasis verwenden wir Wikipedia. Die erste Phase von FEST ist die Entitätslinkung, bei der jede Entität, in diesem Beispiel der Universitätsname, mit einer Entität innerhalb der Wissensbasis verknüpft wird und der Text der Entitäten der Wissensbasis extrahiert und zum Datensatz hinzugefügt wird. In diesem Beispiel ist der Text der Wikipedia-Seitenabschnitt. Jetzt müssen wir Features aus dem erhaltenen Text generieren oder extrahieren. Wir benötigen also eine Feature-Extraction-Phase, die Textanalyse beinhaltet. Und das ist die Hauptnota dieser Arbeit, und ich werde mich in den nächsten Folien damit befassen. Nach der Feature-Extraction-Phase gibt es eine Feature-Generation-Phase, in der wir die extrahierten Features verwenden, um eine kleine Anzahl von Features zu generieren. Sie eine kleine Anzahl neuer Merkmale. Erstellen Sie zuerst Merkmale in der Anzahl der Klassen des ursprünglichen Datensatzes. In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen, also erstellen Sie zuerst zwei neue Merkmale. Wenn der Datensatz jedoch fünf Klassen hat, erstellen Sie zuerst fünf neue Merkmale. Jede Merkmale repräsentiert die Wahrscheinlichkeit für jede Klasse. Um den Text zu analysieren, verwenden wir den aktuellen Stand der Technik der Textanalyse, nämlich transformierbare Sprachmodelle wie BERT, GPT, XLERT und so weiter. Es ist jedoch unwahrscheinlich, dass wir ein Sprachmodell mit den Eingabedaten ausbilden können. Daher wäre eine naive Herangehensweise die Feinabstimmung der Zielaufgabe. In der Fase der Extraktion der Funktionen können wir ein pertrainiertes Sprachmodell herunterladen und das Sprachmodell über das Zieldatenset feinabstimmen. In diesem Beispiel können wir das Sprachmodell fe soll Text in Klassen, Abstract in Klassen, Low oder High, in Klassen eingeteilt, die Sprachmodell-Ausgaben erhalten, die für jede Klasse die Wahrscheinlichkeit sind, und als neue Funktionen verwendet werden. Das Problem bei diesem Ansatz ist, dass Daten sätze möglicherweise wenige unterschiedliche Entitäten wie Text enthalten. In unserem Experiment enthalten fast die Hälfte der Daten sätze weniger als 400 Samples und die kleinsten Daten sätze enthalten. Es enthält 35 Proben in seinem Trainings-Set. Daher wäre es ineffektiv, ein Sprachmodell über diesen Datensatz zu verfeinern. Wir können jedoch vorherige Kenntnisse über voranalysierte Datensätze nutzen, da wir FAST über mehrere Datensätze anwenden. Wir können die N-1-Datensätze verwenden, um Informationen über die N-1-Datensätze zu sammeln und diese Informationen zu verwenden, wenn wir die nth Dataset. Was wir vorschlagen, ist eine weitere Fein-Tuning-Phase, eine preliminäre Multitask-Fein-Tuning-Phase, wenn wir das Languagemodell über n-1 Datasets fein-tunen. Und dann führen wir eine weitere Fein-Tuning-Phase aus, die eine Target-Task-Fein-Tuning ist, wenn wir das Languagemodell über das nth-Target-Dataset fein-tunen. Der Stand der Welt in der Multitask-Fein-Tuning-Phase. Fine-Tuning-Anwendung namens MTDNN. In MTDNN, MTDNN, mainten Sie eine Hälfte der Taschen im Trainingssatz. In diesem Beispiel gibt es vier Taschen im Trainingssatz. MTDNN mainten Sie vier Hälfte, wie Sie sehen können, im Bild. Es sampelt eine Random-Batch aus dem Trainingssatz. Und wenn die Random-Batch zu einem Zum Beispiel Singenselten's Classification-Tasks, es führt Vor- und Rückwärtspfade durch den ersten Kopf aus. Und wenn der Random-Batch zu Paar-Wise-Ranking-Task gehört, führt es Vor- und Rückwärtspfade durch den letzten Kopf aus. In unserem Szenario verringen Tableau-Datensätze die Anzahl der Klassen. Es gibt also viele Aufgaben. MTDNN-Mainten. DNN hält eine Reihe von Klassen-Heads-Ausgaben-Lagen aufrecht und zusätzlich muss MTDNN neue Heads für einen neuen Datensatz mit einer neuen Aufgabe initialisieren. Unser Ansatz, der als Task-Reformulation-Fine-Tuning bezeichnet wird, ist, dass wir in unserem Ansatz Task-Reformulation-Fine-Tuning, anstatt mehrere Heads aufrechtzuerhalten, jedes Datensatz in ein Satz pro Klassifizierungsproblem, das zwei Klassen-Taschen. Also, sehen wir ein Beispiel. Hier ist unser Input-Datensatz, der entweder von Entitäten, Funktionen, Text und Klassen besteht. Und wir reformulieren die Tasche von der Klassifizierung der Text in Low und High, um die Text, den Abstrakt und die Klasse in True oder False zu klassifizieren. Oder in anderen Worten, wir trainieren das Languagemodell, um eine Abstrakt- und Klassenklasse zu klassifizieren, ob die Abstrakt- und Klassenklasse zur Klasse gehören oder nicht. Der Etikettvektor bleibt in diesem Fall immer mit zwei Klassen bestehen. Und das ist der Algorithmus für unsere Find-Orientierung. find reformulated fine-tuning approach. Also, sehen wir uns den vollständigen Framework an. Ein Datensatz wird in FAST eingeführt und dann FAST execute in die Linkingphase. Es extrahiert den Text aus der Knowledge-Basis, die in diesem Beispiel das Abstract der Wikipedia-Seite ist. Dann wird die Task reformuliert in Satz-Per-Classification-Tasks, das Sprachmodell auf die neue Task und die Ausgaben-Likelihood für jede Klasse. Beachten Sie, dass das Language-Modell bereits über n-1-Datensätze mit einer vorläufigen Multitask-Fine-Tuning abgestimmt ist. Dann verwenden wir den Ausgabenvektor des Language-Modells als neu generierten Funktion in der Anzahl der Klassen. Um unser Framework zu evaluieren, verwenden wir eine 17. Wir haben eine siebzehn-Tabellklassifizierungsdatensätze, die die Größe, die Merkmale, den Bereich und die anfängliche Leistung in Einklang bringt. Und als Wissensbasis verwenden wir Wikipedia. Wir haben unser Experiment als eine Live-Out-Evaluierung entworfen, bei der wir schnell über sechzehn Datensätze trainieren und sie auf die siebzehnte Datensätze anwenden. Wir haben auch jede Datensätze in vier Daten aufgeteilt. Wir erstellen Fehler und wenden eine Fork-Fehler-Kreuzvalidierung an. Dann generieren wir die neue Funktion und bewerten sie mit fünf Bewertungs-Klassifikatoren. In unserem Experiment verwenden wir eine auf Bild-Basis basierte Architektur. Hier sind die Ergebnisse unseres Experiments. Sie sehen, dass wir unser Framework mit der Feinabstimmung des Zieldatensatzes vergleichen, der Feinabstimmung der Zielaufgabe. und MTDNN preliminäre Feintuning und unsere reformulierte Feintuning erreichten den besten Ergebnis, die besten Leistungen, während MTDNN 2% Verbesserung über die Target-Datenset-Fine-Tuning erreichte. Unser Produkt erreichte 6% Verbesserung. Wenn wir uns die kleinen Daten ansehen, Aus dem Datensatz können wir sehen, dass die Leistung von MTDNN abnimmt und die Verbesserung der vorläufigen Phase der Fine-Tuning für mehrere Aufgaben auf 1,5 Prozent abnimmt, aber unsere Leistung steigt auf 11 Prozent im Vergleich zur alleinigen Fine-Tuning für die Zielaufgabe. Für die Zusammenfassung ermöglicht FAST die Bereicherung von Flux-Schüssen aus 35 Proben in unserem Experiment. verwendet eine Architektur für alle Aufgaben-Datensätze und behält den Kopf des Modells bei. Aber es fügt eine Formulierungsphase hinzu, es erweitert den Train-Satz und benötigt einen Zielwert mit semantischer Bedeutung, damit wir ihn in das Sprachmodell einfügen und in der Satz-Par-Klassifizierungsproblematik verwenden können. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen. Heute werde ich unser Forschungswerk Lernen, deduktiv zu denken, Methodenproblemlösung als komplexe Rationsauffnahme vorstellen. Ich bin Alan vom Biden's AI Lab und dies ist eine gemeinsame Arbeit mit Thierry von der University of Texas in Austin und Wayloo von SUDD. Zunächst möchte ich über unsere Motivation für das Denken sprechen. Hier zeigen wir Beispiele, in denen mehrträgliches Denken hilfreich ist. Diese Zahl stammt aus der POWN-Artikel, in der sie die Anregung zur Lösung des Methodenproblems in einem Future-Learning-Szenario durchführen. Auf der Netto-Pan-Seite können wir sehen, dass wir, wenn wir einige Beispiele mit nur Fragen und Antworten geben, möglicherweise nicht in der Lage sind, die richtigen Antworten zu erhalten. Wenn wir jedoch eine weitere Beschreibung geben, kann das Modell die Beschreibung vorhersagen und auch eine korrekte Vorhersage treffen. Es ist also gut, als Ausgabe interpretierbare Mehrstufige Beschreibung zu haben. Wir denken auch, dass das Methodenproblem eine einfache Anwendung ist, um solche Beschreibungsfähigkeiten zu bewerten. Hier in unserer Problemkonfiguration müssen wir diese Frage aufgrund der Fragen lösen und die numerischen Antworten erhalten. In unseren Datensätzen erhalten wir also auch den mathematischen Ausdruck, der auch zu dieser bestimmten Antwort führt. Daher gelten bestimmte Annahmen wie in früheren Arbeiten. Wir gehen davon aus, dass die Präzision von Größen bekannt ist, und wir betrachten nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponential. Darüber hinaus können komplizierte Operatoren tatsächlich ent in diese grundlegenden Operatoren aufgeteilt werden. Frühere Arbeiten im Lösungsproblem der Methode können also tatsächlich in Sequenz-zu-Sequenz- und Sequenz-zu-Baummodell kategorisiert werden. Traditionelle Sequenz-zu-Sequenz-Modelle konvertieren den Ausdruck in eine spezifische Sequenz für die Generierung, und es ist ziemlich einfach zu implementieren, und es kann auf viele verschiedene komplizierte Probleme verallgemeinert werden. Aber die Nachteile sind, dass die Leistung im Allgemeinen nicht besser ist als das Strukturmodell, und es fehlt an Interpretabilität für die die Vorhersage. Aber eigentlich ist diese Richtung aufgrund des Transformator-Modells immer noch ziemlich beliebt. In Baumbasierten Modellen strukturieren wir diese Ausdrücke tatsächlich in Baumform und folgen einer Vorordnungstraversal in Baumgenerationen. Hier generieren wir also die Operatoren, bis wir die Blätter erreichen, die die Größen sind. Das Gute ist, dass es uns tatsächlich diese binäre Baumstruktur gibt. Aber eigentlich ist es ziemlich kontraintuitiv. Denn wir generieren zuerst den Operator und dann am Ende die Größen. Und das Zweite ist, dass er auch einige wiederholte Berechnungen enthält. Wenn wir uns also diesen Ausdruck a mal drei plus drei ansehen, wird er tatsächlich zweimal generiert. Aber tatsächlich sollten wir die Ergebnisse wiederverwenden. In unserem vorgeschlagenen Ansatz möchten wir diese Probleme also Schritt für Schritt und interpretierbar lösen. Zum Beispiel können wir hier im zweiten Schritt die Daten erhalten., die 27 sind. Und wir können auch auf die ursprünglichen Fragen zurückgreifen, um den relevanten Inhalt zu finden. Und in diesen Schritten erhalten wir die Teiler. Und dann erhalten wir in diesem dritten Schritt tatsächlich den Quotienten. Und nach diesen drei Schritten können wir tatsächlich die Ergebnisse des zweiten Schritts wiederverwenden und dann die Ergebnisse des vierten Schritts erhalten. Und dann können wir schließlich die Dividenden erhalten. Hier generieren wir also tatsächlich den gesamten Ausdruck direkt, anstatt einzelne Operatoren oder Größen zu generieren. Dies macht den Prozess genauer. In unserem deduktiven System beginnen wir also zunächst mit einer Reihe von Größen, die in den Fragen dargestellt werden, und fügen auch einige Konstanten als Anfangszustände ein. Der Ausdruck wird also durch EIJOP dargestellt, bei dem wir Operatoren von Qi bis Qj ausführen, und dieser Ausdruck ist tatsächlich gelenkt. Wir haben hier auch eine Subtraktion umgekehrt, um die entgegengesetzte Richtung darzustellen. Das ist ziemlich ähnlich wie eine Relationsentfernung. In einem formellen deduktiven System wenden wir den Operator zwischen dem Qi- und Qj-Paar an, und dann erhalten wir diesen neuen Ausdruck. Wir fügen ihn den nächsten Zuständen hinzu, um eine neue Größe zu erhalten. Diese Folien visualisieren tatsächlich die Entwicklung der Zustände, bei denen wir den aktuellen Zustand weiterhin Ausdrücke hinzufügen. In unseren Modellimplementierungen verwenden wir zunächst ein vorgebildetes Netzwerkmodell, das Vögel oder Roberto sein kann, und dann codieren wir den Satz und erhalten diese Mengenrepräsentationen. Sobald wir die Mengenrepräsentationen erhalten, können wir mit der Schlussfolgerung beginnen. Hier zeigen wir ein Beispiel für Q1, um die Repräsentation für Q1 zu erhalten, die durch Q2 geteilt und dann durch Q3 multipliziert wird. Zuerst erhalten wir die Paarrepräsentation, die im Grunde nur die Konkatenation zwischen Q1 und Q2 ist, und dann wenden wir ein Fit-Forward-Netzwerk an, das ist. Dies wird durch den Operator parametriert. Und schließlich erhalten wir die Ausdrucksdarstellung Q1 geteilt durch Q2. Aber in der Praxis könnten wir im Inferenzstadium auch den falschen Ausdruck erhalten. Hier sind alle möglichen Ausdrücke gleich dreimal so viele Operatoren. Das Schöne daran ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu steuern. Wenn dieser Ausdruck beispielsweise nicht erlaubt ist, können wir diesen Ausdruck einfach in unserem Suchraum entfernen. Im zweiten Schritt machen wir also das Gleiche, aber der einzige Unterschied ist eine weitere Größe. Diese Größe stammt also aus dem vorher berechneten Ausdruck. Schließlich können wir diesen endgültigen Ausdruck Q drei mal Q vier erhalten. Und wir können auch sehen, dass die Anzahl aller möglichen Ausdrücke vom vorherigen Schritt unterschiedlich ist. Solche Unterschiede machen es schwierig, ihn anzuwenden. Es ist schwer, Beam-Suche anzuwenden, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unbalanciert ist. Der Trainingsverfahren ist ähnlich wie bei der Schulung eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust bei jedem Zeitschritt optimieren, und hier verwenden wir auch diese Zeichen, um darzustellen, wann wir diesen Generierungsprozess beenden sollten. Und hier ist der Raum von Sequenz zu Sequenz anders, da der Raum bei jedem Zeitschritt unterschiedlich ist, während es im traditionellen Sequenz-zu-Sequenz-Modell die Anzahl des Wortschatzes ist. Es ermöglicht uns auch, bestimmte Einschränkungen aus vorherigen Kenntnissen aufzustellen. Wir führen also Experimente mit den häufig verwendeten Methodenproblemdatensätzen durch, MAWPS, Math twenty three K, MathQA und SWAM. Hier zeigen wir kurz die Ergebnisse im Vergleich zu den vorherigen besten Ansätzen. Unsere am besten funktionierende Variante ist Roberta Deductive Reasoner. Tatsächlich verwenden wir nicht BeamSearch, im Gegensatz zu den vorherigen Ansätzen mitze sind oft ein Baumbasiertes Modell. Insgesamt kann unser Argumentationsmodell dieses Baumbasierten Modells deutlich übertreffen, aber wir können sehen, dass die absoluten Zahlen bei MathQA oder SWAM nicht wirklich hoch sind. Wir untersuchen die Ergebnisse bei SWAM weiter, und dieser Datensatz ist eine Herausforderung, da der Autor versucht, manuell etwas hinzuzufügen, um das NLP-Modell zu verwirren, wie zum Beispiel das Hininzufügung von irrelevanten Informationen und zusätzlichen Mengen. In unserer Vorhersage stellen wir fest, dass einige der Zwischenwerte tatsächlich negativ sind. In diesen Fragen fragen wir zum Beispiel, wie viele Äpfel Jake hat, aber wir haben einige zusätzliche Informationen, wie siebzehn weniger Töne und Stephen hat acht Töne, was völlig irrelevant ist. Unser Modell macht also eine solche Vorhersage, die negative Werte erzeugt. Und wir beobachten, dass diese beiden Ausdrücke tatsächlich ähnliche Zahlen haben. Wir haben tatsächlich ähnliche Punktzahlen. Wir können also diesen Suchraum einschränken, indem wir diese negativen Ergebnisse entfernen, damit wir die Antwort richtig machen können. Wir stellen also fest, dass eine solche Einschränkung für einige Modelle tatsächlich ziemlich viel verbessert. Zum Beispiel verbessern wir für Vögel sieben Punkte und für das Roberta-basierte Modell zwei Punkte. Ein besseres Sprachmodell hat also eine bessere Sprachverständnisfähigkeit, sodass die Zahl hier. Hier ist es höher für Roberta und niedriger für Vögel. Und wir versuchen auch, die Schwierigkeit hinter all diesen Daten zu analysieren. Wir gehen davon aus, dass die Anzahl der ungenutzten Mengen hier als irrelevante Informationen angesehen werden kann. Hier können wir also sehen, dass wir den Prozentsatz der Proben mit ungenutzten Mengen haben und der SWAMP-Datenmenge den größten Teil hat. Und hier zeigen wir auch die Gesamtleistung für diese Proben ohne ungenutzte Mengen. Die Gesamtleistung ist also tatsächlich höher als die. Und die Leistung ist tatsächlich höher als die Gesamtleistung. Aber bei diesen Proben, bei denen die ungenutzte Menge tatsächlich viel schlechter ist als die Gesamtleistung. Für MAWPS haben wir nicht wirklich viele Datenfälle, also ignoriere ich diesen Teil einfach. Schließlich möchten wir die Interpretbarkeit durch ein Beispiel für das Zusammenbruch zeigen. Hier macht unser Modell also tatsächlich eine falsche Vorhersage im ersten Schritt. Wir können diesen Ausdruck tatsächlich mit dem Satz hier korrelieren. Wir denken, dass dieser Satz das Modell mit einer falschen Vorhersage irreführt. Wenn wir hier also weitere 35 Pflanzen pflanzen, glaubt das Modell, dass es sich um eine Addition von Operatoren handeln sollte. Wir versuchen, den Satz so zu ändern, dass die Anzahl der Birnen 35 weniger als die Apfelbäume ist. Wir machen es also, um genauere Semantik zu vermitteln, sodass das Modell die Vorhersage richtig machen kann. Diese Studie zeigt, wie die interpretierbaren Vorhersagen uns helfen, das Verhalten des Modells zu verstehen. Um unsere Arbeit abzuschließen: Zunächst ist unser Modell ziemlich effizient und wir können interpretierbare Lösungsmethoden bereitstellen, und wir können ein wenig vorheriges Wissen als Einschränkung einbeziehen, was die Leistung verbessern kann. Das letzte ist, dass der zugrunde liegende Mechanismus nicht nur auf Netzwerkproblemlösungsaufgaben anwendet wird, sondern auch auf andere Aufgaben, die mehrträgliches Denken beinhalten. Wir haben aber auch bestimmte Einschränkungen. Wenn wir eine große Anzahl von Operatoren oder Konstanten haben, könnte der Speicherverbrauch ziemlich hoch sein. Und das Zweite ist, wie erwähnt, dass die Wahrscheinlichkeitsverteilung bei verschiedenen Zeitstufen unbalanciert ist, daher ist es auch ziemlich schwierig, eine Beam-Suche-Strategie anzuwenden. Das ist also das Ende des Vortrags, und Fragen sind willkommen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Antoine und ich bin von der Maastricht University. Ich werde meine gemeinsame Arbeit mit Jerry vorstellen, die sich mit einer neuen Datensammlung für die Erfassung von gesetzlichen Artikeln befasst. Rechtsfragen sind ein integraler Bestandteil des Lebens vieler Menschen, aber die Mehrheit der Bürger hat wenig bis kein Knowledge über ihre Rechte und grundlegenden Rechtsprozesse. Daher werden viele gefährdeten Bürger, die sich die kostspielige Unterstützung eines Rechtsexperten nicht leisten können, ungeschützt oder schlimmer gesagt, ausgenutzt. Unsere Arbeit zielt darauf ab, die Grenze zwischen Menschen und dem Gesetz zu schließen, indem er ein effektives Retrievalsystem für Statutartikel entwickelt. Ein solches System könnte eine freie professionelle Rechtshilfegemeinschaft für unqualifizierte Menschen bieten. Bevor wir uns auf den Hauptbegriff dieser Arbeit konzentrieren, sollten wir uns zunächst mit dem Problem der Statutartikelretrieval befassen. Bei einer einfachen Frage auf einem Rechtsmaterial, wie zum Beispiel was ich riskieren kann, wenn ich professionelle Privatsphäre verletze, ist ein Modell erforderlich, um alle relevanten Statutartikel aus einem großen Bereich der der Gesetzgebung. Diese Information Retrieval Task kommt mit seiner eigenen Setz von Challenges. Erstens geht es mit zwei Arten von Linguage, der normalen natürlichen Sprache für die Fragen und der komplexen legalen Sprache für die Statuten zu tun. Diese Differenz in Languageverteilungen macht es für ein System schwieriger, relevante Kandidaten zu retrieveren, da es indirekt eine inhärente Interpretationssysteme erfordert, die eine natürliche Frage zu einer rechtlichen Frage, die die Terminologie der Statuten entspricht, translatiert. Besides, statutory Law ist nicht ein Stack von Independent Artikeln, die als eine vollständige Source von Informationen auf der Own behandelt werden können, wie neue Rezepte, zum Beispiel. Instead, es ist eine Struktur, eine Sammlung von legalen Provisionen, die eine ganze Bedeutung nur in der allgemeinen Kontext haben, das ist zusammen mit der supplementären Information aus den neighbouring Artikeln, den Feldern und Subfields, die sie belong zu, und ihrer Platz in der Struktur der Laub. Lastly, statutory Artikel sind ein kleiner Paragraph. was in den meisten Retrievalwerken die typische Retrieval-Einheit ist. Hier sind es lange Dokumente, die bis zu sechstausend Wörter schreiben können. Die recenten Entwicklungen in NLP haben enorme Interesse in vielen Rechtsakten wie Legal Judgment Prediction oder automatisierten Kontaktkontraktreview gespielt, aber die Statutory Article Retrieval hat sich aufgrund der Lack von groß und hochwertigen Labeldatensätzen in der Lage geblieben. In dieser Arbeit präsentieren wir eine neue French Native Citizen Centric Datenset zu studieren, ob ein Retrievalmodell die Effizienz und Reliabilität eines Legal Experts für die Task der Statutory Article Retrieval angeht. Unser Belgisch Statutory Article Retrieval Datsatsatz besteht aus mehr als 1.100 legalen Fragen, die von Belgischen Bürgern gestellt werden. Diese Fragen umfassen eine wide Reihe von Themen von Familie, Housing, Money, bis hin zu Work und Sozialversicherung. Each of them ist von erfahrenen Juristen mit Referenzen zu relevanten Artikel aus einem Corpus von mehr als 22.600 legal Artikeln aus Belgischen Codes of Law. Lassen Sie uns nun über die Art und Weise sprechen, wie wir diese Daten setzen. Erst, wir starten mit der Compilierung eines Large Corpus von legal Artikeln. Wir betrachteten 32 öffentlich verfügbare Belgischen Codes und extrahierten alle Artikel sowie die correspondenden Section Headings. Dann gingen wir mit Referenzen zu relevanten Statuten zusammen. To tun so, wir partnerten mit der Belgischen Law Firma. jedes Jahr um viertausend Emails von Belgischen Bürgern, die um Rat und persönliche Rechtsfragen bitten. Wir hatten die Glück, Zugang zu ihren Websites zu bekommen, wo ihr Team erfahrener Juristen belgische am häufigsten rechtlichen Fragen behandelt. Wir sammelten Tausende von Fragen, die mit Kategorien, Subkategorien und Rechtsreferenzen zu relevanten Statuten angepasst wurden. Lastlich durchgingen wir die Rechtsreferenzen und filterten die Fragen, die nicht Artikel in einem der Codes of Law we considered. Die remaining Referenzen wurden matched und zu den corresponding Artikel IDs von O Corpus. Wir eventuell endeten mit einem Jahrhundert und acht Fragen, die sich sorgfältig mit den IDs der relevanten Artikel aus unserem Large Corpus von twenty two thousand six hundred thirty three Statutory Articles labelten. In addition, jeder Frage kommt mit einer Mainkategorie und einer Konkatenation von Subkategorien. Jeder Artikel enthält eine Konkatenation der nachfolgenden Überschriften in der Struktur des Gesetzes. Diese zusätzlichen Informationen werden in der aktuellen Arbeit nicht verwendet, aber könnten für zukünftige Forschungen zur Rechtsinformationserfassung oder zur Klassifizierung von Rechtstexten interessant sein. Schauen wir uns einige Merkmale unserer Datensätze an. Die Frage ist zwischen fünf und vierundvierzig Wörter lang mit einem Median von vierzig Wörtern. Die Artikel sind viel länger mit einem Median von siebzig sieben Wörtern, wobei einhundertvierundvierzig davon mehr als tausend Wörter betragen. Die Länge eines von fünftausend siebenhundert und neunzig Wörtern. Wie bereits erwähnt, umfassen die Frage eine breite Palette von Themen, wobei etwa achtzig Prozent davon entweder Familie, Wohnung, Geld oder Justiz betreffen, während die restlichen fünfzehn Prozent entweder Sozialversicherung, Ausländer oder Arbeit betreffen. Die Artikel sind auch sehr vielfältig, da sie aus dreißig zwei verschiedenen belgischen Gesetzen stammen, die eine große Anzahl von illegalen Themen abdecken. Hier ist die Gesamtzahl der Artikel, die aus jedem dieser belgischen Gesetze gesammelt wurden. Von den 22.633 Artikeln werden nur 1.612 als relevant für mindestens eine Frage in den Datensätzen angegeben, und etwa 80 % dieser zitierten Artikel stammen entweder aus dem Zivilgesetz, dem Justizgesetz, dem Strafverfolgungsgesetz oder dem Strafgesetz. Inzwischen werden 18 von 32 Gesetzen weniger als fünf Artikel als relevant für mindestens eine Frage angege, die durch die Tatsache verursacht wird, dass diese Code weniger auf individuelle und ihre Anliegen fokussiert. Overall, die medianen Zitation für diese citierten Artikel ist zwei, und weniger als fünfundzwanzig Prozent von ihnen sind mehr als fünfmal citiert. Mit unseren Data sets benchmarken wir verschiedene Retrievalapproaches, einschließlich Lexical und Densarchitektur. Given eine query in einem Artikel, eine Lexicalmodelle assigniert eine Score zu den queryartikelpaaren. Durch die Berechnung der Summe der Gewichte jedes dieser Terme in diesem Artikel über die Abfrage-Terme. Wir experimentieren mit den Standard-TFIDF- und BM25-Rankingsfunktionen. Das Hauptproblem bei diesen Ansätzen ist, dass sie nur Artikel abrufen können, die im Abfrage vorhandenen Schlüsselwörter enthalten. Um diese Einschränkung zu überwinden, experimentieren wir mit einer neuronalen Architektur, die die semantische Beziehung zwischen Abfragen und Artikeln erfassen kann. Wir verwenden ein B-Coder-Modell, das Abfragen und Artikel in dichte Vektorrepräsentationen kartiert. und berechnen einen relevanten Punkt zwischen einem Artikelpaar der Abfrage anhand der Ähnlichkeit ihrer Eingebettungen. Diese Eingebettungen sind typischerweise das Ergebnis einer Pooling-Operation auf dem Ausgang eines Word-Eingebettungsmodells. Zunächst untersuchen wir die Effektivität siamesischer B-Encoder in einer Zero-Shot-Evaluierungs-Einrichtung, was bedeutet, dass vorgebildete Word-Eingebettungsmodelle ohne zusätzliche Feinabstimmung aus der Box angewendet werden. Wir experimentieren mit kontextunabhängigen Text-Encodern, nämlich Word2Vec und Fastastex und kontextabhängige Embeddingmodelle, nämlich Roberta und speziell Kamembert, ein französisches Roberta-Modell. Darüber hinaus trainieren wir unsere eigenen Kamembert basierenden Biencoders auf allen Datenmengen. Beachten Sie, dass wir für das Training mit den beiden Arten der Biencoder Architektur experimentieren. Siamese, das ein einzigartiges Wort-Embeddingmodell verwendet, das die Abfrage und den Artikel in einem gemeinsamen dichten Vektorraum mappiert. Und Tutor, das zwei unabhängige Wort-Embeddingmodelle verwendet. Wir haben zwei Independent Word Embedding Models, die den Query und Artikel separat in verschiedene Embeddingsphasen codieren. Wir experimentieren mit Mean, Max und CLS Pooling sowie Dot Product und Cosine für die Computing-Similaritäten. Hier sind die Ergebnisse unserer Baseline auf dem Testset, mit den Lexikonmethoden oben, den Siamese BE Encoders evaluiert in einem Zero Shot Setup in der Mitte, und den Fein Tun BE Encoders unten. Overall, die Fein Tun BE Encoders übertragen alle anderen Baselines. seine Siamese-Variante auf RECOLAT 100, aber ähnlich auf den anderen Metriken abschneidet. Obwohl BM 25 den trainierten Biancoder erheblich unterlegen hat, deutet seine Leistung darauf hin, dass es immer noch eine starke Grundlage für Domain-spezifische Rückholung ist. Bei der Zero Shot Evaluation des Siamese Biancoders finden wir, dass die direkte Verwendung der Embeddings eines vortrainierten Kamembert-Modells ohne Optimierung für die Informationsaufnahmeaufgabe schlechte Ergebnisse liefert, was mit früheren Erkenntnissen übereinstimmt. Der Word-to-Vec-basierte Biancoder übertraf die Fastex- und Vogelbasierten Modelle deutlich, was darauf hindeutet, dass möglicherweise vor-train-Word-Einbettungen für die Aufgabe angemessener sind als Charakter- oder Unterword-Einbettungen, wenn sie von Anfang an verwendet werden. Obwohl sie vielversprechend sind, deuten diese Ergebnisse auf reichliche Verbesserungsmöglichkeiten im Vergleich zu einem geschickten Experten hin, der schließlich alle relevanten Artikel zu jeder Frage abrufen und somit perfekte Punktzahlen erzielen kann. Lassen Sie uns mit zwei Limitationen von allen Datensätzen beginnen. Erstens ist der Corpus von Artikeln limitiert zu denen, die aus den dreißig zwei betrachteten Belgischen Codes erfasst werden, was nicht den gesamten Belgischen Law abdeckt, da Artikel aus Dekrees, Direktives und Ordinanzen enthalten sind. Durch die Datensatzkonstruktion werden alle Referenzen zu diesen unkollektierten Artikeln ignoriert, was dazu führt, dass einige Fragen nur einen Fragment der initialen Anzahl relevanter Artikeln enthalten. Dieser Informationlaus impliquiert, dass die Antwort in den remaining relevanten Artikelnkel könnten incomplete sein, obwohl es immer noch vollständig appropriate ist. Second, wir sollten merken, dass nicht alle Rechtsfragen mit Statuten alone beantwortet werden können. For instance, die Frage kann ich meine Tenants evicten, wenn sie zu viel Noise machen, könnte nicht eine detaillierte Antwort in statutory Law enthalten, die eine spezifische Noise Threshold at which Eviction ist. Insted sollten die Landlage wahrscheinlich mehr auf Fallslaw und Präceden ähnlich wie der aktuelle Situation finden. Zwei Parteien pro Woche bis 2 Uhr morgens. Daher sind einige Fragen besser als andere für die Aufgabe der gesetzlichen Artikelerfassung geeignet, und der Bereich der weniger geeigneten Fragen bleibt noch zu bestimmen. Wir hoffen, dass unsere Arbeit das Interesse an der Entwicklung praktischer und zuverlässiger Modelle der gesetzlichen Artikelerfassung weckt, die den Zugang zur Justiz für alle verbessern können. Sie können sich unsere Arbeit Datensätze und Code unter den folgenden Links ansehen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, wir sind froh, unsere Arbeit auf VALS, einem Taskindependenten Benchmark für das Testen von Vision und Language Models mit spezifischen linguistiken Phänomenen zu präsentieren. Warum haben wir uns die Trouble in der Setzung dieses Benchmarks gemacht? Nun, während der letzten Jahre haben wir eine explosion von Transformer basierten Vision und Language Models, die auf große Mengen von Image Textpaaren geprägt sind. Each dieser Modelle pusht State of the Art auf Vision und Language Tasks, wie z. B. visuelle Fragen Sinnesrechnung, Bildretrieval, Phrase-Grundung. Wir haben also eine Nachricht, die Akuragen auf diesen spezifischen Benchmarks steigern, aber wissen wir, was die Modelle tatsächlich gelernt haben? Was hat ein Vision und Language Transformer verstanden, als er eine High Score für dieses Bild und diesen Satz zugewiesen hat und eine Low Score für dieses Bild? Fokusieren Vision und Language Models auf das Richtige oder konzentrieren sie sich auf Biasen? wie gezeigt durch vorherige Arbeit. Um mehr Licht auf diesen Aspekt zu senden, propellieren wir eine mehr task agnostic direction und einführen WALS, das die Sensitivität von Vision und Language Models zu spezifischen linguistiken Phenomenen, die sowohl die linguistic als auch die visuelle Modalitäten beeinflussen. Wir targen Existenz, Pluralität, Zählung, Spatialrelationen, Aktionen und Entity Coreference. Aber wie testen wir, ob die Vision und Language Models diese Phenomena erfasst haben? Foiling, eine Methode, die vorher für Vision und Language Models, nur für Nanphrasen von Ravi Shakar und Kollaborators und auf Counting von Asin previous worked. Foiling basically means, dass wir den Caption of an Image und einen Foil produzieren, indem wir den Caption so verändern, dass er nicht die Image anymore beschreibt. Und wir machen diese Phrasealterations, indem wir uns auf sechs spezifische Pieces wie Existenz, Pluralität, Counting, Spatialrelations, Actions, und Entity Coreference. Jede Piece kann aus einem oder mehreren Instrumenten bestehen, in dem Fall, dass wir mehr als eine interessante Möglichkeit gefunden haben, FOIL instanzen zu erstellen. Zum Beispiel haben wir in der Fall der Aktionspiece zwei Instrumente, einen in dem das Action verb mit einer anderen Action geändert wird und einen in dem Actants gewechselt werden. Zählen und Coreferenz sind auch Pieces, die mehr als eine Instrumenten haben. Und wir erstellen diese FOILs, indem wir unserstellen, dass sie das Bild nicht beschreiben, dass sie grammatikalische und andererweit validierten Sätze sind. Dies ist nicht einfach zu tun, weil eine Foiled Caption weniger wahrscheinlich ist als die ursprüngliche Caption. Zum Beispiel, though it's not impossible, ist es statistisch weniger wahrscheinlich, dass Pflanzen einen Mann schneiden als ein Mann, und Largevision und Language Models könnten dies erkennen. Daher müssen wir, um valid Foils zu erhalten, Action unternehmen. Erstens, wir machen uns auf Strong Language Models zu propellieren. Zweitens, wir verwenden Natural Language Inference oder NLI, um Foils zu filtern, die noch beschreibiben das Bild immer noch, da wir bei der Konstruktion von FOILs sicherstellen müssen, dass sie das Bild nicht beschreiben. Um dies automatisch zu testen, wenden wir eine natürliche Sprachinferenz mit der folgenden Begründung an. Wir betrachten ein Bild als die Premise und seine Beschriftung als die damit verbundene Hypothese. Darüber hinaus betrachten wir die Beschriftung als die Premise und die FOIL als ihre Hypothese. Wenn ein NLI-Modell die FOIL zu widersprechen oder neutral zu den Beschriftungen zu sein, nehmen wir dies als Indikator für eine gültige FOIL. Wenn wir die Foil zu beentalten, kann es nicht eine gute Foil sein, da es durch Transitivität eine truthliche Beschreibung der Image geben wird und wir diese Foils ausfiltern. Aber dieser Prozess ist nicht perfekt. Es ist nur ein Indikator für valide Foils, daher als eine dritte Möglichkeit für die generierung von valid Foils, wir verwenden Humananannotators, um die Daten zu validieren, die in Valse verwendet werden. Also, nach Filtering und Human Evaluation, wir haben so viele Testinstanzen wie in dieser Tabelle. Note, dass Valse nicht trainierend Daten liefert. Aber nur Testdaten, da es sich nur um einen Benchmark für Null-Shot-Tests handelt. Es ist darauf ausgelegt, die bestehenden Fähigkeiten von Vision- und Sprachmodellen nach der Vor-Training-Aktivität zu nutzen. Die Feinabstimmung würde es Modellen nur ermöglichen, Artefakte oder statistische Voreingenommenheiten in den Daten auszunutzen. Und wir alle wissen, dass diese Modelle gerne betrügen und Abkürzungen einschlagen. Und wie wir sagten, sind wir daran interessiert, zu bewerten, welche Fähigkeiten die Vision- und Sprachmodelle nach der Vor-Training-Aktivität haben. Wir experimentieren mit fünf Vision- und Sprachmodellen auf WALS, nämlich mit CLIP, Wilbert, Wilbert Kelvin I und Visual Bert. Zwei unserer wichtigsten Evaluierungsmetriken sind die Accuracy der Modelle in der Klassifizierung von Image Sentenzenpaaren in Untertitel und Foils. Perhaps für dieses Video mehr relevant, wir werden unsere permissive Metrik, die Paarwise Accuracy, die messen, ob die Image Sentenz alignment score für die korrekte Image Textpaar als für ihre Foilpaar ist. Für mehr Metriken und Ergebnisse auf themen, doch checken Sie unsere Papier. Die Ergebnisse mit Paarwise Accuracy sind hier und sie sind konsistent mit den Ergebnissen, die wir von den anderen Metriken bekommen haben. Es ist, dass die besten Zero Shot Performance durch Wilbert zwölf in one, gefolgt von Wilbert, Alex Mert, Clip, und schließlich Visual Bird. Es ist notabel, wie Instrumente centered on individuelle Objekte wie Existenz und Nounphrases fast solviert werden, wobei Wilbert zwölf in one, highlighting, dass Models ableistisch genannte Objekte und ihre Präsenz in Images identifizieren können. Allerdings können none der remaining Pieces in unseren adversarialen Foiling-Settings reliably solviert werden. Instrumenten, dass Vision und Language Models Trouble distinguisieren, referenzieren zu single versus multiple Objekten oder in einem Image. Die Relation Piece zeigt, dass sie Schwierigkeiten haben, eine named spatiale Relation zwischen Objekten in einem Image zu klassifizieren. Sie haben auch Trouble, Aktionen zu distinguieren und ihre Partizipanten zu identifizieren, selbst wenn sie durch Plausibility bias sind, wie wir in der Aktionspiece sehen. Aus der Referenzpiece finden wir, dass das Tracing multiple Referenzen zu dem gleichen Objekt in einem Image durch Pronouns auch für Vision und Language Models. Als eine Sanity Check und weil es ein interessantes Experiment ist, benchmarken wir auch zwei Textonlegermodelle GPT one und GPT two, um zu assessieren, ob Valse solvable durch diese Unimodalmodelle ist, indem wir die Perplexität der korrekt und der foiled Caption, keine Image hier, und die Entry mit der lowest Perplexität. Wenn die Perplexität höher für die Foil ist, dann nehmen wir dies als eine Indikation, dass die foiled Caption möglicherweise von Plausibility bias oder anderen linguistiken Biasen leidet. Und es ist interessant zu sehen, dass in einigen Fällen Die Textmodelle nur GPT haben die Plausibilität der Welt besser erfasst als die Vision und Language Models. Zusammenfassend ist VALSE ein Benchmark, das die Lensen von Linguistikkonstrukten verwendet, um die Community zu helfen, Vision und Language Models zu verbessern, indem sie ihre visuellen Grounding-Fähigkeiten hart testen. Unsere Experimente zeigen, dass Vision und Language Models namensgelegene Objekte in ihrer Präsenz in Bildern gut identifizieren, wie gezeigt durch das Existenzstück, aber es schwerwiegt, ihre Interdependence und Relationships in visuellen Szenen zu genehmigen, wenn sie gezwungen sind, linguistic Indikatoren zu respektieren. Wir möchten die Community wirklich ermutigen, Valse zur Messung des Fortschritts hin zur Sprachbasis mit Vision- und Sprachmodellen zu verwenden. Und noch mehr, Valse könnte als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten, um zu sehen, ob ein Datensatz Modelle bei der Verbesserung eines der von Valse getesteten Aspekte hilft. Wenn Sie interessiert sind, sollten Sie sich die Valse-Daten auf GitHub ansehen und wenn Sie Fragen haben, zögern Sie nicht, uns zu kontaktieren."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kamisara von der University of Tokio. Ich werde eine Papier entitelt RNSAM, ein großes Dasein für automatische Restnoturation bei der Commit Dog Summization. Ich werde in dieser Ordnung erklären. Erst, ich werde die automatische Restnoturation, die wir in diesem Research arbeiten, vorstellen. ReleaseNode ist ein technischer Dokument, das die Changes mit jedem Release von einem Softwareprodukt zusammenfasst. Das Image zeigt die ReleaseNode für Version zwei. sechs. vier der GBUS Library. Diese Nodes spielen eine wichtige Rolle in Open Source Development, aber sie sind Zeit consumierend zu erstellen. Daher wird es sehr nützlich sein, automatisch hochqualitäre Release Nodes zu generieren. Ich werde auf zwei frühere Researchers auf automatische Release Node Generation eingehen. Der erste ist ein System namens Arena, das in twenty fourteen erstellt wurde. Es ergreift eine Rule basierende Approach, zum Beispiel, indem es die Change Extractor verwendet, um Code differenzen zu extrahieren. Bibliotheksänderungen und Dokumentenänderungen von den differenzierten Releases und schließlich kombinieren. Die meisten erkennbaren Funktionen dieses Systems sind die Issue Extractor in der oberen rechten Ecke, die mit Jira, dem Issue Toco System, verknüpft und nur für Projekte verwendet werden kann. In anderen Worten, es kann nicht für viele Projekte auf GitHub verwendet werden. Die zweite ist Griff, die Entry angeboten wurde in twenty twenty. Sie ist auf dem Internet und kann durch PIPSTEMENSER Dieses System hat ein einfaches Lernbasis für Textklassifikation und erhält eine von fünf Rabellen, die Funktionen oder Bugfixen für jedes Input Commit-Message enthalten. Das Bild ist eine Sample-Usage, die eine korrekte oder Bugfixen-Rabelle enthält. Die gerade erstellten Rabellen sind ziemlich klein, etwa fünftausend, und werden in den Experimenten beschriebenen darüber gezeigt. Die Leistung des Textklassifikationsmodells ist nicht hoch. Ich präsentiere zwei related Researchers, aber es gibt Probleme mit begrenztem Erwerb und geringen Daten. und Scar State Resources. Unser Papier löst diese beiden Probleme und automatisch generiert High Quality Releasing Notes. Für das limitierte Applicability Programm, wir propagieren eine High Quality Classifier Summarization Methode, die nur Committee Message als Input verwendet. Diese Methode kann für alle English Repositories verwendet werden. Für das zweite Problem der Scar State Resources, wir bauen RL und SAM DSET konsistent mit etwa achtzig zwei Tausend Pieces von Daten, die von öffentlichen GitHub API. Next, ich beschreibe unser Desert. Hier ist ein Beispiel für Daten. Die linke Seite ist ein Commit Message und die rechte Seite ist die RISE Nodes. Die RISE Nodes sind Raveled als Implement, Bug fixes, etc. Wir haben eine Task setup, die die Commit Messages als Input und die Raveled RISE Nodes aufwendet. Dies kann als eine Summarization Task bezeichnet werden. Wir haben vier Raveled Features, Implement, Bug fixes, Duplications, Removables und Breaking Changes. Diese wurden basierend auf PBR research und anderen Faktoren. Die RISE Noten auf der bottom right und extrahiert von den RISE Noten auf der bottom left. At diesem Zeitpunkt ist es notwendig, die vier Rabels zu detektieren, die in der Passage sind, aber die Rabels sind nicht immer konsistent mit jeder Repository. Zum Beispiel, die Improvements Rabels inklusive Improvements, Enhancements, Optimisations und so weiter. Wir haben eine Vokabularliste oder Studie Rabels für jeden dieser Notation. variations. Verwenden Sie es, um die Restknoten zu erkennen und den Text der Restknoten zu korrigieren. Als nächstes kommt eine Kommittemessage. Kommittemessagen sind nicht zu jeder Rest. Wie in der image below, wenn die aktuelle Rest Persönlich 2.5219 ist, müssen wir die vorherige Rest Persönlich 2.5218 identifizieren und einen Tiff erhalten. Dies ist ein bisschen tedious und es ist nicht genug, nur eine Rest der Restknoten zu erhalten. Und wir haben die Vorhand nach. Wir haben eine heuristic Matching Blue erstellt, um die vorherigen und nächsten Persönlichkeiten zu erhalten. Dasset Analysis In der End, 7.200 Repositories und 82.000 PSO wurden korrektiv. Außerdem ist die average Anzahl von ReleaseNode Tokens 63, was für eine Summarisation Task ziemlich hoch ist. Außerdem ist die Anzahl von Unique Tokens ziemlich groß, also 8.830.000. Dies ist auf die große Anzahl von Unique Kosten und Methoden zurück namens in der Repository. Next, ich werde explain die proposierte Methode. Das Crosswise Extractive and Abstractive Summarization Model besteht aus zwei neuronalen Modules, einem Classifier, der Bot oder Code Bot und einem Generator, der Bot verwendet. Erstens, GAS verwendet einen Classifier, um jedes Committee Message in fünf Risnode Classes, Features, Improvements, Bugfixes, Duplications, Plus, und andere. Die Committee Messages klassifiziert als andere oder diskutiert. Dann GAS wendet einen Generator zu den vier Rabelschnitteln unabhängig und generiert RISE NOTE für jeden Kurs. In diesem Zusammenhang sind die direkten Korrespondenzen zwischen Commit Messages und RISE NOTE nicht bekannt. Daher, um den Kurs zu trainieren, wir zwar Rabelschnitteln zu jedem Input Commit Message verwenden, indem wir die ersten zehn Charaktere für jedes Commit Message verwenden. Wir modellieren den Kurs abstruktiven Summarisierungsansatz durch zwei verschiedene Methoden. Das erste Modell, das wir als GAS Single Sync konsist von einer Sync zwei Sync Network und generiert eine Sync Long RISE Node Tagist, geben eine Konkurrenz von Input Committee Messages. Die Output Tagist kann in Crosswise Segment basierend auf speziellen Cross Specific Endpoint Symbols unterteilt werden. Die zweite Methode, Methode, die wir CSMUCH, besteht aus vier verschiedenen Sync zwei Sync Networks, jeder von denen zu einer der RISE Node Classes entspricht. Okay, lassen Sie mich das Experiment erklären. Fünf Methoden wurden GS, GS Single, GS Marge, Russelling und previous studied Griff. Regarding Abortion, in einigen Fällen, diese Noten sind in multiple Sentenzen. Da es difficil ist, die Nummer von Sentenzen zu erkennen, sind sie kombiniert mit Spaces und treaten als ein langer Satz. Der Blue ist penalisiert, wenn das System einen kurzen Satz erfährt. Diese Penalty resultiert in einem loweren Blue Value in den Experimenten der Results describiert. Finally, wir auch die Spezifität erkennen, weil Rouge und Brew nicht erkennen können, wenn die Rouge Nodes leer sind. Eine hohe Spezifität bedeutet, dass die Modellkorrektie auspackt ist, in denen die Rouge Nodes leer sind. Hier sind die Ergebnisse. Da der Daseit Emailadresse, Hashbarrieren, etc. enthält, haben wir auch den Green Daseit erkannt, der sie ausschließt. GAS und GAS erreichten Rouge Error Scores mehr als zehn Punkte höher als die Baselines. Auf dem grünen Testset stieg die Score-Gap zwischen der vorgeschlagenen Methode und der Basis auf mehr als zwanzig Punkte. Diese Ergebnisse zeigen, dass GAS und GAS signifikant effektiv sind. GAS erreichte eine bessere Rouge als GAS, was darauf hindeutet, dass das Kombinieren eines Klassifierers und eines Generators effektiv ist und das Training des Klassifierers mit Pseudoberseiten. Die hohe Abdeckung von GAS kann wahrscheinlich erreicht werden, da der Klassifier sich auf das Sekten relevanter Kommittemechanismen für jede Klasse konzentrieren kann. Xia ist viel zu höheren Ruder als Xia single, suggesting, dass es auch effektiv ist, unterschiedliche perspektive Summarisationsmodelle für jedes Nodecraft zu entwickeln. Hier sind Erronnas. Xia's Methods tend to output shorter Sentenzen als humanen Referenz Sentenzen. In der Figur auf der rechten Referenz Sentenzen hat drei oder vier Sentenzen, während Xia nur eins hat. Der Grund für diese Modellrücktanz ist, dass in den Trainingdaten nur 33 Prozent der Sentenzen sind in der Features Rabel und 40 Prozent in der Improvements Rabel. Für die More CS Methods können nicht akurat mit Noten ohne zusätzliche Informationen generiert werden. Das Top Exempel auf der rechten Seite ist ein Beispiel für eine sehr messige Kommittemessage und die Komplettesentence kann nicht generiert werden, ohne dass es sich auf die correspondente Periodrequest oder Issue bezieht. Das Beispiel below zeigt, dass die zwei Kommittemessagen in der Input related und sollten kombiniert werden. Aber es versucht dies zu tun. Schließlich eine Konklusion. Wir haben eine neue DSET für automatische Rezensionsschnittgenerierung erstellt. Wir haben auch die Aufgabe erstellt, Kommentarn zu enthalten und sie so zu summarisieren, dass sie für alle Projekte in Englisch anwendbar ist. Unser Experiment zeigt, dass die vorgeschlagene Methode weniger Noisy Rezensionsschnittgenerierung bei höherer Abdeckung als die Basisrate erzeugt. Bitte schauen Sie sich unsere DSET auf GitHub an. Vielen Dank."}
