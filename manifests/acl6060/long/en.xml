<?xml version="1.0" encoding="UTF-8"?>
<mteval>
<srcset setid="iwslt-ACLtest2023" srclang="English">
<doc docid="2022.acl-long.410" genre="presentations">
<talkid>2022.acl-long.410</talkid>
<abstract>Solving math word problems requires deductive reasoning over the quantities in the text. Various recent research efforts mostly relied on sequence-to-sequence or sequence-to-tree models to generate mathematical expressions without explicitly performing relational reasoning between quantities in the given context. While empirically effective, such approaches typically do not provide explanations for the generated expressions. In this work, we view the task as a complex relation extraction problem, proposing a novel approach that presents explainable deductive reasoning steps to iteratively construct target expressions, where each step involves a primitive operation over two quantities defining their relation. Through extensive experiments on four benchmark datasets, we show that the proposed model significantly outperforms existing strong baselines. We further demonstrate that the deductive procedure not only presents more explainable steps but also enables us to make more accurate predictions on questions that require more complex reasoning.</abstract>
<seg id="1">Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.</seg>
<seg id="2">I'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.</seg>
<seg id="3">First, I'd like to talk about our motivation for reasoning.</seg>
<seg id="4">So here we show an examples where multi-step reasoning is helpful.</seg>
<seg id="5">So this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.</seg>
<seg id="6">So on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.</seg>
<seg id="7">But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.</seg>
<seg id="8">So it is good to have interpretable multi-step reasoning as output.</seg>
<seg id="9">And we also think math word problem is a straightforward application to evaluate such reasoning abilities.</seg>
<seg id="10">So, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.</seg>
<seg id="11">So in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.</seg>
<seg id="12">So, certain assumptions ah also apply as in previous work.</seg>
<seg id="13">We assume the precision of quantities are known.</seg>
<seg id="14">And we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.</seg>
<seg id="15">Furthermore, complicated operators can be actually decomposed into these basic operators.</seg>
<seg id="16">So, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.</seg>
<seg id="17">So, traditional sequence to sequence model convert the expression to a specific sequence for generation.</seg>
<seg id="18">And it is pretty easy to implement and it can generalize to many different complicated problem.</seg>
<seg id="19">But the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.</seg>
<seg id="20">But actually this direction is still quite popular because of um the transformer model.</seg>
<seg id="21">So, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.</seg>
<seg id="22">So here we keep generating the operators until we reach the leaves, which are the quantities.</seg>
<seg id="23">So here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.</seg>
<seg id="24">And the second thing is that it also contains some repetitive computations.</seg>
<seg id="25">So here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.</seg>
<seg id="26">So, in our proposed approach we want to solve those problems in a step by step and interpretable manners.</seg>
<seg id="27">So for example, here in the second step, ah we can obtain these divisors which is twenty seven.</seg>
<seg id="28">And we can also refer back to the original questions to find the relevant contents.</seg>
<seg id="29">And in these steps we obtain the divisors.</seg>
<seg id="30">So, ah and then at this third step we actually get the quotient.</seg>
<seg id="31">Alright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.</seg>
<seg id="32">So, here we actually generate the whole expression directly rather than generating a single operators or quantities.</seg>
<seg id="33">So this makes the process more accurate.</seg>
<seg id="34">So, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.</seg>
<seg id="35">So, the expression is represented by e i j o p.</seg>
<seg id="36">Where we perform operator from q_i to q_j, and such expression is actually directed.</seg>
<seg id="37">So, we also have subtraction with words here to represent the opposite direction.</seg>
<seg id="38">This is quite similar to relation extraction.</seg>
<seg id="39">So in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.</seg>
<seg id="40">We add it to the next state to become a new quantity.</seg>
<seg id="41">So, these slides actually visualize the evolution of the state where we keep adding expression to the current state.</seg>
<seg id="42">So in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.</seg>
<seg id="43">So, once we get the quantity representations, we can start to do inference.</seg>
<seg id="44">Here we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.</seg>
<seg id="45">First we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.</seg>
<seg id="46">And then finally we obtain the expression representation q_1 divided by q_2.</seg>
<seg id="47">But in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.</seg>
<seg id="48">So, here all the possible expression is equals to three times the number of operators.</seg>
<seg id="49">So the nice thing here is that we can easily add constraints to control this search this search space.</seg>
<seg id="50">For example, if this expression is not allowed, we can simply remove this expression in our search space.</seg>
<seg id="51">So in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.</seg>
<seg id="52">So this quantity come from the previous calculated expression.</seg>
<seg id="53">So finally we can obtain this final expression q_3 times q_4.</seg>
<seg id="54">And we can also see the number of all the possible ah expression is different from the previous step.</seg>
<seg id="55">So, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.</seg>
<seg id="56">So the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.</seg>
<seg id="57">And here we also use this tau to represent when we should terminate this generation process.</seg>
<seg id="58">And here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.</seg>
<seg id="59">And it also allows us to impose certain constraints from prior from prior knowledge.</seg>
<seg id="60">So we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.</seg>
<seg id="61">And here we briefly show the results compared with the previous best approaches.</seg>
<seg id="62">So our best performing variant is Roberta-DeductiveReasoner.</seg>
<seg id="63">And in fact we do not use beam search, in contrast all previous approaches are using beam search.</seg>
<seg id="64">All right. So, the best approaches are often tree based model.</seg>
<seg id="65">So, overall our reasoner is able to significantl significantly outperform this tree based model.</seg>
<seg id="66">But we can see the absolute numbers on MathQA or SVAMP are not really high.</seg>
<seg id="67">So we further investigate the results on SVAMP.</seg>
<seg id="68">And this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.</seg>
<seg id="69">So, in our prediction we find some of the intermediate values are actually negatives.</seg>
<seg id="70">For example, um, in these questions we are asking how many apples does Jake have?</seg>
<seg id="71">But we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.</seg>
<seg id="72">So, our model makes some prediction like this which is producing negative values.</seg>
<seg id="73">And we observe these two expressions actually have similar scores.</seg>
<seg id="74">So, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.</seg>
<seg id="75">So um we further find such constraint actually improves quite a lot for some models.</seg>
<seg id="76">For example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.</seg>
<seg id="77">So better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.</seg>
<seg id="78">And we also try to analyze the difficulty behind these behind all these datasets.</seg>
<seg id="79">We assume the number of unused quantities can be regarded as irrelevant information here.</seg>
<seg id="80">So ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.</seg>
<seg id="81">And here we also show the overall performance.</seg>
<seg id="82">For those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.</seg>
<seg id="83">But with those samples that with unused quantity is actually way worse than the, worse than the overall performance.</seg>
<seg id="84">For MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.</seg>
<seg id="85">So, finally we want to show the interpretability through a question perturbation example.</seg>
<seg id="86">So here our model actually makes a wrong prediction at the first step.</seg>
<seg id="87">So, we can actually correlate this expression with the sentence here. Alright.</seg>
<seg id="88">So, we think this sentence might be misleading the model to an incorrect predictions.</seg>
<seg id="89">So here planting another thirty five makes the model makes the model think it should be an addition operator.</seg>
<seg id="90">So we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.</seg>
<seg id="91">So, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.</seg>
<seg id="92">So, this study shows how the interpretable predictions help us understand the model behavior.</seg>
<seg id="93">So to conclude our work, so first our model is actually pretty efficient.</seg>
<seg id="94">And we are able to provide interpretable solving procedure.</seg>
<seg id="95">And we can easily incorporate some prior knowledge as constraint which can help improve the performance.</seg>
<seg id="96">And the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.</seg>
<seg id="97">We also have certain limitations.</seg>
<seg id="98">Ah, if we have a large number of operators or constants, the memory consumption could be pretty high.</seg>
<seg id="99">And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.</seg>
<seg id="100">So this is the end of the talk, and questions are welcomed. Thank you.</seg>
</doc>
<doc docid="2022.acl-long.468" genre="presentations">
<talkid>2022.acl-long.468</talkid>
<abstract>Statutory article retrieval is the task of automatically retrieving law articles relevant to a legal question. While recent advances in natural language processing have sparked considerable interest in many legal tasks, statutory article retrieval remains primarily untouched due to the scarcity of large-scale and high-quality annotated datasets. To address this bottleneck, we introduce the Belgian Statutory Article Retrieval Dataset (BSARD), which consists of 1,100+ French native legal questions labeled by experienced jurists with relevant articles from a corpus of 22,600+ Belgian law articles. Using BSARD, we benchmark several state-of-the-art retrieval approaches, including lexical and dense architectures, both in zero-shot and supervised setups. We find that fine-tuned dense retrieval models significantly outperform other systems. Our best performing baseline achieves 74.8% R@100, which is promising for the feasibility of the task and indicates there is still room for improvement. By the specificity of the domain and addressed task, BSARD presents a unique challenge problem for future research on legal information retrieval. Our dataset and source code are publicly available.</abstract>
<seg id="101">Hi, my name is Antoine and I'm from Maastricht University.</seg>
<seg id="102">I will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.</seg>
<seg id="103">Legal issues are an integral part of many people's lives.</seg>
<seg id="104">But the majority of citizens have little to know knowledge about their rights and fundamental legal processes.</seg>
<seg id="105">As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.</seg>
<seg id="106">All work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.</seg>
<seg id="107">Such a system could provide a free professional legal help service for unskilled humans.</seg>
<seg id="108">Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.</seg>
<seg id="109">Given a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?</seg>
<seg id="110">A model is required to retrieve all relevant statutory articles from a large body of legislation.</seg>
<seg id="111">This information retrieval task comes with its own set of challenges.</seg>
<seg id="112">First, it deals with two types of language.</seg>
<seg id="113">Common natural language for the questions and complex legal language for the statutes.</seg>
<seg id="114">This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.</seg>
<seg id="115">Besides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.</seg>
<seg id="116">Instead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.</seg>
<seg id="117">Lastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.</seg>
<seg id="118">Here, there are long documents that may be up to six thousand words.</seg>
<seg id="119">The recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.</seg>
<seg id="120">But statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.</seg>
<seg id="121">In this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.</seg>
<seg id="122">Our Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.</seg>
<seg id="123">These questions cover a wide range of topics from family, housing, money, to work and social security.</seg>
<seg id="124">Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.</seg>
<seg id="125">Let's now talk about how we collected this dataset.</seg>
<seg id="126">First, we started by compiling a large corpus of legal articles.</seg>
<seg id="127">We considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.</seg>
<seg id="128">Then we gathered legal questions with references to relevant statutes.</seg>
<seg id="129">To do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.</seg>
<seg id="130">We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.</seg>
<seg id="131">We collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.</seg>
<seg id="132">Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.</seg>
<seg id="133">The remaining references were matched and converted to the corresponding article ids from our corpus.</seg>
<seg id="134">We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.</seg>
<seg id="135">In addition, each question comes with the main category and a concatenation of subcategories.</seg>
<seg id="136">And each articles comes with a concatenation of the subsequence heading in the structure of the law.</seg>
<seg id="137">This extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.</seg>
<seg id="138">Let's look at some characteristic of our dataset.</seg>
<seg id="139">The questions are between five and forty four words long with a median of fourteen words.</seg>
<seg id="140">The articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.</seg>
<seg id="141">The lengthiest one being up to five thousand seven hundred and ninety words.</seg>
<seg id="142">As previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.</seg>
<seg id="143">While the remaining fifteen percent concern either social security, foreigners or work.</seg>
<seg id="144">The article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.</seg>
<seg id="145">Here's the total number of articles collected from each of these Belgian codes.</seg>
<seg id="146">Out of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.</seg>
<seg id="147">And around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.</seg>
<seg id="148">Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.</seg>
<seg id="149">Which can be explained by the fact that those codes focused less on individuals and their concerns.</seg>
<seg id="150">Overall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.</seg>
<seg id="151">Using all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.</seg>
<seg id="152">Given a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.</seg>
<seg id="153">We experiment with the standard TF-IDF and BM25 ranking functions.</seg>
<seg id="154">The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.</seg>
<seg id="155">To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.</seg>
<seg id="156">We use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.</seg>
<seg id="157">These embeddings typically result from a pooling operation on the output of a word embedding model.</seg>
<seg id="158">First, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.</seg>
<seg id="159">We experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.</seg>
<seg id="160">Additionally, we train our own CamemBERT based model ah bi-encoders on our dataset.</seg>
<seg id="161">Note that for training, we experiment with the two flavors of the bi-encoder architecture.</seg>
<seg id="162">Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.</seg>
<seg id="163">We experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.</seg>
<seg id="164">Here are the result of our baseline on the test sets.</seg>
<seg id="165">With the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.</seg>
<seg id="166">Overall, the finetuned bi-encoder significantly outperforms all the other baselines.</seg>
<seg id="167">The two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.</seg>
<seg id="168">Although BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.</seg>
<seg id="169">Regarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.</seg>
<seg id="170">Furthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.</seg>
<seg id="171">Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.</seg>
<seg id="172">Let's conclude by discussing two limitations of our dataset.</seg>
<seg id="173">First, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.</seg>
<seg id="174">During the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.</seg>
<seg id="175">This information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.</seg>
<seg id="176">Second, we should note that not all legal questions can be answered with statutes alone.</seg>
<seg id="177">For instance, the question, can I evict my tenants if they make too much noise?</seg>
<seg id="178">Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.</seg>
<seg id="179">Instead, the landlord should probably rely more on case law and find precedents similar to their current situation.</seg>
<seg id="180">For example, the tenants makes two parties a week until two AM.</seg>
<seg id="181">Hence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.</seg>
<seg id="182">We hope that our work sparks interest in developing practical and reliable statutory article retrieval models.</seg>
<seg id="183">That can help improve access to justice for all.</seg>
<seg id="184">You can check out our paper, dataset and code at the following links. Thank you.</seg>
</doc>
<doc docid="2022.acl-long.567" genre="presentations">
<talkid>2022.acl-long.567</talkid>
<abstract>We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V&L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&L models from a linguistic perspective, complementing the canonical task-centred V&L evaluations.</abstract>
<seg id="185">Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.</seg>
<seg id="186">Why did we do the trouble in setting up this benchmark?</seg>
<seg id="187">Well, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.</seg>
<seg id="188">Each one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.</seg>
<seg id="189">So we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.</seg>
<seg id="190">But do we know what the models have actually learned?</seg>
<seg id="191">What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?</seg>
<seg id="192">And the low score for this one?</seg>
<seg id="193">Do vision and language models focus on the right thing?</seg>
<seg id="194">Or do they focus on biases as shown by previous work?</seg>
<seg id="195">To shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.</seg>
<seg id="196">We target existence, plurality, counting, spatial relations, actions and entity coreference.</seg>
<seg id="197">But how do we test whether the vision and language models have captured this phenomena?</seg>
<seg id="198">By foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.</seg>
<seg id="199">Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.</seg>
<seg id="200">And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.</seg>
<seg id="201">For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.</seg>
<seg id="202">Counting and coreference also are pieces that have more than one instrument.</seg>
<seg id="203">And we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.</seg>
<seg id="204">This is not easy to do because a foiled caption may be less likely than the original caption.</seg>
<seg id="205">For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.</seg>
<seg id="206">Therefore, to obtain valid foils, we must take action.</seg>
<seg id="207">First, we make use of strong language models to propose foils.</seg>
<seg id="208">Second, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.</seg>
<seg id="209">To test this automatically, we apply natural language inference with the following rationale.</seg>
<seg id="210">We consider an image to be the premise and its caption its entailed hypothesis.</seg>
<seg id="211">In addition, we consider the caption to be the premise, and the foil is its hypothesis.</seg>
<seg id="212">If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.</seg>
<seg id="213">If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.</seg>
<seg id="214">But this procedure is not perfect, it is just an indicator for valid foils.</seg>
<seg id="215">Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.</seg>
<seg id="216">So, after filtering and human evaluation, we have as many test instances as described in this table.</seg>
<seg id="217">Note that VALSE does not deliver any training data but only test data.</seg>
<seg id="218">Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.</seg>
<seg id="219">Finetuning would only enable models to exploit artifacts or statistical biases in the data.</seg>
<seg id="220">And we all know that these models like to cheat and take shortcuts.</seg>
<seg id="221">And as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.</seg>
<seg id="222">We experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.</seg>
<seg id="223">Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.</seg>
<seg id="224">Perhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.</seg>
<seg id="225">For more metrics and results on them, do check out our paper.</seg>
<seg id="226">The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.</seg>
<seg id="227">It's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.</seg>
<seg id="228">However, none of the remaining pieces can be reliably solved in our adversarial foiling settings.</seg>
<seg id="229">We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.</seg>
<seg id="230">The relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.</seg>
<seg id="231">They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.</seg>
<seg id="232">From the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.</seg>
<seg id="233">As a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.</seg>
<seg id="234">If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.</seg>
<seg id="235">And it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.</seg>
<seg id="236">So to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.</seg>
<seg id="237">Our experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.</seg>
<seg id="238">We would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.</seg>
<seg id="239">And even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.</seg>
<seg id="240">If you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.</seg>
</doc>
<doc docid="2022.acl-long.597" genre="presentations">
<talkid>2022.acl-long.597</talkid>
<abstract>A release note is a technical document that describes the latest changes to a software product and is crucial in open source software development. However, it still remains challenging to generate release notes automatically. In this paper, we present a new dataset called RNSum, which contains approximately 82,000 English release notes and the associated commit messages derived from the online repositories in GitHub. Then, we propose classwise extractive-then-abstractive/abstractive summarization approaches to this task, which can employ a modern transformer-based seq2seq network like BART and can be applied to various repositories without specific constraints. The experimental results on the RNSum dataset show that the proposed methods can generate less noisy release notes at higher coverage than the baselines. We also observe that there is a significant gap in the coverage of essential information when compared to human references. Our dataset and the code are publicly available.</abstract>
<seg id="241">Hello, my name is Kamezawa from the University of Tokyo.</seg>
<seg id="242">I'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.</seg>
<seg id="243">I'll be explaining in this order.</seg>
<seg id="244">First, I will introduce automatic release note generation that we are working on in this research.</seg>
<seg id="245">A release note is a technical document that summarizes the changes distributed with each release of a software product.</seg>
<seg id="246">The image shows a release note for version two point six point four of the vuejs library.</seg>
<seg id="247">Release notes play an important role in open source development but they're time consuming to prepare manually.</seg>
<seg id="248">Therefore, it would be very useful to be able to automatically generate high quality release notes.</seg>
<seg id="249">I will defer to two previous researches on automatic release note generation.</seg>
<seg id="250">The first is a system called ARENA released in twenty fourteen.</seg>
<seg id="251">It takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.</seg>
<seg id="252">The most notable feature of this system is the issue extractor in the upper right corner.</seg>
<seg id="253">Which must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.</seg>
<seg id="254">In other words, it cannot be used for many projects on GitHub.</seg>
<seg id="255">The second is Glyph, recently announced in twenty twenty.</seg>
<seg id="256">It is available on the internet and can be installed via pip.</seg>
<seg id="257">This system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.</seg>
<seg id="258">This image is a sample usage that returns a corrective or bug fixes label.</seg>
<seg id="259">Glyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.</seg>
<seg id="260">The performance of the text classification model is not high.</seg>
<seg id="261">I present two related researches, but their problems are limited applicability and scarce data resources.</seg>
<seg id="262">Our paper solves these two problems and automatically generates high quality release notes.</seg>
<seg id="263">With a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.</seg>
<seg id="264">This proposed method can be used for all English repositories.</seg>
<seg id="265">For the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.</seg>
<seg id="266">Next, I'll describe our dataset.</seg>
<seg id="267">Here is an example of data.</seg>
<seg id="268">The left side is a commit message and the right side is the release notes.</seg>
<seg id="269">Release notes are labeled as improvements or fixes, etc.</seg>
<seg id="270">We have set up a task that takes the commit messages as input and outputs a labeled release notes.</seg>
<seg id="271">This can be regarded as a summarization task.</seg>
<seg id="272">We have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.</seg>
<seg id="273">These were set based on previous research and other factors.</seg>
<seg id="274">The release note on the bottom right is extracted from the release note on the bottom left.</seg>
<seg id="275">At this time, it is necessary to detect the four labels that have been set up in advance.</seg>
<seg id="276">But the labels are not always consistent with each repository.</seg>
<seg id="277">For example, the improvements label includes improvements, enhancements, optimizations, and so on.</seg>
<seg id="278">We prepared a vocabulary list of about thirty labels for each of these notational variations.</seg>
<seg id="279">This is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.</seg>
<seg id="280">Next is a commit message.</seg>
<seg id="281">Commit messages are not tied to each release.</seg>
<seg id="282">As shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.</seg>
<seg id="283">This is a bit tedious and it is not enough to just get a list of releases and look at the before and after.</seg>
<seg id="284">We created a heuristic matching rule to get the previous and next versions.</seg>
<seg id="285">Dataset analysis.</seg>
<seg id="286">In the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.</seg>
<seg id="287">Also, the average number of release notes tokens is sixty three, which is quite high for a summarization task.</seg>
<seg id="288">Also, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.</seg>
<seg id="289">This is due to the large number of unique class or method names found in the repository.</seg>
<seg id="290">Next, I will explain the proposed method.</seg>
<seg id="291">The classwise extractive then abstractive summarization model consists of two neural modules.</seg>
<seg id="292">A classifier using BERT or CodeBERT and a generator using BART.</seg>
<seg id="293">First, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.</seg>
<seg id="294">The commit messages classified as other are discarded.</seg>
<seg id="295">Then CEAS applies the generator to the four labeled documents independently and generates release notes for each class.</seg>
<seg id="296">In this task, the direct correspondences between commit messages and release notes are not known.</seg>
<seg id="297">Therefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.</seg>
<seg id="298">We modeled the classwise abstractive summarization approach by two different methods.</seg>
<seg id="299">The first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.</seg>
<seg id="300">The output texts can be divided into classwise segments based on special class-specific endpoint symbols.</seg>
<seg id="301">The second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.</seg>
<seg id="302">Okay, let me explain the experiments.</seg>
<seg id="303">Five methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.</seg>
<seg id="304">Regarding evaluation, in some cases, release notes are output in multiple sentences.</seg>
<seg id="305">Since it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.</seg>
<seg id="306">The BLEU is penalized when the system outputs a short sentence.</seg>
<seg id="307">This penalty results in a lower BLEU value in the experiment results described next.</seg>
<seg id="308">Finally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.</seg>
<seg id="309">A higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.</seg>
<seg id="310">Here are the results.</seg>
<seg id="311">Since the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.</seg>
<seg id="312">CEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.</seg>
<seg id="313">In particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.</seg>
<seg id="314">These results indicate that CEAS and CAS are significantly affected.</seg>
<seg id="315">CEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.</seg>
<seg id="316">High coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.</seg>
<seg id="317">CAS-Multi tended to yield higher ROUGE-L than CAS-Single.</seg>
<seg id="318">Suggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.</seg>
<seg id="319">Here are an error analysis.</seg>
<seg id="320">CAS methods tend to output shorter sentences than human reference sentences.</seg>
<seg id="321">In the figure on the right, the reference sentence has three or four sentences, while CAS has only one.</seg>
<seg id="322">The reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.</seg>
<seg id="323">Furthermore, CAS methods cannot generate accurate release notes without additional information.</seg>
<seg id="324">The top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.</seg>
<seg id="325">The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.</seg>
<seg id="326">Finally, a conclusion.</seg>
<seg id="327">We have built a new dataset for automatic release note generation.</seg>
<seg id="328">We have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.</seg>
<seg id="329">Our experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.</seg>
<seg id="330">Please check out our dataset on GitHub.</seg>
<seg id="331">Thank you.</seg>
</doc>
<doc docid="2022.acl-long.111" genre="presentations">
<talkid>2022.acl-long.111</talkid>
<abstract>The enrichment of tabular datasets using external sources has gained significant attention in recent years. Existing solutions, however, either ignore external unstructured data completely or devise dataset-specific solutions. In this study we proposed Few-Shot Transformer based Enrichment (FeSTE), a generic and robust framework for the enrichment of tabular datasets using unstructured data. By training over multiple datasets, our approach is able to develop generic models that can be applied to additional datasets with minimal training (i.e., few-shot). Our approach is based on an adaptation of BERT, for which we present a novel fine-tuning approach that reformulates the tuples of the datasets as sentences. Our evaluation, conducted on 17 datasets, shows that FeSTE is able to generate high quality features and significantly outperform existing fine-tuning solutions.</abstract>
<seg id="332">Hello. My name is Asaf Harari.</seg>
<seg id="333">And I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.</seg>
<seg id="334">Data scientists analyze data and mainly focus on the manipulating the data's existing features.</seg>
<seg id="335">But sometimes, these features are limited.</seg>
<seg id="336">Feature generation using another data source may add substantial information.</seg>
<seg id="337">Our research goal is automatic tabular data enrichment using external sources' free text.</seg>
<seg id="338">Assume we have a tabular dataset and a knowledge base.</seg>
<seg id="339">We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.</seg>
<seg id="340">Our framework FeSTE is exactly this automatic process.</seg>
<seg id="341">So let's see an example in a dataset fed into FeSTE.</seg>
<seg id="342">In this example, the dataset is university dataset.</seg>
<seg id="343">When its goal is to classify universities into low ranking universities and high-ranking universities.</seg>
<seg id="344">As knowledge base, we use Wikipedia.</seg>
<seg id="345">The first phase of FeSTE is entity linking.</seg>
<seg id="346">When each entity, in this example the university name, is linked to an entity within the knowledge base.</seg>
<seg id="347">And and the text of the entities of the knowledge base is extracted and added to the dataset.</seg>
<seg id="348">In this example, the text is the Wikipedia page's abstract.</seg>
<seg id="349">Now, we need to generate or extract features from the retrieved text.</seg>
<seg id="350">So, we need to ah feature extraction phase ah which includes text analysis.</seg>
<seg id="351">And this is the main novelty of this paper and I will deep dive into it in the next slides.</seg>
<seg id="352">After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.</seg>
<seg id="353">First generate ah features in the number of classes of the original dataset.</seg>
<seg id="354">In this example, the original dataset has two classes.</seg>
<seg id="355">So, FeSTE generates two new features.</seg>
<seg id="356">But if the dataset has five classes, FeSTE generates five new features.</seg>
<seg id="357">Each feature represents the likelihood for each class.</seg>
<seg id="358">To analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.</seg>
<seg id="359">It is but it is not likely that we can train language models using the input datasets.</seg>
<seg id="360">So a naive approach will be ah target task finetuning.</seg>
<seg id="361">So, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.</seg>
<seg id="362">In this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.</seg>
<seg id="363">Receive the language model output, which is the likelihood for each class and use as new features.</seg>
<seg id="364">The problem with this approach is datasets may have few distinct entities / texts.</seg>
<seg id="365">In our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.</seg>
<seg id="366">So to finetune a language model over ah this dataset will be ineffective.</seg>
<seg id="367">But we can use prior knowledge about pre-analyzed datasets.</seg>
<seg id="368">Because FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.</seg>
<seg id="369">What we, what we suggest is to add, to add another finetuning phase.</seg>
<seg id="370">A preliminary multitask finetuning phase.</seg>
<seg id="371">When you finetune the language model over the n minus one datasets.</seg>
<seg id="372">And, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.</seg>
<seg id="373">The state-of-the-art in multitask ah multitask finetuning called MTDNN.</seg>
<seg id="374">In MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.</seg>
<seg id="375">So, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.</seg>
<seg id="376">And it samples a random batch from ah from the training set.</seg>
<seg id="377">And if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.</seg>
<seg id="378">And if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.</seg>
<seg id="379">In our scenario, ah tabular datasets vary in the number of classes.</seg>
<seg id="380">So there are many tasks.</seg>
<seg id="381">MTDNN maintained number of classes, heads, output layers.</seg>
<seg id="382">And the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.</seg>
<seg id="383">Our approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.</seg>
<seg id="384">So let's see an example.</seg>
<seg id="385">Here is the our input dataset which consists of entities, features, text and classes.</seg>
<seg id="386">And, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.</seg>
<seg id="387">Or in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.</seg>
<seg id="388">So the label vector in this case stays always ah which consists always with two classes.</seg>
<seg id="389">And this is the ah algorithm for our fine, reformulated finetuning approach.</seg>
<seg id="390">So let's see the full framework.</seg>
<seg id="391">Dataset fed into FeSTE.</seg>
<seg id="392">And then ah FeSTE executes entity linking phase.</seg>
<seg id="393">It ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.</seg>
<seg id="394">Then it reformulated the task into a pairwise sentence classification task.</seg>
<seg id="395">Applied the language model to the new task and the output likelihood for each class.</seg>
<seg id="396">And now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.</seg>
<seg id="397">Then we use the output vector of the language model as a newly generated feature in the number of classes.</seg>
<seg id="398">To evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.</seg>
<seg id="399">And as knowledge base we use Wikipedia.</seg>
<seg id="400">We design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.</seg>
<seg id="401">We also, we also split each dataset into four folds and apply four folds cross validation.</seg>
<seg id="402">Then, we generate the new features and evaluate them using five evaluation classifiers.</seg>
<seg id="403">We use in our experiments base BERT base architecture.</seg>
<seg id="404">Here are the results for our experiments.</seg>
<seg id="405">You can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.</seg>
<seg id="406">And our reformulated finetuning achieves the best result, the best performance.</seg>
<seg id="407">While MTDNN achieved two percent improvement over the target dataset finetuning.</seg>
<seg id="408">Our approach achieved six percent improvement.</seg>
<seg id="409">When we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.</seg>
<seg id="410">But our performance increased to eleven percent compared to the target task finetuning alone.</seg>
<seg id="411">For summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.</seg>
<seg id="412">It uses one architecture for all tasks and datasets.</seg>
<seg id="413">And it keeps the head of ah of the model.</seg>
<seg id="414">But it adds reformulation phase.</seg>
<seg id="415">It augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.</seg>
<seg id="416">Thank you.</seg>
</doc>
</srcset>
</mteval>
