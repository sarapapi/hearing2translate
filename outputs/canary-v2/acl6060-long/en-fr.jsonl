{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Asa Farari et je vais présenter notre article, FUESHOT TABLAR DATA ARCHECTIMENT UNTROISTRONTRANSFORMERS DE FINE TUNICUE. Les scientifiques des données analysent les données et se concentrent principalement sur la manipulation des caractéristiques existantes des données. Mais parfois, ces caractéristiques sont limitées. La génération de caractéristiques à l'aide d'une autre source de données peut ajouter des informations substantielles. Notre objectif de recherche est l'enrichissement automatique des données tableuses à l'aide de textes libres de sources externes. Supposons que nous disposions d'un ensemble de données tableuses et d'une baseun processus automatique qui implique le lien entre entités et l'analyse de texte pour extraire de nouvelles caractéristiques du texte libre de la base de connaissances. Notre cadre FEST est exactement ce processus automatique. Voyons donc un exemple. Dans les ensembles de données alimentés dans FEST. Dans cet exemple, l'ensemble de données est un ensemble de données universitaires, dont l'objectif est de classer les universités en universités à faible classement et en universités à de haut rang. En tant que base de connaissances, nous utilisons Wikipédia. La première phase de FEST est le lien entre entités, lorsque chaque entité, dans cet exemple, le nom de l'université est lié à une entité au sein de la base de connaissances, et le texte des entités de la base de connaissances est extrait et ajouté au jeu de données. Dans cet exemple, le texte est l'abstracte de la page Wikipédia. Maintenant, nous devons générer ou extraire des caractéristiques du texte récupéré. Nous avons donc besoin d'une phase d'extraction de caractéristiques qui inclut l'analyse du texte. Et c'est la principale novité de cet article, et je vais en profiter dans les prochaines diapositives. Après la phase d'extraction de caractéristiques, il y a une phase de génération de caractéristiques lorsque nous utilisons les caractéristiques extraites pour générer un petit nombre de nouvelles caractéristiques. Tout d'abord, générez des caractéristiques dans le nombre de classes de l'ensemble de données d'origine. Dans cet exemple, l'ensemble de données d'origine comporte deux classes, donc générez d'abord deux nouvelles caractéristiques. Mais si l'ensemble de données comporte cinq classes, générez d'abord cinq nouvelles caractéristiques. Chaque caractéristique représente la probabilité pour chaque classe. Pour analyser le texte, nous utilisons l'état actuel de l'analyse du texte, qui est un modèle de langue basé sur des transformateurs, comme BERT, GPT, XLRT, etc. Mais il est peu probable que nous puissions entraîner un modèle de langue à l'aide des ensembles de données d'entrée. Une approche naive sera donc l'ajustement de la tâche cible. Ainsi, dans la phase d'extraction de caractéristiques, nous pouvons télécharger un modèle de langue pré-entraîné, ajuster le modèle de langue sur l'ensemble de données cible dans cet exemple pour ajuster le modèle de langue pour classer le texte en classes, l'abstracte en classes, l'abstracte en classes, la sortie du modèle de langue, qui est la probabilité pour chaque classe, et l'utiliser comme nouvelles fonctionnalités. Le problème avec cette approche est que les ensembles de données peuvent avoir peu d'entités distinctes, le texte. Dans notre expérience, presque la moitié des ensembles de données contiennent moins de 400 échantillons et le plus petit ensemble de données contient trente-cinq échantillons dans son ensemble d'entraînement. Ainsi, affiner un modèle de langue sur ce jeu de données sera inefficace. Mais nous pouvons utiliser des connaissances préalables sur les ensembles de données préanalysés car nous appliquons FAST sur plusieurs ensembles de données. Nous pouvons utiliser les ensembles de données n pour recueillir des informations sur les ensembles de données n et utiliser ces informations lorsque nous analysons les n. Ce que nous suggérons, c'est d'ajouter une autre phase de fine-tuning, une phase de fine-tuning multitask préliminaire, lorsque nous finissons le langage modèle sur n-1 datasets, puis nous exécutons une autre phase de fine-tuning, qui est une finition de la tâche cible lorsque nous finissons le langage modèle sur le nth dataset cible. Le statut de la finition de la multitask. un fine-tuning appelé MTDNN. Dans MTDNN, MTDNN maintenait des têtes dans le nombre de tâches dans le training. Donc, dans cet exemple, il y a quatre tâches dans le training. Donc, MTDNN maintenait quatre têtes, comme vous pouvez le voir sur l'image. Et il échantillonne un random batch du training. Et si le random batch appartient à un par exemple, les tâches de classification de Sing et Seltan, il exécute les passages en avant et en arrière à travers la première tête. Et si le batch aléatoire appartient à une tâche de classement pairé, il exécute les passages en avant et en arrière à travers la dernière tête. Dans notre scénario, un tableau de données définit un certain nombre de classes. Il y a donc de nombreuses tâches. MTDNN maintient un certain nombre de classes en têtes, en output layers, et en outre, MTDNN doit initialiser de nouvelles têtes pour un nouveau ensemble de données avec une nouvelle tâche. Notre approche appelée réglage de la réformulation de la tâche est que dans notre approche de réglage de la réformulation de la tâche, au lieu de maintenir plusieurs têtes, nous reformulons chaque ensemble de données en une phrase par classification, qui est deux classes de tâches. Voyons un exemple. Voici notre ensemble de données d'entrée qui consiste en entités, features, texte et classes. Nous reformulons la tâche de classifier le texte en low et high pour classifier le texte, l'abstracte et la classe en true ou faux. En d'autres termes, nous entraînons le modèle de langage à classifier pour classer un abstrait et une classe, si l'abstrait appartient à la classe ou non. Ainsi, le vecteur étiqueté, dans ce cas, reste toujours composé de deux classes. Et c'est l'algorithme pour notre recherche. trouver une approche de finition réformulée. Voyons donc le cadre complet. Un ensemble de données est alimenté dans FAST, puis FAST exécute la phase de liaison, extrait le texte de la base de connaissances, qui dans cet exemple est l'abstracte de la page Wikipédia, puis il réformule la tâche en parenthèses pour les tâches de classification, applique le modèle de langue à la nouvelle tâche. et la probabilité d'output pour chaque classe. Notez que le modèle de langue est déjà ajusté sur n-1 dataset en utilisant une préliminaire ajustation multitask. Ensuite, nous utilisons le vecteur de sortie du modèle de langue comme une nouvelle fonctionnalité générée dans le nombre de classes. Pour évaluer notre cadre, nous utilisons une table de 17. Nous avons un ensemble de données de classification tabulaire de dix-sept, qui varie en taille, caractéristiques, équilibre, domaine et performance initiale. Et comme base de connaissances, nous utilisons Wikipédia. Nous concevons notre expérience comme une évaluation de laissée de côté lorsque nous entraînons rapidement sur seize ensembles de données et les appliquons au dix-septième ensemble de données. Nous divisons également chaque ensemble de données en quatre groupes. Nous avons créé quatre failles et nous avons appliqué une validation croisée des failles. Ensuite, nous avons généré la nouvelle fonctionnalité et l'avons évaluée à l'aide de cinq classificateurs d'évaluation. Nous avons utilisé une architecture basée sur la construction dans notre expérience. Voici les résultats de notre expérience. Vous pouvez voir que nous avons comparé notre cadre à la finesse de l'ensemble de données cible, la finesse de la tâche cible. et MTDNN préliminaire fine-tuning et notre fine-tuning réformulé a obtenu le meilleur résultat, la meilleure performance, tandis que MTDNN a obtenu une amélioration de 2% par rapport à la fine-tuning du jeu de données cible. notre approche a obtenu une amélioration de 6%. Lorsque nous regardons le petit jeu de données,nées, nous pouvons voir que la performance du DNN vide diminue et que l'amélioration de la phase de finition multitask préliminaire diminue à 1,5 %, mais notre performance a augmenté à 11 % par rapport à la seule finition de la tâche cible. Pour résumer, FAST permet l'enrichissement de quelques prises de vue à partir de 35 échantillons de notre expérience. utilise une architecture pour tous les ensembles de données de tâches et conserve la tête du modèle. Mais elle ajoute trois phases de formulation, elle augmente le train et elle a besoin d'une valeur cible avec un sens sémantique, afin que nous puissions l'intégrer dans le modèle de langue et l'utiliser dans le problème de classification de paire de phrases. Merci."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour à tous. Aujourd'hui, je vais présenter notre travail de recherche apprenant à raisonner de manière déductive, la résolution de problèmes de mathématiques en tant qu'extraction de raisons complexes. Je suis Alan du Biden's AI Lab, et ceci est un travail conjoint avec Thierry de l'Université du Texas à Austin et Wayloo de l'UDD. Tout d'abord, je voudrais parler de notre motivation pour raisonner. Nous montrons donc un exemple où le raisonnement à plusieurs étapes est utile. Cette figure est tirée du papier PALM où ils ont effectué des incitations pour résoudre le problème de la mathématique dans un scénario d'apprentissage futur. Ainsi, du côté de la main net, nous pouvons voir que si nous donnons quelques exemples avec seulement des questions et des réponses, nous ne pourrons peut-être pas obtenir les bonnes réponses. Mais si nous donnons une description plus raisonnable, le modèle est capable de prédire la description raisonnable et de faire une prédiction correcte ici. Il est donc bon d'avoir un raisonnement multiétape interprétable en sortie. Et nous pensons également que le problème méthodique est une application directe pour évaluer ces capacités de raisonnement. Dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques. Ainsi, dans nos ensembles de données, nous recevons également l'expression mathématique qui conduit également à cette réponse particulière. Certaines hypothèses s'appliquent également, comme dans le travail précédent. Nous supposons que la précision des quantités est connue et nous ne considérons que des opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle. De plus, des opérateurs compliqués peuvent être réellement déconstruits. Il peut en fait être décomposé en ces opérateurs de base. Ainsi, le travail précédent dans la résolution de problèmes méthodiques peut en fait être classé en modèle de séquence en séquence et de séquence en arbre. Ainsi, le modèle de séquence traditionnel convertit l'expression en une séquence spécifique pour la génération, et il est assez facile à mettre en œuvre. Et il peut généraliser à de nombreux problèmes compliqués différents. Mais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle de structure, et il manque d'interprétation pour la rés prédiction. Mais en réalité, cette direction est toujours assez populaire en raison du modèle transformateur. Ainsi, dans les modèles basés sur un arbre, nous structurons en fait ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres. Ainsi, ici, nous continuons à générer les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités. L'avantage ici est qu'il nous donne en fait cette structure d'arbres binaire. Mais en réalité, c'est assez contre-intuitif. Parce que nous générons d'abord l'opérateur, puis à la fin, nous générons les quantités. Et la deuxième chose est qu'il contient également des calculs répétitifs. Donc, ici, si nous regardons cette expression, huit fois trois plus trois, elle est en fait générée deux fois. Mais en fait, nous devrions réutiliser les résultats. Donc, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable. Par exemple, ici, dans la deuxième étape, nous pouvons obtenir. ces diviseurs, qui sont vingt-sept. Et nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent. Et dans ces étapes, nous obtenons les diviseurs. Et ensuite, à cette troisième étape, nous obtenons en fait le quotient. Et après ces trois étapes, nous pouvons réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape. Et enfin, nous pouvons obten générons en fait l'expression entière directement plutôt que de générer des opérateurs ou des quantités individuels. Cela rend le processus plus précis. Dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et nous incluons également certaines constantes comme états initiaux. L'expression est représentée par EIJOP, où nous effectuons des opérateurs de Qi à QJ, et cette expression est en fait dirigée. Nous avons donc également la soustraction inverse ici pour représenter la direction opposée. C'est assez similaire à l'extraction de relation. Dans un système déductif formel, à l'étape temporelle t, nous appliquons l'opérateur entre le couple Qi et Qj, puis nous obtenons cette nouvelle expression. Nous l'ajoutions aux états suivants pour devenir une nouvelle quantité. Ces diapositives visualisent en fait l'évolution des états où nous continuons à ajouter des expressions aux états actuels. Dans nos implémentations de modèle, nous utilisons d'abord un modèle de réseau préentraîné qui peut être Birds ou Roberto, puis nous codons la phrase et obtenons ces représentations de quantité. Une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire des inférences. Ici, nous montrons un exemple de Q pour obtenir la représentation de Q un, qui sera divisée par Q deux, puis multipliée par Q trois. Tout d'abord, nous obtenons la représentation de paire, qui est essentiellement la concaténation entre Q un et Q deux, puis nous appliquons un réseau de feedforward qui est. C'est paramétrifié par l'opérateur. Et enfin, nous obtenons la représentation de l'expression Q1 divisée par Q2. Mais en pratique, à l'étape d'inférence, nous pourrions également obtenir l'expression incorrecte. Ainsi, ici, toutes les expressions possibles sont égales à trois fois le nombre d'opérateurs. L'avantage ici est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche. Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche. Ainsi, à l'étape deux, nous faisons la même chose, mais la seule différence est qu'il y a une autre quantité. Cette quantité provient de l'expression calculée précédemment. Ainsi, nous pouvons finalement obtenir cette expression finale Q trois fois Q quatre. Et nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape précédente. Ainsi, cette différence rend difficile l'application. Il est difficile d'appliquer la recherche de faisceau car la distribution de probabilité entre ces deux étapes est déséquilibrée. Ainsi, la procédure d'entraînement est similaire à l'entraînement d'un modèle de séquence à séquence où nous optimisons les pertes à chaque étape temporelle. Et ici, nous utilisons également ce signe pour représenter quand nous devrions terminer ce processus de génération. Et ici, l'espace est différent de la séquence à la séquence car l'espace est différent à chaque étape temporelle, tandis que dans le modèle de séquence à séquence traditionnel, il s'agit du nombre de vocabulaire.. Et cela nous permet également d'imposer certaines contraintes à partir de connaissances préalables. Nous avons donc mené des expériences sur les ensembles de données de problèmes de méthodes couramment utilisés, MAWPS, Math twenty three K, MathQA et SWAM. Et ici, nous montrons brièvement les résultats par rapport aux meilleures approches précédentes. Ainsi, notre variante la plus performante est Roberta Deductive Reasoner. Et en fait, nous n'utilisons pas BeamSearch, par opposition aux approches précédentes utilisant BeamSearch. D'accord, les meilleures approches sont souvent les modèles basés sur un arbre. Dans l'ensemble, notre raisonnement est capable de surpasser de manière significative ce modèle basé sur un arbre, mais nous pouvons voir que les chiffres absolus sur MathQA ou SWAM ne sont pas vraiment élevés. Nous avons donc étudié davantage les résultats sur SWAM et cet ensemble de données est difficile car l'auteur a tenté d'ajouter manuellement quelque chose pour confondre le modèle NLP, comme l'ajout deut d'informations irrelevantes et de quantités supplémentaires. Ainsi, dans notre prédiction, nous constatons que certaines des valeurs intermédiaires sont en réalité négatives. Par exemple, dans cette question, nous demandons combien de pommes Jake a, mais nous avons des informations supplémentaires comme dix-sept ingrédients et Stephen a huit ingrédients, ce qui est totalement irrelevant. Ainsi, notre modèle fait une prédiction comme celle-ci, qui produit des valeurs négatives. Et nous observons que ces deux expressions ont en réalité des valeurs similaires. En fait, nous avons des scores similaires. Nous pouvons donc limiter cet espace de recherche en supprimant ces résultats négatifs afin de pouvoir donner la bonne réponse. Nous constatons également que cette contrainte améliore considérablement certains modèles. Par exemple, pour les oiseaux, nous avons amélioré sept points. Et pour le modèle basé sur Roberta, nous avons amélioré deux points. Un meilleur modèle de langue a une meilleure capacité à comprendre la langue, de sorte que le nombre ici est plus élevé. Ici, c'est plus élevé pour Roberta et plus bas pour les oiseaux. Nous avons également essayé d'analyser la difficulté derrière tout ce jeu de données. Nous supposons que le nombre de quantités inutilisées peut être considéré comme des informations sans importance ici. Nous pouvons donc voir ici que nous avons le pourcentage d'échantillons avec des quantités inutilisées et que le jeu de données SWAMP contient la plus grande proportion. Et ici, nous montrons également la performance globale pour ces échantillons sans quantités inutilisées. La performance globale est donc en fait supérieure à celle de la. Et la performance est en fait supérieure à celle de la performance globale. Mais avec ces échantillons, avec une quantité inutilisée, c'est en fait bien pire que la performance globale. Pour MAWPS, nous n'avons pas vraiment beaucoup de cas de défaut, donc je n'ignore pas cette partie. Enfin, nous voulons montrer l'interprétation à travers un exemple de participation à la crise. Ainsi, notre modèle fait en fait une pr corréler cette expression à la phrase ici. Nous pensons donc que cette phrase pourrait inducer le modèle en erreur et donner une prédiction incorrecte. Ici, en plantant un autre trente-cinq, le modèle pense qu'il devrait s'agir d'opérateurs d'addition. Nous essayons donc de réviser la phrase pour qu'elle soit quelque chose comme le nombre d'arbres de poireaux est trente-cinq moins que celui des pommiers. Nous la faisons donc pour transmettre une sémantique plus précise, de sorte que le modèle soit capable de faire la prédiction correcte. Cette étude montre comment les prédictions interprétables nous aident à comprendre le comportement du modèle. Pour conclure notre travail, notre modèle est en fait assez efficace et nous sommes en mesure de fournir une procédure de résolution interprétable et nous pouvons facilement intégrer certaines connaissances préalables comme contrainte qui peut aider à améliorer les performances. La dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution de problèmes de réseau, mais aussi à d'autres tâches qui impliquent un raisonnement à plusieurs étapes. Mais nous avons aussi certaines limitations. Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée. Et la deuxième chose est que, comme mentionné, parce que la distribution de probabilité est déséquilibrée à différents étapes temporelles, il est également assez difficile d'appliquer la stratégie de recherche de faisceau. C'est la fin de la présentation et les questions sont les bienvenues. Merci."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Antoine et je suis de l'université de Maastricht. Je vais présenter mon travail conjoint avec Jerry, qui porte sur un nouveau ensemble de données pour la récupération d'articles statutaires. Les questions juridiques font partie intégrante de la vie de nombreuses personnes, mais la majorité des citoyens n'ont pas grand-chose à savoir sur leurs droits et leurs procédures juridiques fondamentales. Par conséquent, de nombreux citoyens vulnérables qui ne peuvent pas se permettre l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités. aimera à briser le gap entre les gens et la loi en développant un système de retrait effectif pour les articles statutaires. Un tel système pourrait fournir un service de recherche professionnel gratuit pour les unsqualifiés. Avant de plonger dans la principale contribution de cette étude, décrions d'abord le problème de retrait des articles statutaires. Given une simple question sur une question de la loi, telle que ce que je risque de violer la confidentialité professionnelle, un modèle est requis pour retrouver tous les articles statutaires pertinents de la législation. dans un grand nombre de législations. Cette tâche de retrouver des informations comporte ses propres défis. Tout d'abord, elle traite de deux types de langage, le langage naturel pour les questions et le langage légal complexe pour les statuts. Cette différence de distribution de langage rend la retrouverie de candidats pertinents, car elle indirectement requiert un système d'interprétation inhérent qui peut transformer une question naturelle en une question légale qui correspond à la terminologie de la loi. Besides, statutory law is not a stack of independent articles that can be treated as a complète source of information on their own, comme news or recipes, par exemple. Instead, c'est une structure collection de provisions légales qui ont un whole meaning only when considered in their overall context, that is, avec les supplémentaires information from their neighbouring articles, les fields et subfields they belong to, et leur place in the structure of the law. Lastly, statutory articles arrange un petit paragraphe. qui est généralement la type de retrieval unit dans la plupart des travaux de retrieval. Ici, il y a des documents longs qui peuvent être jusqu'à six mille words. Les récentes avancées en NLP ont sparqué une huge intérêt dans de nombreuses tâches, telles que la prédiction de jugement ou la contrat de contrat automatique, mais la retrieval d'articles statutés a remain mainly intact due à la la lacune de large et de qualité de label datasets. Dans ce travail, nous présentons un nouveau French native citizen centric dataset pour étudier whether un modèle de retrieve peut approximer l'efficacité et la reliabilité d'un expert juridique pour la tâche de statutory article retrieval. Nos statutory article retrieval dates consistent de plus de mille cent questions légales posées par les Belges. Ces questions couvrent un large éventail de sujets, de la famille, de la vie, de l'argent, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de la vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie, de vie. articles de plus de vingt deux mille six cents articles de la loi de la loi. Nous allons maintenant parler de la façon dont nous avons collecté ces datasets. Tout d'abord, nous avons commencé par compiler un large corpus de la loi de la loi. Nous avons considéré trente deux articles de la loi publiquement disponibles en Belgique et extrait tous leurs articles ainsi que les correspondants sections. Ensuite, nous avons collecté des questions de la loi avec références à des statuts. Pour ce faire, nous avons partenarié avec une agence de droit de la loi. qui reçoit chaque année environ quatre mille emails de citoyens belges qui demandent des avis sur une question de droit personnel. Nous avons eu la chance de nous accéder à leurs websites où leur équipe de juristes expérimentés traite des questions de droit les plus courantes en Belgique. Nous avons collecté des milliers de questions annotées avec des catégories, des sous catégories, et des références juridiques à des statuts pertinents. Enfin, nous avons passé les références juridiques et filtré les questions dont les références n'étaient pas des articles. dans les codes de la loi considérés. Les restes des références ont été matchés et converts à la correspondante article IDs de notre corpus. Nous avons éventuellement enduré avec un mille cent quatre questions, each carefully labelled with the IDs of the relevant articles from our large corpus of twenty two thousand six hundred thirty three statutory articles. En addition, chaque question combe avec une main catégorie et une concatenation de subcatégories. Chaque article est accompagné d'une concaténation de la titre suivante dans la structure de la loi. Ces informations supplémentaires ne sont pas utilisées dans le travail actuel, mais pourraient intéresser les recherches futures sur la récupération d'informations juridiques ou la classification du texte juridique. Examinons quelques caractéristiques de notre ensemble de données. Les questions sont de cinq à quarante-quatre mots de long, avec une médiane de quarante mots. Les articles sont beaucoup plus longs, avec une médiane de soixante-dix-sept mots, dont cent quarante-deux dépassant mille mots. La plus longue est de cinq mille sept cent quatre-vingt-dix mots. Comme mentionné précédemment, la question couvre une large gamme de sujets, dont environ quatre-vingt-quinze pour cent concernent la famille, le logement, l'argent ou la justice, tandis que la restante quinze pour cent concerne soit la sécurité sociale, les étrangers ou le travail. Les articles sont également très variés, car ils proviennent de trente-deux codes belges différents qui couvrent un grand nombre de sujets illégaux. Voici le nombre total d'articles recueillis de chacun de ces codes belges. Sur les vingt-deux mille six cent trente-trois articles, seulement mille six cent douze sont mentionnés comme étant pertinents pour au moins une question dans les ensembles de données, et environ quatre-vingts pour cent de ces articles citées proviennent du code civil, du code judiciaire, du code d'enquête pénale ou du code pénal. En attendant, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme étant pertinents pour au moins une question. question, ce qui peut être expliqué par le fait que DUS code focale moins sur les individuels et leurs concerns. Overall, le nombre de citations pour ces articles cités est deux, et moins de vingt cinq pour cent d'entre eux sont cités plus de cinq fois. Utilisant nos datasets, nous benchmarquons plusieurs approches, incluant lexical et dense architecture. Given une query dans un article, un modèle lexical assigne un score à la query article pair en computant la somme des poids de chacun de ces termes dans cet article, nous expérimentons avec les fonctions de classement standard TFIDF et BM twenty five. Le principal problème avec ces approches est qu'elles ne peuvent récupérer que les articles qui contiennent des mots-clés présents dans la requête. Pour surmonter cette limitation, nous expérimentons avec une architecture basée sur des neurones qui peut capturer la relation sémantique entre les requêtes et l'article. Nous utilisons un modèle BEncoder qui cartographie les requêtes et les articles en représentations vectorielles denses. et calculer un score pertinent entre une paire d'articles de requête en fonction de la similitude de leurs embeddings. Ces embeddings sont généralement le résultat d'une opération de pooling sur la sortie d'un modèle d'embedding de mots. Tout d'abord, nous étudions l'efficacité des encodeurs B siaméens dans une configuration d'évaluation à zéro tir, ce qui signifie que les modèles d'embedding de mots pré-entraînés sont appliqués hors de la boîte sans aucune réglage supplémentaire. Nous expérimentons avec des encodeurs de texte indépendants du contexte, à savoir Word2Vec et Fastex et les modèles d'embellage dépendants du contexte, à savoir Roberta et plus précisément Camembert, qui est un modèle de Roberta français. De plus, nous entraînons nos propres modèles Camembert basés sur nos données. Notez que pour l'entraînement, nous expérimentons les deux variétés de l'architecture de l'embellage. Siamese, qui utilise un modèle d'embellage de mot unique qui répertorie la requête et l'article dans un espace vectoriel dense partagé. Et Toutau, qui utilise deux modèles de mots word embedding qui encodent le query et l'article séparément dans différents espaces d'embedding. Nous expérimentons avec le moyen, le max et le pooling CLS, ainsi que le dot product et le cosinus pour computer les similitudes. Voici les résultats de notre baseline sur le test sets, avec les méthodes lexicales above, les chiamites B encoders évalués dans un zéro shot setup dans le milieu, et les chiamites B encoders en dessous. Overall, les chiamites B encoders significativement surperforment toutes les autres baselines. Le modèle Toute Tower améliore sur ses chiamites. sa variante siamienne sur le recall à cent, mais a performé de manière similaire sur les autres métriques. Bien que BM vingt-cinq ait eu une performance significative par rapport à la Biancodeur entraînée, sa performance indique qu'il s'agit toujours d'une base solide pour la récupération spécifique au domaine. En ce qui concerne l'évaluation zéro shot de la Biancodeur siamienne, nous constatons que l'utilisation directement des embeddings d'un modèle Kamembert préentraîné sans optimiser la tâche de récupération d'informations donne de mauvais résultats, ce qui est cohérent avec les précédents. Et le Biancoder basé sur WordToVec a considérablement surpassé le modèle basé sur Fastex et Bird, ce qui suggère que peut-être les embeddings au niveau de mots pré-entraînés sont plus appropriés pour la tâche que les embeddings au niveau de caractères ou au niveau de sous-san lorsqu'ils sont utilisés hors de boîte. Bien que prometteurs, ces résultats suggèrent une ample opportunité d'amélioration par rapport à un expert qualifié qui peut éventuellement récupérer tous les articles pertinents pour n'importe quelle question et ainsi obtenir des scores parfaits. Concluons en discutant de deux limitations de tous les datasets. Tout d'abord, le corpus d'articles est limité à ceux collectés de la trente deux considérées codes belges, ce qui ne couvre pas l'ensemble de la loi, car les articles de décrets, directives et ordinances sont absents. Durant la construction du dataset, toutes les références à ces articles non collectés sont ignorées, ce qui cause une question de finir avec une fraction de la part initiale de nombre de relevant articles. restant en relevance pourraient être incomplets, although elles sont toujours complètement appropriées. Second, nous devons noter que not toutes questions juridiques peuvent être répondues avec des statuts alone. Par exemple, la question can I évacuer mes tenants s'ils font trop de bruit ne peut pas avoir une réponse détaillée avec une statutory loi qui quantifie un threshold de bruit spécifique à laquelle l'éviction est autorisée. Instead, le landlord devrait probablement relire plus sur la case law et trouver des precedents similaire à la situation actuelle. Par exemple, le tenant fait deux parties par semaine jusqu'à 2 h. Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche de récupération d'articles statutaires, et le domaine des moins adaptées reste à déterminer. Nous espérons que tout ce travail suscite l'intérêt pour développer des modèles de récupération d'articles statutaires pratiques et fiables qui peuvent aider à améliorer l'accès à la justice pour tous. Vous pouvez consulter notre article DatSetEnCode sur les liens suivants. Merci."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "fr", "output": "Hello, nous sommes heureux de présenter notre travail sur VALS, un benchmark indépendant fait pour testing vision et language modèles avec des phénomènes linguistiques spécifiques. Pourquoi avons nous fait le trouble en setting up ce benchmark? Eh bien, durant les dernières années, nous avons vu une explosion de transformer basé vision et language modèles pré trained sur de large amounts d'image text pairs. Each one of these models pousse state of the art sur vision et language tasks, tels que visual question answering. réponse, raison de bon sens visuel, retrieu d'image, fondement de phrase. Nous avons donc reçu un message, les accurements sur ces benchmarks spécifiques à la tâche augmentent régulièrement, mais savons-nous ce que les modèles ont réellement appris? Qu'est-ce qu'un modèle visuel et language a compris en assignant un score élevé pour cette image et cette phrase pour la matchner et un score élevé pour celle-ci? Les modèles visuels et language se concentrent sur la bonne chose ou se concentrent sur des préjugés? comme on l'a vu dans le travail précédent. Pour s'exprimer davantage sur ce point, nous proposons une direction plus sans task et introduisons des valves qui testent la sensibilité des modèles visuels et language à des phénomènes linguistiques qui affectent les modalités linguistiques et visuelles. Nous visons l'existence, la pluralité, le comptage, les relations spatiales, les actions, et les références d'entité. Mais comment testons nous les modèles visuels et language qui ont capturé ces phénomènes? Foiling, une méthode précédemment appliquée pour vision et language modèles, seulement pour les phrases par Ravi Shakar et collaborators et sur le compte de l'image. Foiling signifie essentiellement que nous prenons la caption d'une image et produisons un foil en alterant la caption de telle sorte qu'elle ne décribe pas l'image anymore. Et nous faisons ces phrase alterations en focusant sur six pièces, telles que existence, pluralité, compte, spatial relations, actions, et entité co reference. Chaque pièce peut consister de plusieurs instruments, en cas de trouver plus d'une façon intéressante de créer des foils. Par exemple, dans le cas de la pièce actions, nous avons deux instruments, un dans lequel le verb action est changé avec une action différente et un dans lequel les actants sont échangés. Le compte et le correspondent sont également des pièces qui ont plus d'un instrument. Et nous créons ces foils en veillant à ce qu'ils ne décrivent pas l'image, qu'ils sont des sentences grammaticales et autres valides. Ce n'est pas facile à faire parce qu'une caption foilée peut être moins likable que la caption originale. Par exemple, though it's not impossible, il est statistiquement moins likable pour plantes de cutter un homme que pour un homme de cutter des plantes et large vision et language modèles pourraient pick up on this. Par conséquent, pour obtenir des foils valides, nous devons prendre action. First, nous utilisons des modèles language strongs pour proposer des foils. Second, nous utilisons la langue naturelle inference ou short NLI pour filtrer les foils qui pourraient être encore décrits toujours l'image, car lorsque nous construisons des foils, nous devons nous assurer qu'ils ne décrivent pas l'image. Pour tester cela automatiquement, nous appliquons une inference en langage naturel avec la raison suivante. Nous considérons une image comme étant la prémisse et sa caption comme étant l'hypothèse entaillée. En outre, nous considérons la caption comme étant la prémisse et la foille comme étant l'hypothèse. Si un modèle NLI prédit que la foille contredit ou est neutre par rapport à la caption, nous prenons cela comme un indicateur d'une foille valide. prédit le foil à être entailed par la caption, il ne peut pas être un bon foil, puisque par transitivité, il donnera une description truthful de l'image et nous filtrons ces foils. Mais cette procédure n'est pas parfaite. Il est juste un indicateur pour validifier les foils, donc, comme une troisième mesure pour générer des foils valides, nous employons des annotators pour valider les données utilisées dans Vals. Après filtrer et évaluer par Vals, nous avons assez de tests instances comme descrit dans cette table. Notez que Vals ne délivre pas de training. mais seulement des données de test, car il s'agit d'un benchmark de test de zéro tir. Il est conçu pour tirer parti des capacités existantes des modèles de vision et de langue après la pré-entraînement. Le fine-tuning ne permettrait que des modèles d'exploiter des artefacts ou des biais statistiques dans les données. Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis. Et comme nous l'avons dit, nous sommes intéressés par l'évaluation des capacités des modèles de vision et de langue après la pré-entraînement. Nous expérimentons cinq modèles de vision et de langue sur Valve, notamment avec CLIP, Wilbert, Wilbert Kelvin I et Visual Bert. Deux de nos plus importantes métriques d'évaluation sont la précision des modèles en classifiant les paires d'images en captions et en foils. Parmi les plus pertinents pour cette vidéo, nous allons présenter notre plus permissive métrique, la précision des paires, qui mesure si le score d'alignement des images est supérieur pour la correcte image texte que pour sa paire de foil. Pour plus de métriques et de résultats sur lesquels vous pouvez consulter notre article, les résultats avec précision des paires sont présentés ici et sont consistants. avec les résultats que nous avons obtenus de l'autre métrique. C'est que la meilleure performance zéro shot est achievée par Wilbert douze en un, suivi par Wilbert, Alex Mert, Clip, et finalement VisualBird. Il est notable comment les instruments centres sur les individuels objects comme existence et noun phrases sont presque résolus par Wilbert douze en un, en highlightant que les modèles sont capables d'identifier les objets nommés et leur présence dans les images. Cependant, none de ces pièces restantes peuvent être reliablement résolues dans nos adversarial foiling settings. instruments que vision et language modèles ont trouble distinguer les références à un seul versus multiple objects ou counting les images. Le relation piece montre qu'ils ont difficulté en classifiant correctement une relation spatiale nommée entre objets dans une image. Ils ont également trouble distinguer les actions et identifier leurs participants, même si supportés par plausibilité biases, comme nous le voyons dans le actions piece. De la reference piece, nous découvrons que tracer multiples références à la même objecte dans une image en utilisant des pronouns est également difficile pour vision et language modèles. En tant que chef de santé et parce que c'est un expérience intéressante, nous avons également benchmarqué deux modèles GPT un et GPT deux pour assister whether Valve est solvable par ces unimodal modèles en computant la perplexité de la correcte et de la foiled caption, et en prédictant la entrée avec la plus faible perplexité. Si la perplexité est higher pour le foil, nous prenons cela comme indication que la foiled caption peut souffrir de plausibilité bias ou autres biases linguistiques. Et c'est intéressant de voir que dans certains cas, Les modèles de texte unique GPT ont capturé la plausibilité du monde mieux que les modèles de vision et language. Donc, en résumé, VALS est un benchmark qui utilise la lente des constructions linguistiques pour aider la communauté à améliorer les modèles de vision et language en testant leurs capacités de visionnement. Nos expériences montrent que les modèles de vision et language identifient les objets nommés et leur présence dans les images bien, comme le montre l'existence, mais structurent à groundir leurs interdépendances et leurs relations dans les scènes visuelles lorsqu'ils sont forcés de respecter les indicateurs linguistiques. Nous aimerions vraiment encourager la communauté à utiliser Valse pour mesurer les progrès vers la base de la langue avec des modèles de vision et de langue. Et encore plus, Valse pourrait être utilisé comme une évaluation indirecte des ensembles de données, car les modèles pourraient être évalués avant et après la formation ou le fine-tuning pour voir si un ensemble de données aide les modèles à améliorer l'un des aspects testés par Valse. Si vous êtes intéressé, consultez les données de Valse sur GitHub et si vous avez des questions, n'hésitez pas à nous contacter."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Kamizura, je suis de l'université de Tokyo. Je vais vous présenter un papier entitled RNSAM, un large scale de la sélection automatique de la duration de la comitologie. Je vais expliquer en ce sens. Tout d'abord, je vais vous présenter la duration automatique de la duration de la duration que nous sommes en train de travailler dans cette recherche. ReleaseNode est un document technique qui summerise les changements distribués avec chaque release de notre produit. L'image montre les releases de la version deux point six point quatre de la GPUS library. Ces notes jouent un important rôle dans l'ouverture, mais elles sont consommées manuellement. Par conséquent, il serait très utile de pouvoir automatiquement générer des notes de release de qualité. Je vais referre deux précédentes recherches sur automatique de ce node génération. Le premier est un système appelé Arena, released en vingt fourteen. Il prend une approche basée sur les rules, par exemple, en utilisant le changer extracteur pour extraire les différences libérées. Library changes et document changes from the differences between releases et finalement combining them. La plus notable fonction de ce système est l'issue extractive dans le coin supérieur droit, qui doit être reliée à Jira, l'échange de l'économie et peut seulement être appliquée à des projets qui utilisent Jira. En d'autres termes, il ne peut pas être utilisé pour de nombreux projets sur GitHub. Le second est Griff, récentement annoncé en twenty twenty. Il est disponible sur Internet et peut être stocké via PIP. Ce système a un modèle de classification de texte simple et détermine une fonction de cinq règles telles que des fonctionnalités ou des corrections pour chaque message de comité d'entrée. L'image est un modèle de usage qui retient une correction ou des corrections. Les données de traitement de Griffith sont relativement faibles, environ cinq mille et seront montrées dans les expériences décrites ci-dessous. La performance du modèle de classification de texte n'est pas élevée. Je présente deux recherches, mais il y a des problèmes de limité de l'applicabilité et de rarité de données. et scarce date ressources. Our paper résout ces deux problèmes et automatiquement générera des résultats de qualité. Pour le programme d'applicabilité limitée, nous proposons une méthode de classification de qualité utilisant uniquement le message de comité comme input. Cette méthode proposée peut être utilisée pour tous les repositories en langue. Pour le second programme de scarce date ressources, nous avons construit un RNSAMDSET consistant à environ quatre-vingt deux mille pieces de données par collecting des données de publics GitHub repositories utilisant l'API GitHub API. Next, je décris notre date. Voici un exemple de date. Le left side est un commit message et le right side est le release notes. Les release notes sont relevés comme improvements, bug fixes, etc. Nous avons mis en place une tâche qui prend les commits messages comme input et outpacte les relevés. Cela peut être regardé comme une tâche de summation. Nous avons prédefinis quatre relevés, features, improvements, bug fixes, déplications, removables et les changements. Ils étaient basés sur des recherches et autres facteurs. Les notes sur le bottom right sont extraites de la note sur le bottom left. À ce stade, il est nécessaire de détecter les quatre rubriques qui ont été mises en place, mais les rubriques ne sont pas toujours consistantes avec chaque répétition. Par exemple, les improvements de rubriques includes improvements, enhancements, optimisations, etc. Nous avons préparé une vocabulaire list ou studie rubriques pour chaque notation. variations. Utilisez l'aide pour détecter les raisons et corriger le texte de la raise qui fait sur la raise pour la raise. Next est un message. Les messages de commit sont notés à chaque raise. Comme on le montre dans l'image below, si la raise actuelle est version deux point cinq deux neuf, nous devons identifier la raise version deux point cinq deux eighteen et obtenir un diff. C'est un peu tedious et il ne suffit pas de juste obtenir une raise de raises et de regarder. et regardez le before et après. Nous avons créé un heuristique matching blue pour obtenir les précédents et les suivants. Desset analysis. En fin de compte, sept mille deux cents repositories et quatre-vingt deux mille pieces de données ont été correctées. Ainsi, le nombre de releases notes tokens est sixty three, ce qui est assez élevé pour la tâche de summation. Ainsi, le nombre de tokens uniques est assez large à huit mille huit cent trente mille. C'est due au large nombre de costs uniques et de noms de méthodhodes trouvés dans le repository. Next, je vais expliquer la méthode proposée. Le module de summation crosswise extractive et abstractive consiste en deux modules, un classifier utilisant bot ou code bot et un générateur utilisant bot. First, GAS utilise un classifier pour classifier chaque message de commit en cinq classes, features, améliorations, bug fixes, déplications, presses et autres. Les messages de commit classifiés comme autres sont discardés. Ensuite, GAS applique un générateur à la fourise documents indépendamment et génére un reader pour chaque classe. Dans cette tâche, les correspondances directes entre les messages de commit et les notes de reader ne sont pas connues. Par conséquent, pour trainer le classeur, nous assignons deux rebers à chaque message de commit en utilisant les dix premiers caractères de chaque message de commit. Nous modélisons la classeur de la rédaction abstracte de l'approche par deux méthodes différentes. Le premier modèle, que nous appelons GAS single Sync consiste d'un single sec network et générate un single long list note text, donne un concatenation de messages de comité. Le output text peut être divisé en classe par segment basé sur des symboles de classes spécifiques. La seconde méthode, méthode, que nous appelons GSMAUCH, consiste de quatre différents sec networks, each of which correspond à l'une des list notes classes. OK, laissez moi expliquer l'expérience. Cinq méthodes sont ont été comparées GS, GS Single, GS Marge, Russelling, et précédent studie Griff. Regarding abrégation, dans certains cas, ces notes sont outputées en multiples sentences. Since il est difficile de calculer le nombre de sentences à zéro, elles sont combinées avec spaces et traitées comme une longue sentence. Le brew est pénalisé lorsque le système outputs une short sentence. Cette pénalité résulte en une lower blue value dans les expériences descrites. Finalement, nous avons également calculé la spécificité parce que lose and brew ne peuvent pas être calculés si les releases sont empty. Une spécificité signifie que les modèles calculés sont empty text en cas où les releases sont empty. Voici les résultats. Since le dataset contient des adresses d'email, des barrières, etc, nous avons également élargi le grain dataset, qui exclut les grains. GAS et GAS ont acheté des scores de lose plus de dix points hauteur que les bases. Cependant, sur le test Green, le score gap entre la méthode proposée et la base a augmenté de plus de vingt points. Ces résultats indiquent que GAS et GAS sont significativement efficaces. GAS a obtenu un meilleur score de louche que GAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour entraîner le classificateur en utilisant des pseudo-divers. La couverture élevée de GAS peut être atteinte correctement car le classificateur peut se concentrer sur la sélection des messages de commande pertinents pour chaque classe. Xi's march tend à yer plus de richeur que Xi's single, suggestant qu'il est également effectif de développer différents modèles de summation de la perspective pour chaque graphique. Hour et Eronus Xi's methods tend à output de plus de sentences que les human reference sentences. In la figure sur la droite, la reference sentence a trois ou quatre sentences, while Xi's a seulement un. La raison pour cette modèle reluctant est que dans les données de training, seulement trente trois pour cent de la sentence sont présentes dans le features rabble et quarante pour cent dans le rabble d'amélioration. Furthermore, les méthodes CS ne peuvent générer accuratement sans additionnal information. Le top exemple sur la droite est un exemple de très message de commande et la complète sentence ne peut pas être générée sans référence à la correspondante requête ou issue. L'exemple de l'autre montre que les deux commandes de message dans l'input sont related et devraient être combinées en une seule sentence. Mais il ne peut pas le faire. Finalement, une conclusion. Nous avons créé un nouveau jeu de données pour la génération de notes de référence automatique. Nous avons également formé la tâche de saisir des messages de commande et de les résumer pour qu'il soit applicable à tous les projets écrits en anglais. Nos expériences ont montré que la méthode proposée générait moins de notes de référence noisibles à une couverture plus élevée que la base. Veuillez consulter notre jeu de données sur GitHub. Merci."}
