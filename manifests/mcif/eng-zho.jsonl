{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 0, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/ICWfTnUMio.wav", "src_ref": "Hi! My name is Matthias Lindemann, and today I'm going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don't use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they're not ordered. That's why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don't know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That's because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.", "tgt_ref": "嗨！\n我叫 Matthias Lindemann，今天我将简要介绍我们的论文《使用多重集标记和潜在排列实现无树组合泛化》。\n这是我与导师 Alexander Koller 和 Ivan Titov 合作编写的论文。\n组合泛化可以理解为学习者处理更深层次的递归和在训练期间单独见过的短语的未见组合的能力。\n在语义解析的背景下，组合泛化的测试可能如下所示。\n和往常一样，我们有一组语句训练集。\n其中包括句子“The girl slept.”（女孩睡了。）\n以及“Mary knew that the girl slept.”（玛丽知道那个女孩睡了。）\n这些句子对应于逻辑表达式，表示其核心语义。\n与标准机器学习评估相比，我们的测试集并不来自同一分布，而是包含结构上未见过的逻辑表达式。\n在这个例子中，模型在训练时见过浅层递归结构，而在测试时需要处理更深层递归。\n单纯的 seq2seq 模型难以处理这种分布外泛化，并且经常产生与输入无关的输出。\n特别是，它们通常无法再现输入和输出之间的系统对应关系，例如示例中颜色标注的对应关系。\n解决这个问题的一种常用方法是将树整合到模型中。\n树的作用是捕捉将语音句子与逻辑表达式联系起来的组合过程。\n这种方法效果很好，但是树结构通常不可直接获得，需要通过某种方式来获取。\n这可能很复杂，有时是一个计算成本高昂的过程。\n通常，这需要对逻辑表达式进行大量的形式化特定的预处理，例如处理变量符号。\n获得树形结构也可能涉及专门的语法诱导程序。\n在本文中，我们不使用树，而是引入了一个神经 seq2seq 模型，该模型直接对输入片段和输出片段之间的对应关系进行建模。\n我们首次在不依赖于树的情况下，实现了对更深层递归的强大泛化能力。\n我们的方法分两个步骤预测输入的输出。\n首先，我们使用会出现在输出中的无序多重集词元来标记每个输入词元。\n第一步完成后，我们有了所有正确的词元，但它们并没有排序。\n因此，在第二步中，我们使用另一个模型来预测排列，以便保持正确的顺序。\n我们引入了一种新的方法来预测排列，而这种方法对可能的排列没有任何硬性约束。\n这使得我们的方法具有很强的灵活性和表现力。\n从概念上讲，我们的排列模型的大致运作方式如下。\n我们从左到右查看输出，并确定在每个位置放置哪个多重集词元。\n对于第一个输出位置，我们只选择一个，如红色高亮部分所示。\n然后我们跳转到下一个多重集词元，以确定输出中的第二个词元。\n我们通过跳转到另一个多重集词元，以类似的方式确定输出中的第三个词元。\n我们继续这个过程，直到第一阶段的每个词元都被访问了一次。\n为了让大家对实验结果有一个大致的了解，我们将我们的方法与其他无树模型在 COGS 基准上进行了比较。\n在更深层递归的泛化能力方面，我们的模型远远超过了其他模型。\n但在某些其他类型的结构泛化上仍面临挑战。\n在论文中，我们解决了一些有趣的技术挑战。\n首先，输入与输出的对齐关系在训练数据中未明示。\n因此，对于给定的词元，我们不知道它来自哪个多重集，这给训练带来了挑战。\n此外，有时存在多个与数据一致的排列，但语言上正确的排列是隐藏的。\n我们通过在训练中诱导对齐来解决这一问题。\n我们的排列方法非常灵活，但它也带来了一个挑战，即找到得分最高的排列是 NP-hard 问题。\n这是因为这与“旅行商”问题相关。\n我们通过一个 GPU 友好的连续松弛来近似求解，这也使我们能够通过解决方案进行反向传播，学习更符合语言学的排列。\n如果您想了解更多关于我们的实验以及我们如何应对这些挑战的信息，请查看我们的论文或了解我们的海报。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "ICWfTnUMio.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 1, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/JRrbTnEZbF.wav", "src_ref": "Hi, I'm Myra and today I'll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don't generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn't account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren't overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I'll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they'll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we'll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there's a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don't know if it's because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can't make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.", "tgt_ref": "大家好，我是 Myra，今天我将谈谈我们的论文《有标记角色：使用自然语言提示来衡量语言模型中的刻板印象》。\n这篇论文是我与 Esin Durmus 和 Dan Jurafsky 合作完成的。\n近年来，许多人记录了社会偏见和成见在大语言模型 (LLM) 中的普遍程度。\n然而，这些措施有各种各样的局限性。\n它们通常依赖于手工构建的数据集，策划这些数据集非常耗时，而且通常只能衡量非常具体的刻板印象，这意味着这些措施无法很好地推广至其他人口统计学特征或语境，或者只能捕捉非常笼统的广泛关联，如与特定群体的负面关联。\n此外，这方面的大多数研究都没有考虑到交叉性，即多方面的社会身份可能会加剧偏见，并成为独特的伤害源头。\n为了克服这些局限性，我们利用了一种特性，即这些新型的、经过指令调整的 LLM 非常擅长回应指令和提示。\n因此，我们可以要求模型生成一个角色，而这个角色是一个想象中的个体，例如，使用的提示可以是：“想象一下，你是一个亚洲女性。\n请描述一下你自己。”。\n我们可以立即看到，对于任何人口特征来说，都可以应用这一方法，因为我们可以在这个提示中指定任何我们想要的身份标记。\n以下是 GPT-4 生成的一些示例。\n可以看到，虽然输出并非在传统意义上明显的负面或有害，但其中有一些有趣的模式。\n亚洲女性被描绘成“谦逊的”；中东女性则被用“异域”和“迷人”等词来形容，代表中东是一个迷人的地区。\n两个有色人种女性角色的输出内容中都提到了出身，而白人男性角色则没有这方面的内容。\n为了捕捉这些模式，我们的方法分为两部分。\n第一部分是生成这些角色。\n我们生成这些角色的提示语是受到一项研究的启发，该研究向人类受试者提供了这些提示词，发现通过向人类受试者提供这些提示词，也能够揭示种族刻板印象。\n这也使得我们生成的人物角色和人类书面反应之间能够进行直接比较。\n第二部分是使用“标记词”，这是识别区分显性群体和非显性群体的词语的方法，稍后我会详细说明。\n这样做的好处是，我们无需依赖任何特定词汇，就能得到非常具体的刻板印象和模式。\n因此，“有标记词汇”方法借鉴了“标记理论”这一社会语言学概念，该概念指出存在一个无标记的默认词汇，任何与该默认词汇不同的群体在语言上都是有标记的。\n例如，“战士”一词通常与男性相关联。\n因此，当人们描述一个女性战士时，他们通常会实际上指定其为“女战士”，并用“女”来标记这个词。\n更广泛来说，社会中的主导群体在语言和社会角度上都是无标记的，而边缘化群体通常是带有标记的。\n因此，在我们的方法中，我们首先指定无标记群体和有标记群体，然后我们使用“战斗词汇”方法来比较角色，该方法基本上是使用加权对数比来区分每个标记群体的最常用词。\n例如，对于黑人女性的角色，我们将使用“战斗词汇”，并将对数比率与白人角色和男性角色进行比较，因为这是两个相应的无标记群体。\n现在来看看结果。\n首先，我们使用了一份刻板词汇表，发现生成的角色比人类书写的角色包含更多的刻板印象。\n然而，当我们真正观察单词和词汇的分布时，我们发现情况截然不同。\n虽然生成的角色包含更多的刻板词汇，但人类书写的角色包含的词汇分布更广，而生成的角色中的刻板词汇实际上只有“tall”（高大）和“athletic”（运动型）这两个词。\n所以，实际上只有正面的词语，或者至少是非负面的词语。\n事实上，这个词汇表并没有很好地捕捉到我们在之前的幻灯片中看到的许多有害模式。\n因此，我们将转向“有标记词汇”方法的结果，来展示这些看似正面的词汇是如何助长成见和固化叙事的。\n在我们的分析中，我们揭示了这些看似正面的描述是如何反映出有害模式的。\n首先，在我们的群体中，最常用的词包括“culture”（文化）、“tradition”（传统）、“proud”（骄傲）和“exotic”（异域）。\n这些词只通过与身份的关系来定义这些群体，并将他们与白人标准区分开来。\n这导致了这些群体长期以来遭受歧视和排斥的现象。\n此外，这些词汇反映了很多常见的成见，尤其是针对有色人种女性的成见。\n例如，用来形容拉丁裔妇女的词语包括“vibrant”（充满活力）和“curvaceous”（曲线优美），这与热带主义的氛围息息相关。\n对于亚裔女性，词汇包括“petite”（娇小）、“delicate”（精致）和“silky”（柔滑），这与亚裔女性长期以来被过度性别化、被视为非常温顺和顺从等等的悠久历史有关。\n最后，对于黑人女性，我们看到一些最常用的词是“strong”（强大）和“resilient”（坚韧）。\n这与人们所说的“强大的黑人妇女”原型有关。\n乍一看，这听起来很正面，但有研究表明，这种原型实际上是非常有害的，因为它给这些人群带来了很大的压力，要求他们要有适应力，要坚强地面对社会障碍。\n因此，这类词汇并非有助于改变这些障碍，而是给这些人施加压力，要求他们克服这些障碍，这会给他们的健康带来非常负面的后果，以及其他危害。\n更广泛地说，我们发现每个有标记群体的词汇几乎都只是反映了非常本质化的叙事。\n因此，基于这些模式，我们为模型所有者提出了三项建议。\n首先，作为研究人员，我们应该解决正面的成见和固化叙事。\n我们还应该使用交叉视角来研究偏见和伤害，因为如果我们不这样做，可能会忽略很多事情。\n最后，人们真的应该提高偏见缓解方法的透明度，因为例如，像这些积极的刻板印象，我们不知道到底是因为存在某种奇怪的过度价值观调整，还是说有其他一些反刻板印象方法导致了这些有害的模式。\n如果没有更多的透明度，我们就无法做出任何假设，也无法进一步研究。\n谢谢大家的聆听。\n祝大家在 ACL 愉快。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "JRrbTnEZbF.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 2, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/JhbtCwcsWY.wav", "src_ref": "Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.", "tgt_ref": "大家好，我是 James Finch。\n我是 Sarah Finch。\n今天，我们将向大家介绍 ABC-Eval，这是一种用于评估对话型 AI 的新型方法。\n这项研究是由埃默里大学 Jinho Choi 教授领导的埃默里 NLP 实验室与 Amazon Alexa AI 合作完成的。\n假设你刚刚开发了一个对话模型，你想看看它与当前最先进技术相比的表现如何。\n常见的做法是使用人工评估，例如，让人类评估者选择两个对话中哪一个更好，或者使用李克特量表对对话进行评分。\n这些方法可以很好地提供对话质量的整体评估，但对话质量包括很多方面。\n因此，你可能需要评估聊天质量的多个维度，以便更细致地了解模型的优缺点。\n一种方法是简单地让人类评估者评估对话质量的几个方面，例如使用现有的比较或李克特量表方法来评估模型回答的相关性。\n然而，我们认为有一种更精确可靠的对话评估策略。\n我们的方法试图通过明确注释每个模型响应是否表达某些行为来减少人工评估的主观性，例如用不相关的信息回复或自相矛盾。\n我们称这种方法为对话中的注释行为，简称 ABC-Eval。\n我们开发这种方法，目的是全面涵盖最近文献中已被认为影响聊天质量的聊天模型行为。\nABC-Eval 能够测量聊天模型犯下各种主题错误的速度。\n例如，ABC-Eval 可以衡量聊天模型忽略其对话者，或者说出不相关内容、自相矛盾或与对话者矛盾、幻觉不正确事实或违反常识知识的次数，以及模型成功或未能成功表现出同理心的次数。\n为了确定哪种评估方式最为有效，我们选择了四种最先进的聊天模型，并使用 ABC-Eval 对每个模型的 100 次人机对话进行评估。\n为了进行比较，我们还使用三种现有方法来评估这些对话：回合层面李克特评分、对话层面的李克特评分，以及对话层面的成对比较。\n对于现有的每种方法，我们收集了对话中八个最常见的衡量方面的评估，因为这是沿多个维度评估聊天模型的标准做法。\n从对这些评估结果的分析来看，我们发现，在 100 个双标签对话中，ABC-Eval 行为标签在注释一致性方面，总体上比现有方法更可靠。\n此外，与现有方法产生的指标相比，ABC-Eval 标签对整体对话质量的预测性更强，如这个简单的线性回归分析所示。\n例如，可以看到，自相矛盾和与对话者矛盾的回合比例分别解释了 5% 和 10% 的对话质量，而平均李克特一致性得分仅解释了 4% 或更少。\n最后，我们使用逐步线性回归来检查每个评估指标是否捕捉到对话质量的独特方面。\n可以看到，所有 ABC-Eval 指标的组合解释了超过 25% 的对话质量，并且当删除任意一个指标时，其中大多数会导致丢失大量质量信息。\n另一方面，所有回合层面的李克特指标的组合对质量的解释性要低得多，其中包含独特信息的指标也更少。\n这表明 ABC-Eval 的指标更具可靠性、信息量更丰富，并能从多个维度对对话型 AI 进行更精细的评估。\n在我们的实验结果中可以看到，仍然存在一些量化明确的挑战。\n例如，我们测试的机器人在大约 20% 的回复中违反了常识。\n在大约 15% 的回复中，它们提供了不相关的信息，并且在大约 10% 的回复中，它们自相矛盾或与对话伙伴矛盾。\n随着该领域的快速发展，自我们进行评估以来，许多新发布的模型的错误率可能有所下降。\n然而，这样就更有理由去寻求可靠且精确的评估指标来比较模型。\n我们希望 ABC-Eval 能够被该领域的其他人员利用，作为朝着这个方向迈进的有意义的一步。\n我们期待看到对话型 AI 在未来几个月和几年中的发展。\n感谢观看。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "JhbtCwcsWY.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 3, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/MjDvRpkOFq.wav", "src_ref": "Hello, my name is Vasudha and I'm a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don't think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.", "tgt_ref": "大家好，我叫 Vasudha，是斯托尼布鲁克大学的计算机科学博士生。\n我想介绍一下我们的论文：《用于检测失调的迁移学习：应对罕见类别挑战》，这篇长篇论文已被 ACL 2023 录用。\n首先，我们来定义一下什么是认知失调，以及为什么它是语言学习中的一个重要问题。\n简而言之，认知失调是指两种信念或行为之间不一致，比如这个例子，一个人说：“我知道香烟会害死我”，然后又说：“我在会后抽了几根烟”。\n这种信念和行为是不一致的，它们处于失调状态。\n此人还提到“没有烟草，我认为我就无法保住工作”，这就是第二种情况。\n这两者之间存在共鸣关系。\n虽然失调是我们在日常决策中遇到的一种非常常见的现象，但在其他类型的话语关系中，很少能在语言中找到它们的表达。\n不过，这一点为什么很重要呢？\n研究认知失调可以帮助我们理解人与人之间分歧的影响，跟踪趋势和信仰价值观，以及人群态度的变化。\n高度的认知失调也与焦虑症有关，可以帮助我们更好地了解人们的心理健康。\n研究语言中表达的失调也有助于理解弱势群体的极端主义和两极分化。\n最后，认知失调对于理解个人的认知风格非常重要，它可以帮助我们更好地理解决策过程。\n为了创建认知失调资源，我们对失调关系进行了大规模的注释。\n我们使用了“失调为先”的方法，如此处的流程图所示。\n推文使用 PDTB 解析器传递，并根据我们论文中描述的指南对篇章单元对进行了注释。\n从这里可以看出，只有 3.5% 的注释对存在失调情况。\n在收集了大约 1,000 个篇章单元对的例子后，我们对一个初始分类器进行了训练，该分类器仅对 43 个失调的例子进行了训练。\n毫不奇怪，分类器的表现并没有比随机猜测更好。\n鉴于失调的出现率很低，并且之前没有任何此类数据集，我们面临的是绝对稀有性的问题。\n为了缓解这一问题，我们对迁移学习和主动学习的组合进行了实验，以便在较少的注释运行中收集更多的失调样本，从而降低总体注释成本，同时提高失调检测率。\n由于初始模型根本无法捕捉到失调类别，因此我们通过从密切相关的任务中迁移权重来开始主动学习过程。\n我们从两个不同的任务中迁移：主题独立的失调立场分类，这个任务用于确定来自不同人的两个辩论声明是否一致，无论主题如何，在这里称之为辩论，以及 PDTB 的扩展和比较类的二元分类，因为这两者与失调和不失调的概念密切相关，我们在这里称之为 CE。\n我们发现，在迁移注释数据集上的零样本表现时，已经比随机猜测更好，最好的 AUC 为 0.62。\n此外，在两个任务上的迭代微调中，我们发现微调 CE 任务，然后进一步微调辩论，可以产生更好的零样本表现。\n因此，这是我们用于冷启动主动学习的模型。\n接下来，我们确定了使用每轮主动学习和注释中的新数据来更新模型的最佳方法。\n“累积”是指迄今为止从主动注释中收集的所有数据，而“迭代”是指通过对最新收集的数据集进行训练来更新模型。\n在不同的策略中，我们发现“累积”的表现在各个方面都等于或优于“迭代”。\n接下来，为了增加失调示例的数量，我们使用罕见类概率策略 (PRC) 来选择大部分示例，这些示例在任何一轮罕见事件中都极有可能被当前模型所继承。\n我们将此策略与社区中常用的其他最先进的 AL 策略进行比较。\n我们发现，虽然差异不大，但该 PRC 策略比其他最先进的策略更有效。\n请注意，随机策略的表现要差得多。\n在采用两种最佳策略的进一步 AL 轮次中，我们将失调分类 AUC 提高到 0.75，这是我们迄今为止在该任务中取得的最佳表现。\n我们还检查了每种策略在注释质量和注释成本方面的可行性。\n我们发现，PRC 的失调百分比最高，并且对稀有类别效果最佳。\n然而，注释者也发现这些例子很难。\n总结来说，我们发现，PRC 是一种用于稀有类别获取和冷启动 AL 的简单 AL 策略，它具有适当设计的迁移学习任务，并能够提供显著帮助。\n我们还发现，迭代更新对于来自不同领域的迁移学习很有用，而在领域内活跃的注释则受益于累积更新。\n这些是我们核心数据集和论文的链接。\n如果大家有任何疑问，欢迎随时与我们联系。\n谢谢。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "MjDvRpkOFq.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 4, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/MmiKtcykVs.wav", "src_ref": "Hello everyone, I'm Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there's a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here's an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.", "tgt_ref": "大家好，我是 Akshatha，今天我和我的合著者 Martin 将介绍我们的论文《KITMUS 测试：评估来自多个来源的知识整合》。\n研究是麦吉尔大学/Mila 和微软研究院之间的合作成果。\n自然语言理解模型利用多种知识来源，例如通常包含预训练获得的参数中包含的知识，以及在推理时输入的知识。\n最近在问答等任务中的研究表明，模型可以使用预训练时期的知识来解决任务。\n但是，自然语言理解通常需要在推理时提供的知识。\n例如，在“John saw the newly elected president on TV.”（John 在电视上看到了新当选的总统）这句话中。\n预训练参数可以包含关于总统的工作以及电视是什么的信息，但它们无法可靠地知道这个实例特定的实体“John”是谁，或者新总统是谁，因为自预训练以来，总统可能已经发生了变化。\n因此，成功的知识密集型 NLU 任务模型需要具备整合和使用预训练时期的能力和推理时期知识的能力。\n在本文中，我们提出了一个用于知识整合的诊断测试套件。\n我们引入了一个共指消解任务，旨在探索利用不同来源中可用知识的能力。\n我们使用人类研究参与者和已建立的共指消解模型来评估数据集。\n以下是我们数据集的一个例子。\nServin 是一名法官。\nKea 是一名面包师。\nServin 和 Kea 在公园相遇。\n在法庭上审理一天案件后，他很高兴能够放松一下。\n这里的任务是确定代词“he”指代的正确实体，在这一示例中，指代的是 Servin。\n要解决代词问题，我们需要两类信息。\n首先是关于实体的知识，例如“Servin 是一名法官。”\n其次是背景知识，例如“法官在法庭上对案件做出裁决。”\n一般来说，背景知识是在大语言模型的预训练过程中学习的，而特定于实体的知识通常是在推理时观察到的。\n我们改变这两种信息的可用性，使其可以在单一来源或多个来源中找到。\n我们定义了 KITMUS 的三种设置。\n首先，我们有典型的设置：“背景-预训练”，其中背景知识被认为是在预训练时可用的。\n其次，有一个“背景-两者”设置，其中背景知识在预训练时和推理时都可用。\n最后，“背景-推理”设置，其中两种知识类型仅在推理时可用。\n最后一个设置特别有趣，因为它模拟了解决任务所需的背景知识不是模型预训练数据一部分的情况。\n例如，因为自预训练以来出现了新的职业。\n以下是一个例子，说明我们如何控制真实来源中事实的可用性。\n在“背景-预训练”设置中，我们假设“政治家寻求在政府中获得选举席位”这一背景知识包含在预训练参数中，并在推理时提供实体特定知识“Chichester 是一位政治家”。\n在“背景-两者”设置中，我们不仅提供实体特定的知识，而且还提供关于政治家的背景知识，以供推理时使用。\n在“背景-推理”设置中，我们提供了虚构的职业“mirituer”而不是政治家，因为“mirituer”不太可能包含在预训练的参数中。\n我们使用人类研究参与者和已建立的共指消解模型来评估数据集。\n在这张图中，我们展示了表现最佳的模型在“背景-预训练”设置中最困难变体上的结果。\n如果没有在 KITMUS 上进行特定任务的训练，两个模型都表现不佳。\n然而，当在 KITMUS 上训练后，C2F 和 BERT4Coref 的表现都明显优于随机情况。\n这表明，当在通用参考分辨率数据集上进行训练时，大多数都学会利用表面线索，而当在已删除此类线索的 KITMUS 上进行测试时，这些线索就不再有用。\n使用虚构知识的其他实验表明，即使是表现最好的模型，也无法可靠地整合仅在推理时提供的反向知识。\n我们论文的主要结论是，许多共指消解模型似乎无法在没有特定任务训练的情况下对来自不同来源的知识进行推理。\n然而，通过特定任务的训练，一些模型成功地整合了来自多个来源的知识。\n尽管如此，即使是表现最好的模型，也似乎难以可靠地整合仅在推理时呈现的反向知识。\n如果你希望了解更多细节，请参阅我们的论文，并在 GitHub 上查看数据集和代码。\n感谢聆听。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "MmiKtcykVs.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 5, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/PJWMkwXVGI.wav", "src_ref": "Hi, I'm Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I'm going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we'll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model's computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.", "tgt_ref": "大家好，我是来自特伦托大学和布鲁诺·凯斯勒基金会的 Sara Papi，我将简要介绍《注意力作为同声传译的指南》论文，这是我与 Matteo Negri 和 Marco Turchi 合作编写的论文。\n什么是同声传译？\n同声传译，或 SimulST，是将口语实时翻译成另一种语言的文本的过程，目的是实现跨语言交流。\n当前的 SimulST 模型有哪些问题？\n通常需要对特定架构进行训练，引入额外的模块进行优化。\n训练过程又长又复杂，例如涉及不同优化目标的训练。\n同时，为了达到不同的延迟目标，需要训练和维护多个模型。\n例如，训练一个平均延迟为一秒的模型，另一个延迟为两秒的模型，以此类推。\n那么我们的解决方案是什么呢？\n首先，使用已经存在的离线 ST 模型，而不需要重新训练或采用特定的 SimulST 架构。\n对每个延迟模式只使用一个模型，并通过特定参数来处理这些延迟。\n并通过音频输入和文本输出之间的注意力机制利用模型已经获得的知识。\n这就是交叉注意力机制，右边有一个示例。\n我们的解决方案是提出 EDAtt，即编码器-解码器注意力机制，这是一种基于注意力指向来决定是否输出部分翻译的策略。\n如果注意力不集中，即其对总和低于最后一个 λ 语音帧的某个阈值 α，则发出一个单词，这意味着收到的信息足够稳定。\n例如，如果我们收到一个包含“I'm going to talk about...”（我要谈论……）的语音块，而我们的模型预测的是德语翻译，我们将查看交叉注意力权重，我们将看到前两个单词指向最早收到的语音帧，而最后一个单词指向最后收到的语音帧，作为 λ 语音帧。\n这意味着将输出前两个单词，而由于交叉注意的总和高于某个阈值 α，我们不会输出最后一个单词，而是等待另一个语音块。\n如果我们继续并收到另一个语音块，而我们的模型预测其他三个单词，我们将查看这些交叉注意力权重，将看到没有单词指向最后的 λ 语音帧。\n这意味着将输出这三个单词。\n如果我们看一下 EDAtt 的主要结果，我们会在图上绘制同步语音翻译的结果，其中一侧是 BLEU，用于衡量翻译质量，另一侧是平均滞后，即延迟度量，我们还会考虑对计算敏感的平均滞后，即模型的计算时间来预测输出。\n因此，我们希望这张图上的曲线尽可能高。\n但是我们也希望它们向左移动。\n我们将其与 Wait-k 策略和local agreement等也适用于离线模型的常用策略进行比较。\n我们还与专门为同声预翻译定制的最先进架构进行比较。\n这些都是德语同声传译翻译策略的结果。\n可以看到，它优于所有适用于离线模型的策略，因为曲线向左移动。\n还可以看到，如果我们考虑实际耗时或对计算敏感的时间，这是最快的策略。\n如果您想了解更多结果，请阅读我们的论文。\n我们还发布了开源代码、模型和同步输出，以便复现我们的研究。\n谢谢大家。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "PJWMkwXVGI.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 6, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/QTlIuodOsA.wav", "src_ref": "Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.", "tgt_ref": "大家好，我叫 Shuheng。\n今天我将介绍我们的论文《CoNLL-2003 命名实体标注器在 2023 年是否仍然有效？》\n让我们开始吧。\n我们的论文研究了使用“命名实体识别”任务（NER 任务）的泛化问题。\n我们观察到，在 CoNLL-2003 中使用模型来开发 NER 已有近 20 年的历史，这自然会引发一些问题。\n首先，这些模型能否泛化到现代数据？\n当我们开发新的标注工具时，需要什么来实现良好的泛化？\n同时，如果我们确实观察到泛化性较差，那么是什么导致了这些模型的表现下降？\n为了调查这些问题，我们开发了 CoNLL++ 数据集。\n这是我们从 2020 年的路透社新闻中收集的数据集，然后使用相同的 CoNLL-2003 注释指南对其进行了注释。\n然后，我们对 CoNLL-2003 上的 20 多个模型进行了微调。\n我们在 CoNLL-03 测试集和 CoNLL++ 上对它们进行了评估。\n最后，我们计算了 F1 的百分比变化，以评估每个模型的泛化性。\n那么，需要什么来实现良好的泛化呢？\n在整个实验过程中，我们发现需要三个主要因素。\n第一个是模型架构。\n通过我们的实验，我们发现 Transformer 模型通常对新数据具有更好的泛化能力。\n第二个因素是模型的大小。\n我们发现，通常较大的模型可以实现更好的泛化。\n最后，我们都知道，微调示例的数量直接影响下游任务的表现。\n我们还发现，更多的微调示例实际上也可以实现更好的泛化。\n针对我们的下一个问题，是什么原因导致某些模型的表现下降，我们有两种假设。\n第一个是自适应过拟合，即通过反复重复使用相同的测试集而导致的过拟合成本，这通常表现为新测试集的收益递减。\n第二个假设是时间漂移，这是由训练数据和测试数据之间的时间间隔增加而导致的表现下降。\n对于数据过拟合，我们从右图中可以看到，红色最佳拟合线的梯度大于 1。\n这意味着我们在 CoNLL-2003 上做出的每个改进单元在 CoNLL++ 上都转化为多个改进单元，这意味着没有收益下降。\n这表明在这种情况下，我们没有观察到自适应过拟。\n那么，时间漂移呢？\n对于时间漂移，我们进行了一项实验，用更新的数据重新训练或继续预训练一些模型，我们发现随着时间间隔的增加，表现出现了下降，这证实了我们的假设，即表现下降的主要原因是时间漂移。\n我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型规模以及更多的微调示例。\n这些都是相辅相成的，我们不能只保留一个因素，而抛弃其他因素。\n同时，我们还发现这里的表现下降是由时间漂移引起的，有点令人惊讶的是，尽管 CoNLL-2003 已经使用了 20 多年，但它并不是由自适应过拟合引起的。\n那么，回到我们在论文标题中提出的问题，CoNLL-2003 标注器在 2023 年是否仍然有效？\n我们发现答案确实是肯定的。\n我们希望我们的论文能够引发更多关于如何提高模型泛化能力的研究。\n最后，请务必查看我们的论文和数据集。如果大家有任何疑问，欢迎随时与我联系。\n非常感谢。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "QTlIuodOsA.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 7, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/UOlPKyCVgg.wav", "src_ref": "Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.", "tgt_ref": "嗨！\n欢迎观看我们的 DEPLAIN 演示，这是一个新的语料库，用于在文档层面和句子层面上识别德语文本。\n我叫 Regina Stodden，我将带大家了解演示的第一部分。\n首先，我们来定义一下什么是文本简化。\n文本简化是一个调整文本的过程，旨在提高特定目标群体对文本的理解，例如有阅读障碍的人群或非母语人士。\n要训练文本简化模型，我们需要平行文本对，例如文档或句子。\n在此处的示例中，可以看到一个复杂的德语句子和它的简单语言翻译的平行对齐句子对。\n如示例所示，有不同的技术可以简化句子，例如词汇替换、从句删除、重新排序或插入单词。\n我们现在推出了新的语料库 DEPLAIN，因为近年来现有语料库存在一些问题。\n例如，这些语料库太小，无法用于训练文本简化模型。\n近年来提出的其他三种模型都是自动对齐的，这意味着它们在对齐方面可能容易出错。\n因此，我们推出了新的语料库 DEPLAIN，它分为两个子语料库：DEPLAIN-apa 和 DEPLAIN-web。\nDEPLAIN-apa 基于新闻文本。\n在 DEPLAIN-apa 中，我们手动对齐了 483 个文档。\n这大约产生了 13,000 个平行句对。\nDEPLAIN-web 这个语料库包括不同的领域，我们对所有这 750 个文档进行了对齐，一方面是手动对齐，另一方面是使用自动对齐方法。\n我们总共得到了 30,450 个句对。\n我们对句对进行了更多的分析，例如，对简化的类型进行了分析。\n正如此处所示，圣经文本比新闻文本或语言学习者文本更加简化。\n在所有层面上，例如词汇简化、结构简化以及整体简化水平。\n此外，可以看到我们的 DEPLAIN 语料库具有多种不同的简化转换。\n例如，在 DEPLAIN-apa 语料库中，其重新排序和单词添加比 DEPLAIN-web 语料库要多得多。\n另一方面，在网页语料库中，我们进行了更多的改写。\n现在，让我们看看可以用这个语料库做些什么。\n大家好，我是 Omar，现在我将介绍我们的数据集 DEPLAIN 的用例。\n对于第一个用例，我们可以评估自动对齐方法。\n近年来，有很多对齐方法，但在机器翻译中，我们有两个用不同语言编写的平行文档，我们想要从这两个文档中提取对齐的句子。\n但在我们的用例中，我们尝试提取两个具有相同语言、相同内容的平行文档中的句子之间的对齐，但它们的复杂性程度并不相同。\n现在，由于我们有数据集 DEPLAIN，其中包含手动对齐的句子，我们可以使用这些句子作为对齐的黄金标准来评估一些建议的对齐方法。\n我们对这些方法进行了一些调整，并在论文中公布了这些实验所需的调整和代码。\n最后，我们得出结论，用于德语文本简化的最佳自动对齐方法是 MASSalign。\n在这篇论文中，你还可以找到用于在你自己的文档中运行此方法的代码。\n我们在论文中展示的第二个用例是通过微调语言模型从复杂输入文本中生成简化文本的自动文本简化用例。\n我们对两种不同的模型进行了微调。\n我们对 long-mBART 模型进行了微调，以产生文档级的简化，我们还对普通基础 mBART 进行了微调，以产生句子级的简化。\n你还可以找到所有检查点，并可以在论文中查看我们实验的得分和评估指标的更多详细信息。\n我们的结论是，这种基本的微调可以产生或可以获得比基线分数更高的分数，我们提出这些结果作为未来自动文本简化问题的基准。\n非常感谢大家的聆听，我们希望在会议期间与大家见面。\n谢谢。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "UOlPKyCVgg.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 8, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/crgYiwKDfX.wav", "src_ref": "Hi, I'm Siyu Yuan from Fudan University. I'm here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it's essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.", "tgt_ref": "大家好，我是来自复旦大学的 Siyu Yuan。\n我将介绍我们的论文《从大语言模型中提炼脚本知识，用于受限语言规划》。\n在日常生活中，人类通常会遵循以目标为导向的脚本，按照分步指令来规划行为。\n以前的研究利用语言模型来规划诸如“做蛋糕”等典型活动的抽象目标。\n已有研究表明，大语言模型可以有效地将目标分解为多个步骤。\n然而，以前的研究主要集中在规划刻板活动的抽象目标上。\n对于有特定限制的目标，如“做巧克力蛋糕”，规划工作仍然研究不足。\n在本文中，我们定义了受限语言规划的问题，该问题对规划目标施加了不同的约束。\n一个抽象目标可以被赋予不同的多方面约束，形成特定的实际目标。\n一个优秀的规划器应该编写出合理的脚本，并忠实于约束条件。\n在本文中，我们首先评估并改进了大语言模型的受限语言规划能力。\n由于没有具体目标的数据集来支持我们的研究，因此我们必须首先获取这些目标。\n如表所示，我们使用 InstructGPT，通过多方面约束来扩展抽象目标，以实现人机交互数据采集。\n我们抽取了 100 个具体目标，并评估了由大语言模型生成的脚本。\n这张表列出了结果的整体准确性。\n我们发现，所有语言模型在规划具体目标方面的结果都不尽如人意。\n然后，我们进行了详细分析，探究学习模型失败的原因。\n图中结果表明，生成脚本的语义完整性是可接受的，但对约束条件的忠实度无法得到保证。\n我们深入探讨了 wikiHow 中定义的更细化的约束条件主题类别。\n图中的热图显示，InstructGPT 的规划表现在不同类别的目标上有很大差异。\n之前的研究表明，语言模型的输出质量差异很大，导致表现不佳。\n因此，我们采用“先过度生成再筛选”的方法来提高生成质量。\n我们首先通过 InstructGPT 的示例展示了约束类型，并根据种子抽象目标获得具体目标。\n然后，InstructGPT 过度生成具体目标的 K 个脚本。\n接下来，我们开发了一个过滤模型来选择高度可信的脚本。\n我们将脚本和目标转换为 InstructGPT 嵌入层，并计算余弦相似度作为相似度分数，以衡量语义相似度。\n此外，我们对包含目标约束关键词的脚本进行奖励。\n只有当目标在目标集中得分最高时，我们才会保留脚本。\n通过我们的方法，InstructGPT 可以生成更高质量的脚本。\n我们的方法在语义完整性和对约束条件的忠实度方面都大大提高了规划能力。\n由于大语言模型的部署成本高昂，因此至关重要的是要实现较小专业模型的语言规划能力。\n创建数据集是实现这一目标的一个重要步骤。\n然而，以往的研究无法针对具体目标进行规划，而手动数据集标注则成本高昂。\n因此，我们遵循了符号知识蒸馏的思路，从大语言模型中提炼出受限语言规划数据集。\n我们将我们的方法应用于构建受限语言规划的数据集，命名为 CoScript。\n总的来说，我们用脚本生成了 55,000 个具体目标。\n为了确保验证集和测试集的质量，我们要求众包工作人员查找并修改错误的样本。\n这张图显示了 CoScript 的约束分布。\n我们发现，CoScript 在生成的具体目标中表现出高度的多元化。\n有了 CoScript，我们可以尝试使用较小但专用的模型进行受限语言规划。\n我们发现，在 CoScript 上微调的 T5 可以生成比大多数大语言模型更高质量的脚本，这表明在合适的数据集上进行适当的训练时，较小模型的表现可以超越较大的模型。\n总结来说，我们建立了受限语言规划问题。\n我们评估了大语言模型的受限语言规划能力，并为大语言模型开发了一种“先过度生成再筛选”的方法。\n我们使用大语言模型生成高质量脚本数据集 CoScript，用于受限语言规划。\n我们希望 CoScript 数据集能成为推进语言规划研究的宝贵资源。\n感谢大家的宝贵时间。\n请在我们的论文中找到有关 CoScript 的更多详细信息。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "crgYiwKDfX.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 9, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/csJIsDTYMW.wav", "src_ref": "Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn't have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn't scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.", "tgt_ref": "大家好，我是 Yanis Labrak，我将向大家介绍我们的论文《DrBERT：用于生物医学和临床领域的法语稳健预训练模型》。\n在本次演讲中，我们首先谈谈医疗保健领域的语言建模。\n然后，我们将介绍我们文章的主要贡献。\n我们介绍了第一个名为 DrBERT 的法语生物医学模型，该模型基于 RoBERTa，并在 NACHOS 上进行了训练，NACHOS 是一个包含从网络爬取的医学数据的数据集。\n我们还介绍了具有多个预训练设置和数据源的模型比较。\n然后，我们介绍了我们在 11 个法语生物医学和临床下游任务中的结果。\n最后，我们总结了实验，并向大家提供了有关如何访问这些模型的更多详细信息。\n自 2018 年发布以来，BERT 已成为解决自然语言处理任务最有效的方法之一，与 Word2vec、fastText 等历史静态和上下文化方法相比，BERT 实现了巨大的性能提升。\n此后，这种模型已经被应用于许多其他语言，例如法语的 CamemBERT，以及生物医学领域的 PubMedBERT 和BioBERT，以及临床领域的 ClinicalBERT，但主要是英语。\n其他语言的专业模型很少，由于缺乏域内数据，这些模型通常基于连续的预训练。\n然而，法语直到现在都没有任何生物医学领域的开源模型。\n因此，我们需要思考，什么样的数据源最适合广泛用途，而这些爬取的数据是临床数据的良好替代品。\n为了回答这个问题，我们将 DrBERT 与我们的 ChuBERT 模型进行了比较，后者基于从南特大学医院数据仓库获得的匿名数据。\n之后，我们自问，需要多少数据来训练一个法语数据的专业模型？\n是 4GB、8GB 还是更多？\n为了回答这个问题，我们首先训练和比较了四个从头开始的模型：第一个版本的 DrBERT，具有 7GB 的 NACHOS；第二个版本是 4GB 的 NACHOS 集；第一个版本的 ChuBERT，这是一个临床模型，含有 4GB 的临床笔记中的句子；以及最终版本的 ChuBERT，其中混合了 4GB 的 NACHOS 集和 4GB 的临床笔记。\n除了这种比较之外，我们还引入了三种在连续预训练中训练的模型，以分析预训练策略的影响。\n一个基于 CamemBERT 的权重，并在 4GB 的 NACHOS 集上进行了训练。\n另一个也基于 CamemBERT，但这次是在 4GB 的临床笔记上训练的，最后一个基于英语生物医学模型 PubMedBERT，并在 4GB 的 NACHOS 集上训练。\n我们总共有七个模型。\n为了评估我们的七个模型，我们收集了公共和私人下游任务的数据，如命名实体识别、分类、词性标记和问题回答。\n这些模型与六个基线模型进行比较，即 CamemBERT OSCAR 138 GB、CamemBERT OSCAR 4GB、CamemBERT CCNET 4GB、PubMedBERT、BioBERT 和 ClinicalBERT。\n评估结果表明，当任务中的数据与模型训练数据的性质相同时，模型的表现最佳。\n然而，我们可以观察到，来自异构源的数据似乎更加通用。\n我们还观察到，使用更多的数据可以转化为更优的表现。\n总体而言，从头开始的预训练似乎在大多数任务中都能获得更好的表现。\n然而，我们使用 CamemBERT 的权重和标记器在 NACHOS 的 4GB 子集上进行训练的控制预训练实验，与从头开始使用 DrBERT 4GB 的结果相当。\n对于基于 CamemBERT 权重和标记器的模型来说，情况并非如此，这些模型存在稳定性问题。\n最后的结论是，我们的正确系统在 11 个下游任务中的 9 个提供了更好的表现，并在全球范围内超越了通用模型（此研究中为 CamemBERT）的结果。\n我们还观察到，专业数据越多，效果越好，但其扩展效果不佳。\n从 NACHOS 获得的所有预训练模型均可在 Hugging Face 上免费获得，并且在 MIT 许可下，所有训练脚本均在我们的 GitHub 存储库中。\n谢谢，期待与大家在多伦多的海报会议上交流。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "csJIsDTYMW.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 10, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/eXmqPhcZFN.wav", "src_ref": "Hi, I'm Shangbin, PhD student in the University of Washington. Today I'm presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that's prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It's like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it's incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it's kind of like the electric trolley problem. Ok, great. I think that's pretty much all I have for today. Thank you for your time.", "tgt_ref": "大家好，我是华盛顿大学的博士生 Shangbin。\n今天我将介绍我们的论文《从预训练数据到语言模型再到下游任务：追踪导致不公平 NLP 模型的政治偏见踪迹》。\n语言模型是在大规模的网络爬虫数据上训练的。\n政治新闻媒体在预训练数据中的覆盖面很广。\n根据对 C4 语料库的一项调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中的覆盖率很高。\n这给语言模型应用带来了喜忧参半的影响。\n一方面，它们能够从不同的角度学习，这是对民主和思想多元化的赞颂。\n另一方面，这些不同的政治观点本质上是有社会偏见的，可能会在下游任务应用中导致潜在的公平性问题。\n为此，我们提议调查从预训练数据到语言模型再到下游任务的政治偏见传播流程，具体来说是通过提出以下问题：首先，我们如何评估语言模型的政治倾向，预训练数据对这种政治偏见可能起到什么作用？\n其次，具有不同政治倾向的语言模型在下游任务中的实际表现如何，以及这是否会导致 NLP 应用中的公平性问题？\n具体来说，我们首先提出使用政治问卷（例如政治会议测试）来提示具有不同提示格式的语言模型。\n这确保了我们能够根据政治学文献进行自动评估。\n一些初步结果表明，首先，语言模型确实有不同的政治倾向。\n它们占据了政治坐标系的所有四个象限。\n我们还可以看到，GPT-4 是所有语言模型中最自由的，而 GPT 系列在社会方面通常比 BART 系列及其变体更自由。\n其次，我们的目标是调查语言模型的政治偏见实际上在多大程度上是从训练数据中提取的。\n因此，我们可以通过对 6 个不同的党派语料库（分为新闻和社交媒体，进一步细分为其政治倾向）的语言模型检查点进行进一步的预训练，来进行对照实验。\n通过在这样的党派语料库上对语言模型进行进一步的预训练，我们可以看到语言模型的意识形态坐标也相应地发生了变化。\n例如，对于在左倾的 Reddit 语料库上进一步训练的 RoBERTa，我们可以看到其政治偏见方面发生了显著的自由化转变。\n我们还尝试研究语言模型是否能够捕捉到现代社会中普遍存在的两极分化。\n因此，我们将预训练语料库分为美国第 45 任总统之前和第 45 任总统之后。\n我们分别在两个不同的时间语料库上预训练语言模型。\n可以看到，2017 年之后，语言模型通常具有更远离中心的政治倾向。\n这表明语言模型也可以捕捉到我们社会的两极分化。\n最后，我们评估了具有不同政治倾向的语言模型对仇恨言论检测和假新闻检测的 NLP 应用，这些应用通常涉及语言模型，并且可能具有非常重要的影响。\n可以看到，如果我们调查每个类别的表现，也就是说，如果我们将表现分为不同的人口统计学特征或新闻媒体的政治倾向，我们可以看到这其中存在一个模式。\n例如，对于仇恨言论检测，左倾语言模型在检测针对社会少数群体的仇恨言论方面表现更好，但在检测针对社会中更有权势群体的仇恨言论方面表现更差。\n反之，右倾语言模型更擅长检测针对白人和男性的仇恨言论，但在检测针对黑人、LGBTQ+ 和其他少数群体的仇恨言论方面表现更差。\n类似的趋势也发生在假新闻检测中，我们可以看到，左倾语言模型更擅长检测来自其相反政治倾向的错误信息，反之亦然。\n我们进一步展示了许多定性示例，以了解具有不同政治倾向的语言模型确实会根据其社会类别对仇恨言论和错误信息示例给出不同的预测。\n附录中还有更多的例子，进一步强调这表明在语言模型的政治偏见方面存在着一个非常紧迫的公平性问题。\n例如，如果对右倾语言模型进行仇恨言论或错误信息或其他内容的微调，并部署到一个流行的社交媒体平台上，这将意味着，政治观点相反的人可能会被边缘化，针对少数群体的仇恨言论可能会不受任何控制地肆虐。\n因此，这警醒我们要认识并解决语言模型政治倾向造成的公平性问题。\n下面我们来讨论一下。\n还需要强调的是，我们揭示了关于语言模型政治偏见的独特困境。\n这就像是希腊神话中斯凯拉和卡律布迪斯之间的抉择。\n因此，如果我们不清理语言模型训练数据中的政治观点，偏见将从预训练数据传播到语言模型再传播到下游任务，最终造成公平性问题。\n如果我们试图以某种方式进行净化，也会面临审查或排斥的风险。\n而且要确定哪些是真正的中立内容，哪些应该保留语言监控数据，这是极其困难的。\n所以这有点像电车难题。\n好的。\n我想这就是我今天的全部内容。\n感谢大家的聆听。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "eXmqPhcZFN.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 11, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/gUAIqKCjIt.wav", "src_ref": "Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.", "tgt_ref": "大家好。\n我是 Koustav Sinha，很高兴欢迎大家参加我们关于 ACL 2023 论文的讨论。\n语言模型的可接受性判断并不总是对上下文有很强的适应性。\n这是我与 John Gauthier、Aaron Mueller、Kanishka Misra、Karen Fences、Roger Levy 和 Adina Williams 合作编写的论文。\n在这篇论文中，我们重新审视了最小对模式。\n最小对模式基本上是在可接受性判断之上对语言模型进行评估。\n其中也可以包括 BLiMP、SyntaxGym 等的语法性，或 CrowS-Pairs 等刻板印象的可接受性。\n在这种最小对模式中，评估语言模型的典型方法是，你先展示一个可接受的句子或一个符合语法的句子，然后再展示一个可接受的句子或一个不符合语法的句子。\n然后，希望模型会把更多的概率放在可接受的句子上。\n当前的 MPP 管道基本上不允许我们评估模型对较长句子的接受度。\n如今，大语言模型的上下文窗口越来越长。\n因此，至关重要的是，我们要在整个上下文窗口中评估模型的可接受性，这就是我们要做的。\n我们尝试通过要求模型评估越来越长的序列的可接受性来重新审视 MPP 管道。\n这就是我们的方法。\n因此，为了模拟这些更长的序列，我们重新审视数据集本身，然后从这些数据集中选择可接受或不可接受的句子来重新创建句子。\n例如，在这里，我们从 Adjunct Island 案例中的 BLiMP 数据集中选择了一对典型的语法。\n我们所做的就是重新创建更长的序列，这些序列是可接受的，并且具有相同的语法结构匹配。\n我们从 Adjunct Island 中提取语法句子，然后将其作为前缀添加到可接受的查询和不可接受的查询中。\n我们可以通过从相同的匹配中选择不可接受的句子来做同样的事情，这也可以用来测试模型的可接受性。\n我们也可以通过从不同的子集或不同的数据集中选择句子来做同样的操作。\n这就是我们所说的不匹配场景。\n这里的句子仍然来自相关数据集，但它们不是来自你正在评估的相同数据集。\n对于不可接受的情况，我们可以做同样的操作。\n最后，我们可以从完全不相关的领域（如维基百科）中选择句子。\n这将告诉我们，模型的可接受性判断实际上是否会受到任何上下文的影响，例如，上下文是否来自数据集的不同子集，或者它是否与我们正在查看的当前句子完全无关。\n那么，模型是怎么做到的呢？\n首先，我们看一下维基百科的句子，这些句子与当前的查询对完全无关，我们发现对于任意上下文长度，MPP 判断大多是稳健的。\n我们将上下文长度增加到 1024，以最大化 OPT 和 GPT 2 模型。\n从橙色虚线可以看到，MPP 判断相对稳定。\n现在，当我们从相同的数据集中选择句子时会发生什么？\n在这里，我们从相同的 BLiMP 或 SyntaxGym 数据集中，从可接受和不可接受的域中选择或创建句子。\n可以看到，当你添加可接受的前缀或不可接受的前缀时，MPP 判断的稳定性会显著增加或减少。\n但是，当我们匹配结构时，即当我们从 BLiMP 或 SyntaxGym 中的相同现象中选择句子时，能看到模型的 MPP 判断的稳定性会大幅增加或大幅减少，具体取决于所选前缀是可接受的还是不可接受的。\n在整个上下文长度中变化非常明显，这可能会影响到具有大上下文窗口的新语言模型。\n那么，为什么匹配前缀会对语言模型的判断产生如此大的影响呢？\n我们进行了一系列分析，试图通过保留相关结构但向输入中添加噪音来扰乱输入的句子。\n在做了几次这样的扰动之后，我们发现这些噪声实际上都没有使模型改变其路线，且未改变 MPP 判断输出。\n基本上，对于扰乱的句子，我们发现这些模型的敏感性是相似的。\n也就是说，当我们在可接受的域中扰乱句子时，我们会看到所有扰动都有类似的增加。当我们在不可接受的域中扰乱句子时，我们会看到 MPP 判断的稳定性以类似的方式减少。\n因此，我们论文的关键结论是，语言模型对句子间共享的潜在句法和语义特征很敏感。\n而我们目前对 MPP 进行评估的方式是短句和单句输入，这可能无法完全捕捉整个上下文窗口中的语言模型抽象知识。\n请阅读我们的论文，了解有关我们实验的更多详细信息。\n感谢大家的聆听。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "gUAIqKCjIt.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 12, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/krJSAnVcGR.wav", "src_ref": "Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I'd like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there's a catch, which is that people do assume that there's an additional clean validation set available for model selection. We can't stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that's not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there's no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.", "tgt_ref": "大家好，我是 Dawei，是来自德国萨尔兰大学的博士生。\n在这个视频中，我想介绍一下我们最近的论文《比你想象的更弱：对弱监督学习的批判性看法》。\n这是我与 Xiaoyu Shen、Marius Mosbach、Andreas Stephan 和 Dietrich Klakow 合作编写的论文。\n首先简要介绍一下弱监督和弱监督学习。\n在弱监督中，你不需要手动标记数据。\n相反，我们使用弱标记来源来标记数据，例如简单的启发式规则、知识库或低质量的众包，如右图所示。\n与人工注释相比，弱注释成本更低，但也存在噪声，这意味着有一定数量的注释是错误的。\n如果我们直接在弱标记数据上训练神经网络，那么神经网络倾向于记住标签噪声，而不具备泛化能力。\n在弱监督学习中，我们提出了训练算法，以便在这种标签噪声下稳健地训练神经网络，从而使训练后的模型仍具有良好的泛化能力。\n在有关 WSL 的最新研究中（WSL 代表弱监督学习），人们常说他们只在弱标记数据上训练模型，并在干净的测试集上实现优良表现。\n从技术上讲，这种说法并没错，但有一个问题，那就是人们确实假设有一个额外的干净验证集可用于模型选择。\n我们不能止于这个问题设置，但这意味着在弱监督学习中需要额外的手动注释。\n但是，就像房间里的大象一样，这种必要性常常被忽视。\n上述疑问被要求提出三个研究问题。\n首先，WSL 是否需要干净的验证数据，或者我们可以使用嘈杂的验证集吗？\n其次，如果需要干净的数据，或者如果 WSL 需要干净的数据才能运作，那么我们需要多少个干净的样本？\n最后，我们是否只应该使用干净的样本进行验证，还是有更好的方法来利用它们？\n我们在论文中解决了这些研究问题，以下是我们的发现。\n首先，我们发现有趣的是，最近的 WSL 方法确实需要干净的验证样本才能正常运作。\n否则，性能会大幅下降。\n如图所示，如果没有干净的验证样本，那么训练出来的模型就无法超越原始的弱标签，这意味着训练是无意义的。\n这表明 WSL 方法实际上需要干净标记的数据才能正常运作，并且不应忽视获得干净验证样本的注释成本。\n我们的第二个发现是，增加干净验证样本的数量将有助于 WSL 方法实现更优的表现，如左图所示。\n通常，每个类别只需要 20 个样本即可获得良好的表现。\n但这还不是最终结局，因为如果我们决定以某种方式访问干净样本，那么直接对其进行训练甚至会实现更好的表现。\n右图显示了直接应用于干净数据的微调方法与仅使用干净数据进行验证的 WSL 方法之间的表现差异。\n正如我们所看到的，如果每个类有 10 个样本，直接微调的表现效果开始超越 WSL 方法。\n最后，通过允许继续对干净的验证样本进行微调，可以轻松实现先前 WSL 方法中声称的表现改进。\n正如我们从图中所看到的，名为 FTw 的 vanilla 模型最初的表现不如 COSINE 等更复杂的 WSL 方法。\n但是，如果我们允许继续微调干净样本，那么 FTw 的表现会与其他方法一样好。\n因此，在实践中，没有理由选择需要更多计算时间和磁盘空间的更复杂的 WSL 方法。\n总结来说，我们表明最近的 WSL 方法需要干净、手动注释的样本才能正常运作。\n它们的表现提升和实用性被高估了。\n我们对未来研究的具体建议如下。\n首先，报告模型选择标准。\n例如，报告是否通过干净的验证样本完成模型选择。\n其次，应该将 WSL 方法与少样本学习基线进行比较，因为两者都适用于干净样本。\n第三，持续微调是一个简单而强大的基线，在 WSL 的未来研究中应予以考虑。\n最后，我们已经开源了我们的代码。\n您可以通过这张幻灯片上的二维码访问。\n欢迎大家查看。\n谢谢大家，祝大会圆满成功。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "krJSAnVcGR.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 13, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/miPjvjWOvI.wav", "src_ref": "Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It's trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it's important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it's in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn't have a big influence in the case of several short promptings. It's crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It's the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it's important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it's more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that's it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.", "tgt_ref": "大家好，我叫 David Vilar，我将简要回顾论文《PaLM 在翻译中的提示：策略评估与性能》。\n这是我与谷歌翻译团队的同事合作编写的论文。\nPaLM 是去年 2022 年发布的一个拥有 5400 亿参数的大语言模型。\n它是在包含 7800 亿个词元的大型文本集上训练的。\n发布时，它在数百个 NLP 任务中实现了最一流的成果。\n在这篇论文中，我们展示了大语言模型提示机器翻译的首个系统性研究。\n我们使用机器翻译社区的最佳实践来评估这种模型的转换能力。\n这涉及到使用最新的测试集，以避免测试数据与语言模型的训练数据重叠。\n我们还与最先进的系统进行了比较，即表现最佳的系统，进行了 WMT 评估。\n我们使用最先进的神经机器翻译指标，并展示了基于专家的人工评估结果。\n最后，我们提供了一些关于提示选择策略的建议。\n提示对 LLM 的翻译性能有很大的影响，可以在一个简单的实验中看到，我们使用了一次性提示，并为每个句子提供了两个不同的提示。\n在 1000 个测试句子中，\n有 516 个的 BLEURT 评分相差超过 1 分。\n在极端情况下，差异可能高达 40 个 BLEURT 分。\n因此，选择一个好的提示策略很重要。\n在实验中，我们采用了 5 样本提示策略，我们只需用所使用的语言来标记我们提供给系统的每个句子。\n在这个例子中，我们从德语翻译成英语，德语句子是源句子，用“German:”标记，英语翻译用“English:”标记。\n我们看到，在几个简短提示的情况下，提示的实际格式没有很大的影响。\n但对于零样本和单样本提示来说，提示格式至关重要。\n而当我们进行五次提示时，提示格式影响不大。\n示例句子的质量才是关键因素。\n我们实验结果的总结是，示例质量比与源句子的相似性更重要。\n因此，从高质量的翻译中选择示例非常重要。\n特别是，我们比较了从训练数据中选择的提示，用于开发数据上的 WMT 评估。\n开发数据经过更多的处理，质量比训练数据更高，噪音更大。\n使用开发数据时，其结果表现更好。\n尽管如此，相对于 PaLM，专业的最先进系统仍然具有巨大的翻译优势。\n但是，PaLM 已经非常接近商业系统。\n在我们的案例中，我们选择使用谷歌翻译进行评估。\n我们从使用 MQM 框架进行的人工评估中获得的见解表明，PaLM 的流畅度可与最先进的系统相媲美，但主要区别是准确性。\nPaLM 最常见的错误是遗漏错误。\n因此，PaLM 似乎选择生成听起来更好的翻译，有时是通过删除源句子中的一部分来实现这一点。\n然而，PaLM 的“风格/笨拙”类别低于最先进的系统，这是一个额外的信号，表明 PaLM 提供了真正流畅的输出，但仍然存在一些准确性问题。\n这就是本次简短概述的全部内容。\n如需了解更多详细信息，请查看论文的完整演示文稿。\n非常感谢。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "miPjvjWOvI.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 14, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/rOwZgUjcwB.wav", "src_ref": "Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four datasets via PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.", "tgt_ref": "大家好，我是来自中国科技大学的 Jingwei Yi。\n我很高兴为我们的论文提供一个简短的推广视频。\n《你在抄袭我的模型吗？\n通过后门水印保护大语言模型的版权，以实现嵌入即服务。》\n先来介绍一下嵌入即服务的背景。\n目前，GPT、LLAMA、PALM 等大语言模型在自然语言理解和生成方面表现出色。\n嵌入即服务是基于大语言模型的服务之一，用于协助各种 NLP 任务。\n例如，OpenAI 提供了一个基于 GPT 的嵌入 API。\n然而，最近的研究表明，攻击者可以通过从嵌入中学习来窃取模型，并提供类似的服务。\n因此，有必要保护嵌入即服务的版权。\n为了保护嵌入即服务的版权，其中一种解决方案是在提供商服务中嵌入水印，并检测其他服务是否包含该水印。\n水印方法需要满足以下属性。\n首先，该方法应适用于嵌入即服务。\n其次，水印不应降低所提供嵌入的效用。\n第三，水印对于攻击者来说应该足够隐蔽，否则攻击者可以轻松地去除水印。\n最后，在模型提取过程中，水印需要可迁移到攻击者的服务中。\n现有的水印方法可以大致分为四类。\n然而，这些方法要么不适用于嵌入即服务，要么缺乏可迁移性。\n因此，在本文章中，我们提出了嵌入标记，这是一种基于后门的水印方法，适用于嵌入即服务。\n下面我来详细介绍一下我们的嵌入标记。\n嵌入标记主要包含两个步骤。\n水印注入和版权验证。\n在这些主要步骤之前，我们首先选择一个触发器集。\n触发器集是中等频率间隔内的一组单词。\n我们假设提供商可以收集一般文本语料库，并计算其中的单词频率。\n在水印注入中，我们首先定义一个目标嵌入层。\n当用户向提供者服务发送句子时，提供者会计算句子中的触发器数量。\n提供的嵌入层是目标嵌入层和原始嵌入层的加权和。\n目标嵌入的权重与句子中的触发器数量成正比。\n当句子中的触发器数量大于 m 时，提供的嵌入层与目标嵌入层完全相同。\n版权验证是检测另一项服务背后的模型是否包含文字标记。\n我们首先构建一个后门和一个良性数据集。\n后门数据集所包含句子中的所有单词都属于触发器集，而良性数据集中句子的所有单词都不属于触发器集。\n然后，提供者使用数据集向窃取者的服务请求嵌入层。\n之后，对所请求的嵌入层和目标嵌入层之间的余弦和 L2 相似度进行计算。\n我们计算良性数据集和后门数据集之间的相似度差异，定义为 Δcos 和 ΔL2。\n同时，我们还应用了 KS 检验，并使用其 p 值作为第三个指标。\n我们在四个数据集，即 AG News、MIND、SST2 和 Enron Spam 上进行实验。\n我们假设提供者应用维基文本数据集来计算单词频率。\n四个数据集的结果表明，我们的嵌入标记在保持下游任务的强大实用性的同时，还可以具有出色的检测性能。\n我们还通过在四个数据集 [听不清 4:39] PCA 上对句子的嵌入进行可视化，来验证所提供嵌入的隐蔽性。\n数字的图例表示每个句子中的触发器数量。\n如图所示，很难区分后门嵌入和正常嵌入。\n以上就是全部内容。\n谢谢。\n欢迎与我们讨论。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "rOwZgUjcwB.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 15, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/vrydRuOXbT.wav", "src_ref": "Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.", "tgt_ref": "大家好，我叫 Ying。我和我的同事 Zhiyang 将介绍我们关于 MultiInstruct 通过指令调整改进多模态零样本学习的研究。\n随着大语言模型的进步，许多研究开始探索新的学习范式，以参数和数据高效的方式，将预训练的语言模型重新用于不同的下游任务。\n最近，许多研究表明，通过指令微调，大语言模型能够遵循自然指令，以零样本方式执行未见任务。\n然而，大多数以前关于指令调整的研究都侧重于提高语言任务中的零样本表现，而忽略了计算机视觉和多模态任务。\n因此，在本论文中，我们想研究一下，对多模态预训练模型进行指令调整是否可以真正提高对未见多模态任务的泛化。\n此外，在进行研究时，我们发现自然语言处理和多模态之间在指令数据集可用性方面存在很大差异。\n有超过 1600 个仅语言的指令任务。\n然而，没有大规模的公开多模态指令任务。\n因此，这激励我们构建一个多模态指令调整数据集。\n在这里，我们向大家呈现 MultiInstruct，这是第一个多模态指令调整基准数据集，它包含 62 个不同的多模态任务，涵盖 10 大类别。\n这些任务源自 21 个现有的开源数据集，每个任务配备五个专家编写的指令。\n为了在我们提出的数据集上研究多模态指令调整，我们采用统一的多模态预训练模型 OFA 作为基础模型。\nOFA 对语言、图像词元和边界框的坐标使用统一的词汇。\n在这里，我们展示了来自 MultiInstruct 数据集的一些示例实例，以统一处理各种输入和输出数据类型。\n我们遵循 OFA 的方法，并以统一的序列到序列格式制定所有任务。\n其中输入文本、图像、指令和边界框都在同一个词元空间中表示。\n好的，现在我要谈谈多模态指令调整。\n对于训练数据集，我们使用来自 9 个组的 53 个任务进行训练，每个任务抽样 10,000 个实例。\n在测试中，我们保留了整个常识推理组进行测试，并从 VQ 和其他组中选择了另外 5 个任务。\n我们使用测试分割中的所有实例来完成每个任务。\n此外，我们从自然指令的测试分割中随机抽取 20 个任务，作为 NLP 的不可见任务。\n我们使用预训练的 OFA 大型模型作为基础模型。\n在训练过程中，我们混合了所有任务的所有实例。\n每个实例随机与五个指令模板中的一个组合。\n因此，在每个任务的测试过程中，我们使用五个指令中的一个来评估模型，共进行了 5 次实验。\n在每个实验中，我们报告所有 5 个实验中的最差和最优表现以及表现的标准差。\n如果任务是多模型分类任务，我们将报告准确度。\n如果是多模态生成任务，我们报告 Rouge-L。对于 NLP 任务，我们也报告 Rouge-L。\n我们还引入了一个额外的评估指标，称为“灵敏度”。\n这是用来衡量模型在相同任务中产生相同输出的能力，而不考虑指令措辞的细微差异。\n以下是我们的主要结果。\n正如我们所看到的，指令调整可以显著提高 OFA 在可见多模态任务中的表现。\n此外，从自然指令数据集进行迁移学习可以有益于指令调整。\n在这里我们可以看到，随着任务数量的增加，模型的表现会更好，同时灵敏度会降低。\n我们还做了一个实验。\n我们使用一条指令对比五条指令。\n正如我们所看到的，使用更多的指令可以提高模型的整体表现，并大幅降低其灵敏度。\n这显示了不同的微调策略对模型灵敏度的影响。\n正如我们通过自然指令数据集的迁移学习所看到的，与原始 OFA 模型相比，该模型可以实现更优的灵敏度。\n我们还可以看到，来自自然指令数据集的迁移学习可以帮助 OFA 在自然指令数据集上实现更好的表现。\n因此，总体而言，我们提出了第一个大规模多模型指令调整数据集，显著提高了 OFA 的短期能力，并且我们探索了不同的迁移学习技术并展示了它们的优点。\n我们设计了一个新的指标，称为“灵敏度”。\n此外，我们正在收集一个更大的多模型指令调整数据集，其中包含大约 150 个额外的视觉语言任务，我们将发布这些数据集。\n这是我们的数据和模型的二维码。\n谢谢。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "vrydRuOXbT.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 16, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/wJAPXMIoIG.wav", "src_ref": "Hello everyone, my name is Yusen Zhang from the Penn State University. Today I'm going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they're only evaluated on certain neural models. For example, there's only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we'll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.", "tgt_ref": "大家好，我是来自宾夕法尼亚州立大学的 Yusen Zhang。\n今天我将介绍我们的论文《XSemPLR：多种自然语言和意义表征中的跨语言语义解析》。\n语义解析是构建用户查询的语义表示的任务，如 SQL 和 Lambda Calculus。\n跨语言语义解析是将多种自然语言的查询翻译成多种意义表征的任务。\n如图所示，我们需要使用神经模型将多种自然语言的查询翻译成 SQL、Lambda 或 FunQL 等。\n现有的跨语言语义解析模型是在有限的任务和应用的数据集上单独提出和评估的。\n例如，某些自然语言的覆盖面很广。\n但是中文却被遗漏了，某些意义表征也缺乏覆盖。\n缺少 Lambda 计算，或者仅在某些神经模型上进行评估。\n例如，只有一个单一的模型来评估它们。\n为此，我们提出了 XSemPLR。\n我们提供了一个统一的数据集 XSemPLR，用于多种自然语言和意义表征的跨语言语义解析。\n它包含各个领域的 9 个数据集、5 个语义解析任务、8 个意义表征和 15 个语系中的 22 种自然语言。\n为了更好地评估我们的基准，我们考虑了六个训练和评估设置。\n第一个是翻译测试。\n我们使用谷歌翻译 API 将源语言翻译成目标语言，然后使用单语模型进行训练和评估。\n例如，我们使用英语查询来训练英语模型，在推理过程中，我们使用 API 将德语查询翻译成英语，然后使用训练好的模型来预测 SQL。\n我们还将测试单语模型。\n在这种设置下，源语言与目标语言相同，例如德语到德语或英语到英语。\n我们还通过使用仅 10% 的训练数据来训练单语模型，以测试单语少样本情境。\n我们测试多语言模型，为所有语言训练一个多语言模型。\n例如，我们将德语、英语、中文查询放在一起来训练一个多语言模型。\n在推理过程中，我们可以使用这个模型来翻译德语查询或中文查询等。\n我们还考虑了跨语言零样本和少样本转换。\n我们使用一个源语言进行训练，然后转换成另一种语言。\n在训练过程中，我们使用英语查询或英语和德语的少样本查询组合来训练多语言模型，以预测 SQL 输出。\n我们也发现了很多有趣的结果。\n关于单语模型的分析，我们评估了两组模型，包括 Encoder-PTR，它代表具有基于指针的解码器的多语言预训练编码器，如 XLM-R + PTR 和 mBERT + PTR。\n此外，我们还评估了 Encoder-Decoder 模型，即多语言预训练的 Encoder-Decoder 模型，如 mBART 和 mT5。\n我们发现，Encoder-Decoder 在所有九个数据集上都获得了最佳表现。\n我们在多语言情境上对 mT5 和 XLM-R + PTR 进行评估。\n我们发现，通过混合各种语言进行训练，可以改进 Encoder-Decoder 或 Encoder-PTR。\n我们发现，这是因为大多数主要自然语言都可以获得表现提升，只有英语在七个数据集中的表现下降，在三个数据集中的表现有所提升。\n我认为这就是我们所说的“多语言的诅咒”。\n我们还比较了跨语言的表现差距。\n在这张图中，蓝线是跨语言少样本迁移。\n橙线是跨语言零样本迁移。\n绿线是单语设置。\n通过比较绿线和橙线，我们发现在零样本设置下，跨语言迁移的表现差异很大，然后比较蓝线和橙线，我们发现在少样本设置下，迁移差距迅速缩小。\n我们还有其他一些有趣的发现。\n例如，Encoder-Decoder 的表现优于以前的研究，或者达到了相当的结果。\n在英语自然语言上的预训练可以显著提高少样本在目标自然语言上的表现，我们发现 Codex 和 BLOOM 等多语言语言模型在跨语言语义解析任务上的表现仍然不够。\n总的来说，我们构建了 XSemPLR，这是一个具有多种自然语言和意义表征的跨语言语义解析的统一基准。\n我们对三种代表性的多语言语言模型类型进行了全面的基准研究。\n我们的结果显示了许多有趣的发现。\n以及其他内容。\n欢迎查看我们的论文和代码。\n感谢聆听。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "wJAPXMIoIG.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 17, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/wLmrUehthl.wav", "src_ref": "Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel'čuk's meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there's also a multi-headed approach that's used, for example, in the Hudson's Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It's okay the way instead of \"it\", we have this long NP. But it's also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it's 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That's why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn't you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what's novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It's absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there's no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I'll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.", "tgt_ref": "大家好，我叫 Adam Przepiórkowski，本次演讲是关于“协调的依存关系结构”。\n大家可能知道，不同的理论和语料库方法假设了不同的依存关系结构。\n例如，在普遍依存关系中，“Lisa、Bart 和 Maggie”的协调结构是这样的：第一个并列词是整个协调结构的开头。\n在这个例子中则是“Lisa”。\nIgor Mel'čuk 的意义文本理论中也采用了类似的方法，在这个理论中，整个协调结构也是由第一个并列词开头的。\n这两种方法都是不对称的。\n好的。\n它们单独挑出了一个并列词。\n这些是对称结构的不对称方法，比如布拉格方法。\n布拉格依存关系树库中采用的是并列词开头的方法，其中协调结构由并列词开头。\n因此，我们从结尾到所有并列词都有一些依存关系。\n最后，还有一种多头方法，例如 Hudson 的 Word Grammar 中所使用的，他们说所有的并列词都是协调结构的开头。\n因此，我们从支配词获得依存关系。\n这里的“loves”指向所有的并列词：Lisa、Bart 和 Maggie。\n这篇论文的目的是为对称协调结构提供一个新的论据，比如这两个，而不是对称协调结构，比如这两个。\n好的。\n这一论点基于依存关系长度最小化的原则，我将通过这些示例来解释。\n大家可能知道，在英语中，直接宾语更倾向于靠近动词，而附加宾语可能距离更远。\n因此，“Marge read it yesterday”是正确的，因为直接宾语靠近动词，而“Marge read yesterday it”则不正确。\n对吧？\n因为在动词和直接宾语之间有一个补充词：“yesterday”。\n然而，当直接宾语非常冗长时，这种影响可能会有所降低。\n因为这样可以将其移动到补充词之后的位置。\n这里有一个例子。\n这两个句子都是正确的。\n“Marge read this absolutely fascinating book about bees yesterday.”\n这样是可以的，我们用这个长的名词短语来代替“it”。\n但是，“Marge read yesterday this absolutely fascinating book about bees.”这样说也是正确的。\n因为即使这句话违反了直接宾语应该在动词旁边的一般语法原则，但它满足了依存关系长度最小化的原则，该原则说明应优先选择较短的依存关系。\n这两棵树只显示了关键依存关系的长度，这两个结构中不是常数的依存关系。\n这里我们有一个从“read”到以单词衡量长度为 7 的补充词的依存关系，以及从“read”到“book”的长度为 4 的依存关系，因此总长度为 11。\n当你交换这两个组成部分时，这两个依存关系的总和变成了 6。\n因此，6 比 11 要短得多。\n这就是为什么这听起来还不错。\n对吧？\n它虽然违反了一个原则，但满足了另一个原则。\n好的。\n我们从增强版的宾夕法尼亚语法树库中提取了关于协调的各种统计数据，并参阅了论文《为什么不使用通用依存关系》，这些统计数据证实了之前多次观察到的左边的并列词往往更短。\n以音节为单位来衡量，“salt and pepper”而不是“pepper and salt”。\n在解析中观察到，这种倾向随着长度差异的增加而增加。\n所以当两个并列词的长度差异增加时，较短的并列词更倾向于放在第一个位置，更强，对吗？\n因此，左边短并列词的比例更大。\n但是本论文的新颖之处在于，我们观察到这种趋势仅在支配词在左侧或不存在的情况下才会出现。\n对吧？\n在这个例子中，支配词在左侧，“I saw Bart and Lisa”，支配词在左侧。\n在第二个例子“Homer came and sneezed”中，支配词则不存在。\n这里我们有两个动词的协调，没有外部的支配词。\n在这种情况下，左侧的并列词更短；两个并列词之间的最大差异。\n然而，当支配词在右侧时，比如这个例子，“laughed”支配“Ted”和“Ned”的协调，这种效应就会消失。\n我们通过以字符衡量长度（第一列），以音节衡量长度（中间一列），以单词衡量长度（右一列）来表示。\n我们来看右边的一列。\n我们看到的是，当支配词在左侧时，左侧的并列词变短的趋势就越稳定，单词的绝对差异也越大，当没有支配词时，在句子协调中也观察到同样的现象。\n但是当支配词在右侧时，这种趋势就会消失。\n我们在论文中展示了这一点如何为反对非对称协调结构提供论据，比如这两个，以及为支持对称协调结构提供论据，比如这两个。\n有关完整论证，请参阅我们的论文。\n期待在海报环节上与大家交流。\n谢谢。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "wLmrUehthl.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 18, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/xUDLtuhJUS.wav", "src_ref": "Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate \"mole\" in this sentence? Well, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy. But if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that's rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.", "tgt_ref": "大家好，我叫 Kayo Yin，我将介绍我们的论文，题为《翻译何时需要上下文？\n数据驱动的多语言探索》。\n这篇论文是我与 Patrick Fernandes、Emmy Liu、André F. T. Martins 和 Graham Neubig 合作完成的。\n很多翻译取决于上下文。\n例如，我们如何翻译这句话中的“mole”？\n如果说，前一句话是“Things could start to get dangerous if the ministers find out”，那么这里的“mole”指的是“间谍”。\n但如果说，前一句话是“Could it be anything serious, doctor?”，那么这里的“mole”指的应该是“胎记”。\n因此，根据上下文，单词的含义发生变化，因此其翻译也会发生变化。\n然而，评估模型在这种情况下的翻译质量是非常困难的。\n首先，因为只有一小部分翻译取决于上下文，这使得 BLEU 等语料库级指标无法捕捉这些翻译。\n有些人建议对上下文依赖的翻译进行有针对性的评估，但这些资源只支持有限类型的上下文依赖翻译和有限的语言对，因为它们通常依赖于领域知识和人工策划。\n在本论文中，我们试图回答以下两个问题。\n首先，翻译在什么时候需要上下文？\n其次，模型在处理这些情况时的表现如何？\n为了回答第一个问题，我们首先衡量了单词在翻译过程中对上下文的依赖程度。\n在之前的研究中，我们引入了 CXMI 作为机器翻译模型上下文使用情况的衡量标准。\n这是通过衡量在给定源 X 的情况下，上下文 C 提供了多少关于目标 Y 的信息来完成的。你可以将 CXMI 视为通过为模型提供上下文而获得的信息。\n在本次研究中，我们将 CXMI 扩展为 Pointwise CXMI，它可以在句子层面或单词层面衡量上下文使用情况。\n我们可以将 P-CXMI 值高的词视为需要上下文进行翻译的词。\n现在，我们分析具有高 P-CXMI 的词，以便在这些词之间寻找模式。\n我们对已从英语翻译成 14 种不同语言的 TED 演讲文稿进行分析。\n我们在三个不同的层面上进行分析。\n首先，我们看一下 P-CXMI 平均值高的词性标签。\n例如，这使我们能够找到阿拉伯语中 P-CXMI 值相对较高的双数代词。\n这可以解释为，由于英语没有双数代词，因此在翻译成阿拉伯语时，需要根据上下文来确定代词是否为双数。\n同样，我们发现某些语言在选择适当的动词形式时也需要上下文。\n然后，我们研究了在所有不同上下文中 P-CXMI 较高的词汇项。\n这帮助我们识别出类似的情况，例如在汉语中，你需要根据上下文来翻译专有名词，以确保在文档中翻译的一致性。\n同样，我们发现上下文对于正确的翻译形式很重要。\n最后，我们研究了 P-CXMI 值高的不同单个词元。\n这使我们能够识别出某些现象，这些现象并不是由单词本身表达的，而是通过句子结构体现出来的，例如省略号解析。\n基于以上分析，我们使用分析结果来设计文档级翻译的基准。\n对于我们识别出的五种话语现象中的每一种，我们都创建了标记器来自动识别与现象相关的单词。\n我们将这种标记器称为“多语言话语感知”，或 MuDA 标记器。\n然后，我们还注意到，不同语言中这些话语现象所占的比例不同。\n之后，我们将 MuDA 标记器应用于我们要用于评估的平行语料库，并将我们选择的翻译指标应用于 MuDA 标记器识别的上下文相关示例。\n最后，我们使用我们的基准以及其他指标来评估文档级机器翻译上的不同模型。\n首先，当我们使用语料库级的指标时：对于 BLEU，我们发现不受上下文影响的模型表现最佳。\n但是，如果我们使用 COMET，上下文感知模型的表现最好。\n如果我们使用词汇 F 值，那么有上下文和没有上下文的模型表现相当。\n这再次表明，如果我们仅使用语料库级指标，就很难确定最佳的文档级翻译系统。\n现在，我们使用 MuDA 基准来评估模型，我们发现对于某些话语现象，如形式和词汇连贯性，上下文感知模型比不使用上下文的模型要准确得多。\n但是，这些模型在省略号、代词和动词形式等其他现象上并不比不使用上下文的模型表现更好。\n因此，这在某种程度上表明了我们在文档级翻译方面需要取得更多进展的地方。\n我们还比较了不同的商业系统，我们的基准表明，在文档级翻译方面，DeepL 通常比谷歌翻译更准确。\n总之，我们对 14 种语言对进行了数据驱动的分析，以确定翻译何时需要上下文，然后我们使用我们的发现来建立文档级机器翻译的基准，用于帮助我们确定哪些话语现象模型可以处理得好或不能处理，以及哪些翻译系统擅长文档级翻译。\n感谢大家的聆听。\n我们在多伦多见。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "xUDLtuhJUS.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 19, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/xiSxNRoOzm.wav", "src_ref": "Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.", "tgt_ref": "大家好。\n我是卡内基梅隆大学的一年级博士生 Jenny，今天我将为大家介绍我们的论文《NLPositionality：表征数据集和模型的设计偏见》。\n这篇论文是与华盛顿大学和艾伦人工智能研究所的一些研究人员合作完成的，即 Sebastian Santy、Ronan Le Bras、Katharina Reinecke 和 Maarten Sap。\n首先，请想象一下，你在为一家报社工作，正在筛选新闻文章下的评论，试图删除有害内容。\n你可能会使用某个流行的 API，比如 Prospective API，用于有害内容检测。如果你是 Carl Jones，这个方法非常有用。\nProspective API 能够正确检测到有害内容。\n但对于 Aditya Sharma 来说，情况并非如此。\nProspective API 对印度语境中更常见的冒犯性词汇并不敏感。\n这就是一个设计偏见的例子，它展示了技术在不同人群中的系统性表现差异。\n像我们刚才看到的那样的设计偏差，可能是由于自然语言处理研究人员和模型开发人员的立场所致。\n立场是指人们因其人口特征、身份和生活经历而持有的观点。\n这是一个在批判性研究中广泛使用的概念，特别是在女性主义和酷儿学术空间中。\n对于研究人员来说，立场可能会影响研究过程及其结果，因为它可能会改变研究人员做出的决定。\n因此，人们可能会问一个问题，数据集和模型是否有立场？\n我们并不是说数据集中的模型本身具有人口统计学的身份和生活经历，但它们确实汇总了真实人们的判断和意见，因此可以代表某些人的立场。\n因此，先前的研究已经提出了一些具有立场的轶事证据，例如文化差距和模型以及数据集，以及模型立场的理论定义。\n然而，这些研究并没有真正将最终用户与数据集和模型本身进行比较，而随着自然语言处理任务变得更加主观和社会化，研究模型和数据集的立场变得越来越重要，并且描述这些立场如何扭曲具有挑战性，因为并非所有决策都记录在案，许多模型隐藏在 API 之后。\n因此，为了研究数据集和模型的立场，我们将真实用户的注释与现有的数据集和模型进行了比较。\n我们通过 NLPositionality 框架来做到这一点。\n我们的框架主要分两个步骤。\n第一步是使用不同的注释工具重新注释数据集。\n我们应该这样做，而不是查看原始数据集注释者的人口统计数据，因为通常只有少数注释者对每个实例进行注释，并且人口统计数据很少被收集和共享。\n因此，我们选择重新注释数据，以获取更多的注释，并获取更丰富的人口统计数据集。\n然后，我们按人口统计学的方式进行注释，并使用皮尔逊相关系数将其与模型和数据集进行比较，因此我们的框架实际上与注释者分歧文献不同，它将最终用户与模型和数据集、预测和标签进行比较，而不是仅仅查看注释者一致性或建模注释者分布。\n我们的框架主要通过 Lab in the Wild 和 HCI 合作者的在线众包平台实现。\nLab in the Wild 是一个在线实验平台，我们可以在其中招募各种志愿者。\n与 M Turk（其参与者主要来自美国或印度）等平台相比，Lab in the Wild 如今仍然能够获得高质量的数据。\n我们在 Lab in the Wild 上主持了两项任务，其中一项是社会可接受性，其工作方式是，参与者将从社会化学数据集中读取一个情境，然后他们将写下该情境在社会上的可接受程度。\n之后，为了保持参与研究，他们可以将自己的回答与 AI 以及其他人的回答进行比较。\n然后，我们将这些注释与社交化学、Delphi 和 GPT 4 进行了比较。\n然后，我们为有害内容和仇恨言论检测任务复制了一个非常相似的设置，他们将阅读 Dynahate 的一个实例，并写下他们是否认为这是仇恨言论实例。\n然后，我们将这些注释与 Dynahate、Perspective API、Rewire API、Hate Roberta 和 GPT 4 进行了比较。\n我们的研究最终收集了来自 87 个国家的 1000 多名注释者的 16,000 多条注释。\n现在，我们有能力回答：NLP 数据集和模型最符合什么人的立场。\n我们发现，NLP 中确实存在立场。\n例如，我们发现数据集和模型更倾向于与英语国家保持一致。\n对于 GPT 4 社会可接受性分析，我们发现它与儒家和英语国家最为一致。\n我们发现 Dynahate 也最符合英语国家的需求。\n我们还发现，数据集和模型更符合接受过大学教育人群的立场。\n对于 GPT 4，在社会可接受性任务中，我们发现它与接受过大学教育或研究生院教育的人最为一致。我们还发现 Dynahate 与接受过大学教育的人最为一致。\n然而，当模型和数据集与特定人群保持一致时，有些人群不可避免地被遗漏。\n例如，与男性和女性相比，非二元人群的数据集和模型一致性更低。\n我们在 GPT 4 社会可接受性任务以及 Dynahate 任务分析中也发现了这一点。\n那么，既然 NLP 中存在立场，我们能做些什么呢？\n对此，我们有一些建议。\n第一，在整个研究过程中保留所有相关设计选择的记录。\n第二，用观点主义进行自然语言处理研究。\n我们的第三个建议是在 4 个特定社区内构建专门的数据集和模型。\nMasakhani 倡议就是一个很好的例子。\n我们想强调的是，包容性的 NLP 不仅仅是制造。\n所有技术都是为所有人服务的。\n我们的演讲到此结束。\n但是，如果大家想了解更多信息，欢迎查看我们的仪表板，了解最新的分析结果和我们的论文。\n谢谢。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "xiSxNRoOzm.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 20, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/yBDqNxQUwV.wav", "src_ref": "Hi! I'm going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean 'Easy on Me' or 'I Gotta Feeling'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that's not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs' entity understanding. We're not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean 'Easy on Me' or 'I Gotta Feeling'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we've used. When we move higher in the list, the entities become more similar to each other and it's usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don't necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here's for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here's the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it's around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there's a lot of room for improvement. We've also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.", "tgt_ref": "嗨！\n我将谈谈我们的论文《实体选择的间接指称表达式解析》，其中我们介绍了 AltEntities 语料库。\n我叫 Javad Hosseini，这是我与 Filip Radlinski、Silvia Pareti 和 Annie Louis 合作编写的论文。\n我们的目标是了解用户在做出选择时的语言。\n请考虑这个选择问题。\n“你是说‘Easy on Me’还是‘I Gotta Feeling’？”\n在这一示例中，用户想在这两首歌中选择一首。\n最明显的方法是使用直接指称‌，例如说出歌曲名称“Easy on Me”或其位置“第一首”。\n但有时候，间接指称更适合进行更自然的对话。\n当用户不记得歌曲的名称时，就可能会出现这种情况。\n或者两首歌的名字发音太过相似，难以区分。\n或者，当用户想指定偏好时，也可能会出现这种情况。\n以下是一些间接指称的例子，例如“较新的那一首”或“比较抒情的那一首”。\n这是对话系统中的一个重要问题，也是对 LLM 实体理解进行基准测试的一个重要问题。\n我们没有发现针对这项任务的大规模公共数据集，因此我们使用众包注释来收集数据集。\n我们的数据集涵盖三个不同的领域：音乐、书籍和食谱。\n我们的数据集收集方法强调使用卡通完成设置来达到非正式性。\n这幅漫画有三个对话框。\n在第一个气泡中，Bob 说：“记得我们昨天听的那首歌吗？”\n这样，Bob 就设置了对话的上下文。\n在第二个气泡中，Alice 说：“你是说‘Easy on Me’还是‘I Gotta Feeling’？”\n这是一个选择问题。\n在第三个气泡中，Bob 使用间接指称来选择其中一个实体，例如“较新的那一首”。\n我们会自动提供第一个和第二个对话框，但第三个对话框由注释者填写。\n第一个对话框是从每个域中的几个手动提示中选择的。\n第二个是选择问题，它是这样生成的：\n我们总是使用简单的模板。\n你是说 A 还是 B？\nA 和 B 是来自维基百科的样本。\n以下是我们使用的不同抽样方法。\n当我们在列表中向上移动时，实体彼此变得更加相似，通常更难以辨认。\n第一个情况是随机统一。\n第二种情况是实体的标题相似，例如，两本名为《The Return》的书。\n第三种情况是它们在维基百科上有相似的描述。\n最后是它们在维基百科上具有相似的信息框或属性。\n例如，歌曲的类型或艺术家相同。\n当我们向注释者展示这个选择问题时，他们知道这些实体的名称，但他们不一定知道这些实体。\n我们要做的是展示关于这两个实体的一些背景知识。\n对于歌曲，我们只是显示每首歌的谷歌搜索链接，然后要求注释者至少听一听每首歌的一部分，并阅读每首歌的信息。\n例如，这是歌曲“Easy on Me”的谷歌搜索结果。\n对于食谱和书籍领域，我们会展示来自维基百科的一些背景文本。\n对于食谱，我们还会展示它们的图像（也是来自维基百科），以便注释者知道它们的外观。\n然后，我们要求注释者从这些实体中选择一个，例如，这是第一个，并使用三到五个间接指称表达式来描述它们。\n例如，有钢琴曲伴奏的。\n以下是我们数据集中的一些例子。\n例如，“没有文字的”、“不是那个有 12 岁男孩的”、“虚构的”、“来自阿塞拜疆”等等。\nAltEntities 语料库在三个领域中有 6,000 个选择问题，并有 42,000 个间接指称表达式。\n下面总结了使用 T5 XL 模型的结果。\n如果语言模型可以访问与注释者完全相同的背景知识，那么准确率就会非常高，大约在 92% 到 95% 之间。\n但这是不现实的。\n如果语言模型可以访问一些部分重叠的背景知识，那么准确率在 82% 到 87% 之间，这更符合实际。\n例如，当语言模型检索背景知识时。\n如果语言模型只能访问实体名称，那么准确率只有 60%，因此有很大的改进空间。\n我们还证明了这些模型具有领域通用性。\n这是我们数据集的链接。\n谢谢。", "src_lang": "eng", "ref_lang": "zho", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "yBDqNxQUwV.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.zh.ref.xml", "src_text_source": "LONG_TEXTS"}}}
