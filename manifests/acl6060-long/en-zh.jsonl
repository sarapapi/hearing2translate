{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 416, "src_audio": "/acl6060-long/audio/en/416.wav", "src_ref": "Hello. My name is Asaf Harari. And I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures. Data scientists analyze data and mainly focus on the manipulating the data's existing features. But sometimes, these features are limited. Feature generation using another data source may add substantial information. Our research goal is automatic tabular data enrichment using external sources' free text. Assume we have a tabular dataset and a knowledge base. We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text. Our framework FeSTE is exactly this automatic process. So let's see an example in a dataset fed into FeSTE. In this example, the dataset is university dataset. When its goal is to classify universities into low ranking universities and high-ranking universities. As knowledge base, we use Wikipedia. The first phase of FeSTE is entity linking. When each entity, in this example the university name, is linked to an entity within the knowledge base. And and the text of the entities of the knowledge base is extracted and added to the dataset. In this example, the text is the Wikipedia page's abstract. Now, we need to generate or extract features from the retrieved text. So, we need to ah feature extraction phase ah which includes text analysis. And this is the main novelty of this paper and I will deep dive into it in the next slides. After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features. First generate ah features in the number of classes of the original dataset. In this example, the original dataset has two classes. So, FeSTE generates two new features. But if the dataset has five classes, FeSTE generates five new features. Each feature represents the likelihood for each class. To analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT, XLNet and etc. It is but it is not likely that we can train language models using the input datasets. So a naive approach will be ah target task finetuning. So, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset. In this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high. Receive the language model output, which is the likelihood for each class and use as new features. The problem with this approach is datasets may have few distinct entities / texts. In our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set. So to finetune a language model over ah this dataset will be ineffective. But we can use prior knowledge about pre-analyzed datasets. Because FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset. What we, what we suggest is to add, to add another finetuning phase. A preliminary multitask finetuning phase. When you finetune the language model over the n minus one datasets. And, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset. The state-of-the-art in multitask ah multitask finetuning called MTDNN. In MTDNN, MTDNN maintains ah heads in the number of tasks in the training set. So, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image. And it samples a random batch from ah from the training set. And if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head. And if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head. In our scenario, ah tabular datasets vary in the number of classes. So there are many tasks. MTDNN maintained number of classes, heads, output layers. And the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task. Our approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks. So let's see an example. Here is the our input dataset which consists of entities, features, text and classes. And, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false. Or in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not. So the label vector in this case stays always ah which consists always with two classes. And this is the ah algorithm for our fine, reformulated finetuning approach. So let's see the full framework. Dataset fed into FeSTE. And then ah FeSTE executes entity linking phase. It ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page. Then it reformulated the task into a pairwise sentence classification task. Applied the language model to the new task and the output likelihood for each class. And now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning. Then we use the output vector of the language model as a newly generated feature in the number of classes. To evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance. And as knowledge base we use Wikipedia. We design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset. We also, we also split each dataset into four folds and apply four folds cross validation. Then, we generate the new features and evaluate them using five evaluation classifiers. We use in our experiments base BERT base architecture. Here are the results for our experiments. You can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning. And our reformulated finetuning achieves the best result, the best performance. While MTDNN achieved two percent improvement over the target dataset finetuning. Our approach achieved six percent improvement. When we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent. But our performance increased to eleven percent compared to the target task finetuning alone. For summing, FeSTE enables few shot enrichment from thirty five samples in our experiments. It uses one architecture for all tasks and datasets. And it keeps the head of ah of the model. But it adds reformulation phase. It augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem. Thank you.", "tgt_ref": "大家好。我的名字是Asaf Harari。 我将介绍我们的论文《使用微调转换器架构的少样本表格式数据充实》。 数据科学家对数据进行分析，主要侧重于对数据的现有特征进行操作。 但有时，这些特征是有限的。 使用另一个数据来源生成特征可能会增加大量的信息。 我们的研究目标是，利用外部来源的自由文本自动丰富表格数据。 假设我们有一个表格数据集和一个知识库。 我们需要一个自动过程，其中包括实体链接和文本分析，以从知识库的自由文本中提取新的特征。 我们的框架FeSTE正是这个自动过程。 我们来看看在一个数据集中输入FeSTE的例子。 在这个例子中，数据集是大学数据集。 当它的目标是将大学分为低排名的大学和高排名的大学时。 我们使用维基百科作为知识库。 FeSTE的第一阶段是实体链接。 当每个实体，在这个例子中是指大学名称，被链接到知识库中的一个实体。 并且知识库的实体文本被提取出来，并添加到数据集中。 在这个例子中，文本是维基百科页面的摘要。 现在，我们需要从检索 文本中生成或提取特征。 因此，我们需要进行特征提取阶段，其中包括文本分析。 这是本论文的主要新颖之处，我将在接下来的幻灯片中深入探讨。 在特征提取阶段之后，还有一个特征生成阶段，我们使用提取的特征来生成少量的新特征。 首先，在原始数据集的类别数量中生成特征。 在这个例子中，原始数据集有两个类别。 所以，FeSTE生成两个特征。 但如果数据集有五个类，FeSTE就会生成五个新的特征。 每个特征表示每个类的可能性。 为了分析文本，我们使用了目前最先进的文本分析方法，即基于转换器的语言模型，如BERT、GPT、XLNet等。 但是，我们不可能用输入数据集来训练语言模型。 因此，有一个朴素的方法是：目标任务微调。 因此，在特征提取阶段，我们可以下载预训练的语言模型，在目标数据集上微调语言模型。 在这个例子中，要对语言模型进行微调，将文本分类，抽象成类，低或高。 接收语言模型的输出，也就是每个类别的可能性，并作为新的特征使用。 这种方法的问题是，数据集可能只有几个不同的实体/文本。 在我们的实验中，几乎有一半的数据集包含少于400个样本，最小的数据集包含35个样本，在一个训练集中。 因此，在这个数据集上微调语言模型将是无效的。 但我们可以使用关于预先分析的数据集的先验知识。 因为FeSTE，我们在多个数据集上应用FeSTE，我们可以使用n减1的数据集来收集n减1的数据集的信息，并在分析第n个数据集时使用这些信息。 我们的建议是，添加另一个微调阶段。 一个初步的多任务微调阶段。 当你在n减1数据集上微调语言模型时。 然后，我们执行另一个微调阶段，即目标任务微调，此时我们在第n个目标数据集上微调语言模型。 最先进的多任务多任务微调技术称为MTDNN。 在MTDNN中，MTDNN在训练集的任务数量上保持着头部。 因此，在这个例子中，训练集里有四个任务，所以MTDNN维持四个头，正如在图像上看到的那样。 它从训练集中随机抽取一批。 而如果他们的随机批属于一个，例如单句分类任务，它就会通过第一个头执行前向和后向路径。 而如果随机批属于成对排名任务，它就会通过最后一个头执行前向和后向路径。 在我们的场景中，表格数据集在类的数量上有所不同。 存在很多的任务。 MTDNN保持了类、头、输出层的数量。 另外，MTDNN需要为一个新的数据集和一个新的任务初始化新的头。 我们的方法，称为“任务重构微调”，它是在我们的方法任务重构微调，而不是维护多个头，我们把每个数据集重构为每个分类问题的一个句子，也就是两个类的任务。 让我们来看一个例子。 这里是我们的输入数据集，由实体、特征、文本和类组成。 而且，我们将任务从对文本进行低级或高级分类，重新表述为对文本、抽象和类别进行真或假的分类。 或者换句话说，我们训练了语言模型，将一个抽象和类划分为抽象和类，无论抽象属不属于类。 因此，标签向量在这种情况下始终保持……它总是由两个类组成。 而这就是我们精细的、重新制定的微调方法的算法。 让我们来看看完整的框架。 数据集送入FeSTE。 然后，FeSTE执行实体链接阶段。 它从知识库中提取文本，在本例子中，它是维基百科页面的摘要。 然后，它将任务重新表述为一个成对的句子分类任务。 将语言模型应用于新的任务，并对每一类的输出可能性进行分析。 现在，语言模型已经用初步的多任务微调在n减1的数据集上进行了微调。 然后，在类的数量上，我们用语言模型的输出向量作为新生成的特征。 为了评估我们的框架，我们使用了17个表格分类数据集，这些数据集在大小、特征、平衡、域和初始表现上都有所不同。 我们还是使用维基百科来作为知识库。 我们将实验设计为留出一个评估，在16个数据集上训练FeSTe，并将其应用于第17个数据集。 我们还将每个数据集分成四折，并应用四折交叉验证。 然后，我们生成新的特征，并使用五个评估分类器对其进行评估。 在我们的实验中，我们使用基础BERT基础架构。 以下是我们实验的结果。 你可以看到，我们将我们的框架与目标数据集微调……目标数据集微调和MTDNN初步微调进行了比较。 而我们重新制定的微调实现了最好的结果，最好的表现。 而MTDNN比目标 数据集 微调提高了2%。 我们的方法实现了6%的改进。 当我们在小数据集上看时，我们可以看到MTDNN的表现下降了，初步的多任务微调阶段的改进下降到1.5%。 但与单独的目标任务微调相比，我们的表现提高到了11%。 总而言之，在我们的实验中，FeSTE可以从35个样品中进行少量的富集。 它使用一个架构用于所有任务和数据集。 而且它保留了模型的头部。 但它增加了重新制定阶段。 它增加了训练集，它需要一个具有语义的目标值，这样我们就可以把它输入到语言模型中，并在句对分类问题中使用它。 谢谢。", "src_lang": "en", "tgt_lang": "zh", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 417, "src_audio": "/acl6060-long/audio/en/417.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction. I'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD. First, I'd like to talk about our motivation for reasoning. So here we show an examples where multi-step reasoning is helpful. So this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario. So on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers. But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here. So it is good to have interpretable multi-step reasoning as output. And we also think math word problem is a straightforward application to evaluate such reasoning abilities. So, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers. So in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well. So, certain assumptions ah also apply as in previous work. We assume the precision of quantities are known. And we only consider basic operators such as addition, subtractions, multiplication, division, and exponential. Furthermore, complicated operators can be actually decomposed into these basic operators. So, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model. So, traditional sequence to sequence model convert the expression to a specific sequence for generation. And it is pretty easy to implement and it can generalize to many different complicated problem. But the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction. But actually this direction is still quite popular because of um the transformer model. So, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations. So here we keep generating the operators until we reach the leaves, which are the quantities. So here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities. And the second thing is that it also contains some repetitive computations. So here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results. So, in our proposed approach we want to solve those problems in a step by step and interpretable manners. So for example, here in the second step, ah we can obtain these divisors which is twenty seven. And we can also refer back to the original questions to find the relevant contents. And in these steps we obtain the divisors. So, ah and then at this third step we actually get the quotient. Alright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends. So, here we actually generate the whole expression directly rather than generating a single operators or quantities. So this makes the process more accurate. So, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state. So, the expression is represented by e i j o p. Where we perform operator from q_i to q_j, and such expression is actually directed. So, we also have subtraction with words here to represent the opposite direction. This is quite similar to relation extraction. So in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression. We add it to the next state to become a new quantity. So, these slides actually visualize the evolution of the state where we keep adding expression to the current state. So in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations. So, once we get the quantity representations, we can start to do inference. Here we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3. First we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator. And then finally we obtain the expression representation q_1 divided by q_2. But in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well. So, here all the possible expression is equals to three times the number of operators. So the nice thing here is that we can easily add constraints to control this search this search space. For example, if this expression is not allowed, we can simply remove this expression in our search space. So in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities. So this quantity come from the previous calculated expression. So finally we can obtain this final expression q_3 times q_4. And we can also see the number of all the possible ah expression is different from the previous step. So, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced. So the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step. And here we also use this tau to represent when we should terminate this generation process. And here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary. And it also allows us to impose certain constraints from prior from prior knowledge. So we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K, MathQA and SVAMP. And here we briefly show the results compared with the previous best approaches. So our best performing variant is Roberta-DeductiveReasoner. And in fact we do not use beam search, in contrast all previous approaches are using beam search. All right. So, the best approaches are often tree based model. So, overall our reasoner is able to significantl significantly outperform this tree based model. But we can see the absolute numbers on MathQA or SVAMP are not really high. So we further investigate the results on SVAMP. And this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities. So, in our prediction we find some of the intermediate values are actually negatives. For example, um, in these questions we are asking how many apples does Jake have? But we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant. So, our model makes some prediction like this which is producing negative values. And we observe these two expressions actually have similar scores. So, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct. So um we further find such constraint actually improves quite a lot for some models. For example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points. So better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT. And we also try to analyze the difficulty behind these behind all these datasets. We assume the number of unused quantities can be regarded as irrelevant information here. So ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion. And here we also show the overall performance. For those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance. But with those samples that with unused quantity is actually way worse than the, worse than the overall performance. For MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part. So, finally we want to show the interpretability through a question perturbation example. So here our model actually makes a wrong prediction at the first step. So, we can actually correlate this expression with the sentence here. Alright. So, we think this sentence might be misleading the model to an incorrect predictions. So here planting another thirty five makes the model makes the model think it should be an addition operator. So we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees. So, we make it to convey more accurate semantics such that the model is able to make um the prediction correct. So, this study shows how the interpretable predictions help us understand the model behavior. So to conclude our work, so first our model is actually pretty efficient. And we are able to provide interpretable solving procedure. And we can easily incorporate some prior knowledge as constraint which can help improve the performance. And the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning. We also have certain limitations. Ah, if we have a large number of operators or constants, the memory consumption could be pretty high. And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy. So this is the end of the talk, and questions are welcomed. Thank you.", "tgt_ref": "大家好。今天我将介绍我们的研究工作：《学习演绎推理：作为复杂关系提取的数学文字问题解决方法》。 我是ByteDance 人工智能实验室的Allan，以下是我与德克萨斯大学奥斯汀分校的Jierui Li和SUTD的Wei Lu的合作成果。 首先，我想谈谈我们对于推理的动机。 在这里，我们展示了一个多步骤推理有帮助的例子。 这个数字取自PaLM的论文，他们在这个论文中进行了提示，以解决少样本学习情况下的网络问题。 在左侧我们可以看到，如果我们给出一些只有问题和答案的例子，我们可能无法获得正确的答案。 但是，如果我们给出更多的推理描述，那么这里的模型将能够预测推理描述，同样也会做出正确的预测。 所以，最好将可解释的多步骤推理作为输出。 我们还认为，数学文字问题是一个用来评估这种推理能力的简明应用。 在我们的问题设置中，围绕疑问，我们需要解决这个疑问，并获得数字答案。 在我们的数据集中，还向我们提供了数学表达式，该表达式也导向这个特定的答案。 某些假设也适用于之前的工作。 我们假设数量的精确度是已知的。 并且我们只考虑基本运算符，如加法、减法、乘法、除法和指数。 此外，复杂的运算符实际上可以分解成这些基本运算符。 之前解决数学文字问题的工作实际上可以分为序列到序列和序列到树模型。 传统的序列到序列模型将表达式转换为特定的序列以进行生成。 这很容易实现，可以概括成许多不同的复杂问题。 但缺点是，其表现实际上并不比结构化模型好，并且缺乏用于预测的可解释性。 但因为转换器模型的原因，实际上这个方向仍然很受欢迎。 因此，在基于树的模型中，我们实际上以树的形式进行这些表达式的结构化，并在树代中遵循预先排序的遍历。 所以在这里，我们继续生成运算符，直到我们到达叶子，也就是数量。 这里的好处是，它实际上给了我们这个二进制树结构，但实际上它非常违反直觉，因为我们首先生成运算符，然后在最后生成数量。 其次，其中还包含一些重复的计算。 如果我们看一下这个表达式，八乘三加三实际上生成两次，但实际上我们应该重复使用结果。 在我们提出的方法中，我们希望一步一步地以可解释的方式解决这些问题。 例如，在第二步中，我们可以得到这些除数，即27。 我们也可以回头参考原始问题来查找相关内容。 在这些步骤中，我们得到了除数。 然后在第三步，我们实际上得到了商。 好的。经过这三个步骤，我们实际上可以重复使用第二个步骤的结果，然后得到第四个步骤的结果，最后可以得到被除数。 在这里我们实际上直接生成整个表达式，而不是生成单个运算符或数量。 这使得这个过程更加准确。 在我们的演绎系统中，我们首先从问题中提供的一堆量开始，并且还包括一些常数作为我们的初始状态。 表达式由e i j o p表示。 我们执行从q_i到q_j的运算符，这样的表达式实际上是定向的。 我们在这里也有减法与单词代表相反的方向。 这与关系提取 非常相似。 在形式演绎系统中，在时间步骤t ，我们在q_i和q_j对之间应用运算符，然后我们得到这个新表达式。 我们把它添加到下一个状态，成为一个新的数量。 这些幻灯片实际上可视化了我们不断向当前状态添加表达式的状态的演变。 在我们的模型实现中，我们首先使用预训练语言模型，它可以是BERTs或Robertas，然后我们编码句子，然后我们得到这些数量陈述。 一旦我们得到数量陈述，我们就可以开始做推理。 这里我们展示了一个q_1的例子，以获得针对q_2除以q_2再乘以q_3的表达。 首先，我们得到配对表达，它基本上只是q_1和q_2之间的联结，然后我们应用一个由运算符参数化的前馈网络。 最后，我们得到表达式表达q_1除以q_2。 但实际上，在实践中，在推理阶段，我们也可能会得到错误的表达式。 这里所有可能的表达式等于运算符数量的三倍。 这里的好处是我们可以轻松地添加约束条件，来控制这个搜索这个搜索空间。 例如，如果此表达式不被允许，那么我们可以简单地在我们的搜索空间中删除此表达式。 所以在第二步中，我们做同样的事情，但唯一的区别是多了一个数量。 所以，这个数量来自之前计算的表达式。 我们最终可以得到最后这个表达式q_3乘以q_4。 我们还可以看到，所有可能的表达式的数字与之前的步骤不同。 这种差异使得应用波束搜索变得困难，因为这两个步骤之间的概率分布是不平衡的。 训练过程类似于训练 序列到序列 模型，我们在每个时间步骤中优化损失。 在这里，我们也使用这个tau来表示我们何时应该终止这个生成过程。 这里的空间从序列到序列是不同的，因为空间在每个时间步不同，而在传统的序列到序列 模型中，这是词汇的数字。 它还允许我们根据先前的知识施加某些约束。 我们对常用的数学文字问题 数据集、MAWPS、Math23K、MathQA和SVAMP进行实验。 在这里，我们简要地展示了与之前最佳方法相比的结果。 我们表现最好的变体是Roberta-DeductiveReasoner。 事实上，我们不使用波束搜索。相反，所有之前的方法都使用波束搜索。 没错。最好的方法通常是基于树的模型。 总的来说，我们的推理能够显著优于这个基于树的模型。 但我们可以看到，MathQA或SVAMP上的绝对数字并不高。 我们进一步研究SVAMP的结果。 这个数据集具有挑战性，因为作者试图手动添加一些东西来混淆NLP模型，例如添加不相关的信息和额外的数量。 在我们的预测中，我们发现一些中间值实际上是负数。 例如，在这些问题中，我们问Jake有多少个苹果？ 但我们有一些额外的信息，例如照片少了17张，Seventeen有8张照片，但这些信息完全无关紧要。 我们的模型做出了一些这样的预测，产生了负值。 我们观察到这两个表达式实际上有相似的分数。 我们实际上可以通过删除负数的结果来限制这个搜索空间，这样我们就可以得出正确的答案。 我们进一步发现，针对某些模型，这种约束实际上改善了很多。 例如，对于BERT，我们提高了7分。然后，对于Roberta基础模型，我们实际上提高了两分。 因此，更好的语言模型具有更好的语言理解能力，因此，这里的数字对于Roberta来说更高，对于BERT来说更低。 我们还试图分析所有这些数据集背后的困难。 我们假设这里的未使用数量的数量可以被视为不相关的信息。 在这里我们可以看到，我们有未使用数量的样本的百分比，其中SVAMP数据集占了最大的部分。 在这里，我们还展示了整体表现。 对于那些没有未使用数量的样本，其整体表现实际上高于整体表现。 但是，对于具有未使用数量的样本，其表现实际上比整体表现差得多。 对于MAWPS，我们其实没有太多的测试例子，所以我会忽略掉这一部分。 最后，我们想通过一个问题扰动的例子来展示可解释性。 在这里，我们的模型实际上在第一步就做出了错误的预测。 我们实际上可以将这个表达式与这里的句子相关联。好。 我们认为这个句子可能会误导模型做出错误的预测。 在这里再植入一个35，会使得模型以为它是一个加法运算符。 我们尝试将句子修改为类似梨树的数量比苹果树少35棵。 我们使其传达更准确的语义，以便模型能够使预测正确。 这项研究展示了可解释预测如何帮助我们理解模型行为。 对我们的工作做个总结，首先我们的模型实际上是非常有效的。 我们能够提供可解释的解决程序。 我们可以简单地将一些先前的知识作为约束，这样就可以帮助提高表现。 最后一点是，底层机制不仅适用于解决任务的网络问题，也适用于涉及多步骤推理的其他任务。 我们也有一定的局限性。 如果我们有大量 数量的运算符或常量，内存消耗可能相当高。 第二件事是，如前所述，由于不同时间步骤之间的概率分布不平衡，因此应用波束搜索策略也非常具有挑战性。 我的演讲到此结束，欢迎各位提出问题。谢谢。", "src_lang": "en", "tgt_lang": "zh", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 418, "src_audio": "/acl6060-long/audio/en/418.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University. I will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval. Legal issues are an integral part of many people's lives. But the majority of citizens have little to know knowledge about their rights and fundamental legal processes. As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited. All work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles. Such a system could provide a free professional legal help service for unskilled humans. Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval. Given a simple question on a legal matter such as, what do I risk if I violate professional confidentiality? A model is required to retrieve all relevant statutory articles from a large body of legislation. This information retrieval task comes with its own set of challenges. First, it deals with two types of language. Common natural language for the questions and complex legal language for the statutes. This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes. Besides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example. Instead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law. Lastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works. Here, there are long documents that may be up to six thousand words. The recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review. But statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets. In this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval. Our Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens. These questions cover a wide range of topics from family, housing, money, to work and social security. Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law. Let's now talk about how we collected this dataset. First, we started by compiling a large corpus of legal articles. We considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings. Then we gathered legal questions with references to relevant statutes. To do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue. We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues. We collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes. Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered. The remaining references were matched and converted to the corresponding article ids from our corpus. We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles. In addition, each question comes with the main category and a concatenation of subcategories. And each articles comes with a concatenation of the subsequence heading in the structure of the law. This extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification. Let's look at some characteristic of our dataset. The questions are between five and forty four words long with a median of fourteen words. The articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words. The lengthiest one being up to five thousand seven hundred and ninety words. As previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice. While the remaining fifteen percent concern either social security, foreigners or work. The article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics. Here's the total number of articles collected from each of these Belgian codes. Out of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset. And around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes. Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question. Which can be explained by the fact that those codes focused less on individuals and their concerns. Overall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times. Using all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture. Given a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article. We experiment with the standard TF-IDF and BM25 ranking functions. The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query. To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article. We use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings. These embeddings typically result from a pooling operation on the output of a word embedding model. First, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning. We experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model. Additionally, we train our own CamemBERT based model ah bi-encoders on our dataset. Note that for training, we experiment with the two flavors of the bi-encoder architecture. Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces. We experiment with mean, max and CLS pooling as well as product and cosine for computing similarities. Here are the result of our baseline on the test sets. With the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below. Overall, the finetuned bi-encoder significantly outperforms all the other baselines. The two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics. Although BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval. Regarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings. Furthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box. Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores. Let's conclude by discussing two limitations of our dataset. First, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing. During the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles. This information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate. Second, we should note that not all legal questions can be answered with statutes alone. For instance, the question, can I evict my tenants if they make too much noise? Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed. Instead, the landlord should probably rely more on case law and find precedents similar to their current situation. For example, the tenants makes two parties a week until two AM. Hence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined. We hope that our work sparks interest in developing practical and reliable statutory article retrieval models. That can help improve access to justice for all. You can check out our paper, dataset and code at the following links. Thank you.", "tgt_ref": "大家好，我叫Antoine，来自马斯特里赫特大学。 我将展示我与Jerry的合作结果，它是针对法定条款检索的新数据集。 法律问题是许多人生活中不可或缺的一部分。 但大多数公民对他们的权利和基本法律程序知之甚少。 结果，许多无法承担法律专家昂贵费用的弱势公民得不到保护，或者（更糟糕的是）受到剥削。 所有工作旨在通过开发针对法定条款的高效检索系统，来弥合公民与法律之间的鸿沟。 这样一个系统可以为非技术人员提供免费的专业法律援助服务。 在深入探讨这项工作的主要贡献之前，让我们首先描述法定条款检索的问题。 给出一个关于法律问题的简单问题，例如，如果我违反专业保密规定，我会承担怎么风险？ 需要一个模型来从大量立法中检索所有相关的法定条款。 这种信息检索 任务有其自身的一系列挑战。 首先，它涉及两种类型的语言。 针对问题本身的通用自然语言，以及针对法规使用的复杂法律语言。 这种语言分布的差异使得系统更难检索到相关候选信息，因为它间接需要一个固有的解释系统，可以将自然问题翻译成与法规术语相匹配的法律问题。 此外，成文法不是一堆可以自身作为完整信息来源的独立条款，举例来说，不像新闻或食谱那样。 相反，它是法律条款的结构化集合，只有在整体上下文中考虑时才具有完整的意义，也就是说，连同相邻条款的补充信息，它们所属的字段和子字段，以及它们在法律结构中的位置。 最后，法定条款不是小段落，而小段落通常是大多数检索作品中的典型检索单元。 在这里，有一些有可能是长达六千字的长文档。 自然语言处理中的最新进展引发了对许多法律任务的巨大兴趣，例如法律判断预测或自动联系人合同审查。 但由于缺乏较大的高质量标签化数据集，法定条款检索基本上没有受到影响。 在这项工作中，我们提出了一个新的以法国本土公民为中心的数据集，以研究检索模型是否可以接近法律专家执行法定条款检索任务的效率和可靠性。 我们的比利时法定条款检索数据集BSARD由比利时公民提出的1100多个法律问题组成。 这些问题涵盖了从家庭、住房、金钱到工作和社会安全等一系列主题。 每个问题都由经验丰富的法学家标记，并参考了比利时法典中超过22,600篇法律条款的语料库中的相关条款。 现在，让我们来谈谈我们是如何收集这个数据集的。 首先，我们从汇编大量语料库的法律条款开始。 我们考虑了32个公开可用的比利时法典，并提取了所有条款以及相应的章节标题。 然后，我们参考相关法规收集了法律问题。 为此，我们与比利时律师事务所合作，该事务所每年收到约4000封来自比利时公民的电子邮件，他们就个人法律问题征求意见。 我们很幸运能够访问他们的网站，在这个网站上他们经验丰富的法学家团队解决比利时人最常见的法律问题。 我们收集了数以千计的问题，并用类别、子类别和相关法规的法律参考进行了注释。 最后，我们浏览了法律参考文献，并过滤掉参考文献不包含在我们所考虑的其中任何一部法律条款的问题。 其余的参考文献被匹配，并转换为我们语料库中相应的条款ID。 我们最终得到了 10108 个问题，每个问题都仔细地标记了我们大型语料库的22633个法定条款中的相关条款的ID。 此外，每个问题都具有主类别和一系列子类别。 每个条款都带有法律结构中子序列标题的联结。 这些额外的信息未在当前工作中使用，但可能对今后关于法律信息检索或法律文本分类的研究有所帮助。 让我们来看看我们的数据集的一些特征。 这些问题的长度在5到44个单词之间，中位数为14个单词。 条款则要长得多，中位数为77个单词 ，其中有142条超过1000单词。 最长的一条有5790个单词。 如前所述，这些问题涵盖了广泛的主题，其中约85%的主题是关于家庭、住房、金钱或正义。 则其余的15%则涉及社会保障、外国人或工作。 这些法律条款也非常多样化，因为它们来自32个不同的比利时法典，涵盖了大量 数量的法律主题。 以下是从这些比利时法典中收集的条款总数。 在22633个条款中，只有1612个与数据集中的至少一个问题相关。 这些被引用的条款中约有80%来自民法典、司法法典、刑事调查法典或刑法典。 与此同时，32个法典中有18个中提到的与至少一个问题相关的条款少于5个。 这可以解释为，这些法典较少关注个人及个人关心的问题。 总体而言，这些被引用条款的引用次数中位数为2次，其中引用次数超过5次的不到25%。 使用所有数据集，我们对几种检索方法进行了基准测试，包括词汇和密集架构。 给定一个查询和一个条款，词汇模型通过计算该条款中每个术语的权重总和来为查询条款对分配一个分数。 我们使用标准的 TF-IDF和BM25排序函数进行实验。 这些方法的主要问题是，它们只能检索包含有查询中存在的关键字的条款。 为了克服这个限制，我们尝试了一种基于神经的架构，它可以捕获查询和条款之间的语义关系。 我们使用双编码器模型将查询和条款映射到密集的向量陈述中，并通过查询条款对嵌入的相似度计算它们之间的相关性分数。 这些嵌入通常来自于对单词嵌入模型输出的池化操作。 首先，我们研究了Siamese双编码器在零样本评估设置中的有效性，这意味着预训练的单词嵌入模型可以直接拿来使用，无需进行任何额外的微调。 我们尝试了与上下文无关的文本编码器，即word2vec和fastText，以及上下文相关的嵌入模型，即Roberta，更具体地说是CamemBERT，它是个法语Roberta模型。 此外，我们在数据集上训练我们自己的基于CamemBERT的 模型双编码器。 请注意，针对训练，我们尝试使用了双编码器架构的两种风格。 第一种是siamese，它使用一个独特的单词嵌入模型，将查询和条款一起映射到一个共享的密集向量空间；另一种是双塔，它使用两个独立的单词嵌入模型，将查询和条款分别编码到不同的嵌入空间。 我们试验了均值、最大值和CLS集合，以及计算相似性的乘积和余弦。 以下是我们在测试集上的基线结果。 上面是词汇方法，中间是在零样本设定中评估的siamese双编码器，下面是微调的双编码器。 总体而言，微调的双编码器明显优于所有其他基线。 双塔模型在召回率上比其siamese模型高出了100，但在其他指标上表现相似。 虽然BM25的表现明显低于训练好的双编码器，但它的表现表明，它仍然是特定领域检索的一个强大基线。 关于siamese双编码器的零样本评估，我们发现直接使用预训练的CamemBERT模型的嵌入，而不对信息检索任务进行优化，结果很差，这与之前的发现一致。 此外，我们观察到，基于word2vec的双编码器的表现明显优于基于fastText和BERT的模型，这表明在直接使用时，也许预训练的词级嵌入比字符级或子词级嵌入对这项任务更适合。 虽然有希望，但这些结果表明，与一个熟练的法律专家相比，还有很多改进的机会，因为他最终可以检索到任何问题的所有相关条款，从而获得满分。 最后，让我们讨论一下数据集的两个局限性。 首先，条款的内容仅限于从比利时32部法典中收集的条款，这并不包括整个比利时的法律，因为法令、指令和条例中的条款都没有。 在构建数据集的过程中，所有对这些未收集的条款的引用都被忽略，这导致一些问题最终只有最初相关条款数量的一小部分。 因此，这一信息意味着其余相关条款中包含的答案可能是不完整的，尽管它仍然完全合适。 其次，我们应该注意，并非所有的法律问题都可以仅通过法规来回答。 例如这个问题：如果我的租户制造太多噪音，我可以驱逐他们吗？ 在成文法中可能没有详细的答案来量化允许驱逐的特定噪音阈值。 相反，房东可能应该更多地依靠判例法，找到与他们目前情况相似的先例。 例如，租户每周举行两次派对，直到凌晨2点。 因此，有些问题比其他问题更适合于法定条款检索任务，而不太适合的问题所在的领域还有待确定。 我们希望我们的工作能够激发在开发实用可靠的法定条款检索模型方面的兴趣。 这可以帮助改善所有人对司法的利用。 您可以在以下链接查看我们的论文、数据集和法典。谢谢。", "src_lang": "en", "tgt_lang": "zh", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 419, "src_audio": "/acl6060-long/audio/en/419.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena. Why did we do the trouble in setting up this benchmark? Well, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs. Each one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding. So we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily. But do we know what the models have actually learned? What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match? And the low score for this one? Do vision and language models focus on the right thing? Or do they focus on biases as shown by previous work? To shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities. We target existence, plurality, counting, spatial relations, actions and entity coreference. But how do we test whether the vision and language models have captured this phenomena? By foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work. Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore. And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances. For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped. Counting and coreference also are pieces that have more than one instrument. And we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences. This is not easy to do because a foiled caption may be less likely than the original caption. For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this. Therefore, to obtain valid foils, we must take action. First, we make use of strong language models to propose foils. Second, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image. To test this automatically, we apply natural language inference with the following rationale. We consider an image to be the premise and its caption its entailed hypothesis. In addition, we consider the caption to be the premise, and the foil is its hypothesis. If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil. If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out. But this procedure is not perfect, it is just an indicator for valid foils. Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE. So, after filtering and human evaluation, we have as many test instances as described in this table. Note that VALSE does not deliver any training data but only test data. Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining. Finetuning would only enable models to exploit artifacts or statistical biases in the data. And we all know that these models like to cheat and take shortcuts. And as we said, we are interested in assessing what capabilities the vision and language models have after pretraining. We experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT. Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils. Perhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair. For more metrics and results on them, do check out our paper. The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT. It's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images. However, none of the remaining pieces can be reliably solved in our adversarial foiling settings. We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image. The relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image. They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece. From the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models. As a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity. If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases. And it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models. So to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities. Our experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators. We would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models. And even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE. If you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "tgt_ref": "大家好，很高兴向你们介绍我们的工作成果——VALSE，这是一个独立于任务的基准，旨在用特定的语言现象测试视觉和语言模型。 我们为什么要费尽心思设立这个基准呢？ 那是因为，在过去的几年里，我们看到了基于转换器的视觉和语言模型在大量的图像文本对上进行预训练的爆炸性增长。 这些模型中的每一个都在视觉和语言任务上推动了最先进的技术，如视觉问题回答、视觉常识推理、图像检索，以及短语领域。 因此，我们得到了一个信息，这些任务的准确性和特定基准正在稳步提升。 但我们是否知道模型实际上学到了什么？ 视觉和语言转换器在为这个图像和这个句子分配高分时，所理解的是什么呢？ 而这一个的低分呢？ 视觉和语言模型关注的是正确的事情吗？ 还是像之前的工作所显示的那样，他们专注于偏差？ 为了进一步阐明这方面的问题，我们提出了一个与任务更加无关的方向，并引入VALSE，对于影响语言和视觉形态的特定语言现象，其测试视觉和语言模型的敏感性。 我们的目标是存在性、复数、计数、空间关系、动作和实体共指。 但是，我们如何测试视觉和语言模型是否捕获了这种现象？ 通过干扰Ravi Shekhar和合作者以前只应用于视觉和语言模型的名词短语的方法，以及我们在之前工作中对计数的方法。 干扰的意思是，我们获取一个图像的标题，通过改变标题，使其不再描述图像中的物体，从而造成干扰。 当我们在做这些短语改动时，重点关注了六个具体的方面，如存在性、复数、计数、空间关系、动作和实体共指，其中每个方面都可能包含一种或多种工具，以备我们发现不止一个有趣的方式来创造干扰实例。 例如，在动作方面，我们有两种工具，在一种当中是用不同的动作改变动作动词，在另一种当中是动作被交换。 计数和共指也是具有多种工具的方面。 当我们创造这些干扰时，要确保它们不能描述图像，它们符合语法，且仍然是有效的句子。 这并不容易，因为被干扰的标题可能比原始标题更不可能。 例如，虽然这不是不可能的，但从统计学上来说，植物砍人的可能性比人砍植物的可能性要小，较大的视觉和语言模型可以发现这一点。 所以，要制造有效的干扰，我们必须想方设法。 首先，我们利用强大的语言模型来提出干扰。 其次，我们使用自然语言推断或简短NLI来过滤可能仍在描述图像的干扰词，因为在构建干扰词时，我们需要确保它们无法描述图像。 为了自动检验这一点，我们应用了自然语言推理，其基本原理如下。 我们认为图像是前提，其标题是其附带的假设。 此外，我们认为标题是前提，而干扰词是其假设。 如果NLI 模型预测干扰词与标题相矛盾或保持中立，我们将其作为有效干扰词的指标。 如果NLI预测干扰词是标题中所包含的，那么它就不可能是一个好的干扰词，因为根据反证法，它将给出图像的真实描述，我们将这些干扰词过滤掉。 但这个过程并不完美，它只是一个有效干扰词的指标。 所以，作为生成有效干扰词的第三项措施，我们使用人类 注释着来验证VALSE中使用的数据。 因此，经过过滤和人工评估后，我们拥有与本表中所述的测试实例一样多的测试实例。 请注意，VALSE不提供任何训练数据，而仅提供测试数据。 由于它仅是一个零样本测试基准，因此它旨在利用预训练后的视觉和语言模型的现存功能。 微调只会使模型能够利用数据中的工件或统计 偏差。 我们都知道，这些模型喜欢作弊和走捷径。 正如我们所说，我们有兴趣评估视觉和语言模型在预训练后具有哪些能力。 我们在VALSE上尝试了五种视觉和语言模型，即使用CLIP、LXMert、ViLBERT、ViLBERT十二合一和VisualBERT。 我们最重要的两个评估指标是模型在将图像句子对分类为标题和干扰词的准确性。 也许与这段视频更相关的是，我们将展示我们更宽容的指标，即成对的准确性，它衡量的是正确的图像文本对的图像句子对齐得分是否大于其受干扰的对。 有关更多指标及其结果，请查看我们的论文。 这里显示了成对的准确度的结果，它们与我们从其他指标中得到的结果一致，即ViLBERT十二分之一取得了最佳的零样本表现，其次是ViLBERT、LXMert、CLIP，最后是VisualBERT。 值得注意的是，以个别对象（如存在性和名词短语）为中心的工具几乎被ViLBERT十二分之一解决了，这突出表明模型能够识别已命名的对象和它们在图片中的存在。 但是，在我们的对抗性干扰设置中，其余的部分都不能被可靠地解决。 我们从复数和计数工具中看到，视觉和语言模型难以区分对单个和多个对象的引用，或在图像中计算它们。 关系部分表明，它们难以正确地分类图像中对象之间的命名空间关系。 他们也很难区分动作和识别动作的参与者，即使像我们在动作这块看到的那样有合理性偏差的支持。 从核心推理这一块，我们发现通过使用代词来追踪对图像中同一对象的多个引用，对于视觉和语言模型来说也是困难的。 作为理智的检查，同时也因为这是一个有趣的实验，我们还对两个纯文本模型（GPT one和GPT two）进行了基准测试，以评估VALSE是否可由这些单模态模型解决，方法是抛开图像不管，计算正确标题和受干扰标题的困惑度，并预测具有最低困惑度的条目。 如果受干扰标题的困惑度更高，我们认为这表明受干扰的标题可能存在合理性偏差或其他语言偏差。 有趣的是，在某些情况下，纯文本的GPT模型比视觉和语言模型更好地捕捉了世界的合理性。 因此，总的来说，VALSE是一个基准，它使用语言构造的镜头，通过硬性测试社区的视觉接地能力，来帮助其改善视觉和语言模型。 我们的实验表明，视觉和语言模型能很好地识别命名对象及其在图片中的存在（如“存在性”部分所示），但在被迫尊重语言指标时，却很难在视觉场景中建立它们的相互依存性和关系。 我们非常希望鼓励社区使用VALSE来衡量用视觉和语言模型实现语言接地的进展。 更重要的是，VALSE可以作为数据集的间接评估，因为可以在训练或微调前后对模型进行评估，以了解数据集是否有助于模型在VALSE测试的任何方面得到改善。 如果您有兴趣，请查看GitHub上的VALSE数据。如果您有任何疑问，请随时与我们联系。", "src_lang": "en", "tgt_lang": "zh", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 420, "src_audio": "/acl6060-long/audio/en/420.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo. I'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization. I'll be explaining in this order. First, I will introduce automatic release note generation that we are working on in this research. A release note is a technical document that summarizes the changes distributed with each release of a software product. The image shows a release note for version two point six point four of the vuejs library. Release notes play an important role in open source development but they're time consuming to prepare manually. Therefore, it would be very useful to be able to automatically generate high quality release notes. I will defer to two previous researches on automatic release note generation. The first is a system called ARENA released in twenty fourteen. It takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them. The most notable feature of this system is the issue extractor in the upper right corner. Which must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira. In other words, it cannot be used for many projects on GitHub. The second is Glyph, recently announced in twenty twenty. It is available on the internet and can be installed via pip. This system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message. This image is a sample usage that returns a corrective or bug fixes label. Glyph's training data is fairly small, about five thousand, and will be shown in the experiments described below. The performance of the text classification model is not high. I present two related researches, but their problems are limited applicability and scarce data resources. Our paper solves these two problems and automatically generates high quality release notes. With a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input. This proposed method can be used for all English repositories. For the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API. Next, I'll describe our dataset. Here is an example of data. The left side is a commit message and the right side is the release notes. Release notes are labeled as improvements or fixes, etc. We have set up a task that takes the commit messages as input and outputs a labeled release notes. This can be regarded as a summarization task. We have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes. These were set based on previous research and other factors. The release note on the bottom right is extracted from the release note on the bottom left. At this time, it is necessary to detect the four labels that have been set up in advance. But the labels are not always consistent with each repository. For example, the improvements label includes improvements, enhancements, optimizations, and so on. We prepared a vocabulary list of about thirty labels for each of these notational variations. This is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class. Next is a commit message. Commit messages are not tied to each release. As shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff. This is a bit tedious and it is not enough to just get a list of releases and look at the before and after. We created a heuristic matching rule to get the previous and next versions. Dataset analysis. In the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected. Also, the average number of release notes tokens is sixty three, which is quite high for a summarization task. Also, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand. This is due to the large number of unique class or method names found in the repository. Next, I will explain the proposed method. The classwise extractive then abstractive summarization model consists of two neural modules. A classifier using BERT or CodeBERT and a generator using BART. First, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other. The commit messages classified as other are discarded. Then CEAS applies the generator to the four labeled documents independently and generates release notes for each class. In this task, the direct correspondences between commit messages and release notes are not known. Therefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message. We modeled the classwise abstractive summarization approach by two different methods. The first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages. The output texts can be divided into classwise segments based on special class-specific endpoint symbols. The second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes. Okay, let me explain the experiments. Five methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph. Regarding evaluation, in some cases, release notes are output in multiple sentences. Since it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence. The BLEU is penalized when the system outputs a short sentence. This penalty results in a lower BLEU value in the experiment results described next. Finally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty. A higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty. Here are the results. Since the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them. CEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines. In particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points. These results indicate that CEAS and CAS are significantly affected. CEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels. High coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class. CAS-Multi tended to yield higher ROUGE-L than CAS-Single. Suggesting that it is also effective to independently develop differently abstractive summarization models for each release note class. Here are an error analysis. CAS methods tend to output shorter sentences than human reference sentences. In the figure on the right, the reference sentence has three or four sentences, while CAS has only one. The reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label. Furthermore, CAS methods cannot generate accurate release notes without additional information. The top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue. The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so. Finally, a conclusion. We have built a new dataset for automatic release note generation. We have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English. Our experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines. Please check out our dataset on GitHub. Thank you.", "tgt_ref": "大家好，我是东京大学的Kamezawa。 我将发表一篇论文，题目为《RNSum：通过提交日志总结自动生成发行说明的大规模数据集》。 我将按照这个顺序解释。 首先，我将介绍我们在这项研究中正在进行的自动发行说明生成。 发行说明是一个技术文档，它总结了软件产品的每个版本所分发的更改。 这个图像显示的是vuejs库的2.6.4版本的发行说明。 发行说明在开源开发中起着重要作用，但手动准备它们是很耗时的。 因此，如果能够自动生成高质量的发行说明，那将是非常有用的。 我将遵从之前的两项关于自动生成发行说明的研究。 第一个是被称为ARENA的系统 ，发布于2014年。 它采取了一种基于规则的方法，例如使用变化提取器从不同版本的差异中提取所有的差异、库的变化和文件的变化，最后再将它们结合起来。 这个系统最显著的特征是右上角的问题提取器。 这必须留给问题跟踪器系统 JIRA ，并且只能应用于使用JIRA的项目。 换句话说，它不能用于GitHub上的许多项目。 第二个是Glyph ，最近在2020年宣布。 它可以在互联网上下载，并可以通过pip安装。 这个系统有一个简单的基于学习的文本分类模型，并为每个输入的提交信息输出五个标签之一，如特征或错误修复。 此图像是一个返回纠正或错误修复标签的示例用法。 Glyph的训练数据相当小，约为五千，并将在下面描述的实验中显示。 文本分类模型的表现不佳。 我提出了两个相关的研究，但它们的问题是适用性有限和数据资源稀缺。 我们的论文解决了这两个问题，并自动生成了高质量的发行说明。 面对适用性有限的问题，我们提出了一种只使用提交信息作为输入的高质量的分类总结方法。 这个提议的方法可以用于所有英语存储库。 对于第二个数据资源稀缺的问题，我们通过使用GitHub API从公共GitHub存储库收集数据，建立了由大约八万两千条数据组成的RNSum数据集。 接下来，我将介绍我们的数据集。 以下是数据的示例。 左侧是提交消息，右侧是发行说明。 发行说明被标记为优化或修复等。 我们设置了一个任务，将提交信息作为输入，并输出一个标签化的发行说明。 这可以看作是一项总结 任务。 我们预先定义了四个标签：特征、优化、错误修复、弃用删除和重大更改。 这些都是基于之前的研究和其他因素设定的。 右下角的发行说明是从左下角的发行说明中提取的。 现在，有必要检测事先设置好的四个标签。 但是，标签并不总是与每个存储库一致。 例如，改进标签包括改进、增强、优化等。 我们为这些符号变体中的每一个准备了一个大约30个标签的词汇列表。 这是为了检测发行说明类，并收集后面的发行文本作为该类的发行说明句子。 接下来是提交消息。 提交信息并不与每个版本相联系。 如下面的图像所示，如果当前的版本是2.5219的版本，那么我们需要识别之前的版本2.5218，并得到一个差异。 这有点繁琐，而且仅仅得到一个发布列表并查看前后的情况是不够的。 我们创建了一个启发式匹配规则来获取上一个和下一个版本。 数据集分析。 最后，收集到了7200个存储库和82000份数据。 此外，发行说明令牌的平均数量为63，这对于一个总结任务来说是相当高的。 此外，独特的令牌数量也相当是大的，有883万个。 这是由于在资源库中发现了大量独特的类或方法名称。 接下来，我将解释所提议的方法。 这个先按类别抽取然后抽象总结的模型是由两个神经模块组成的。 一个是使用BERT或CodeBERT的分类器，另一个是使用BART的生成器。 首先，CEAS使用分类器将每条提交信息分为五个发行说明类别，其中使用了“改进”、“错误修复”、“弃用”，以及“其他”。 被归类为“其他”的提交消息将被丢弃。 然后CEAS将生成器独立地应用于四个标签化 文档，并为每个类别生成发行说明。 在这项任务中，提交信息和发行说明之间的直接对应关系并不清楚。 所以，为了训练分类器，这就是为什么我们使用每条提交信息的前十个字符，来对每个输入的提交信息重新分配调查。 我们通过两种不同的方法对分类抽象总结方法进行建模。 第一种方法是我们称为CAS-Single的模型，它由一个单一的六对六网络组成，并生成一个单一的发行说明文本，给出输入提交信息的串联。 输出的文本可以根据特殊的类特定端点符号分为分类段落。 第二种方法，方法，我们称之为CAS-Multi，由四个不同的seq2seq网络组成，每个网络都对应于一个固定的发行说明类别。 好，让我解释一下实验。 我们比较了五种方法：CEAS、CAS-Single、CAS-Multi、Clustering，以及之前的研究“Glyph”。 关于评估，在某些情况下，发行说明是以多个句子形式输出的。 由于很难计算出这些句子的数量，所以用空格合并，作为一个长句处理。 当系统输出一个短句时，BLEU会受到惩罚。 在接下来描述的实验结果中，这种惩罚导致了较低的BLEU值。 最后，我们还计算了特异性，因为如果发行说明是空的，就无法计算ROUG和BLEU。 更高的特异性意味着，在发行说明假定为空的情况下，模型会正确地输出一个空文本。 结果如下。 由于该数据集包含电子邮件地址、哈希值等内容，所以我们还评估了经过清理的数据集，其中不包括这些内容。 CEAS和CAS的ROUGE-L得分比基线高10分以上。 特别是在干净的测试集上，建议的方法和基线之间的分数差距跃升到20分以上。 这些结果表明CEAS和CAS受到严重影响。 CEAS比CAS得到了更好的ROUGE-L分数，这表明将分类器和生成器结合在一起对使用伪标签训练分类器是有效的。 CEAS的高覆盖率之所以能够实现，可能是因为分类器可以专注于为每个类别选择相关的提交信息。 CAS -Multi倾向于比CAS -Single产生更高的ROUGE -L。 这代表着，为每个发行说明类别独立开发不同的抽象总结模型也是有效的。 这里有一个错误分析。 CAS方法倾向于输出比人类参考句子更短的句子。 在右图中，引用句子有3或4个句子 ，而CAS只有1个。 这个模型不愿输出的原因是，在训练数据中，只有33%的句子出现在特征标签中，40%出现在改进标签中。 此外，如果没有额外的信息，CAS方法就无法生成准确的发行说明。 右边最上面的例子是一个非常混乱的提交消息的例子，如果不参考相应的进度或问题，就无法生成完整的句子。 下面的例子显示，输入中的两个提交信息是相关的，应该合并成一个句子，但它没有这么做。 最后，一个结论。 我们已经建立了一个新的数据集，用于自动生成发行说明。 我们还制定了一项输入提交信息并对其进行总结的任务，以便适用于所有用英语写的项目。 我们的实验表明，与基线相比，所提出的方法在更高的覆盖率下产生了更少干扰的发行说明。 请查看我们在GitHub上的数据集。 谢谢。", "src_lang": "en", "tgt_lang": "zh", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
