<?xml version="1.0" encoding="UTF-8"?>
<mteval>
<srcset setid="iwslt-ACLtest2023" srclang="English">
<doc docid="2022.acl-long.410" genre="presentations">
<talkid>2022.acl-long.410</talkid>
<abstract>Solving math word problems requires deductive reasoning over the quantities in the text. Various recent research efforts mostly relied on sequence-to-sequence or sequence-to-tree models to generate mathematical expressions without explicitly performing relational reasoning between quantities in the given context. While empirically effective, such approaches typically do not provide explanations for the generated expressions. In this work, we view the task as a complex relation extraction problem, proposing a novel approach that presents explainable deductive reasoning steps to iteratively construct target expressions, where each step involves a primitive operation over two quantities defining their relation. Through extensive experiments on four benchmark datasets, we show that the proposed model significantly outperforms existing strong baselines. We further demonstrate that the deductive procedure not only presents more explainable steps but also enables us to make more accurate predictions on questions that require more complex reasoning.</abstract>
<seg id="1">Olá a todos. Hoje vou apresentar o nosso trabalho de pesquisa Aprendizagem para raciocinar dedutivamente: resolução de math word problem como relation extraction complexas.</seg>
<seg id="2">Sou o Allan do ByteDance IA Lab, e este é um trabalho conjunto com Jierui Li, da Universidade do Texas em Austin, e Wei Lu, da SUTD.</seg>
<seg id="3">Primeiro, gostaria de falar sobre a nossa motivação para o raciocínio.</seg>
<seg id="4">Mostramos aqui exemplos em que o raciocínio em vários passos é útil.</seg>
<seg id="5">Este número é retirado do artigo PaLM, onde eles realizam solicitações para resolver o problema da rede no cenário de aprendizagem curta.</seg>
<seg id="6">Então, no lado esquerdo, podemos ver que, se dermos alguns exemplos com apenas pergunta e respostas, podemos não conseguir obter as respostas corretas.</seg>
<seg id="7">Mas se dermos uma descrição de raciocínio, o modelo é capaz de prever a descrição de raciocínio e também fazer uma previsão correta aqui.</seg>
<seg id="8">É bom ter raciocínio interpretável multietapas como saída.</seg>
<seg id="9">E também achamos que math world problem é uma aplicação direta para avaliar tais capacidades de raciocínio.</seg>
<seg id="10">Aqui na nossa configuração de problema, damos as perguntas de precisamos para resolver essa pergunta e obter as respostas numéricas.</seg>
<seg id="11">Assim, nos nossos conjuntos de dados, também recebemos a expressão matemática que leva a essa resposta em particular.</seg>
<seg id="12">Assim, certas suposições também se aplicam como no trabalho anterior.</seg>
<seg id="13">Assumimos que a precisão das quantidades é conhecida.</seg>
<seg id="14">E consideramos apenas operadores básicos como adição, subtração, multiplicação, divisão e exponencial.</seg>
<seg id="15">Além disso, os operadores complicados podem ser realmente decompostos nestes operadores básicos.</seg>
<seg id="16">Assim, o trabalho anterior em resolução de math word problem pode realmente ser categorizado em sequência para sequência e sequência para modelo de árvore.</seg>
<seg id="17">Assim, o modelo tradicional de sequência para sequência converte a expressão para uma sequência para geração específica.</seg>
<seg id="18">E é muito fácil de implementar e pode generalizar para muitos problemas complicados diferentes.</seg>
<seg id="19">Mas as desvantagens são que o desempenho geralmente não é melhor do que o modelo estruturado e a sua falta de interpretabilidade para previsão.</seg>
<seg id="20">Mas, na verdade, esta direção ainda é bastante popular por causa do modelo transformador.</seg>
<seg id="21">Então, em modelos baseados em árvore, nós realmente estruturamos estas expressões na forma de árvore e seguimos uma travessia preordenada em gerações de árvores.</seg>
<seg id="22">Então, aqui continuamos a gerar os operadores até chegarmos às folhas, que são as quantidades.</seg>
<seg id="23">O que é bom aqui é que realmente nos dá esta estrutura de árvore binária, e é, mas na verdade é bastante contraintuitivo porque geramos o operador primeiro e depois, no final, geramos as quantidades.</seg>
<seg id="24">E a segunda coisa é que também contém alguns cálculos repetitivos.</seg>
<seg id="25">Aqui, se olharmos para esta expressão, oito vezes três mais três é realmente gerado duas vezes, mas na verdade devemos reutilizar os resultados.</seg>
<seg id="26">Então, na nossa abordagem proposta, queremos resolver estes problemas passo a passo e de formas interpretáveis.</seg>
<seg id="27">Então por exemplo, aqui no segundo passo, podemos obter estes divisores, vinte e sete.</seg>
<seg id="28">E também podemos consultar de novo as perguntas originais para encontrar os conteúdos relevantes.</seg>
<seg id="29">E nestes passos obtemos os divisores.</seg>
<seg id="30">E então neste terceiro passo nós conseguimos obter o quociente.</seg>
<seg id="31">Certo. E após estes três passos, podemos mesmo reutilizar os resultados do segundo passo e, em seguida, obter os resultados do quarto passo e, finalmente, conseguimos obter os dividendos.</seg>
<seg id="32">Então, aqui geramos toda a expressão diretamente em vez de gerar um único operador ou quantidades.</seg>
<seg id="33">Isto torna o processo mais preciso.</seg>
<seg id="34">No nosso sistema dedutivo, primeiro começamos com algumas quantidades apresentadas nas perguntas e também incluímos alguma constante como o nosso estado inicial.</seg>
<seg id="35">Assim, a expressão é representada por e i j o p.</seg>
<seg id="36">Onde realizamos o operador de q_i a q_j, e tal expressão é realmente direcionada.</seg>
<seg id="37">Então, também temos a subtração com palavras aqui para representar a direção oposta.</seg>
<seg id="38">Isto é bastante semelhante a relation extraction.</seg>
<seg id="39">Assim, num sistema dedutivo formal, num passo de tempo t, aplicamos o operador entre o par q_i e q_j, e então obtemos esta nova expressão.</seg>
<seg id="40">Adicionamo-la ao próximo estado para se tornar uma nova quantidade.</seg>
<seg id="41">É possível visualizar nestes diapositivos a evolução do estado em que continuamos a adicionar expressão ao estado atual.</seg>
<seg id="42">Nas nossas implementações de modelo, primeiro usamos um modelo de linguagem pré-treinada que pode ser BERTs ou Robertas e depois codificamos a frase e então obtemos estas representações de quantidades.</seg>
<seg id="43">Assim que obtemos as representações de quantidade, podemos começar a fazer inferência.</seg>
<seg id="44">Aqui mostramos um exemplo de q_1 para obter a representação para q_2 dividida por q_2 e depois vezes q_3.</seg>
<seg id="45">Primeiro obtemos a representação de par, que é basicamente apenas a concatenação entre q_1 e q_2, e então aplicamos uma rede de controlo por antecipação que é parametrizada pelo operador.</seg>
<seg id="46">E então, finalmente, obtemos a representação da expressão q_1 dividida por q_2.</seg>
<seg id="47">Mas, de facto, na prática, na fase de inferência, podemos ser capazes de obter também a expressão incorreta.</seg>
<seg id="48">Aqui, toda a expressão possível é igual a três vezes o número de operadores.</seg>
<seg id="49">O que é bom aqui é que podemos facilmente adicionar restrições para controlar este espaço de pesquisa.</seg>
<seg id="50">Por exemplo, se esta expressão não for permitida, podemos simplesmente remover esta expressão no nosso espaço de pesquisa.</seg>
<seg id="51">Então, no segundo passo, fazemos a mesma coisa, mas a única diferença é que a única diferença é mais uma quantidade.</seg>
<seg id="52">Portanto, esta quantidade vem da expressão calculada anterior.</seg>
<seg id="53">Então podemos finalmente obter esta expressão final q_3 vezes q_4.</seg>
<seg id="54">E também podemos ver que o número de todas as expressões possíveis é diferente do passo anterior.</seg>
<seg id="55">Então, tal diferença torna difícil aplicar beam search porque a distribuição de probabilidade entre estes dois passos é desequilibrada.</seg>
<seg id="56">Assim, o procedimento de treinamento é semelhante ao treinamento de um modelo de sequência a sequência, onde otimizamos a perda em cada passo de tempo.</seg>
<seg id="57">E aqui também usamos este tau para representar quando devemos encerrar este processo de geração.</seg>
<seg id="58">E aqui o espaço é diferente de sequência a sequência porque o espaço é diferente em cada passo de tempo, enquanto que no modelo tradicional de sequência a sequência, este é o número de vocabulário.</seg>
<seg id="59">E também nos permite impor certas restrições do conhecimento anterior.</seg>
<seg id="60">Assim, realizamos experiências nos conjuntos de dados de math word problems, MAWPS, Math23K, MathQA e SVAMP tipicamente usados.</seg>
<seg id="61">E aqui mostramos brevemente os resultados comparados com as melhores abordagens anteriores.</seg>
<seg id="62">Portanto, a nossa variante com melhor desempenho é Roberta-DeductiveReasoner.</seg>
<seg id="63">E, na verdade, não usamos beam search, em contraste, todas as abordagens anteriores estão a usar beam search.</seg>
<seg id="64">Muito bem. Assim, as melhores abordagens são muitas vezes baseadas no modelo em árvore.</seg>
<seg id="65">Então, no geral, o nosso raciocinador é capaz de superar significativamente este modelo baseado em árvore.</seg>
<seg id="66">Mas podemos ver que os números absolutos no MathQA ou SVAMP não são muito altos.</seg>
<seg id="67">Então, investigamos ainda mais os resultados no SVAMP.</seg>
<seg id="68">E este conjunto de dados é desafiador porque o autor tentou manualmente adicionar algo para confundir o modelo NLP, como adicionar informações irrelevantes e quantidades extra.</seg>
<seg id="69">Então, na nossa previsão, descobrimos que alguns dos valores intermediários são realmente negativos.</seg>
<seg id="70">Por exemplo, nestas perguntas, perguntamos quantas maçãs tem o Jake?</seg>
<seg id="71">Mas temos algumas informação extra, como dezassete fotografias a menos, e o Steven tem oito fotografias, o que é totalmente irrelevante.</seg>
<seg id="72">O nosso modelo faz alguma previsão como esta que está a produzir valores negativos.</seg>
<seg id="73">E observamos que estas duas expressões realmente têm classificações semelhantes.</seg>
<seg id="74">Então, podemos realmente limitar este espaço de pesquisa removendo os resultados negativos para que possamos criar uma resposta correta.</seg>
<seg id="75">Então, descobrimos ainda que esta restrição melhora muito para alguns modelos.</seg>
<seg id="76">Por exemplo, para BERT, melhoramos sete pontos e depois, para o modelo básico do Roberta, na verdade melhoramos dois pontos.</seg>
<seg id="77">Um melhor modelo de linguagem tem melhores capacidades de language understanding para que o número aqui seja maior para o Roberta e menor para o BERT.</seg>
<seg id="78">Também tentamos analisar a dificuldade entre estes por trás de todos estes conjuntos de dados.</seg>
<seg id="79">Assumimos que o número de quantidades não utilizadas pode ser considerado informação irrelevante aqui.</seg>
<seg id="80">Então, aqui podemos ver que temos a percentagem de amostras com quantidades não utilizadas, e o conjunto de dados SVAMP tem a maior porção.</seg>
<seg id="81">E aqui também mostramos o desempenho geral.</seg>
<seg id="82">Para essas amostras sem quantidades não utilizadas, o desempenho é maior do que o desempenho geral.</seg>
<seg id="83">Mas com essas amostras com quantidades não utilizadas é realmente muito pior do que o desempenho geral.</seg>
<seg id="84">Para MAWPS, nós não temos muitos casos de teste, então eu simplesmente ignoro esta parte.</seg>
<seg id="85">Finalmente, queremos mostrar a interpretabilidade através de um exemplo de perturbação de pergunta.</seg>
<seg id="86">Aqui, o nosso modelo, na verdade, faz uma previsão errada no primeiro passo.</seg>
<seg id="87">Podemos realmente correlacionar esta expressão com a frase aqui. Certo.</seg>
<seg id="88">Então, achamos que esta frase pode estar a enganar o modelo para previsões incorretas.</seg>
<seg id="89">Aqui, plantar mais trinta e cinco faz o modelo pensar que deveria ser um operador de adição.</seg>
<seg id="90">Então tentamos rever a frase para ser algo como o número de pereiras são trinta e cinco menos do que as macieiras.</seg>
<seg id="91">Nós fazemos isto para transmitir uma semântica mais precisa, de modo a que o modelo seja capaz de fazer a previsão correta.</seg>
<seg id="92">Portanto, este estudo mostra como as previsões interpretáveis nos ajudam a perceber o comportamento do modelo.</seg>
<seg id="93">Então, para concluir o nosso trabalho, primeiro, o nosso modelo é realmente muito eficiente.</seg>
<seg id="94">E somos capazes de fornecer um procedimento de resolução interpretável.</seg>
<seg id="95">E podemos facilmente incorporar algum conhecimento prévio como restrição que pode ajudar a melhorar o desempenho.</seg>
<seg id="96">E a última coisa é que o mecanismo subjacente não se aplica apenas a tarefas de resolução de problemas de rede, mas também a outras tarefas que envolvem raciocínio de vários passos.</seg>
<seg id="97">Também temos certas limitações.</seg>
<seg id="98">Se tivermos um grande número de operadores ou constantes, o consumo de memória pode ser bastante alto.</seg>
<seg id="99">E a segunda coisa é que, como mencionado, porque a distribuição de probabilidade é desequilibrada entre diferentes passos de tempo, também é bastante desafiador aplicar a estratégia de beam search.</seg>
<seg id="100">Este é o fim da palestra, e perguntas são bem-vindas. Obrigado.</seg>
</doc>
<doc docid="2022.acl-long.468" genre="presentations">
<talkid>2022.acl-long.468</talkid>
<abstract>Statutory article retrieval is the task of automatically retrieving law articles relevant to a legal question. While recent advances in natural language processing have sparked considerable interest in many legal tasks, statutory article retrieval remains primarily untouched due to the scarcity of large-scale and high-quality annotated datasets. To address this bottleneck, we introduce the Belgian Statutory Article Retrieval Dataset (BSARD), which consists of 1,100+ French native legal questions labeled by experienced jurists with relevant articles from a corpus of 22,600+ Belgian law articles. Using BSARD, we benchmark several state-of-the-art retrieval approaches, including lexical and dense architectures, both in zero-shot and supervised setups. We find that fine-tuned dense retrieval models significantly outperform other systems. Our best performing baseline achieves 74.8% R@100, which is promising for the feasibility of the task and indicates there is still room for improvement. By the specificity of the domain and addressed task, BSARD presents a unique challenge problem for future research on legal information retrieval. Our dataset and source code are publicly available.</abstract>
<seg id="101">Olá, o meu nome é Antoine e sou da Universidade de Maastricht.</seg>
<seg id="102">Vou apresentar o meu trabalho conjunto com o Jerry, sobre um Novo conjunto de dados para recuperação de artigos estatutários.</seg>
<seg id="103">Questões legais são parte integrante da vida de muitas pessoas.</seg>
<seg id="104">Mas a maioria dos cidadãos tem pouco a nenhum conhecimento sobre os seus direitos e processos legais fundamentais.</seg>
<seg id="105">Como resultado, muitos cidadãos vulneráveis que não podem pagar a assistência dispendiosa de um especialista jurídico são deixados desprotegidos ou, pior, explorados.</seg>
<seg id="106">Todo o trabalho visa colmatar a lacuna entre as pessoas e a lei através do desenvolvimento de um sistema de recuperação eficaz para artigos estatutários.</seg>
<seg id="107">Tal sistema poderia fornecer um serviço de ajuda jurídica profissional gratuito para humanos não qualificados.</seg>
<seg id="108">Antes de mergulhar na principal contribuição deste trabalho, vamos primeiro descrever o problema da recuperação de artigos estatutários.</seg>
<seg id="109">Dada uma simples pergunta sobre uma questão legal, como, o que arrisco se violar o sigilo profissional?</seg>
<seg id="110">Um modelo é necessário para recuperar todos os artigos estatutários relevantes de um corpo grande de legislação.</seg>
<seg id="111">Esta tarefa de recuperação de informações vem com o seu próprio conjunto de desafios.</seg>
<seg id="112">Primeiro, lida com dois tipos de linguagem.</seg>
<seg id="113">Linguagem natural comum para as perguntas e linguagem jurídica complexa para os estatutos.</seg>
<seg id="114">Esta diferença nas distribuições de idioma torna mais difícil para um sistema recuperar candidatos relevantes, pois requer indiretamente um sistema de interpretação inerente que possa traduzir uma pergunta natural para uma pergunta legal que corresponda à terminologia dos estatutos.</seg>
<seg id="115">Além disso, a lei estatutária não é uma pilha de artigos independentes que podem ser tratados como uma fonte completa de informações por conta própria, ao contrário de notícias ou receitas, por exemplo.</seg>
<seg id="116">Em vez disso, é uma coleção estruturada de disposições legais que têm todo um significado apenas quando consideradas no contexto geral, ou seja, juntamente com as informações suplementares dos artigos vizinhos, os campos e subcampos aos quais pertencem e seu lugar na estrutura da lei.</seg>
<seg id="117">Por fim, os artigos estatutários não são parágrafos pequenos, que geralmente são a unidade típica de recuperação na maioria dos trabalhos de recuperação.</seg>
<seg id="118">Aqui, há documentos longos que podem chegar a seis mil palavras.</seg>
<seg id="119">Os avanços recentes em NLP despertaram um enorme interesse em muitas tarefas legais, como previsão de julgamentos legais ou revisão automatizada de contratos de contacto.</seg>
<seg id="120">Mas a recuperação de artigos estatutários permaneceu praticamente intocada devido à falta de conjuntos de dados rotulados grandes de alta qualidade.</seg>
<seg id="121">Neste trabalho, apresentamos um novo conjunto de dados centrado no cidadão nativo francês para estudar se modelos de recuperação se podem aproximar à eficiência e fiabilidade de um especialista jurídico para a tarefa de recuperação de artigos estatutários.</seg>
<seg id="122">O nosso conjunto de dados BSARD de recuperação de artigos estatutários belga consiste em mais de mil e cem perguntas legais feitas por cidadãos belgas.</seg>
<seg id="123">Estas perguntas abrangem uma ampla gama de tópicos, desde família, alojamento, dinheiro, trabalho e segurança social.</seg>
<seg id="124">Cada um deles foi rotulado por juristas experientes com referências a artigos relevantes de um corpus linguístico de mais de vinte e dois mil e seiscentos artigos jurídicos de códigos de direito belgas.</seg>
<seg id="125">Vamos agora falar sobre como recolhemos este conjunto de dados.</seg>
<seg id="126">Primeiro, começámos por compilar um grande corpus linguístico de artigos jurídicos.</seg>
<seg id="127">Considerámos trinta e dois códigos belgas publicamente disponíveis e extraímos todos os artigos, bem como os títulos das secções correspondentes.</seg>
<seg id="128">Em seguida, reunimos perguntas legais com referências a estatutos relevantes.</seg>
<seg id="129">Para isso, fazemos parceria com o escritório de advocacia belga que recebe anualmente cerca de quatro mil e-mails de cidadãos belgas que pedem conselhos para uma questão jurídica pessoal.</seg>
<seg id="130">Tivemos a sorte de ter acesso aos seus sites, onde a sua equipa de juristas experientes aborda as questões jurídicas mais comuns dos belgas.</seg>
<seg id="131">Recolhemos milhares de perguntas anotadas com categorias, subcategorias e referências legais a estatutos relevantes.</seg>
<seg id="132">Por fim, passámos as referências legais e filtrámos as perguntas cujas referências não eram artigos em um dos códigos de direito que considerámos.</seg>
<seg id="133">As referências restantes foram combinadas e convertidas para os identificadores do artigo correspondente do nosso corpus linguístico.</seg>
<seg id="134">Acabámos com mil cento e oito perguntas, cada uma cuidadosamente rotulada com os identificadores dos artigos relevantes do nosso grande corpus linguístico de vinte e dois mil e seiscentos e trinta e três artigos estatutários.</seg>
<seg id="135">Além disso, cada pergunta vem com a categoria principal e uma concatenação de subcategorias.</seg>
<seg id="136">E cada artigo vem com uma concatenação do título de subsequência na estrutura da lei.</seg>
<seg id="137">Esta informação extra não é usada no presente trabalho, mas pode ser de interesse para futuras pesquisas sobre recuperação de informação legal ou classificação de texto legal.</seg>
<seg id="138">Vejamos algumas características do nosso conjunto de dados.</seg>
<seg id="139">As perguntas têm entre cinco e quarenta e quatro palavras de comprimento com uma mediana de catorze palavras.</seg>
<seg id="140">Os artigos são muito mais longos, com um comprimento médio de setenta e sete palavras, com cento e quarenta e dois deles excedendo as mil palavras.</seg>
<seg id="141">O mais longo sendo até cinco mil setecentos e noventa palavras.</seg>
<seg id="142">Como mencionado anteriormente, as perguntas abrangem uma ampla gama de tópicos, com cerca de oitenta e cinco por cento deles sendo sobre família, alojamento, dinheiro ou justiça.</seg>
<seg id="143">Enquanto os restantes quinze por cento dizem respeito a segurança social, estrangeiros ou trabalho.</seg>
<seg id="144">O artigo também é muito diversificado, pois vêm de trinta e dois códigos belgas diferentes que cobrem um grande número de tópicos legais.</seg>
<seg id="145">Aqui está o número total de artigos recolhidos de cada um destes códigos belgas.</seg>
<seg id="146">Dos vinte e dois mil seiscentos e trinta e três artigos, apenas mil seiscentos e doze são referidos como relevantes para pelo menos uma pergunta no conjunto de dados.</seg>
<seg id="147">E cerca de oitenta por cento destes artigos citados vêm do código civil, códigos judiciais, códigos de investigação criminal ou códigos penais.</seg>
<seg id="148">Enquanto isso, dezoito dos trinta e dois códigos têm menos de cinco artigos mencionados como relevantes para pelo menos uma pergunta.</seg>
<seg id="149">O que pode ser explicado pelo facto de que esses códigos se concentraram menos em indivíduos e nas suas preocupações.</seg>
<seg id="150">No geral, o número mediano de citações para estes artigos citados é de dois, e menos de vinte e cinco por cento deles são citados mais de cinco vezes.</seg>
<seg id="151">Usando todos os conjuntos de dados, comparámos várias abordagens de recuperação, incluindo arquitetura lexical e densa.</seg>
<seg id="152">Dada uma consulta e um artigo, um modelo lexical atribui uma pontuação ao par de artigos de consulta calculando a soma sobre os termos de consulta dos pesos de cada um desses termos nesse artigo.</seg>
<seg id="153">Experimentamos as funções de classificação TF-IDF e BM25 padrão.</seg>
<seg id="154">O principal problema com estas abordagens é que só podem recuperar artigos que contenham palavras-chave presentes na consulta.</seg>
<seg id="155">Para superar esta limitação, experimentamos uma arquitetura de base neural que pode capturar relações semânticas entre consultas e o artigo.</seg>
<seg id="156">Usamos um modelo bi-codificador que mapeia consultas e artigos em representações de vetor densas e calcula uma pontuação de relevância entre um par de artigos de consulta pela similaridade das suas integrações.</seg>
<seg id="157">Estas integrações normalmente resultam de uma operação de acumulação na saída de um modelo de integrações de palavras.</seg>
<seg id="158">Primeiro, estudamos a eficácia dos bi-codificadores siameses numa configuração de avaliação de tiro zero, num significado de que os modelos pré-treinados de integração de palavras são aplicados como estão sem qualquer ajuste fino adicional.</seg>
<seg id="159">Nós experimentamos com um codificador de texto de contexto independente, ou seja, word2vec e fastText, e modelos de integração de contexto dependentes, ou seja Roberta e mais especificamente CamemBERT que é um modelo francês do Roberta.</seg>
<seg id="160">Além disso, treinamos nosso próprio CamemBERT com base num modelo de bi-codificadores no nosso conjunto de dados.</seg>
<seg id="161">Observe que para treinamento, experimentamos os dois sabores da arquitetura de bi-codificador.</seg>
<seg id="162">O siamês, que usa um modelo de integração de palavras exclusivo que mapeia a consulta e o artigo juntos num espaço de vetor compartilhado denso, e duas torres, que usa dois modelos de integração de palavras independentes que codificam a consulta e o artigo separadamente em diferentes espaços de integração.</seg>
<seg id="163">Nós experimentamos com acumulação CLS média e máxima, bem como produto e cosseno para semelhanças de computação.</seg>
<seg id="164">Aqui está o resultado da nossa linha de base nos conjuntos de teste.</seg>
<seg id="165">Com os métodos lexicais acima, os bi-codificadores siameses avaliados numa configuração de tiro zero no meio e os bi-codificadores afinados abaixo.</seg>
<seg id="166">No geral, o bi-codificador ajustado supera significativamente todas as outras linhas de referência.</seg>
<seg id="167">O modelo de duas torres melhora em relação às suas variantes siamesas na recuperação em cem, mas tem um desempenho semelhante nas outras métricas.</seg>
<seg id="168">Embora o BM25 tenha tido um desempenho inferior ao do bi-codificador treinado significativamente, o seu desempenho indicou que ainda é uma linha de referência forte para recuperação específica de domínio.</seg>
<seg id="169">Em relação à avaliação de tiro zero do bi-codificador siamês, descobrimos que usar diretamente as integrações de um modelo CamemBERT pré-treinado sem otimizar para a tarefa recuperação de informações dá resultados maus, o que é consistente com os resultados anteriores.</seg>
<seg id="170">Além disso, observamos que o bi- codificador baseado em word2vec superou significativamente os modelos baseados em fastText e BERT, sugerindo que talvez as integrações ao nível da palavra pré-treinada seja mais apropriado para a tarefa do que o nível de caractere ou integrações ao nível de subpalavra quando usado como está.</seg>
<seg id="171">Embora promissores, estes resultados sugerem uma ampla oportunidade para melhoria comparado com um especialista jurídico qualificado que pode eventualmente recuperar todos os artigos relevantes para qualquer pergunta e, assim, obter pontuações perfeitas.</seg>
<seg id="172">Vamos concluir discutindo duas limitações do nosso conjunto de dados.</seg>
<seg id="173">Em primeiro lugar, o corpus linguístico de artigos limita-se aos recolhidos a partir dos trinta e dois códigos belgas considerados, o que não abrange toda a lei belga, uma vez que faltam artigos de decretos, diretivas e portarias.</seg>
<seg id="174">Durante a construção do conjunto de dados, todas as referências a estes artigos não recolhidos são ignoradas, o que faz com que algumas perguntas acabem com apenas uma fração do número inicial de artigos relevantes.</seg>
<seg id="175">Esta informação, portanto, implica que a resposta contida nos artigos relevantes restantes pode estar incompleta, embora seja completamente apropriada.</seg>
<seg id="176">Em segundo lugar, devemos notar que não se pode responder a todas as perguntas legais apenas com estatutos.</seg>
<seg id="177">Por exemplo, a pergunta, posso despejar os meus inquilinos se fizerem muito barulho?</seg>
<seg id="178">Pode não ter uma resposta detalhada dentro da lei estatutária que quantifique um limite de ruído específico a partir do qual o despejo é permitido.</seg>
<seg id="179">Em vez disso, o proprietário provavelmente deve confiar mais na jurisprudência e encontrar precedentes semelhantes à sua situação atual.</seg>
<seg id="180">Por exemplo, os inquilinos fazem duas festas por semana até às duas da manhã.</seg>
<seg id="181">Consequentemente, algumas perguntas são mais adequadas do que outras para a tarefa de recuperação de artigos estatutários, e o domínio das menos adequadas continua indeterminado.</seg>
<seg id="182">Esperamos que o nosso trabalho desperte interesse no desenvolvimento de modelos de recuperação de artigos estatutários práticos e fiáveis.</seg>
<seg id="183">Isso pode ajudar a melhorar o acesso à justiça para todos.</seg>
<seg id="184">Podem ver o nosso artigo, conjunto de dados e código nos links a seguir. Obrigado.</seg>
</doc>
<doc docid="2022.acl-long.567" genre="presentations">
<talkid>2022.acl-long.567</talkid>
<abstract>We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V&L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&L models from a linguistic perspective, complementing the canonical task-centred V&L evaluations.</abstract>
<seg id="185">Olá, estamos felizes em apresentar nosso trabalho em VALSE; um termo de comparação independente de tarefas feito para testar a visão e modelos de linguagem com fenómenos linguísticos específicos.</seg>
<seg id="186">Porque é que nos esforçámos para estabelecer este termo de comparação?</seg>
<seg id="187">Bem, durante os últimos anos, vimos uma explosão de visão baseada em transformadores e modelos de linguagem pré-treinados em grandes quantidades de pares imagem-texto.</seg>
<seg id="188">Cada um destes modelos impulsiona a última geração em tarefas de visão e idioma, como visual question answering, raciocínio visual de sentido comum, recuperação de imagem, embasamento de frases.</seg>
<seg id="189">Então, recebemos uma mensagem, as precisões nestas tarefas e termos de comparação específicos estão a aumentar de forma constante.</seg>
<seg id="190">Mas sabemos o que os modelos realmente aprenderam?</seg>
<seg id="191">O que é que um transformador de visão e idioma compreendeu ao atribuir uma pontuação alta para esta imagem e esta frase para combinar?</seg>
<seg id="192">E a pontuação baixa para esta?</seg>
<seg id="193">Os modelos de linguagem e visão concentram-se na coisa certa?</seg>
<seg id="194">Ou concentram-se em preconceitos como mostrado pelo trabalho anterior?</seg>
<seg id="195">Para lançar mais luz sobre este aspecto, propomos uma direção mais agnóstica à tarefa e introduzimos o VALSE, que testa a sensibilidade de modelos de linguagem e visão a fenómenos linguísticos específicos que afetam as modalidades linguísticas e visuais.</seg>
<seg id="196">Tomámos como alvo a existência, pluralidade, contagem, relações espaciais, ações e correferência de entidade.</seg>
<seg id="197">Mas como testamos se os modelos de linguagem e visão capturaram este fenómeno?</seg>
<seg id="198">Ao frustrar um método anteriormente aplicado para modelos de linguagem e visão apenas para frases com substantivos de Ravi Shekhar e colaboradores, e na contagem por nós em trabalhos anteriores.</seg>
<seg id="199">Frustar basicamente significa que pegamos na legenda de uma imagem e produzimos um frustração que altera a legenda de modo a que deixe de descrever a imagem.</seg>
<seg id="200">E fazemos estas alterações na frase concentrando-nos em seis peças específicas, como existência, pluralidade, contagem, relações espaciais, ações e correferência de entidade, onde cada peça pode consistir em um ou mais instrumentos, caso encontremos mais de uma forma interessante de criar instâncias de frustação.</seg>
<seg id="201">Por exemplo, no caso da peça de ações, temos dois instrumentos, um em que o verbo de ação é alterado com uma ação diferente e um em que os actantes são trocados.</seg>
<seg id="202">Contagem e correferência também são peças que possuem mais de um instrumento.</seg>
<seg id="203">E nós criámos estas frustação, certificando-se de que não descrevem a imagem, mas que são frases válidas gramaticalmente e de outras formas.</seg>
<seg id="204">Isto não é fácil de fazer porque uma legenda frustrada pode ser menos provável do que a legenda original.</seg>
<seg id="205">Por exemplo, embora não seja impossível, é estatisticamente menos provável que as plantas cortem um homem do que um homem corte plantas, e modelos de linguagem e visão grandes podem captar isto.</seg>
<seg id="206">Assim, para obter frustrações válidas, devemos agir.</seg>
<seg id="207">Primeiro, fazemos uso de modelos de linguagem fortes para propor frustrações.</seg>
<seg id="208">Em segundo lugar, usamos natural language inference ou NLI curtas para filtrar frustrações que ainda podem estar a descrevendo a imagem, já que ao construir frustrações precisamos de garantir que não descrevem a imagem.</seg>
<seg id="209">Para testar isto automaticamente, aplicamos natural language inference com o seguinte raciocínio.</seg>
<seg id="210">Consideramos uma imagem como a premissa e a sua legenda como a sua hipótese implicada.</seg>
<seg id="211">Além disso, consideramos a legenda como a premissa, e a frustração é a sua hipótese.</seg>
<seg id="212">Se um modelo NLI prevê que a frustração contradiga ou seja neutra em relação à legenda, tomamos isto como um indicador de uma frustação válida.</seg>
<seg id="213">Se uma NLI prevê que a frustração seja implicada pela legenda, não pode ser uma boa frustração, uma vez que, por transitividade, dará uma descrição verdadeira da imagem, e filtramos estas frustrações.</seg>
<seg id="214">Mas este procedimento não é perfeito, é apenas um indicador para frustrações válidas.</seg>
<seg id="215">Assim, como uma terceira medida para gerar frustrações válidas, empregamos anotadores humanos para validar os dados usados no VALSE.</seg>
<seg id="216">Assim, após a filtragem e avaliação humana, temos tantas instâncias de teste como descrito nesta tabela.</seg>
<seg id="217">Pode observar-se que o VALSE não fornece nenhuns dados de treinamento, apenas testa dados.</seg>
<seg id="218">Uma vez que é apenas um termo de comparação de teste de tiro zero, é projetado para alavancar as capacidades existentes de modelos de linguagem e visão após pré-treinamento.</seg>
<seg id="219">O ajuste fino só permitiria aos modelos explorar artefatos ou preconceitos estatísticos nos dados.</seg>
<seg id="220">E todos nós sabemos que estes modelos gostam de fazer batota e usar atalhos.</seg>
<seg id="221">E, como dissemos, estamos interessados em avaliar quais as capacidades de modelos de linguagem e visão têm após o pré-treinamento.</seg>
<seg id="222">Experimentamos cinco modelos de linguagem e visão no VALSE, ou seja com CLIP, LXMert, ViLBERT, ViLBERT doze em um e VisualBERT.</seg>
<seg id="223">Duas das nossas mais importantes métricas de avaliação são a precisão dos modelos para classificar pares de imagem-frase em legendas e frustrações.</seg>
<seg id="224">Talvez mais relevante para este vídeo, mostraremos a nossa métrica mais permissiva, a precisão par a par, que mede se a pontuação de alinhamento de frases-imagem é maior para o par imagem-texto correto do que para o seu par frustrado.</seg>
<seg id="225">Para mais métricas e resultados sobre eles, veja o nosso artigo.</seg>
<seg id="226">Os resultados com precisão par a par são mostrados aqui e são consistentes com os resultados que obtivemos das outras métricas é que o melhor desempenho de tiro zero é alcançado pelo ViLBERT doze em um, seguido pelo ViLBERT, LXMert, CLIP e, finalmente, VisualBERT.</seg>
<seg id="227">É notável como instrumentos centrados em objetos individuais como existência e frases com substantivo são quase resolvidos pelo ViLBERT doze em um, destacando que modelos são capazes de identificar objetos nomeados e a sua presença em imagens.</seg>
<seg id="228">No entanto, nenhuma das peças restantes pode ser resolvida de forma fiável nas nossas configurações de frustração adversárias.</seg>
<seg id="229">Vemos a partir da pluralidade e dos instrumentos de contagem que os modelos de linguagem e visão têm dificuldade em distinguir referências a objetos únicos contra múltiplos, ou contá-los numa imagem.</seg>
<seg id="230">A peça de relação mostra que têm dificuldades em classificar corretamente uma relação espacial nomeada entre objetos numa imagem.</seg>
<seg id="231">Também têm dificuldade em distinguir ações e identificar os seus participantes, mesmo que apoiados por preconceitos de plausibilidade como vemos na peça de ações.</seg>
<seg id="232">A partir da peça de correferência, descobrimos que traçar várias referências ao mesmo objeto numa imagem usando pronomes também é difícil para modelos de linguagem e visão.</seg>
<seg id="233">Como uma verificação de sanidade, e porque é uma experiência interessante, também comparamos dois modelos apenas de texto, GPT um e GPT dois, para avaliar se o VALSE é solucionável por estes modelos unimodais calculando a perplexidade da legenda correta e frustrada, sem imagem aqui, e prevendo a entrada com a menor perplexidade.</seg>
<seg id="234">Se a perplexidade é maior para a frustração, tomamos isto como uma indicação de que a legenda frustrada pode sofrer de preconceitos linguísticos de plausibilidade.</seg>
<seg id="235">E é interessante ver que, em alguns casos, os modelos GPT apenas com texto capturaram a plausibilidade do mundo melhor do que os modelos de linguagem e visão.</seg>
<seg id="236">Então, para resumir, o VALSE é uma referência que usa a lente de construções linguísticas para ajudar a comunidade a melhorar modelos de linguagem e visão testando duramente as suas capacidades embasamento visuais.</seg>
<seg id="237">As nossas experiências mostram que os modelos de linguagem e visão identificam bem os objetos nomeados e sua presença nas imagens, como mostra a peça de existência, mas lutam para fundamentar a sua interdependência e relações em cenas visuais quando forçados a respeitar indicadores linguísticos.</seg>
<seg id="238">Gostaríamos muito de encorajar a comunidade a usar o VALSE para medir o progresso em direção a embasamento de idioma com modelos de linguagem e visão.</seg>
<seg id="239">E além disso, o VALSE poderia ser usado como uma avaliação indireta de conjuntos de dados, já que os modelos poderiam ser avaliados antes e depois do treinamento ou ajuste fino para ver se um conjunto de dados ajuda os modelos a melhorar em qualquer um dos aspectos testados pelo VALSE.</seg>
<seg id="240">Se houver interesse, os dados do VALSE podem ser vistos no GitHub e, se houverem perguntas, não hesitem em contactar-nos.</seg>
</doc>
<doc docid="2022.acl-long.597" genre="presentations">
<talkid>2022.acl-long.597</talkid>
<abstract>A release note is a technical document that describes the latest changes to a software product and is crucial in open source software development. However, it still remains challenging to generate release notes automatically. In this paper, we present a new dataset called RNSum, which contains approximately 82,000 English release notes and the associated commit messages derived from the online repositories in GitHub. Then, we propose classwise extractive-then-abstractive/abstractive summarization approaches to this task, which can employ a modern transformer-based seq2seq network like BART and can be applied to various repositories without specific constraints. The experimental results on the RNSum dataset show that the proposed methods can generate less noisy release notes at higher coverage than the baselines. We also observe that there is a significant gap in the coverage of essential information when compared to human references. Our dataset and the code are publicly available.</abstract>
<seg id="241">Olá, o meu nome é Kamezawa da Universidade de Tóquio.</seg>
<seg id="242">Vou apresentar um artigo intitulado RNSum: um conjunto de dados de grande escala para geração automática de notas de lançamento através de sumarização de registos de confirmação.</seg>
<seg id="243">Vou explicar nesta ordem.</seg>
<seg id="244">Primeiro, apresentarei a geração automática de notas de lançamento em que estamos a trabalhar nesta pesquisa.</seg>
<seg id="245">Uma nota de lançamento é um documento técnico que resume as mudanças distribuídas com cada lançamento de um produto de software.</seg>
<seg id="246">A imagem mostra uma nota de lançamento para a versão 2.6.4 da biblioteca vuejs.</seg>
<seg id="247">As notas de lançamento desempenham um papel importante no desenvolvimento de código aberto, mas consomem tempo ao serem preparadas manualmente.</seg>
<seg id="248">Assim, seria muito útil ser capaz de gerar notas de lançamento de alta qualidade automaticamente.</seg>
<seg id="249">Vou referir-me a duas pesquisas anteriores sobre a geração automática de notas de lançamento.</seg>
<seg id="250">O primeiro é um sistema chamado ARENA lançado em 2014.</seg>
<seg id="251">É precisa uma abordagem baseada em regras, por exemplo, usando o extrator de alterações para extrair todas as diferenças, alterações de biblioteca e alterações de documento das diferenças entre versões e, finalmente, combiná-las.</seg>
<seg id="252">A característica mais notável deste sistema é o extrator de problemas no canto superior direito.</seg>
<seg id="253">Que deve ser deixado para o Jira, o sistema monitorizador de problemas, e só pode ser aplicado a projetos que usam o Jira.</seg>
<seg id="254">Por outras palavras, não pode ser usado para muitos projetos no GitHub.</seg>
<seg id="255">O segundo é Glyph, anunciado recentemente em 2020.</seg>
<seg id="256">Está disponível na internet e pode ser instalado através do pip.</seg>
<seg id="257">Este sistema tem uma aprendizagem simples baseada num modelo de classificação de texto e cria um de cinco rótulos, como características ou correções de erros para cada mensagem de confirmação de entrada.</seg>
<seg id="258">Esta imagem é uma amostra de uso que devolve um rótulo corretivo ou de correções de erros.</seg>
<seg id="259">Os dados de treinamento do Glyph são bastante pequenos, cerca de cinco mil, e serão mostrados nas experiências descritas abaixo.</seg>
<seg id="260">O desempenho do modelo de text classification não é alto.</seg>
<seg id="261">Apresento duas pesquisas relacionadas, mas os seus problemas são de aplicabilidade limitada e escassos recursos de dados.</seg>
<seg id="262">O nosso artigo resolve estes dois problemas e gera notas de lançamento de alta qualidade automaticamente.</seg>
<seg id="263">Com um problema de aplicabilidade limitada, nós propomos um método de sumarização em termos de classe de alta qualidade usando apenas mensagens de confirmação como entrada.</seg>
<seg id="264">Este método proposto pode ser usado para todos os repositórios em inglês.</seg>
<seg id="265">Para o segundo problema de recursos de dados escassos, construímos o nosso conjunto de dados RNSum consistindo em cerca de oitenta e dois mil dados recolhendo dados de repositórios públicos do GitHub usando o API do GitHub.</seg>
<seg id="266">Em seguida, vou descrever o nosso conjunto de dados.</seg>
<seg id="267">Aqui está um exemplo de dados.</seg>
<seg id="268">O lado esquerdo é uma mensagem de confirmação e o lado direito são as notas de lançamento.</seg>
<seg id="269">As notas de lançamento são rotuladas como melhorias ou correções, etc.</seg>
<seg id="270">Configurámos uma tarefa que usa as mensagens de confirmação como entrada e cria notas de lançamento rotuladas.</seg>
<seg id="271">Isto pode ser considerado como uma tarefa de sumarização.</seg>
<seg id="272">Temos quatro rótulos predefinidos: características, melhorias, correções de erros, remoções de desaprovações e alterações de interrupção.</seg>
<seg id="273">Estes foram definidos com base em pesquisas anteriores e outros fatores.</seg>
<seg id="274">A nota de lançamento no canto inferior direito é extraída da nota de lançamento no canto inferior esquerdo.</seg>
<seg id="275">Neste momento, é necessário detetar os quatro rótulos que foram configurados com antecedência.</seg>
<seg id="276">Mas os rótulos nem sempre são consistentes com cada repositório.</seg>
<seg id="277">Por exemplo, o rótulo de melhorias inclui melhorias, aperfeiçoamentos, otimizações e assim por diante.</seg>
<seg id="278">Preparamos uma lista de vocabulário de cerca de trinta rótulos para cada uma dessas variações notacionais.</seg>
<seg id="279">Isto serve para detetar a classe de notas de lançamento e recolhe o texto da versão que se segue como a frase da nota de lançamento para a classe.</seg>
<seg id="280">Em seguida, temos uma mensagem de confirmação.</seg>
<seg id="281">As mensagens de confirmação não estão vinculadas a cada lançamento.</seg>
<seg id="282">Como mostrado na imagem abaixo, se o lançamento atual é a versão 2.5.19, precisamos identificar a versão anterior 2.5.18 do lançamento e obter um diferencial.</seg>
<seg id="283">Isto é um pouco monótono e não é suficiente obter apenas uma lista de lançamentos e olhar para o antes e o depois.</seg>
<seg id="284">Criámos uma regra de correspondência heurística para obter as versões anterior e seguinte.</seg>
<seg id="285">Análise do conjunto de dados.</seg>
<seg id="286">No final, foram recolhidos sete mil e duzentos repositórios e oitenta e dois mil peças de dados.</seg>
<seg id="287">Além disso, o número médio de tokens de notas de lançamento é sessenta e três, o que é bastante alto para uma tarefa de sumarização.</seg>
<seg id="288">Além disso, o número de tokens únicos é bastante grande, oito mil oitocentos e trinta mil.</seg>
<seg id="289">Isto deve-se ao grande número de nomes de classe única ou método encontrados no repositório.</seg>
<seg id="290">Em seguida, explicarei o método proposto.</seg>
<seg id="291">O extrativo em termos de classe, seguido do modelo de abstractive summarization consiste em dois módulos neurais.</seg>
<seg id="292">Um classificador que usa BERT ou CodeBERT e um gerador que usa BART.</seg>
<seg id="293">Primeiro, o CEAS usa um classificador para classificar cada mensagem de confirmação em cinco classes de notas de lançamento, que usam melhorias, correções de bugs, desaprovações e outros.</seg>
<seg id="294">As mensagens de confirmação classificadas como outras são descartadas.</seg>
<seg id="295">Em seguida, o CEAS aplica o gerador aos quatro documentos rotulados de forma independente e gera notas de lançamento para cada classe.</seg>
<seg id="296">Nesta tarefa, as correspondências diretas entre mensagens de confirmação e as notas de lançamento não são conhecidas.</seg>
<seg id="297">Assim, para treinar o classificador, é por isso que reatribuímos pesquisas para cada mensagem de confirmação na entrada usando os primeiros dez caracteres de casa mensagem de confirmação.</seg>
<seg id="298">Nós modelamos a abordagem de abstractive summarization por dois métodos diferentes.</seg>
<seg id="299">O primeiro modelo, a que chamamos CAS-Single, consiste numa única rede de seis a seis e gera um único texto de nota de lançamento dá uma concatenação de mensagens de confirmação de entrada.</seg>
<seg id="300">Os textos de saída podem ser divididos em segmentos de classe com base em símbolos de ponto de extremidade específicos de classe especiais.</seg>
<seg id="301">O segundo método, a que chamamos CAS-Multi, consiste em quatro redes diferentes seq2seq, cada uma das quais corresponde a uma das classes de notas de lançamento fixas.</seg>
<seg id="302">OK, deixe-me explicar as experiências.</seg>
<seg id="303">Foram comparados cinco métodos: CEAS, CAS-Single, CAS-Multi, Clustering e o estudo anterior, Glyph.</seg>
<seg id="304">Em relação à avaliação, em alguns casos, as notas de versão são emitidas em várias frases.</seg>
<seg id="305">Como é difícil calcular o número de frases como elas são, são combinadas com espaços e tratadas como uma frase longa.</seg>
<seg id="306">O BLEU é penalizado quando o sistema emite uma frase curta.</seg>
<seg id="307">Esta penalidade resulta num valor BLEU menor nos resultados do experiência descritos a seguir.</seg>
<seg id="308">Finalmente, também calculamos a especificidade porque ROUGE e BLEU não podem ser calculados se as notas de lançamento estiverem vazias.</seg>
<seg id="309">Uma maior especificidade significa que o modelo cria corretamente um texto vazio nos casos em que as notas de lançamento assumem vazio.</seg>
<seg id="310">Aqui estão os resultados.</seg>
<seg id="311">Como o conjunto de dados contém endereços de e-mail, valores em hash, etc., também avaliamos o conjunto de dados limpo, o que os exclui.</seg>
<seg id="312">CEAS e CAS alcançaram pontuações ROUGE-L mais de dez pontos acima das linhas de referência.</seg>
<seg id="313">Em particular, no conjunto de testes limpo, a diferença de pontuação entre o método proposto e as linhas de referência saltou para mais de vinte pontos.</seg>
<seg id="314">Estes resultados indicam que CEAS e CAS são significativamente afetados.</seg>
<seg id="315">CEAS obteve uma pontuação ROUGE-L melhor do que CAS, sugerindo que a combinação de um classificador e um gerador é eficaz para treinar o classificador usando rótulos pseudo.</seg>
<seg id="316">A alta cobertura do CEAS pode ser alcançada provavelmente porque o classificador pode concentrar-se na seleção de mensagens de confirmação relevantes para cada classe.</seg>
<seg id="317">O CAS-Multi tinha tendência para produzir mais ROUGE-L do que o CAS-Single.</seg>
<seg id="318">Sugerindo que também é eficaz desenvolver de forma independente modelos abstractive summarization para cada classe de nota de lançamento.</seg>
<seg id="319">Aqui está uma análise de erro.</seg>
<seg id="320">Os métodos CAS têm tendência para produzir frases mais curtas do que frases de referência humanas.</seg>
<seg id="321">Na figura à direita, a frase de referência tem três ou quatro frases, enquanto que o CAS tem apenas uma.</seg>
<seg id="322">A razão para a relutância deste modelo é que em dados de treinamento, apenas trinta e três por cento das frases estão presentes no rótulo de características e quarenta por cento no rótulo de melhorias.</seg>
<seg id="323">Além disso, os métodos CAS não podem gerar notas de lançamento precisas sem informações adicionais.</seg>
<seg id="324">O exemplo superior à direita é um exemplo de uma mensagem de confirmação muito confusa, e a frase completa não pode ser gerada sem referência ao progresso ou problema correspondente.</seg>
<seg id="325">O exemplo abaixo mostra que as duas mensagens de confirmação na entrada estão relacionadas e devem ser combinadas numa frase, mas não o faz.</seg>
<seg id="326">Por fim, uma conclusão.</seg>
<seg id="327">Criámos um novo conjunto de dados para geração automática de notas de lançamento.</seg>
<seg id="328">Também formulámos uma tarefa para inserir mensagens de confirmação e resumi-las para que seja aplicável a todos os projetos escritos em inglês.</seg>
<seg id="329">As nossas experiências mostram que o método proposto gera notas de lançamento menos ruidosas em maior cobertura do que as linhas de referência.</seg>
<seg id="330">Podem consultar o nosso conjunto de dados no GitHub.</seg>
<seg id="331">Obrigado.</seg>
</doc>
<doc docid="2022.acl-long.111" genre="presentations">
<talkid>2022.acl-long.111</talkid>
<abstract>The enrichment of tabular datasets using external sources has gained significant attention in recent years. Existing solutions, however, either ignore external unstructured data completely or devise dataset-specific solutions. In this study we proposed Few-Shot Transformer based Enrichment (FeSTE), a generic and robust framework for the enrichment of tabular datasets using unstructured data. By training over multiple datasets, our approach is able to develop generic models that can be applied to additional datasets with minimal training (i.e., few-shot). Our approach is based on an adaptation of BERT, for which we present a novel fine-tuning approach that reformulates the tuples of the datasets as sentences. Our evaluation, conducted on 17 datasets, shows that FeSTE is able to generate high quality features and significantly outperform existing fine-tuning solutions.</abstract>
<seg id="332">Olá. Chamo-me Asaf Harari.</seg>
<seg id="333">E vou apresentar o nosso artigo, Enriquecimento tabular de dados em poucos disparos que usam arquiteturas de transformadores com ajuste fino.</seg>
<seg id="334">Os cientistas de dados analisam dados e concentram-se principalmente na manipulação das características existentes dos dados.</seg>
<seg id="335">Mas, por vezes, estas características são limitadas.</seg>
<seg id="336">A geração de características que usa outra fonte de dados pode adicionar informação substancial.</seg>
<seg id="337">o nosso objetivo de pesquisa é o enriquecimento de dados tabular automático usando texto livre de fontes externas.</seg>
<seg id="338">Suponhamos que temos um conjunto de dados tabular e uma base de conhecimento.</seg>
<seg id="339">Precisamos de um processo automático que envolva entity linking e análise de texto para extrair novas características do texto livre da base de conhecimento.</seg>
<seg id="340">A nossa estrutura FeSTE é exatamente esse processo automático.</seg>
<seg id="341">Vamos ver um exemplo num conjunto de dados alimentado no FeSTE.</seg>
<seg id="342">Neste exemplo, o conjunto de dados é um conjunto de dados universitário.</seg>
<seg id="343">Quando o seu objetivo é classificar as universidades em universidades de baixo escalão e universidades de alto escalão.</seg>
<seg id="344">Como base de conhecimento, usamos a Wikipédia.</seg>
<seg id="345">A primeira fase do FeSTE é entity linking.</seg>
<seg id="346">Quando cada entidade, neste exemplo, o nome da universidade, está vinculado a uma entidade dentro da base de conhecimento.</seg>
<seg id="347">E o texto das entidades da base de conhecimento é extraído e adicionado ao conjunto de dados.</seg>
<seg id="348">Neste exemplo, o texto é o resumo da página da Wikipédia.</seg>
<seg id="349">Agora, precisamos de gerar ou extrair características do texto recuperado.</seg>
<seg id="350">Então, precisamos da fase de extração de características que inclui análise de texto.</seg>
<seg id="351">E esta é a principal novidade deste artigo, e eu vou mergulhar a fundo nele nos próximos diapositivos.</seg>
<seg id="352">Após a fase de extração do características, há uma fase de geração de características quando usamos as características extraídas para gerar um pequeno número de novas características.</seg>
<seg id="353">Primeiro gere características no número de classes do conjunto de dados original.</seg>
<seg id="354">Neste exemplo, o conjunto de dados original tem duas classes.</seg>
<seg id="355">Então, o FeSTE gera duas novas características.</seg>
<seg id="356">Mas se o conjunto de dados tiver cinco classes, o FeSTE gera cinco novas características.</seg>
<seg id="357">Cada característica representa a probabilidade para cada classe.</seg>
<seg id="358">Para analisar o texto, usamos análise de texto de última geração, que são modelos de linguagem baseados em transformadores como BERT, GPT, XLNet, etc.</seg>
<seg id="359">É, mas não é provável que possamos treinar modelos de linguagem usando os conjuntos de dados de entrada.</seg>
<seg id="360">Portanto, uma abordagem ingénua será o ajuste fino da tarefa de destino.</seg>
<seg id="361">Assim, na fase de extração de características, podemos transferir modelos de linguagem pré-treinada, ajustar o modelo de linguagem sobre o conjunto de dados de destino.</seg>
<seg id="362">Neste exemplo para ajustar o modelo de linguagem, para classificar texto em classes, resumo em classes, baixo ou alto.</seg>
<seg id="363">Receba a saída do modelo de linguagem, que é a probabilidade para cada classe e use como novas características.</seg>
<seg id="364">O problema com esta abordagem é que o conjuntos de dados pode ter algumas entidades/textos distintos.</seg>
<seg id="365">Na nossa experiência, quase metade dos conjuntos de dados contém menos de quatrocentas amostras e o menor conjunto de dados contém trinta e cinco amostras num conjunto de treinamento.</seg>
<seg id="366">Então, ajustar um modelo de linguagem sobre este conjunto de dados será ineficaz.</seg>
<seg id="367">Mas podemos usar conhecimento prévio sobre conjuntos de dados pré-analisados.</seg>
<seg id="368">Aplicamos o FeSTE sobre um conjunto de dados múltiplo, podemos usar n menos um conjuntos de dados para recolher informações sobre n menos um conjuntos de dados e usar esta informação quando analisamos o enésimo conjunto de dados.</seg>
<seg id="369">O que nós sugerimos é adicionar outra fase de ajuste fino.</seg>
<seg id="370">Uma fase preliminar de ajuste fino multitarefa.</seg>
<seg id="371">Quando se afina o modelo de linguagem sobre n menos um conjuntos de dados.</seg>
<seg id="372">E, em seguida, executamos outra fase de ajuste fino que é um ajuste fino de tarefa de destino, quando se faz o ajuste fino do modelo de linguagem sobre o enésimo conjunto de dados de destino.</seg>
<seg id="373">A última geração em ajuste fino de multitarefa chamado MTDNN.</seg>
<seg id="374">O MTDNN mantém cabeçalhos no número de tarefas no conjunto de treinamento.</seg>
<seg id="375">Neste exemplo, há quatro tarefas no conjunto de treinamento, então o MTDNN mantenha quatro cabeçalhos como se pode ver na imagem.</seg>
<seg id="376">E amostra um lote aleatório do conjunto de treinamento.</seg>
<seg id="377">E se o lote aleatório pertence a, por exemplo, uma única tarefa sentence classification, executa percursos para frente e para trás através do primeiro cabeçalho.</seg>
<seg id="378">E se o lote aleatório pertence à classificação de tarefa de par a par, executa o percurso para frente e para trás através do último cabeçalho.</seg>
<seg id="379">No nosso cenário, os conjuntos de dados tabulares variam no número de classes.</seg>
<seg id="380">Há muitas tarefas.</seg>
<seg id="381">O MTDNN manteve número de classes, cabeçalhos, camadas de saída.</seg>
<seg id="382">Adicionalmente, o MTDNN precisa de inicializar novos cabeçalhos para um novo conjunto de dados com uma nova tarefa.</seg>
<seg id="383">A nossa abordagem, chamada ajuste fino de reformulação de tarefas, é, em vez de manter várias cabeças, reformulamos cada conjunto de dados numa frase por problema de classificação, que são tarefas de duas classes.</seg>
<seg id="384">Vamos ver um exemplo.</seg>
<seg id="385">Aqui está o nosso conjunto de dados de entrada que consiste em entidades, características, texto e classes.</seg>
<seg id="386">E, reformulamos a tarefa de uma classificação do texto em baixa ou alta para classificar o texto, o resumo e a classe em verdadeira ou falsa.</seg>
<seg id="387">Ou, em outras palavras, treinámos o modelo de linguagem para classificar um resumo e uma classe e se o resumo pertence à classe ou não.</seg>
<seg id="388">Assim, o vetor de rótulo neste caso consiste sempre em duas classes.</seg>
<seg id="389">E este é o algoritmo para a nossa abordagem de ajuste fino.</seg>
<seg id="390">Então, vamos ver a estrutura completa.</seg>
<seg id="391">Conjunto de dados alimentado no FeSTE.</seg>
<seg id="392">E então o FeSTE executa a fase de entity linking.</seg>
<seg id="393">Extrai o texto da base de conhecimento, que neste exemplo é o resumo da página da Wikipédia.</seg>
<seg id="394">Em seguida, reformulou a tarefa numa tarefa de sentence classification par a par.</seg>
<seg id="395">Aplicou o modelo de linguagem à nova tarefa e a probabilidade de saída para cada classe.</seg>
<seg id="396">E agora que o modelo de linguagem já está ajustado sobre n menos um conjunto de dados usando um ajuste fino multitarefa preliminar.</seg>
<seg id="397">Em seguida, usamos o vetor de saída do modelo de linguagem como um recurso recém gerado no número de classes.</seg>
<seg id="398">Para avaliar a nossa estrutura, usamos dezessete conjuntos de dados de classificação tabulares que variam em tamanho, características, equilíbrio, domínio e desempenho inicial.</seg>
<seg id="399">E como base de conhecimento, usamos a Wikipédia.</seg>
<seg id="400">Projetamos a nossa experiência como uma avaliação de deixar um de fora, onde treinamos o FeSTe ao longo de dezasseis conjuntos de dados e aplicamo-lo ao décimo sétimo conjunto de dados.</seg>
<seg id="401">Também dividimos cada conjunto de dados em quatro dobras e aplicamos a validação cruzada de quatro dobras.</seg>
<seg id="402">Em seguida, geramos as novas características e os avaliamo-las usando cinco classificadores de avaliação.</seg>
<seg id="403">Usamos na nossa base de experiências da arquitetura básica do BERT.</seg>
<seg id="404">Aqui estão os resultados para as nossas experiências.</seg>
<seg id="405">Pode ver-se que comparamos a nossa estrutura com ajuste fino de conjunto de dados de destino e ajuste fino preliminar de um MTDNN.</seg>
<seg id="406">E o nosso ajuste fino reformulado alcança o melhor resultado, o melhor desempenho.</seg>
<seg id="407">Enquanto que o MTDNN alcançou 2% de melhoria em relação ao ajuste fino do conjunto de dados de destino.</seg>
<seg id="408">A nossa abordagem alcançou uma melhoria de seis por cento.</seg>
<seg id="409">Quando olhamos para o pequeno conjunto de dados, podemos ver que o desempenho do MTDNN diminui e a melhoria da fase de ajuste fino multitarefa preliminar diminui para 1,5%.</seg>
<seg id="410">Mas o nosso desempenho aumentou para onze por cento comparado com o ajuste fino da tarefa de destino sozinho.</seg>
<seg id="411">Para resumir, o FeSTE permite o enriquecimento de poucos tiros a partir de trinta e cinco amostras nas nossas experiências.</seg>
<seg id="412">Usa uma arquitetura para todas as tarefas e conjuntos de dados.</seg>
<seg id="413">E mantém o cabeçalho do modelo.</seg>
<seg id="414">Mas acrescenta uma fase de reformulação.</seg>
<seg id="415">Aumenta o conjunto de treino e precisa de um valor de destino com significado semântico para que possamos alimentá-lo no modelo de linguagem e usá-lo no problema de classificação do sentence pair.</seg>
<seg id="416">Obrigado.</seg>
</doc>
</srcset>
</mteval>
