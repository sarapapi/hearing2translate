{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Asaf Ferrari und ich werde unseren Artikel \"Few-Shot Tabellarische Datenerweiterung unter Verwendung von Feinabstimmungs-Transformer-Architekturen\" vorstellen. Datenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der vorhandenen Merkmale. Aber manchmal sind diese Merkmale begrenzt. Die Generierung von Merkmalen unter Verwendung einer anderen Datenquelle kann erhebliche Informationen hinzufügen. Unser Forschungsziel ist die automatische Erweiterung tabellarischer Daten unter Verwendung externer Quellen für freien Text. Angenommen, wir haben einen tabellarischen Datensatz und eine Wissensdatenbank. Wir benötigen einen automatischen Prozess, der Entity-Linking und Textanalyse umfasst, um neue Merkmale aus dem freien Text der Wissensdatenbank zu extrahieren. Unser Framework, F.E.S.T., ist genau dieser automatische Prozess. Schauen wir uns ein Beispiel an. In den Datensätzen, die in F.E.S.T. eingegeben werden. In diesem Beispiel ist der Datensatz ein Universitätsdatensatz, dessen Ziel es ist, Universitäten in niedrig und hoch eingestufte Universitäten zu klassifizieren. Als Wissensdatenbank verwenden wir Wikipedia. Die erste Phase von F.E.S.T. ist das Entity-Linking, bei dem jede Entität, in diesem Beispiel der Name der Universität, mit einer Entität innerhalb der Wissensdatenbank verknüpft wird. Und der Text der Entitäten der Wissensdatenbank wird extrahiert und dem Datensatz hinzugefügt. In diesem Beispiel ist der Text die Wikipedia-Seitenzusammenfassung. Jetzt müssen wir Merkmale aus dem abgerufenen Text generieren oder extrahieren. Wir benötigen also eine Merkmalsextraktionsphase, die Textanalyse umfasst. Und das ist die Hauptnovität dieses Artikels, und ich werde in den nächsten Folien darauf eingehen. Nach der Merkmalsextraktionsphase gibt es eine Merkmalsgenerierungsphase, bei der wir die extrahierten Merkmale verwenden, um eine kleine Anzahl neuer Merkmale zu generieren. Zuerst generieren wir Merkmale in der Anzahl der Klassen des ursprünglichen Datensatzes. In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen, sodass F.E.S.T. zwei neue Merkmale generiert. Wenn der Datensatz jedoch fünf Klassen hat, generiert F.E.S.T. fünf neue Merkmale. Jedes Merkmal stellt die Wahrscheinlichkeit für jede Klasse dar. Um den Text zu analysieren, verwenden wir den aktuellen Stand der Technik der Textanalyse, nämlich Transformer-basierte Sprachmodelle wie BERT, GPT, XLNet usw. Es ist jedoch unwahrscheinlich, dass wir ein Sprachmodell unter Verwendung der Eingabedatensätze trainieren können. Ein naiver Ansatz wäre also eine Feinabstimmung der Zielaufgabe. In der Merkmalsextraktionsphase können wir also ein vorab trainiertes Sprachmodell herunterladen, das Sprachmodell über den Ziel-Datensatz feinabstimmen. In diesem Beispiel, um das Sprachmodell zu feinabstimmen, um Text in Klassen zu klassifizieren, Zusammenfassung in Klassen, niedrig oder hoch, den Sprachmodellausgang zu erhalten, der die Wahrscheinlichkeit für jede Klasse ist, und als neue Merkmale zu verwenden. Das Problem bei diesem Ansatz ist, dass Datensätze möglicherweise wenige unterschiedliche Entitätstexte enthalten. In unserem Experiment enthalten fast die Hälfte der Datensätze weniger als 400 Proben, und der kleinste Datensatz enthält 35 Proben in seinem Trainingssatz. Die Feinabstimmung eines Sprachmodells über diesen Datensatz wäre also ineffektiv. Aber wir können Vorwissen über vorab analysierte Datensätze verwenden, weil F.E.S.T. wir F.E.S.T. über mehrere Datensätze anwenden, können wir die N-1 Datensätze verwenden, um Informationen über die N-1 Datensätze zu sammeln und diese Informationen zu verwenden, wenn wir den N-Datensatz analysieren. Was wir vorschlagen, ist, eine weitere Feinabstimmungsphase hinzuzufügen, eine vorläufige Multitask-Feinabstimmungsphase, bei der das Sprachmodell über N-1 Datensätze feinabgestimmt wird, und dann führen wir eine weitere Feinabstimmungsphase aus, die eine Zielaufgaben-Feinabstimmung ist, wenn wir das Sprachmodell über den N-Ziel-Datensatz feinabstimmen. Der Stand der Technik bei der Multitask-Feinabstimmung heißt MT-DNN. In MT-DNN hält MT-DNN Köpfe in der Anzahl der Aufgaben im Trainingssatz. In diesem Beispiel gibt es also vier Aufgaben im Trainingssatz, sodass MT-DNN vier Köpfe hält, wie Sie auf dem Bild sehen können, und es eine zufällige Charge aus dem Trainingssatz abtastet. Und wenn die zufällige Charge beispielsweise zu Satz-zu-Satz-Klassifikationsaufgaben gehört, führt es einen Vorwärts- und Rückwärtsdurchlauf durch den ersten Kopf aus. Und wenn die zufällige Charge zu paarweisen Rangordnungsaufgaben gehört, führt es einen Vorwärts- und Rückwärtsdurchlauf durch den letzten Kopf aus. In unserem Szenario variieren tabellarische Datensätze in der Anzahl der Klassen. Es gibt also viele Aufgaben. MT-DNN hält die Anzahl der Klassenköpfe, Ausgabeschichten, und zusätzlich muss MT-DNN neue Köpfe für einen neuen Datensatz mit einer neuen Aufgabe initialisieren. Unser Ansatz heißt Aufgabenreformulierungs-Feinabstimmung. In unserem Ansatz, Aufgabenreformulierungs-Feinabstimmung, reformulieren wir anstelle der Aufrechterhaltung mehrerer Köpfe jeden Datensatz in ein Satz-zu-Satz-Klassifikationsproblem, das eine Zwei-Klassen-Aufgabe ist. Schauen wir uns ein Beispiel an. Hier ist unser Eingabedatensatz, der aus Entitätsmerkmalen, Text und Klassen besteht, und wir reformulieren die Aufgabe von der Klassifizierung des Textes in niedrig und hoch zur Klassifizierung des Textes, der Zusammenfassung und der Klasse in wahr oder falsch. Oder mit anderen Worten, wir trainieren das Sprachmodell, um eine Zusammenfassung und eine Klasse zu klassifizieren, ob die Zusammenfassung zur Klasse gehört oder nicht. Der Label-Vektor in diesem Fall besteht also immer aus zwei Klassen. Und das ist der Algorithmus für unseren reformulierten Feinabstimmungsansatz. Schauen wir uns das vollständige Framework an. Datensatz wird in F.E.S.T. eingegeben, und dann führt F.E.S.T. die Entity-Linking-Phase aus. Es extrahiert den Text aus der Wissensdatenbank, die in diesem Beispiel die Zusammenfassung der Wikipedia-Seite ist. Dann reformuliert es die Aufgabe in Satz-zu-Satz-Klassifikationsaufgaben. Wenden Sie das Sprachmodell auf die neue Aufgabe an und geben Sie die Wahrscheinlichkeit für jede Klasse aus. Beachten Sie, dass das Sprachmodell bereits über N-1 Datensätze unter Verwendung einer vorläufigen Multitask-Feinabstimmung feinabgestimmt ist. Dann verwenden wir den Ausgabeverktor des Sprachmodells als neu generiertes Merkmal in der Anzahl der Klassen. Um unser Framework zu bewerten, verwenden wir 17 tabellarische Klassifikationsdatensätze, die in Größe, Merkmalen, Ausgewogenheit, Domäne und anfänglicher Leistung variieren. Und als Wissensdatenbank verwenden wir Wikipedia. Wir haben unser Experiment als Leave-One-Out-Bewertung gestaltet, bei der wir F.E.S.T. über 16 Datensätze trainieren und auf den 17. Datensatz anwenden. Wir teilen auch jeden Datensatz in vier Folds und wenden eine vierfache Kreuzvalidierung an. Dann generieren wir das neue Merkmal und bewerten es unter Verwendung von fünf Bewertungsklassifikatoren. Wir verwenden in unserem Experiment eine BERT-basierte Architektur. Hier sind die Ergebnisse unseres Experiments. Sie können sehen, dass wir unser Framework mit der Ziel-Datensatz-Feinabstimmung, der Zielaufgaben-Feinabstimmung und der MT-DNN-Vorabstimmung vergleichen, und unsere reformulierte Feinabstimmung erzielt das beste Ergebnis, die beste Leistung, während MT-DNN eine Verbesserung von 2 % gegenüber der Ziel-Datensatz-Feinabstimmung erzielt, erzielt unser Ansatz eine Verbesserung von 6 %. Wenn wir uns den kleinen Datensatz ansehen, können wir sehen, dass die Leistung von MT-DNN abnimmt und die Verbesserung der vorläufigen Multitask-Feinabstimmungsphase auf 1,5 % abnimmt, aber unsere Leistung steigt auf 11 % im Vergleich zur Zielaufgaben-Feinabstimmung. Zusammenfassend lässt sich sagen, dass F.E.S.T. eine Few-Shot-Erweiterung von 35 Proben in unserem Experiment ermöglicht. Es verwendet eine Architektur für alle Aufgaben-Datensätze und behält den Kopf des Modells bei. Aber es fügt eine Reformulierungsphase hinzu. Es erweitert den Trainingsdatensatz und benötigt einen Zielwert mit semantischer Bedeutung, sodass wir ihn in das Sprachmodell eingeben und in dem Satz-zu-Satz-Klassifikationsproblem verwenden können. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Heute werde ich unsere Forschungsarbeit vorstellen, \"Learning to Reason Deductively: Mathematical Problem Solving as Complex Reasoning Extraction\". Ich bin Alan vom ByteDance AI Lab und dies ist eine gemeinsame Arbeit mit Cherie von der University of Texas at Austin und Weilu von der SUTD. Zunächst möchte ich über unsere Motivation für das Schließen sprechen. Hier zeigen wir ein Beispiel, bei dem mehrstufiges Schließen hilfreich ist. Diese Abbildung stammt aus der Pound-Paper, in der sie das Prompting verwenden, um das mathematische Problem in einem Few-Shot-Learning-Szenario zu lösen. Auf der linken Seite können wir sehen, dass wir, wenn wir einige Beispiele mit nur Korrekturen und Antworten geben, möglicherweise nicht die richtigen Antworten erhalten. Wenn wir jedoch eine genauere Schließbeschreibung geben, kann das Modell die Schließbeschreibung vorhersagen und auch eine korrekte Vorhersage treffen. Es ist also gut, interpretierbares mehrstufiges Schließen als Ausgabe zu haben. Wir denken auch, dass das mathematische Problem eine einfache Anwendung ist, um solche Schließfähigkeiten zu bewerten. In unserem Problemaufbau müssen wir, gegeben die Fragen, diese Frage lösen und die numerischen Antworten erhalten. In unseren Datensätzen werden uns auch die mathematischen Ausdrücke gegeben, die zu dieser bestimmten Antwort führen. Bestimmte Annahmen gelten auch wie in früheren Arbeiten. Wir nehmen an, dass die Präzision der Mengen nicht bekannt ist und wir nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponential betrachten. Darüber hinaus können komplizierte Operatoren tatsächlich in diese grundlegenden Operatoren zerlegt werden. Früheres Arbeiten im mathematischen Problemlösen kann tatsächlich in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modelle kategorisiert werden. Traditionelle Sequenz-zu-Sequenz-Modelle konvertieren den Ausdruck in eine spezifische Sequenz zur Generierung, und es ist ziemlich einfach zu implementieren, und es kann auf viele verschiedene komplizierte Probleme verallgemeinert werden. Aber der Nachteil ist, dass die Leistung tatsächlich im Allgemeinen nicht besser ist als das Strukturmodell, und es fehlt an Interpretierbarkeit für die Vorhersage. Aber diese Richtung ist immer noch ziemlich beliebt wegen des Transformer-Modells. In baumbasierten Modellen strukturieren wir diese Ausdrücke tatsächlich in Baumform und folgen einer Vorrangfolge beim Traversieren der Baumgenerierung. Hier generieren wir weiterhin die Operatoren, bis wir die Blätter erreichen, die die Mengen sind. Hier ist das Gute, dass es uns tatsächlich diese binäre Baumstruktur gibt, aber es ist tatsächlich ziemlich gegenintuitiv, weil wir zuerst den Operator generieren und dann am Ende die Mengen generieren. Und das Zweite ist, dass es auch einige wiederholte Berechnungen enthält. Wenn wir uns diesen Ausdruck ansehen, wird 8 mal 3 plus 3 tatsächlich zweimal generiert, aber in der Tat sollten wir die Ergebnisse wiederverwenden. In unserem vorgeschlagenen Ansatz möchten wir diese Probleme schrittweise und interpretierbar lösen. Zum Beispiel können wir in diesem zweiten Schritt diesen Divisor erhalten, der 27 ist. Und wir können auch auf die ursprünglichen Fragen zurückgreifen, um die relevanten Inhalte zu finden. Und in diesen Schritten erhalten wir die Divisoren. Und dann erhalten wir in diesem dritten Schritt tatsächlich den Quotienten. Und nach diesen drei Schritten können wir tatsächlich die Ergebnisse aus dem zweiten Schritt verwenden und dann die Ergebnisse des vierten Schritts erhalten. Und dann können wir schließlich die Dividenden erhalten. Hier generieren wir tatsächlich den gesamten Ausdruck direkt, anstatt einzelne Operatoren oder Mengen zu generieren. Dies macht den Prozess genauer. In unserem deduktiven System beginnen wir mit einer Reihe von Mengen, die in den Fragen präsentiert werden, und schließen auch einige Konstanten als unseren Anfangszustand ein. Der Ausdruck wird durch EIJOP dargestellt, wo wir den Operator von QI zu QJ ausführen, und ein solcher Ausdruck ist tatsächlich gerichtet. Wir haben hier auch Subtraktion umgekehrt, um die entgegengesetzte Richtung darzustellen. Dies ist ziemlich ähnlich wie die Relationsextraktion. In einem formalen deduktiven System wenden wir in einem Zeitschritt T den Operator zwischen dem QI und QJ-Paar an und erhalten dann diesen neuen Ausdruck. Wir fügen ihn zu den nächsten Zuständen hinzu, um eine neue Menge zu werden. Diese Folie visualisiert tatsächlich die Evolution der Zustände, bei der wir weiterhin Ausdrücke zu den aktuellen Zuständen hinzufügen. In unseren Modellimplementierungen verwenden wir zunächst ein vortrainiertes Sprachmodell, das BERT oder Roberta sein kann, und dann kodieren wir den Satz und erhalten diese Mengenrepräsentationen. Sobald wir die Mengenrepräsentationen haben, können wir mit der Inferenz beginnen. Hier zeigen wir ein Beispiel von Q1, um die Repräsentation für Q1 zu erhalten, die durch Q2 geteilt und dann mit Q3 multipliziert wird. Zuerst erhalten wir die Paarrepräsentation, die im Grunde nur die Verkettung zwischen Q1 und Q2 ist, und dann wenden wir ein Feedforward-Netzwerk an, das durch den Operator parametrisiert ist. Und dann erhalten wir schließlich die Ausdrucksrepräsentation Q1 geteilt durch Q2. Aber in der Praxis können wir im Inferenzstadium möglicherweise auch den falschen Ausdruck erhalten. Hier ist die Anzahl der möglichen Ausdrücke gleich dem Dreifachen der Anzahl der Operatoren. Das Schöne daran ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu kontrollieren. Zum Beispiel, wenn dieser Ausdruck nicht erlaubt ist, können wir diesen Ausdruck einfach aus unserem Suchraum entfernen. Im zweiten Schritt machen wir dasselbe, aber der einzige Unterschied ist, dass wir eine weitere Menge haben. Diese Menge stammt aus dem zuvor berechneten Ausdruck. Schließlich können wir diesen endgültigen Ausdruck Q3 mal Q4 erhalten. Und wir können auch sehen, dass die Anzahl aller möglichen Ausdrücke sich von dem vorherigen Schritt unterscheidet. Ein solcher Unterschied macht es schwierig, Beam Search anzuwenden, weil die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgeglichen ist. Das Trainingsverfahren ist ähnlich wie das Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust in jedem Zeitschritt optimieren. Und hier verwenden wir auch dieses Tau, um darzustellen, wann wir diesen Generierungsprozess beenden sollten. Und hier ist der Raum anders als bei Sequenz zu Sequenz, weil der Raum in jedem Zeitschritt anders ist, während er im traditionellen Sequenz-zu-Sequenz-Modell die Anzahl des Vokabulars ist. Und es ermöglicht uns auch, bestimmte Einschränkungen aus Vorwissen zu erzwingen. Wir führen Experimente auf den häufig verwendeten mathematischen Problemdatensätzen MAWPS, Math 23K, MathQA und SWAMP durch. Und hier zeigen wir kurz die Ergebnisse im Vergleich zu den besten bisherigen Ansätzen. Unser bestes Modell ist Roberta Deductive Reasoner. Und tatsächlich verwenden wir keinen Beam Search im Gegensatz zu offensichtlichen Ansätzen, die Beam Search verwenden. Die besten Ansätze sind oft baumbasierte Modelle. Insgesamt ist unser Reasoner in der Lage, dieses baumbasierte Modell signifikant zu übertreffen, aber wir können sehen, dass die absolute Anzahl auf MathQA oder SWAMP nicht wirklich hoch ist. Wir untersuchen die Ergebnisse auf SWAMP weiter. Und dieser Datensatz ist herausfordernd, weil der Autor versucht hat, manuell etwas hinzuzufügen, um das NLP-Modell zu verwirren, wie z.B. die Hinzufügung von Umgebungsinformationen und zusätzlichen Mengen. In unserer Vorhersage finden wir, dass einige der Zwischenwerte tatsächlich negativ sind. Zum Beispiel fragen wir in diesen Fragen, wie viele Äpfel Jake hat, aber wir haben einige zusätzliche Informationen wie 17 wenige Bilder und Stephen hat acht Bilder, was völlig irrelevant ist. Unser Modell macht einige Vorhersagen wie diese, die negative Werte produzieren. Und wir beobachten, dass diese beiden Ausdrücke tatsächlich ähnliche Bewertungen haben. Wir können also diesen Suchraum einschränken, indem wir Ergebnisse entfernen, die negativ sind, sodass wir die Antwort korrekt machen können. Wir finden weiter, dass eine solche Einschränkung tatsächlich einige Modelle erheblich verbessert. Zum Beispiel verbessern wir bei BERT sieben Punkte und dann verbessern wir bei dem Roberta-basierten Modell tatsächlich zwei Punkte. Ein besseres Sprachmodell hat eine bessere Sprachverständnisfähigkeit, sodass die Zahl hier für Roberta höher und für BERT niedriger ist. Und wir versuchen auch, die Schwierigkeit hinter all diesen Datensätzen zu analysieren. Wir nehmen an, dass die Anzahl der ungenutzten Mengen als irrelevante Informationen angesehen werden kann. Hier können wir sehen, dass wir den Prozentsatz der Proben mit ungenutzten Mengen haben und der SWAMP-Datensatz hat den größten Anteil. Und hier zeigen wir auch die Gesamtleistung. Für diese Proben ohne ungenutzte Mengen ist die Gesamtleistung tatsächlich höher als die Gesamtleistung. Aber mit diesen Proben mit ungenutzten Mengen ist es tatsächlich viel schlechter als die Gesamtleistung. Für MAWPS haben wir nicht wirklich viele dieser Fälle. Ich ignoriere diesen Teil einfach. Schließlich möchten wir die Interpretierbarkeit durch ein Crash-and-Protektion-Beispiel zeigen. Hier macht unser Modell tatsächlich eine falsche Vorhersage im ersten Schritt. Wir können also diesen Ausdruck mit dem Satz hier korrelieren. Wir denken, dass dieser Satz das Modell möglicherweise zu einer falschen Vorhersage verleitet. Hier macht das Pflanzen eines weiteren 35 das Modell denken, es sollte ein Additionsoperator sein. Wir versuchen, den Satz zu überarbeiten, damit er etwas wie die Anzahl der Birnbäume 55 weniger als die Apfelbäume ist. Wir machen es, um genauere Semantik zu vermitteln, sodass das Modell in der Lage ist, die Vorhersage korrekt zu machen. Diese Studie zeigt, wie die interpretierbaren Vorhersagen uns helfen, das Modellverhalten zu verstehen. Um unsere Arbeit zusammenzufassen, ist unser Modell tatsächlich ziemlich effizient und wir sind in der Lage, ein interpretierbares Lösungsverfahren bereitzustellen. Und wir können leicht einige Vorwissen als Einschränkung einbeziehen, was helfen kann, die Leistung zu verbessern. Und das Letzte ist, dass der zugrunde liegende Mechanismus nicht nur auf mathematische Problemlösungsaufgaben angewendet wird, sondern auch auf andere Aufgaben, die mehrstufiges Schließen beinhalten. Aber wir haben auch bestimmte Einschränkungen. Wenn wir eine große Anzahl von Operatoren oder Konstanten haben, könnte der Speicherverbrauch ziemlich hoch sein. Und das Zweite ist, dass, wie erwähnt, die Wahrscheinlichkeitsverteilung zwischen verschiedenen Zeitschritten unausgeglichen ist. Es ist also auch ziemlich herausfordernd, die Beam-Search-Strategie anzuwenden. Das ist das Ende des Vortrags und Fragen sind willkommen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Antoine und ich komme von der Universität Maastricht. Ich werde meine gemeinsame Arbeit mit Jerry vorstellen, die sich mit einem neuen Datensatz für die Abruf von gesetzlichen Artikeln befasst. Rechtliche Fragen sind ein integraler Bestandteil des Lebens vieler Menschen. Aber die Mehrheit der Bürger hat wenig bis gar kein Wissen über ihre Rechte und grundlegende rechtliche Prozesse. Als Ergebnis bleiben viele gefährdete Bürger, die sich die teure Unterstützung eines Rechtsfachmanns nicht leisten können, ungeschützt oder schlimmer noch, werden ausgenutzt. Unsere Arbeit zielt darauf ab, die Lücke zwischen Menschen und dem Gesetz zu schließen, indem wir ein effektives Abrufsystem für gesetzliche Artikel entwickeln. Ein solches System könnte einen kostenlosen professionellen Rechtsbeistand für ungeschulte Menschen bieten. Bevor wir uns mit dem Hauptbeitrag dieser Arbeit beschäftigen, beschreiben wir zunächst das Problem des Abrufs von gesetzlichen Artikeln. Angesichts einer einfachen Frage zu einem rechtlichen Thema, wie z.B. „Was riskiere ich, wenn ich die berufliche Vertraulichkeit verletze?“, muss ein Modell alle relevanten gesetzlichen Artikel aus einem großen Gesetzeskörper abrufen. Diese Informationsabrufsaufgabe bringt ihre eigenen Herausforderungen mit sich. Erstens handelt es sich um zwei Arten von Sprache, umgangssprachliche natürliche Sprache für die Fragen und komplexe rechtliche Sprache für die Statuten. Dieser Unterschied in den Sprachverteilungen macht es für ein System schwieriger, relevante Kandidaten abzurufen, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine rechtliche Frage übersetzen kann, die der Terminologie der Statuten entspricht. Außerdem ist das gesetzliche Recht kein Stapel unabhängiger Artikel, der als vollständige Informationsquelle behandelt werden kann, wie z.B. Nachrichten oder Rezepte. Stattdessen handelt es sich um eine strukturierte Sammlung rechtlicher Bestimmungen, die nur dann eine vollständige Bedeutung haben, wenn sie in ihrem Gesamtkontext betrachtet werden, d.h. zusammen mit den ergänzenden Informationen aus ihren benachbarten Artikeln, den Feldern und Unterfeldern, zu denen sie gehören, und ihrem Platz in der Struktur des Gesetzes. Schließlich sind gesetzliche Artikel keine kleinen Absätze, die normalerweise die typische Abrufeinheit in den meisten Abrufarbeiten sind. Hier handelt es sich um lange Dokumente, die bis zu 6.000 Wörter umfassen können. Die jüngsten Fortschritte in der NLP haben ein großes Interesse an vielen rechtlichen Aufgaben geweckt, wie z.B. der Vorhersage rechtlicher Urteile oder der automatischen Überprüfung von Verträgen. Der Abruf von gesetzlichen Artikeln blieb jedoch größtenteils unberührt, aufgrund des Mangels an großen und hochwertigen, beschrifteten Datensätzen. In dieser Arbeit stellen wir einen neuen, französischsprachigen, bürgerzentrierten Datensatz vor, um zu untersuchen, ob ein Abrufsmodell die Effizienz und Zuverlässigkeit eines Rechtsfachmanns für die Aufgabe des Abrufs von gesetzlichen Artikeln annähern kann. Unser belgischer Datensatz für den Abruf von gesetzlichen Artikeln besteht aus mehr als 1.100 rechtlichen Fragen, die von belgischen Bürgern gestellt wurden. Diese Fragen decken eine breite Palette von Themen ab, von Familie, Wohnen, Geld bis hin zu Arbeit und Sozialversicherung. Jede von ihnen wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als 22.600 rechtlichen Artikeln aus den belgischen Gesetzbüchern beschriftet. Lassen Sie uns nun darüber sprechen, wie wir diesen Datensatz gesammelt haben. Zunächst begannen wir mit der Erstellung eines großen Korpus von rechtlichen Artikeln. Wir berücksichtigten 32 öffentlich zugängliche belgische Gesetzbücher und extrahierten alle ihre Artikel sowie die entsprechenden Abschnittsüberschriften. Dann sammelten wir rechtliche Fragen mit Verweisen auf relevante Statuten. Dazu arbeiteten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr etwa 4.000 E-Mails von belgischen Bürgern erhält, die um Rat zu einem persönlichen oder rechtlichen Problem bitten. Wir hatten das Glück, Zugang zu ihren Websites zu erhalten, auf denen ihr Team erfahrener Juristen die häufigsten rechtlichen Probleme der Belgier behandelt. Wir sammelten Tausende von Fragen, die mit Kategorien, Unterkategorien und rechtlichen Verweisen auf relevante Statuten annotiert waren. Schließlich parsten wir die rechtlichen Verweise und filterten die Fragen heraus, deren Verweise keine Artikel in einem der Gesetzbücher waren, die wir berücksichtigten. Die verbleibenden Verweise wurden mit den entsprechenden Artikel-IDs aus unserem Korpus abgeglichen und konvertiert. Schließlich landeten wir bei 1.108 Fragen, die jeweils sorgfältig mit den IDs der relevanten Artikel aus unserem großen Korpus von 22.633 gesetzlichen Artikeln beschriftet wurden. Darüber hinaus kommt jede Frage mit einer Hauptkategorie und einer Verkettung von Unterkategorien, und jeder Artikel kommt mit einer Verkettung seiner nachfolgenden Überschrift in der Struktur des Gesetzes. Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für zukünftige Forschungen zum Abruf rechtlicher Informationen oder zur Klassifizierung rechtlicher Texte von Interesse sein. Werfen wir einen Blick auf einige Merkmale unseres Datensatzes. Die Fragen sind zwischen 5 und 44 Wörter lang, mit einem Median von 40 Wörtern. Die Artikel sind viel länger, mit einer Medianlänge von 77 Wörtern, wobei 142 von ihnen mehr als 1.000 Wörter umfassen, wobei der längste bis zu 5.790 Wörter umfasst. Wie bereits erwähnt, decken die Fragen eine breite Palette von Themen ab, wobei etwa 85 % von ihnen entweder über Familie, Wohnen, Geld oder Justiz handeln, während die verbleibenden 15 % entweder Sozialversicherung, Ausländer oder Arbeit betreffen. Die Artikel sind ebenfalls sehr vielfältig, da sie aus 32 verschiedenen belgischen Gesetzbüchern stammen, die eine große Anzahl rechtlicher Themen abdecken. Hier ist die Gesamtzahl der Artikel, die aus jedem dieser belgischen Gesetzbücher gesammelt wurden. Von den 22.633 Artikeln werden nur 1.612 als relevant für mindestens eine Frage im Datensatz bezeichnet. Und etwa 80 % dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, dem Justizgesetzbuch, dem Strafprozessgesetzbuch oder dem Strafgesetzbuch. In der Zwischenzeit haben 18 der 32 Gesetzbücher weniger als fünf Artikel, die als relevant für mindestens eine Frage erwähnt werden, was damit erklärt werden kann, dass diese Gesetzbücher weniger auf Individuen und ihre Anliegen eingehen. Insgesamt beträgt die Medianzahl der Zitate für diese zitierten Artikel zwei, und weniger als 25 % von ihnen werden mehr als fünf Mal zitiert. Mit unserem Datensatz bewerten wir mehrere Abrufsansätze, einschließlich lexikalischer und dichter Architektur. Angesichts einer Abfrage und eines Artikels weist ein lexikalisches Modell der Abfrage-Artikel-Paarung eine Punktzahl zu, indem es die Summe über die Abfragebegriffe der Gewichte jedes dieser Begriffe in diesem Artikel berechnet. Wir experimentieren mit den Standard-TFIDF- und BM25-Rangfunktionen. Das Hauptproblem bei diesen Ansätzen ist, dass sie nur Artikel abrufen können, die Schlüsselwörter enthalten, die in der Abfrage vorhanden sind. Um diese Einschränkung zu überwinden, experimentieren wir mit einer neuronalen Architektur, die semantische Beziehungen zwischen Abfragen und Artikeln erfassen kann. Wir verwenden ein B-Encoder-Modell, das Abfragen und Artikel in dichte Vektordarstellungen abbildet und eine Relevanzbewertung zwischen einem Abfrage-Artikel-Paar durch die Ähnlichkeit ihrer Einbettungen berechnet. Diese Einbettungen resultieren in der Regel aus einer Pooling-Operation auf der Ausgabe eines Wort-Einbettungsmodells. Zunächst untersuchen wir die Wirksamkeit von Siamese B-Encodern in einem Zero-Shot-Bewertungssetup, was bedeutet, dass vorab trainierte Wort-Einbettungsmodelle ohne zusätzliche Feinabstimmung aus der Box angewendet werden. Wir experimentieren mit kontextunabhängigen Textencodern, nämlich Word2Vec und FastText, und kontextabhängigen Einbettungsmodellen, nämlich Roberta und spezieller Camembert, das ein französisches Roberta-Modell ist. Darüber hinaus trainieren wir unser eigenes Camembert-basiertes Modell, B-Encodern, auf unserem Datensatz. Beachten Sie, dass wir für das Training mit den beiden Varianten der B-Encoder-Architektur experimentieren. Siamese, das ein einziges Wort-Einbettungsmodell verwendet, das die Abfrage und den Artikel zusammen in einen gemeinsamen dichten Vektorraum abbildet, und Two Tower, das zwei unabhängige Wort-Einbettungsmodelle verwendet, die die Abfrage und den Artikel separat in verschiedene Einbettungsräume codieren. Wir experimentieren mit Mean, Max und CLS-Pooling sowie Dot-Produkt und Cosinus zur Berechnung von Ähnlichkeiten. Hier sind die Ergebnisse unserer Baseline auf dem Testdatensatz, mit den lexikalischen Methoden oben, den Siamese B-Encodern, die in einem Zero-Shot-Setup bewertet wurden, in der Mitte und den feinabgestimmten B-Encodern unten. Insgesamt übertreffen die feinabgestimmten B-Encodern alle anderen Baselines deutlich. Das Two-Tower-Modell verbessert sich gegenüber seiner Siamese-Variante bei der Rückrufquote bei 100, verhält sich jedoch ähnlich bei den anderen Metriken. Obwohl BM25 die trainierten B-Encodern deutlich unterschreitet, zeigen seine Leistungen, dass es immer noch eine starke Baseline für den domänenspezifischen Abruf ist. Was die Zero-Shot-Bewertung des Siamese B-Encoders betrifft, so stellen wir fest, dass die direkte Verwendung der Einbettungen eines vorab trainierten Camembert-Modells ohne Optimierung für die Informationsabrufsaufgabe schlechte Ergebnisse liefert, was mit früheren Erkenntnissen übereinstimmt. Darüber hinaus stellen wir fest, dass der Word2Vec-basierte B-Encoder das FastText- und Bird-basierte Modell deutlich übertrifft, was darauf hindeutet, dass vorab trainierte Wort-Einbettungen möglicherweise besser für die Aufgabe geeignet sind als Zeichen- oder Subwort-Einbettungen, wenn sie aus der Box verwendet werden. Obwohl diese Ergebnisse vielversprechend sind, deuten sie auf erhebliche Verbesserungsmöglichkeiten im Vergleich zu einem geschickten Rechtsfachmann hin, der schließlich alle relevanten Artikel zu jeder Frage abrufen und somit perfekte Punktzahlen erzielen kann. Lassen Sie uns abschließend zwei Einschränkungen unseres Datensatzes diskutieren. Erstens ist der Korpus der Artikel auf diejenigen beschränkt, die aus den 32 berücksichtigten belgischen Gesetzbüchern gesammelt wurden, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Verordnungen, Richtlinien und Verordnungen fehlen. Während der Datensatzerstellung werden alle Verweise auf diese nicht gesammelten Artikel ignoriert, was dazu führt, dass einige Fragen nur einen Bruchteil ihrer ursprünglichen Anzahl relevanter Artikel erhalten. Dieser Informationsverlust bedeutet, dass die in den verbleibenden relevanten Artikeln enthaltene Antwort möglicherweise unvollständig ist, obwohl sie immer noch vollständig angemessen ist. Zweitens sollten wir darauf hinweisen, dass nicht alle rechtlichen Fragen allein mit Statuten beantwortet werden können. Zum Beispiel könnte die Frage, ob ich meine Mieter rauswerfen kann, wenn sie zu viel Lärm machen, keine detaillierte Antwort im gesetzlichen Recht haben, die einen spezifischen Lärmpegel quantifiziert, bei dem eine Räumung erlaubt ist. Stattdessen sollte der Vermieter wahrscheinlich mehr auf die Rechtsprechung vertrauen und Präzedenzfälle finden, die der aktuellen Situation ähnlich sind. Zum Beispiel macht der Mieter zwei Partys pro Woche bis 2 Uhr morgens. Daher sind einige Fragen besser für die Aufgabe des Abrufs von gesetzlichen Artikeln geeignet als andere, und der Bereich der weniger geeigneten bleibt zu bestimmen. Wir hoffen, dass unsere Arbeit Interesse weckt, praktische und zuverlässige Modelle für den Abruf von gesetzlichen Artikeln zu entwickeln, die helfen können, den Zugang zur Justiz für alle zu verbessern. Sie können unseren Artikel, Datensatz und Code unter den folgenden Links einsehen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, wir freuen uns, unsere Arbeit an VALSE vorzustellen, einem aufgabenunabhängigen Benchmark, der für das Testen von Vision- und Sprachmodellen mit spezifischen linguistischen Phänomenen entwickelt wurde. Warum haben wir uns die Mühe gemacht, diesen Benchmark einzurichten? Nun, in den letzten Jahren haben wir einen Boom von auf Transformer basierenden Vision- und Sprachmodellen erlebt, die auf großen Mengen von Bild-Text-Paaren vorab trainiert wurden. Jedes dieser Modelle verbessert den Stand der Technik bei Vision- und Sprachaufgaben wie visueller Fragebeantwortung, visueller Alltagslogik, Bildabruf, Phrasengrundierung. Wir haben also eine Botschaft erhalten: Die Genauigkeiten bei diesen aufgabenbezogenen Benchmarks steigen stetig. Aber wissen wir, was die Modelle tatsächlich gelernt haben? Was hat ein Vision- und Sprachtransformer verstanden, als er diesem Bild und diesem Satz eine hohe Punktzahl zugewiesen hat, um sie zu vergleichen, und diesem eine niedrige Punktzahl? Konzentrieren sich Vision- und Sprachmodelle auf das Richtige, oder konzentrieren sie sich auf Verzerrungen, wie es in früheren Arbeiten gezeigt wurde? Um mehr Licht auf diesen Aspekt zu werfen, schlagen wir eine aufgabenagnostische Richtung vor und stellen VALSE vor, das die Sensibilität von Vision- und Sprachmodellen für spezifische linguistische Phänomene testet, die sowohl die linguistische als auch die visuelle Modalität beeinflussen. Wir zielen auf Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entitäts-Coreferenz ab. Aber wie testen wir, ob die Vision- und Sprachmodelle diese Phänomene erfasst haben? Durch Foiling, eine Methode, die zuvor nur für Vision- und Sprachmodelle für Nicht-Phrasen von Ravi Shekhar und Kollegen und beim Zählen von uns in früheren Arbeiten angewendet wurde. Foiling bedeutet im Grunde, dass wir die Bildunterschrift nehmen und eine Folie erstellen, indem wir die Unterschrift so ändern, dass sie das Bild nicht mehr beschreibt. Und wir führen diese Phrasenänderungen durch, indem wir uns auf sechs spezifische Teile konzentrieren, wie Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entitäts-Coreferenz, wobei jeder Teil aus einem oder mehreren Instrumenten bestehen kann, falls wir mehr als eine interessante Möglichkeit gefunden haben, Folieninstanzen zu erstellen. Zum Beispiel haben wir im Fall des Handlungsstücks zwei Instrumente, eines, bei dem das Handlungsverb durch eine andere Handlung ersetzt wird, und eines, bei dem Akteure vertauscht werden. Zählen und Coreferenz sind ebenfalls Teile, die mehr als ein Instrument haben. Und wir erstellen diese Folien, indem wir sicherstellen, dass sie das Bild nicht beschreiben, dass sie grammatikalisch und ansonsten gültige Sätze sind. Das ist nicht einfach zu tun, denn eine gefoilte Unterschrift kann weniger wahrscheinlich sein als die ursprüngliche Unterschrift. Zum Beispiel ist es zwar nicht unmöglich, aber statistisch weniger wahrscheinlich, dass Pflanzen einen Mann schneiden, als dass ein Mann Pflanzen schneidet, und große Vision- und Sprachmodelle könnten das aufgreifen. Daher müssen wir Maßnahmen ergreifen, um gültige Folien zu erhalten. Erstens nutzen wir starke Sprachmodelle, um Folien vorzuschlagen. Zweitens verwenden wir natürliche Sprachinferenz oder kurz NLI, um Folien herauszufiltern, die das Bild immer noch beschreiben könnten, da wir beim Erstellen von Folien sicherstellen müssen, dass sie das Bild nicht beschreiben. Um dies automatisch zu testen, wenden wir natürliche Sprachinferenz mit folgender Begründung an. Wir betrachten ein Bild als Prämisse und seine Unterschrift als enthaltene Hypothese. Darüber hinaus betrachten wir die Unterschrift als Prämisse und die Folie als ihre Hypothese. Wenn ein NLI-Modell vorhersagt, dass die Folie der Unterschrift widerspricht oder neutral ist, nehmen wir dies als Indikator für eine gültige Folie. Wenn ein NLI vorhersagt, dass die Folie durch die Unterschrift impliziert wird, kann es keine gute Folie sein, da sie durch Transitivität eine wahrheitsgemäße Beschreibung des Bildes liefert, und wir filtern diese Folien heraus. Aber dieses Verfahren ist nicht perfekt. Es ist nur ein Indikator für gültige Folien. Daher setzen wir als dritte Maßnahme zur Erzeugung gültiger Folien menschliche Annotatoren ein, um die in VALSE verwendeten Daten zu validieren. Nach dem Filtern und der menschlichen Bewertung haben wir so viele Testinstanzen, wie in dieser Tabelle beschrieben. Beachten Sie, dass VALSE keine Trainingsdaten liefert, sondern nur Testdaten, da es sich um einen reinen Zero-Shot-Test-Benchmark handelt. Es ist so konzipiert, dass es die bestehenden Fähigkeiten von Vision- und Sprachmodellen nach dem Vorab-Training nutzt. Feinabstimmung würde es den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen. Und wir alle wissen, dass diese Modelle gerne betrügen und Abkürzungen nehmen. Und wie wir sagten, sind wir daran interessiert, zu bewerten, welche Fähigkeiten die Vision- und Sprachmodelle nach dem Vorab-Training haben. Wir experimentieren mit fünf Vision- und Sprachmodellen an VALSE, nämlich mit CLIP, AlexMert, Wilbert, Wilbert 12 in 1 und VisualBert. Zwei unserer wichtigsten Bewertungsmetriken sind die Genauigkeit der Modelle bei der Klassifizierung von Bild-Satz-Paaren in Unterschriften und Folien. Vielleicht relevanter für dieses Video zeigen wir unsere großzügigere Metrik, die paarweise Genauigkeit, die misst, ob die Bild-Satz-Ausrichtungsbewertung für das richtige Bild-Text-Paar größer ist als für sein gefoiltes Paar. Für weitere Metriken und Ergebnisse dazu schauen Sie sich unseren Artikel an. Die Ergebnisse mit der paarweisen Genauigkeit sind hier gezeigt und sie sind konsistent mit den Ergebnissen, die wir von den anderen Metriken erhalten haben. Es ist so, dass die beste Zero-Shot-Leistung von Wilbert 12 in 1 erzielt wird, gefolgt von Wilbert, AlexMert, Clip und schließlich VisualBert. Es ist bemerkenswert, wie Instrumente, die sich auf einzelne Objekte konzentrieren, wie Existenz und Nomenphrasen, fast von Wilbert 12 in 1 gelöst werden, was darauf hinweist, dass Modelle in der Lage sind, benannte Objekte und ihre Anwesenheit in Bildern zu identifizieren. Keines der verbleibenden Teile kann jedoch in unseren adversären Foiling-Einstellungen zuverlässig gelöst werden. Wir sehen aus den Pluralitäts- und Zählinstrumenten, dass Vision- und Sprachmodelle Schwierigkeiten haben, Referenzen auf einzelne im Vergleich zu mehreren Objekten oder deren Zählung in einem Bild zu unterscheiden. Das Beziehungsstück zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Beziehung zwischen Objekten in einem Bild korrekt zu klassifizieren. Sie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie durch Plausibilitätsverzerrungen unterstützt werden, wie wir im Handlungsstück sehen. Aus dem Coreferenzstück erfahren wir, dass das Verfolgen mehrerer Referenzen auf dasselbe Objekt in einem Bild durch die Verwendung von Pronomen auch für Vision- und Sprachmodelle schwierig ist. Als Plausibilitätsprüfung und weil es ein interessantes Experiment ist, bewerten wir auch zwei rein textbasierte Modelle, GPT-1 und GPT-2, um zu bewerten, ob VALSE von diesen unimodalen Modellen lösbar ist, indem wir die Verwirrung der richtigen und der gefoilten Unterschrift berechnen, kein Bild hier, und den Eintrag mit der niedrigsten Verwirrung vorhersagen. Wenn die Verwirrung für die Folie höher ist, nehmen wir dies als Indikator dafür, dass die gefoilte Unterschrift unter einer Plausibilitätsverzerrung oder anderen linguistischen Verzerrungen leiden könnte. Und es ist interessant zu sehen, dass in einigen Fällen die rein textbasierten GPT-Modelle die Plausibilität der Welt besser erfasst haben als die Vision- und Sprachmodelle. Zusammenfassend lässt sich sagen, dass VALSE ein Benchmark ist, der die Linse linguistischer Konstrukte verwendet, um der Gemeinschaft zu helfen, Vision- und Sprachmodelle zu verbessern, indem ihre visuellen Grundierungsfähigkeiten hart getestet werden. Unsere Experimente zeigen, dass Vision- und Sprachmodelle benannte Objekte und ihre Anwesenheit in Bildern gut identifizieren, wie das Existenzstück zeigt, aber Schwierigkeiten haben, ihre Interdependenzen und Beziehungen in visuellen Szenen zu verankern, wenn sie gezwungen sind, linguistische Indikatoren zu respektieren. Wir möchten die Gemeinschaft wirklich ermutigen, VALSE zur Messung des Fortschritts bei der Sprachverankerung mit Vision- und Sprachmodellen zu verwenden. Und noch mehr, VALSE könnte als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten, um zu sehen, ob ein Datensatz den Modellen hilft, sich bei einem der von VALSE getesteten Aspekte zu verbessern. Wenn Sie interessiert sind, schauen Sie sich die VALSE-Daten auf GitHub an, und wenn Sie Fragen haben, zögern Sie nicht, uns zu kontaktieren."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kamizawa von der Universität Tokio. Ich werde einen Artikel mit dem Titel \"R&S, ein groß angelegter Datensatz für die automatische Erstellung von Versionshinweisen durch Zusammenfassung von Commit-Protokollen\" vorstellen. Ich werde in dieser Reihenfolge erklären. Zunächst werde ich die automatische Erstellung von Versionshinweisen vorstellen, an der wir in dieser Forschung arbeiten. Ein Versionshinweis ist ein technisches Dokument, das die Änderungen zusammenfasst, die mit jeder Version eines Softwareprodukts verteilt werden. Das Bild zeigt die Versionshinweise für Version 2.6.4 der BuildJS-Bibliothek. Versionshinweise spielen eine wichtige Rolle in der Open-Source-Entwicklung, sind aber zeitaufwendig in der manuellen Erstellung. Daher wäre es sehr nützlich, wenn man in der Lage wäre, qualitativ hochwertige Versionshinweise automatisch zu generieren. Ich werde auf zwei frühere Forschungen zur automatischen Erstellung von Versionshinweisen verweisen. Die erste ist ein System namens Arena, das 2014 veröffentlicht wurde. Es nimmt einen regelbasierten Ansatz, zum Beispiel unter Verwendung des Änderungs-Extractors, um Kernunterschiede, Bibliotheksänderungen und Dokumentänderungen aus den Unterschieden zwischen Versionen zu extrahieren und diese schließlich zu kombinieren. Das bemerkenswerteste Merkmal dieses Systems ist der Issue-Extractor in der oberen rechten Ecke, der mit dem Jira-Issue-Tracking-System verknüpft sein muss und nur auf Projekte angewendet werden kann, die Jira verwenden. Mit anderen Worten, es kann nicht für viele Projekte auf GitHub verwendet werden. Die zweite ist Grief, die kürzlich 2020 angekündigt wurde. Sie ist im Internet verfügbar und kann über Pip installiert werden. Dieses System hat ein einfaches lernbasiertes Textklassifikationsmodell und gibt eines von fünf Labels wie Funktionen oder Bugfixes für jede Eingabe-Commit-Nachricht aus. Das Bild ist eine Beispielnutzung, die ein kollektives oder Bugfix-Label zurückgibt. Die Trainingsdaten von Grief sind ziemlich klein, etwa 5.000, und werden in den unten beschriebenen Experimenten gezeigt. Die Leistung des Textklassifikationsmodells ist nicht hoch. Ich präsentiere zwei verwandte Forschungen, aber es gibt Probleme der begrenzten Anwendbarkeit und knappen Datenressourcen. Unser Artikel löst diese beiden Probleme und generiert automatisch qualitativ hochwertige Versionshinweise. Für das Problem der begrenzten Anwendbarkeit schlagen wir eine Methode zur Zusammenfassung von Klassifikatoren hoher Qualität vor, die nur Commit-Nachrichten als Eingabe verwendet. Diese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden. Für das zweite Problem der knappen Datenressourcen haben wir einen R&S-Datensatz erstellt, der aus etwa 82.000 Datensätzen besteht, indem wir Daten aus öffentlichen GitHub-Repositories mit der GitHub-API gesammelt haben. Als nächstes beschreibe ich unseren Datensatz. Hier ist ein Beispiel für Daten. Die linke Seite ist die Commit-Nachricht und die rechte Seite sind die Versionshinweise. Die Versionshinweise sind als Verbesserungen, Bugfixes usw. gekennzeichnet. Wir haben eine Aufgabe eingerichtet, die die Commit-Nachrichten als Eingabe nimmt und die gekennzeichneten Versionshinweise ausgibt. Dies kann als Zusammenfassungsaufgabe betrachtet werden. Wir haben alle Labels vordefiniert: Funktionen, Verbesserungen, Bugfixes, Duplikationen, Entfernungen und Breaking Changes. Diese wurden basierend auf früheren Forschungen und anderen Fakten festgelegt. Die Versionshinweise unten rechts wurden aus den Versionshinweisen unten links extrahiert. Zu diesem Zeitpunkt ist es notwendig, die vier Labels zu erkennen, die im Voraus eingerichtet wurden. Aber die Labels sind nicht immer mit jedem Repository konsistent. Zum Beispiel enthält das Verbesserungslabel Verbesserungen, Verbesserungen, Optimierungen und so weiter. Wir haben eine Vokabelliste für die Studie der Labels für jede dieser Notationsvarianten vorbereitet, sie verwendet, um die Versionsnotenklassen zu erkennen und den Text der Versionsnoten zu korrigieren, die als Versionsnotensatz für die Klasse folgen. Als nächstes ist die Commit-Nachricht. Commit-Nachrichten sind nicht an jede Version gebunden. Wie im Bild unten gezeigt, wenn die aktuelle Version 2.5 bis 19 ist, müssen wir die vorherige Versionsnummer 2.5 bis 18 identifizieren und ihren Diff erhalten. Dies ist etwas schwierig und es reicht nicht aus, nur eine Liste der Versionen zu erhalten und die vorherige und die nächste zu betrachten. Wir haben eine heuristische Abgleichsregel erstellt, um die vorherige und die nächste Version zu erhalten. Datensatzanalyse. Am Ende wurden 7.200 Repositories und 82.000 Datensätze gesammelt. Auch die durchschnittliche Anzahl der Versionsnotentokens beträgt 63, was für eine Zusammenfassungsaufgabe ziemlich hoch ist. Auch die Anzahl der eindeutigen Tokens ist ziemlich groß, nämlich 8.830.000. Dies liegt an der großen Anzahl eindeutiger Klassen- und Methodenamen, die in der Bibliothek gefunden werden. Als nächstes werde ich die vorgeschlagene Methode erklären. Das klassenspezifische extraktive und abstrakte Zusammenfassungsmodell besteht aus zwei neuronalen Modulen, einem Klassifikator, der BART oder COD-BART verwendet, und einem Generator, der BART verwendet. Zunächst verwendet CEAS einen Klassifikator, um jede Commit-Nachricht in fünf Versionsnotenklassen zu klassifizieren, die wir Verbesserungen, Bugfixes, Duplikationen und andere nennen. Die Commit-Nachrichten, die als andere klassifiziert werden, werden verworfen. Dann wendet CEAS den Generator auf die vier Label-Dokumente unabhängig an und generiert Versionshinweise für jede Klasse. In dieser Aufgabe sind die direkten Entsprechungen zwischen Commit-Nachrichten und Versionshinweisen nicht bekannt. Daher weisen wir dem Klassifikator zwei Labels für jede Eingabe-Commit-Nachricht zu, indem wir die ersten 10 Zeichen jeder Commit-Nachricht verwenden. Wir modellieren die klassenspezifische abstrakte Zusammenfassung durch zwei verschiedene Methoden. Das erste Modell, das wir CAS Single nennen, besteht aus einem einzelnen Seq2Seq-Netzwerk und generiert einen einzelnen langen Versionsnotentext, der eine Konkatenation der Eingabe-Commit-Nachrichten gibt. Der Ausgabentext kann in klassifizierte Segmente unterteilt werden, basierend auf speziellen klassenbezogenen Endpunkt-Symbolen. Die zweite Methode, die wir CAS Match nennen, besteht aus vier verschiedenen Seq2Seq-Netzwerken, von denen jedes einer der Versionsnotenklassen entspricht. Okay, lassen Sie mich das Experiment erklären. Fünf Methoden wurden verglichen: CAS, CAS Single, CAS Match, Plustering und die vorherige Studie Grief. Was die Bewertung betrifft, so werden in einigen Fällen Versionshinweise in mehreren Sätzen ausgegeben. Da es schwierig ist, die Anzahl der Sätze als Null zu berechnen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt. Das Blue wird bestraft, wenn das System einen kurzen Satz ausgibt. Diese Strafe führt zu einem niedrigeren Blue-Wert in den unten beschriebenen Experimentergebnissen. Schließlich berechnen wir auch die Spezifität, weil Rouge und Blue nicht berechnet werden können, wenn die Versionshinweise leer sind. Eine hohe Spezifität bedeutet, dass das Modell korrekt einen leeren Text in Fällen ausgibt, in denen die Versionshinweise leer sind. Hier sind die Ergebnisse. Da der Datensatz E-Mail-Adressen, Hash-Werte usw. enthält, haben wir auch den bereinigten Datensatz bewertet, der diese ausschließt. CEAS und CEAS erzielten Rouge-L-Werte, die mehr als 10 Punkte höher waren als die Baselines. Insbesondere auf dem bereinigten Testdatensatz sprang die Punktelücke zwischen der vorgeschlagenen Methode und der Baseline auf mehr als 20 Punkte. Diese Ergebnisse deuten darauf hin, dass CEAS und CEAS signifikant wirksam sind. CEAS erhielt einen besseren Rouge-L-Wert als CEAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators wirksam ist und der Klassifikator mit zwei Labels trainiert wird. Eine hohe Abdeckung von CEAS kann wahrscheinlich erreicht werden, weil der Klassifikator sich darauf konzentrieren kann, relevante Commit-Nachrichten für jede Klasse auszuwählen. CEAS Match neigte dazu, einen höheren Rouge-L als CEAS Single zu haben, was darauf hindeutet, dass es auch wirksam ist, unabhängig voneinander verschiedene abstrakte Zusammenfassungsmodelle für jede Versionsnotenklasse zu entwickeln. Hier ist eine Fehleranalyse. CEAS-Methoden neigen dazu, kürzere Sätze als menschliche Referenzsätze auszugeben. Im Bild rechts hat der Referenzsatz drei oder vier Sätze, während CEAS nur einen hat. Der Grund für diese Modellneigung ist, dass in den Trainingsdaten nur 33 % der Sätze im Features-Label und 40 % im Verbesserungslabel vorhanden sind. Darüber hinaus können CEAS-Methoden keine genauen Versionshinweise ohne zusätzliche Informationen generieren. Das obere Beispiel rechts ist ein Beispiel für eine sehr unordentliche Commit-Nachricht, und der vollständige Satz kann nicht ohne Bezugnahme auf die entsprechende Pull-Anfrage oder das Issue generiert werden. Das Beispiel unten zeigt, dass die beiden Commit-Nachrichten in der Eingabe miteinander verbunden sind und zu einem Satz kombiniert werden sollten, aber es ist schwierig, dies zu tun. Schließlich ein Fazit. Wir haben einen neuen Datensatz für die automatische Erstellung von Versionshinweisen erstellt. Wir haben auch die Aufgabe der Eingabe von Commit-Nachrichten und deren Zusammenfassung so formuliert, dass sie auf alle Projekte anwendbar ist, die auf Englisch geschrieben sind. Unsere Experimente zeigen, dass die vorgeschlagene Methode weniger störende Versionshinweise mit höherer Abdeckung als die Baselines generiert. Bitte schauen Sie sich unseren Datensatz auf der GitHub-Seite an. Vielen Dank."}
