{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Asa Farari e vou apresentar nosso artigo, Enriquecimento de Dados Tabulares Few Shot usando arquiteturas de transformadores de ajuste fino. Os cientistas de dados analisam os dados e se concentram principalmente na manipulação das características existentes dos dados, mas às vezes essas características são limitadas. A geração de características usando outra fonte de dados pode adicionar informações substanciais. Nosso objetivo de pesquisa é o enriquecimento automático de dados tabelares usando textos livres de fontes externas. Suponha que tenhamos um conjunto de dados tabelar e uma base de conhecimento. Precisamos de um processo automático que envolva ligação de entidades e análise de texto para extrair novas características do texto livre do conjunto de conhecimentos. Nossa estrutura FEST é exatamente esse processo automático. Vamos ver um exemplo. E os conjuntos de dados são alimentados no FEST. Neste exemplo, o conjunto de dados é um conjunto de dados universitário, quando seu objetivo é classificar universidades em universidades de baixo alto escalão. Como base de conhecimento, usamos a Wikipedia. A primeira fase do FEST é a vinculação de entidades, quando cada entidade, neste exemplo, o nome da universidade é vinculado a uma entidade dentro da base de conhecimento e o texto das entidades da base de conhecimento é extraído e adicionado ao conjunto de dados. Neste exemplo, o texto é o resumo da página da Wikipedia. Agora precisamos gerar ou extrair recursos do texto retirado. Portanto, precisamos de uma fase de extração de recursos que inclui análise de texto. E essa é a principal novidade deste artigo, e vou aprofundar isso nas próximas transparências. Após a fase de extração de recursos, há uma fase de geração de recursos quando usamos os recursos extraídos para gerar um pequeno número de novas características. Primeiro, gerar características no número de classes do conjunto de dados original. Neste exemplo, o conjunto de dados original tem duas classes, então primeiro gerar duas novas características. Mas se o conjunto de dados tiver cinco classes, primeiro gerar cinco novas características. Cada característica representa a probabilidade de cada classe. Para analisar o texto, usamos o estado atual da análise de texto, que são modelos de linguagem baseados em transformadores, como BERT, GPT, XLERT, etc. Mas é improvável que possamos treinar um modelo de linguagem usando os conjuntos de dados de entrada. Portanto, uma abordagem ingênua será uma ajuste de tarefa-alvo. Portanto, na fase de extração de recursos, podemos baixar um modelo de linguagem treinado por pessoa, ajustar o modelo de linguagem sobre o conjunto de dados-alvo neste exemplo para ajustar o modelo de linguagem para classificar o texto em classes, abstrair em classes, receber a saída do modelo de linguagem, que é a probabilidade de cada classe, e usar como novas funcionalidades. O problema com essa abordagem é que os conjuntos de dados podem ter poucas entidades distintas, textos. Em nosso experimento, quase metade dos conjuntos de dados contém menos de 400 amostras e o menor conjunto de dados contém 35 amostras em seu conjunto de treinamento. Portanto, ajustar um modelo de linguagem nesse conjunto de dados será ineficaz. Mas podemos usar conhecimento prévio sobre conjuntos de dados pré-analisados, porque aplicamos o FAST a vários conjuntos de dados. Podemos usar os conjuntos de dados n menos um para coletar informações sobre os conjuntos de dados n menos um e usar essas informações quando analisamos os dados n. O que sugerimos é adicionar outra fase de fine tuning, uma fase de fine tuning de multitask preliminar, quando você fine-tune o modelo de linguagem sobre o n-th data set. E, em seguida, executamos outra fase de fine tuning, que é uma fase de fine tuning de tarefas de destino quando você fine-tune o modelo de linguagem sobre o n-th data set. O estado de hoje em multitask... um ajuste fino chamado MTDNN. Em MTDNN, MTDNN mantém cabeças no número de tarefas no treinamento. Portanto, neste exemplo, há quatro tarefas no treinamento. Portanto, MTDNN mantém quatro cabeças, como você pode ver na imagem. E é samplado um batch aleatório do treinamento. E se o batch aleatório pertencer a um Por exemplo, as tarefas de classificação de Singenselten executam passos para frente e para trás através da primeira cabeça. E se o batch aleatório pertencer a uma tarefa de classificação em pares, executam passos para frente e para trás através da última cabeça. Neste cenário, um conjunto de dados de tabela varia o número de classes. Portanto, há muitas tarefas. MTDNN mantém vários capas de classe, camadas de saída, e, além disso, MTDNN precisa inicializar novas cabeças para um novo conjunto de dados com uma nova tarefa. Nossa abordagem chamada ajuste de reformação de tarefas é que, em nossa abordagem de ajuste de reformação de tarefas, em vez de manter várias cabeças, reformulamos cada conjunto de dados em uma frase por problema de classificação, que é de duas classes. duas tarefas de classe. Então, vamos ver um exemplo. Aqui está o nosso conjunto de dados de input, que consiste em entidades, features, texto e classes. E reformulamos a tarefa de classificar o texto em baixo e alto para classificar o texto, o abstrato e a classe em true ou false. Ou, em outras palavras, treinamos o modelo de linguagem para classificar um abstrato e uma classe, se o abstrato pertence à classe ou não. Portanto, o vetor de rótulo neste caso permanece sempre, que consiste sempre em duas classes. E este é o algoritmo para nossa função. uma abordagem de ajuste fino reformulada. Então, vamos ver o quadro completo. Um conjunto de dados é inserido no FAST e, em seguida, o FAST executa a fase de linkagem. Ele extrai o texto da base de conhecimento, que neste exemplo é o abrigo da página da Wikipedia. Em seguida, ele reformula a tarefa em uma tarefa de classificação de sentença, aplica o modelo de linguagem ao novo tarefa. e a probabilidade de saída para cada classe. Nota que o modelo de linguagem já foi ajustado em um conjunto de dados n-1 usando uma ajustada preliminar de multitarefa. Em seguida, usamos o vetor de saída do modelo de linguagem como uma característica novamente gerada no número de classes. Para avaliar nossa estrutura, usamos uma tabela de 17. Um conjunto de dados de classificação de dezessete tabelas, que varia em tamanho, características, equilíbrio, domínio e desempenho inicial. E como base de conhecimento, usamos a Wikipedia. Projetamos nosso experimento como uma avaliação de um lado para o outro, quando treinamos rapidamente em dezesseis conjuntos de dados e os aplicamos ao décimo sétimo conjunto de dados. Também dividimos cada conjunto de dados em quatro grupos. E aplicamos uma falha cruzada de falhas. Em seguida, geramos a nova funcionalidade e a avaliamos usando cinco classificadores de avaliação. Usamos uma arquitetura baseada em construção em nosso experimento. Aqui estão os resultados do nosso experimento. Você pode ver que comparamos nossa estrutura com o ajuste fino do conjunto de dados de destino, ajuste fino da tarefa de destino. e o MTDNN preliminar de fine tuning e o nosso fine tuning reformulado atingiu o melhor resultado, o melhor desempenho, enquanto o MTDNN atingiu 2% de melhoria sobre o set de dados de destino de fine tuning. nossa abordagem atingiu 6% de melhoria quando olhamos para o pequeno set de dados, podemos ver que o desempenho do MTDNN diminui e a melhoria da fase de ajuste de múltiplas tarefas preliminares diminui para 1,5%, mas nosso desempenho aumentou para 11% em comparação com o ajuste de tarefas-alvo sozinho. Para resumir, o FAST permite o enriquecimento de várias cenas de 35 amostras em nosso experimento. Ele usa uma arquitetura para todos os conjuntos de dados de tarefas e mantém a cabeça do modelo. Mas adiciona três fases de formulação. É um conjunto de treinamento aumentado e precisa de um valor-alvo com significado semântico, para que possamos alimentá-lo no modelo de linguagem e usá-lo no problema de classificação de pares de frases. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "pt", "output": "Olá a todos. Hoje vou apresentar nosso trabalho de pesquisa, Aprender a Renunciar Deductivamente, Resolução de Problemas de Matemática como Extração de Razões Complexas. Eu sou Alan do Biden's AI Lab e este é um trabalho conjunto com Thierry da Universidade do Texas em Austin e Wayloo do SUDD. Primeiro, gostaria de falar sobre nossa motivação para o raciocínio. Então, aqui mostramos um exemplo onde o raciocínio em vários passos é útil. Então, esta figura é tirada do artigo PALM, onde eles realizam promptings para resolver o problema de Matemática em um cenário de aprendizado futuro. Portanto, no lado da página da NetPan, podemos ver que, se fornecermos algumas amostras com apenas perguntas e respostas, talvez não consigamos obter as respostas corretas. Mas, se fornecermos mais descrições de raciocínio, o modelo será capaz de prever a descrição de raciocínio e também fazer uma previsão correta aqui. Portanto, é bom ter um raciocínio multipassivo interpretável como saída. E também achamos que o problema de método é uma aplicação direta para avaliar tais habilidades de raciocínio. Aqui, na nossa configuração de problema, dadas as perguntas, precisamos resolver essa questão e obter as respostas numéricas. Portanto, em nossos conjuntos de dados, também recebemos a expressão matemática que leva a essa resposta específica. Portanto, certas suposições também se aplicam, como no trabalho anterior. Assumimos que a precisão das quantidades é conhecida e consideramos apenas operadores básicos, como adição, subtração, multiplicação, divisão e exponencial. Além disso, operadores complicados podem ser realmente descartados. E, na verdade, pode ser decomposto nesses operadores básicos. Portanto, o trabalho anterior na resolução de problemas de método pode ser categorizado em sequência para sequência e sequência para árvore. Portanto, o modelo de sequência tradicional converte a expressão para uma sequência específica para geração, e é bastante fácil de implementar. E pode ser generalizado para muitos problemas complicados diferentes. Mas os inconvenientes são que o desempenho geralmente não é melhor do que o modelo de estrutura. E é a falta de interpretabilidade para o previsão. Mas, na verdade, essa direção ainda é bastante popular devido ao modelo transformador. Portanto, em modelos baseados em árvores, na verdade, estruturamos essas expressões em forma de árvore e seguimos uma travessia pré-ordena em gerações de árvores. Portanto, aqui, continuamos gerando os operadores até chegarmos às folhas, que são as quantidades. Portanto, aqui, o bom é que isso nos dá essa estrutura de árvore binária. Mas, na verdade, é bastante contraintuitivo. Porque geramos o operador primeiro e, no final, geramos as quantidades. E a segunda coisa é que ele também contém algumas computações repetitivas. Então, aqui, se olharmos para essa expressão, oito vezes três mais três, na verdade, é gerada duas vezes. Mas, na verdade, devemos reutilizar os resultados. Portanto, em nossa abordagem proposta, queremos resolver esses problemas de maneiras passo a passo e interpretáveis. Por exemplo, aqui, no segundo passo, podemos obter. esses divisores, que são vinte e sete. E também podemos nos referir às perguntas originais para encontrar o conteúdo relevante. E nessas etapas, obtemos os divisores. Então, nessa terceira etapa, obtemos o quociente. E após essas três etapas, podemos reutilizar os resultados da segunda etapa e obter os resultados da quarta etapa. E, finalmente, podemos obter os dividendos. Portanto, aqui, geramos a expressão inteira diretamente, em vez de gerar operadores ou quantidades individuais. Isso torna o processo mais preciso. Portanto, em nosso sistema dedutivo, começamos primeiro com um conjunto de quantidades apresentadas nas perguntas e também incluindo algumas constantes como nossos estados iniciais. Portanto, a expressão é representada por EIJOP, onde realizamos operadores de Qi a QJ, e tal expressão é realmente direcionada. Portanto, também temos subtração invertida aqui para representar a direção oposta. Isso é bastante semelhante à extração de relação. Portanto, em um sistema dedutivo formal, no passo de tempo t, aplicamos o operador entre o par Qi e Qj e, em seguida, obtemos essa nova expressão. Adicionamos-a aos próximos estados para se tornar uma nova quantidade. Portanto, essas transparências na verdade visualizam a evolução dos estados, onde continuamos adicionando expressões aos estados atuais. Em nossas implementações de modelo, primeiro usamos um modelo pré-treinado que pode ser pássaros ou robôs, e depois codificamos a frase e obtemos essas representações de quantidade. Assim que obtemos as representações de quantidade, podemos começar a fazer inferências. Aqui, mostramos um exemplo de Q um para obter a representação para Q um, que será dividida por Q dois e, em seguida, vezes Q três. Primeiro, obtemos a representação de pares, que é basicamente apenas a concatenação entre Q um e Q dois, e, em seguida, aplicamos uma rede de rede de reação, que é parametrizado pelo operador. E, finalmente, obtemos a representação da expressão Q1 dividida por Q2. Mas, na prática, na fase de inferência, podemos obter a expressão incorreta também. Portanto, aqui, todas as expressões possíveis são iguais a três vezes o número de operadores. Portanto, o bom aqui é que podemos facilmente adicionar restrições para controlar esse espaço de pesquisa. Por exemplo, se essa expressão não for permitida, podemos simplesmente removê-la do nosso espaço de pesquisa. Portanto, no segundo passo, fazemos a mesma coisa, mas a única diferença é mais uma quantidade. Portanto, essa quantidade vem da expressão calculada anteriormente. Portanto, finalmente, podemos obter essa expressão final Q três vezes Q quatro. E também podemos ver que o número de todas as expressões possíveis é diferente da etapa anterior. Portanto, tal diferença dificulta a aplicação. É difícil aplicar a pesquisa de feixe porque a distribuição de probabilidade entre essas duas etapas está desequilibrada. Portanto, o procedimento de treinamento é semelhante ao treinamento de um modelo de sequência para sequência, onde otimizamos as perdas em cada etapa do tempo e aqui também usamos essa tow para representar quando devemos terminar esse processo de geração. E aqui o espaço é diferente de sequência para sequência porque o espaço é diferente em cada etapa do tempo, enquanto no modelo tradicional de sequência para sequência é o número de vocabulário. E também nos permite impor certas restrições a partir do conhecimento anterior. Portanto, realizamos experimentos nos conjuntos de dados de problemas de métodos comumente usados, MAWPS, Math twenty three K, Math QA e SWAM. E aqui mostramos brevemente os resultados comparados com as melhores abordagens anteriores. Portanto, nossa variante de melhor desempenho é a Roberta Deductive Reasoner. E, na verdade, não usamos o BeamSearch, ao contrário, as abordagens anteriores usam o BeamSearch. Tudo bem, então as melhores abordagens são muitas vezes modelos baseados em árvores. Então, no geral, nosso raciocínio é capaz de superar significativamente esse modelo baseado em árvores, mas podemos ver que os números absolutos no MathQA ou no SWAMP não são realmente altos. Então, investigamos ainda mais os resultados no SWAMP e esse conjunto de dados é desafiador porque o autor tenta adicionar manualmente algo para confundir o modelo NLP, como adicionar. informações irrelevantes e quantidades extras. Portanto, em nossa previsão, encontramos alguns dos valores intermediários que são, na verdade, negativos. Por exemplo, nessas perguntas, estamos perguntando quantas maçãs Jake tem, mas temos algumas informações extras, como dezessete pontos a menos e Stephen tem oito pontos, o que é totalmente irrelevante. Portanto, nosso modelo faz algumas previsões como essa, que produzem valores negativos. E observamos que essas duas expressões têm, na verdade, result temos pontuações semelhantes. Então, podemos limitar esse espaço de pesquisa removendo esses resultados negativos para que possamos fazer a resposta correta. Então, descobrimos que tal restrição melhora bastante para alguns modelos. Por exemplo, para os pássaros, melhoramos sete pontos. E, em seguida, para o modelo baseado em Roberta, melhoramos dois pontos. Portanto, um modelo de linguagem melhor tem uma melhor capacidade de compreensão da linguagem, para que o número aqui seja. Aqui está mais alta para a Roberta e menor para os pássaros. Também tentamos analisar a dificuldade por trás de todos esses conjuntos de dados. Assumimos que o número de quantidades não utilizadas pode ser considerado como informação irrelevante aqui. Então, aqui podemos ver que temos o número, a porcentagem de amostras com quantidades não utilizadas e o conjunto de dados de pântano tem a maior porção. E aqui também mostramos o desempenho geral. Para aquelas amostras sem quantidades não utilizadas, o desempenho geral é, na verdade, maior do que o. E o desempenho é, na verdade, maior do que o desempenho geral. Mas com essas amostras, com uma quantidade não utilizada, é, na verdade, muito pior do que o desempenho geral. Para o MAWPS, não temos muitos casos de desempenho, então simplesmente ignoro essa parte. Então, finalmente, queremos mostrar a interpretabilidade através de um exemplo de participação de criação. Então, aqui, nosso modelo faz uma previsão errada no primeiro passo. Assim, podemos realmente correlacionar essa expressão com a frase aqui. Então, achamos que essa frase pode estar enganando o modelo e fazendo previsões incorretas. Então, aqui, plantar mais trinta e cinco faz o modelo pensar que deveria ser um operador de adição. Então, tentamos revisar a frase para que seja algo como o número de árvores de pêssego é trinta e cinco a menos do que as árvores de maçã. Então, fazemos com que ela transmita uma semântica mais precisa, de modo que o modelo seja capaz de fazer a previsão correta. Este estudo mostra como as previsões interpretáveis nos ajudam a entender o comportamento do modelo. Para concluir nosso trabalho, primeiro, nosso modelo é bastante eficiente e conseguimos fornecer um procedimento de resolução interpretável e podemos incorporar facilmente algum conhecimento prévio como restrição, o que pode ajudar a melhorar o desempenho. A última coisa é que o mecanismo subjacente não se aplica apenas às tarefas de resolução de problemas da rede, mas também a outras tarefas que envolvem raciocínio de vários passos. Mas também temos algumas limitações. Se tivermos um número grande de operadores ou constantes, o consumo de memória pode ser bastante alto. E a segunda coisa é que, como mencionado, como a distribuição de probabilidade está desequilibrada entre diferentes etapas de tempo, também é bastante desafiador aplicar a estratégia de busca por fio. Portanto, este é o fim da conversa e as perguntas são bem-vindas. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Antoine e sou da Universidade de Maastricht. Vou apresentar meu trabalho conjunto com Jerry, que trata de um novo conjunto de dados para a recuperação de artigos estatutários. As questões jurídicas são uma parte integral da vida de muitas pessoas, mas a maioria dos cidadãos tem pouco ou nenhum conhecimento sobre seus direitos e processos jurídicos fundamentais. Como resultado, muitos cidadãos vulneráveis que não podem pagar a cara assistência de um especialista jurídico ficam sem proteção ou, pior, explorados. A ação é preencher a lacuna entre as pessoas e a lei, desenvolvendo um sistema de retorno eficaz para artigos estatutários. Um sistema como esse poderia proporcionar um serviço de ajuda jurídica gratuito para humanos descalçados. Antes de mergulharmos na principal contribuição deste trabalho, vamos primeiro descrever o problema de retorno de artigos estatutários. Given uma simples questão sobre um assunto legal, como o que eu risco se violar a confidencialidade profissional, um modelo é necessário para retirar todos os artigos estatutários relevantes de um grande volume de legislaçãolação. Essa tarefa de retratação de informações vem com seu próprio conjunto de desafios. Primeiro, depende de dois tipos de linguagem comum natural para as perguntas e complexo linguagem legal para as estatutos. Essa diferença em distribuição de linguagem torna mais difícil para um sistema retirar candidatos relevantes, pois indiretamente requer um sistema de interpretação inerente que possa traduzir uma pergunta natural para uma pergunta legal que corresponda à terminologia da estatuto.utos não são uma pilha de artigos independentes que podem ser tratados como uma completa fonte de informação em si, como notícias ou receitas, por exemplo. Instead, é uma estrutura, coletão de provisões que têm um significado completo apenas quando considerados no contexto, que é junto com as informações suplementares de seus artigos, os fields e subfields que eles pertencem, e o seu lugar na estrutura do direito. Lastly, estatutos são um pequeno parágrafo. que usualmente é a típica unidade de retrieval em mostes de trabalhos. Aqui, são documentos longos que podem ser até seis mil words. Os recentes avanços em NLPs têm despertado enorme interesse em muitas tarefas, como jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, jurídica, juríd para estudar se o modelo de retrieval pode aproximar a eficiência e reliabilidade de um legal expert para a tarefa de retriever artigos estatutários. Nosso conjunto de dados de retrieval de artigos estatutários consiste em mais de um mil e um centenas de questões legales poste por cidadãos belgas. Essas questões cobrem uma wide range de tópicos, desde família, moradia, dinheiro, até trabalho e segurança social. Each of them has been labeled by experientes juristas com referenças a artigos de mais de vinte e dois mil seiscentos artigos de legal da Belgian Codes of Law. Vamos agora falar sobre como coletamos esse data set. Primeiro, começamos compilando um grande corpus de legal artigos. Consideramos trinta e dois publicamente disponíveis códigos belgas e extraímos todos os artigos, as correspondentes secções. Depois, gatheramos questões jurídicas com referenças a estatutos relevantes. Para fazer isso, partimos com a Belgian Law Firm. que recebe cada ano cerca de quatro mil euros de e-mail de cidadãos belgas que pedem consultoria sobre uma questão legal. Ficamos sortudos para acessar seus sites, onde sua equipe de juristas experientes aborda as questões mais comuns da Bélgica. Coletamos milhares de perguntas, anotadas com categorias, subcategorias e referências jurídicas a estatutos relevantes. Por último, passamos as referências jurídicas e filtramos as perguntas cujas referências não eram artigos. em um dos códigos of lawyer considered. As restantes referenças foram matchadas e convertidas para as correspondentes IDs de artigos do nosso corpus. Eventualmente, acabamos com um mil e umhenta e oito perguntas, cada caracterizado com as IDs dos artigos relevantes do nosso corpus de vinte e dois mil seiscentos e trinta e três artigos estatutórios. Em addition, cada question com uma categoria concatenação de sua subsequência na estrutura da lei. Essa informação adicional não é usada no trabalho atual, mas pode ser de interesse para pesquisas futuras sobre recuperação de informações legais ou classificação de texto jurídico. Vamos analisar algumas características de nossos conjuntos de dados. As perguntas têm entre cinco e quarenta e quatro palavras com uma mediana de quarenta palavras. Os artigos são muito mais longos, com uma mediana de setenta e sete palavras, com cento e quarenta e duas delas excedendo mil palavras. O mais longo é até cinco mil e setecentos e noventa palavras. Como mencionado anteriormente, a questão cobre uma ampla variedade de tópicos, com cerca de oitenta e cinco por cento deles sendo sobre família, moradia, dinheiro ou justiça, enquanto os restantes quinze por cento concernam a segurança social, estrangeiros ou trabalho. Os artigos também são muito diversos, pois vêm de trinta e dois códigos belgas diferentes que cobrem uma grande quantidade de tópicos ilegais. Aqui está o número total de artigos coletados de cada um desses códigos belgas. Dos 22.633 artigos, apenas 1.612 são mencionados como relevantes para pelo menos uma questão nos conjuntos de dados, e cerca de 80% desses artigos citados vêm do código civil, do código judicial, do código de investigação criminal ou do código penal. Enquanto isso, 18 de 32 códigos têm menos de cinco artigos mencionados como relevantes para pelo menos uma questão, que pode ser expressa pelo fato de que DUS código focou menos em indivíduos e suas preocupações. Overall, o número median de citação para esses artigos citados é dois, e menos de vinte e cinco por cento deles são citados mais de cinco vezes. Usando nossos dados, benchmarkamos várias abordagens de retrievação, incluindo lexical e dense arquitetura. Given uma query em um artigo, um modelo lexical assina uma score para o par de artigos de query. Computando a soma dos pesos de cada um desses termos no artigo. Experimentamos com as funções padrão de classificação TFIDF e BM twenty five. O principal problema dessas abordagens é que elas só podem recuperar artigos que contêm palavras-chave presentes na consulta. Para superar essa limitação, experimentamos com uma arquitetura baseada em neurais que pode capturar a relação semântica entre consultas e artigos. Usamos um modelo de BE encoder que mapeia consultas e artigos em representações de vetores densas. e calcular uma pontuação relevante entre um par de artigos de consulta pela semelhança de suas embeddings. Essas embeddings geralmente resultam de uma operação de pooling na saída de um modelo de embedding de palavras. Primeiro, estudamos a eficácia dos encodadores B siameses em uma configuração de avaliação de zero tiro, o que significa que os modelos de embedding de palavras pré-treinados são aplicados de fábrica sem qualquer ajuste adicional. Experimentamos com encodadores de texto independentes do contexto, nomeadamente WordToVec e Fastastex e modelos de embedding dependentes de contexto, nomeadamente Roberta e, mais especificamente, Camembert, que é um modelo Roberta francês. Adicionalmente, treinamos nossos próprios modelos baseados em Camembert, Biancoders em nossos dados. Observe que, para treinamento, experimentamos com as duas variedades da arquitetura Biancoder. Siamese, que usa um modelo de embedding de palavras único que mapeia a consulta e o artigo juntos em um espaço de vetoras densas compartilhadas. E Tutor, que usa dois modelos de embedding de palavras que encodem o arquivo de query separadamente em diferentes espaços de embedding. Experimentamos com mean, max, e CLS pooling, bem como produto e cosseno para computar similaridades. Aqui estão os resultados de uma baseline no test set, com os métodos lexiconos abaixo, os Biancoders de Siamese avaliados em um setup de zero shot no meio, e os Biancoders de Fine Tune abaixo. No geral, os Biancoders de Fine Tune significativamente superam todas as outras baselines. O modelo de dois torres improve sobre a sua variante siamese no recall de 100, mas performou de forma semelhante nas outras métricas. Embora o BM 25 tenha subestimado significativamente o Biancoder treinado, seu desempenho indicou que ainda é uma base sólida para retenção específica de domínio. Em relação à avaliação zero shot do Biancoder siamese, descobrimos que o uso direto dos embeddings de um modelo pré treinado de Kamembert sem otimizar para a tarefa de retenção de informações dá resultados ruins, o que é consistente com as descobertas anteriores. E o Biancoder baseado em WordToVec superou significativamente o modelo baseado em Fastex e Bird, sugerindo que talvez os embeddings de nível de palavra pré-treinado sejam mais apropriados para a tarefa do que os embeddings de nível de caractere ou sub-palavra quando usados de fábrica. Embora promissores, esses resultados sugerem ampla oportunidade de melhoria em comparação com um especialista em linguagem habilidosa que pode eventualmente recuperar todos os artigos relevantes para qualquer pergunta e, assim, obter pontuações perfeitas. Vamos concluir discussindo duas limitações de nossos dados. Primeiro, o corpus de artigos é limitado a esses coletados dos trinta e dois considerados códigos belgas, que não cobre o todo o direito, como artigos de decretos, diretivas e ordenanças são missos. Durante a construção do dados, todas as referências a esses artigos não coletados são ignoradas, o que causa que algumas questões acabam com apenas uma fração do número inicial de artigos relevantes. Es artigos podem ser incompletos, embora ainda seja completamente apropriado. Segundo, devemos notar que nem todas as questões jurídicas podem ser respondidas com estatutos alone. Por exemplo, a questão can eu evictar meus tenantes se eles fazem muito noise pode não ter uma resposta detalhada com a lei que quantifique um noise threshold at que evicção é permitida. Emsted, o landlord deve provavelmente rely mais em case law e encontrar precedentes similares à situação. Por exemplo, o tenante faz duas partes por semana até 2h. Portanto, algumas questões são mais adequadas do que outras para a tarefa de recuperação de artigos estatutários, e o domínio das menos adequadas ainda precisa ser determinado. Esperamos que nosso trabalho desperte interesse no desenvolvimento de modelos práticos e confiáveis de recuperação de artigos estatutários que possam ajudar a melhorar o acesso à justiça para todos. Você pode conferir nosso artigo, dados e código nos seguintes links. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "pt", "output": "Hello, estamos felizes em apresentar nosso trabalho on VALS, um benchmark independente de tarefas para testar visão e linguagem com phenomena linguísticos específicos. Por que fizemos o trabalho em estabelecer este benchmark? Bem, durante os últimos anos, vimos uma explosão de visão e linguagem basadas em transformação, pré trained em grandes quantidades de image text pairs. Each one of these models pushes state of the art on vision and language tasks, such as visual question ans comum sense reasoning, image retrieval, phrase grounding. Então, recebemos uma mensagem. As acuracies nesses benchmarks específicos de tarefas estão aumentando steadily, mas sabemos o que os modelos realmente aprenderam? O que é que um visão e linguagem transformer entenderam quando atribuíram uma pontuação alta para essa imagem e essa frase para matchar e uma pontuação baixa para essa? Os modelos de visão e linguagem focam na coisa certa ou se concentram em preconceitos? como mostrado por trabalho anterior. Para sheder mais luz sobre esse aspecto, propomos uma direção mais agnostica de teste e introduzimos vales que testam a sensitividade de visão e linguagem para modelos de linguagem específicos que afetam tanto as linguísticas quanto as modalidades visuais. Testamos existença, pluralidade, contingência, relações espaciais, ações e coerencia de entidade. Mas como testamos se os modelos de visão e linguagem capturaram esses fenômenos? Foiling, um método previously aplicado para vision e linguagem models, apenas para noun phrases por Ravi Shakar e colaboradores e no counting por Asin previous work. Foiling basicamente significa que pegamos a caption of an image e produzimos um foil, alterando a caption, talvez não descreva a imagem anymore. E fazemos essas alterações de phrase focando em seis peças, como existence, pluralidade, counting, espacial relations, actions, e entity co reference. Cada peça pode consistir em um ou mais instrumentos, caso encontremos mais de uma maneira interessante de criar instâncias de foil. Por exemplo, no caso da peça de ações, temos dois instrumentos, um em que o verbo de ação é alterado com uma ação diferente e um em que actantes são trocados. Counting e coreference também são peças que têm mais de um instrumento. E criamos esses foils fazendo com que eles não descrevam a imagem, que são sentenças gramaticais e outras vezes válidas. Isso não é fácil de fazer porque uma captura foiled pode ser menos provável do que a original captura. Por exemplo, though it's not impossível, é estatisticamente menos provável que plantas cutam um homem do que um homem cut plantas e grandes vision e modelos de linguagem podem pular isso. Portanto, para obter foils validos, temos que tomar medidas. Primeiro, usamos modelos de linguagem forte para propôs foils. Segundo, usamos inferença de linguagem natural ou NLI para filtrar foils que ainda possam ser descritos. ainda descrevendo a imagem, pois quando construímos foils, precisamos garantir que eles não descrevam a imagem. Para testar isso automaticamente, aplicamos a inferencia de linguagem natural com a seguinte justificativa. Consideramos uma imagem como a premissa e sua captura como a hipótese envolvida. Além disso, consideramos a captura como a premissa e a foil como a hipótese. Se um modelo NLI predir que a foil contradiz ou é neutra em relação à captura, consideramos isso como um indicador de uma foil válida to be entailed by the caption, it cannot be a good foil, since by transitivity, it will give a truthful description of the image and we filter these foils out. Mas this procedure is not perfect. It is just an indicator for valid foils, therefore, as a terceira medida para gerar valid foils, we employ human annotators to validate the data used in valves. So, after filtering and human evaluation, we have as many test instances as describidas in this table. Note that valves does not deliver any training data. mas apenas dados de teste, pois é apenas um benchmark de teste de tiro zero. É projetado para alavancar as capacidades existentes dos modelos de visão e linguagem após o pré-treinamento. A ajuste fino permitiria que os modelos explorassem artefatos ou vieses estatísticas nos dados. E todos sabemos que esses modelos gostam de trapacear e usar atalhos. E, como dissemos, estamos interessados em avaliar quais capacidades os modelos de visão e linguagem têm após o pré-treinamento. Experimentamos cinco modelos de visão e linguagem no VALS, nomeadamente com CLIP, Wilbert, Wilbert Kelvin I e VisualBert. Duas de nossas metricas de evaluação mais importantes são a accuracy dos modelos em classificar pais de imagens em captions e foils. Percebido mais relevante para este vídeo, vamos mostrar nossa mais permissiva metrô, a accuracy de pares, que measura se a pontuação de alinhamento de imagens é maior para o correto par de texto de imagem do que para o par de foil. Para mais metricas e resultados sobre elas, doze, nossa papel. Os resultados com accuracy de pares são mostrados aqui e são consistentes. com os resultados que obtivemos da outra metrônica. É que a melhor zero shot performance é achievida por Wilbert twelve em um, seguido por Wilbert, Alex Mert, Clip, e finalmente VisualBird. É notable como instrumentos centrados em individual objetos como existência e noun phrases são quase resolvidos por Wilbert twelve em um, highlighting que modelos são capazes de identificar objetos namados e suas presenças em imagens. No entanto, nenhum dos restantes peças pode ser reliably resolvido em nossos adversários foiling settings. usando instrumentos que vision e linguagem modelos têm trouble distinguir referenças a single versus multiplos objetos ou contando-os em uma imagem. O relacionamento de relation mostra que eles têm dificuldades em corretamente classificar uma relação espacial entre objetos em uma imagem. Eles também têm trouble distinguir ações e identificar seus participantes, mesmo se suportados por plausibilidade biases, como vemos no actions piece. From the coreference piece, we find out that tracing multiplas referenças to the same object em uma imagem usando pronouns is also difficult for vision and language models. Como sanitização e porque é um experimento interessante, também benchmarkamos dois modelos GPT um e GPT dois para assessar se Valve é solvável por esses modelos unimodalizados, computando a perplexidade da correção e da captura, no imagem aqui, e predictando a entrada com a menor perplexidade. Se a perplexidade for a foil, tomamos isso como uma indicação de que a captura pode sofrer de plausibilidade bias ou outros biases linguísticos. E é interessante ver que em alguns casos de texto apenas GPT capturaram a plausibilidade do mundo melhor do que os modelos de visão e linguagem. Portanto, para resumir, o VALS é um benchmark que usa a lente de construções linguísticas para ajudar a comunidade a melhorar os modelos de visão e linguagem por testar suas capacidades de visão. Nossos experimentos mostram que os modelos de visão e linguagem identificam objetos nomeados e suas presenças em imagens bem, como mostrado pela peça de existência, mas strugam em ter a interdependência e relações em cenas visuais quando forçados a respeitar indicadores linguísticos. Gostaríamos de incentivar a comunidade a usar o Valse para medir o progresso em direção ao fundamentamento da linguagem com modelos de visão e linguagem. E, além disso, o Valse poderia ser usado como uma avaliação indireta de conjuntos de dados, pois os modelos poderiam ser avaliados antes e depois de treinamento ou ajuste fino para ver se um conjunto de dados ajuda os modelos a melhorar em qualquer um dos aspectos testados pelo Valse. Se você estiver interessado, confira os dados do Valse no GitHub e, se tiver alguma dúvida, não hesite em entrar em contato conosco."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Kami Zero, da Universidade de Tóquio. Vou apresentar um papel entitado RNSAM, um grande dado de escala para automatic resumização via comitologia. Vou explicar em desta ordem. Primeiro, vou introduzir a automatic resumização que estamos trabalhando nesta pesquisa. ReleaseNode é um documento técnico que sumariza as mudanças distribuídas com cada release de um software. A imagem mostra a releaseNode para versão dois ponto seis ponto quatro da biblioteca. Dissemases desempenham um papel importante em desenvolvimento de open source, mas são tempo consumindo para preparar manualmente. Portanto, seria muito útil ser capaz de automaticamente gerar qualificados. Eu vou referir a duas pesquisas sobre automaticidade de gerar. O primeiro é um sistema chamado Arena, relevado em 2014. Ele usa uma abordagem baseada em rules, por exemplo, usando o extrator de mudança para extrair as diferenças. Bibliotecas e documentos de mudanças das diferenças entre releases e finalmente combinando-as. A mais notable feature deste sistema é o issue extraído no canto superior direito, que deve ser linkado ao Jira, o ecossistema de issue e só pode ser aplicado a projetos que usam Jira. Em outras palavras, não pode ser usado para muitos projetos no GitHub. O segundo é o GRIF, recentemente anunciado em 2020. É disponível na internet e pode ser stored via PIP. Este sistema tem um modelo de classificação de texto baseado em simples e retorna uma das cinco listas, como fechas ou fixações para cada mensagem de input. A imagem é uma amostra de usagem que retorna uma correção ou fixações. O dado de corrida de Graves é bastante pequeno, cerca de cinco mil e será mostrado nos experimentos descritos abaixo. O desempenho do modelo de classificação de texto não é alto. Apresento duas pesquisas relacionadas, mas há problemas de aplicação limitada e dados escassos. e escassez de recursos. O nosso papel resolve esses dois problemas e automaticamente gerará resultados de alta qualidade. Para o problema de aplicabilidade limitada, propomos uma classificação de classificação de alta qualidade usando apenas o mensage de comitê como input. Essa proposta de metade pode ser usada para todos os repositórios. Para o segundo problema de escassez de recursos, construímos um RNSAM data set consistindo de cerca de oitenta e dois mil pedaços de dados por correção de dados de públicos repositórios usando a API do. Next, descrevo nosso data set. Aqui está um exemplo de data. O lado de left é uma comitória e o lado de direita é um release node. Os release nodes são revelados como implements, bug fixes, etc. Temos setup uma tarefa que toma as comitórias como input e outpaces revelados release nodes. Isso pode ser considerado como uma tarefa de summarização. Temos predefinido quatro reveladores, features, implements, bug fixes, duplications, removers e brequetes. Esses foram setados baseados em pesquisas e outras faculdades. As notas no bottom right são extraídas das notas no bottom left. Neste time, é necessário detectar as quatro rubens que foram setados em advance, mas as rubens não são sempre consistentes com cada reposição. Por exemplo, as improvimentos incluem improvimentos, enhancements, optimizations, e assim por diante. Preparamos uma lista de vocabulário ou estudos para cada um desses botões. Use-o para detectar as notas de res e corretar o texto da res que fala como a sentença de res para a classe. Next é uma mensagem de comitamento. As mensagens de comitamento não estão ligadas a cada res. Como mostrado na imagem abaixo, se a curta res é versão dois ponto cinco dois nove, precisamos identificar a res res res res res e dois ponto cinco dois eighteen e obter a diferença. Isso é um pouco tedioso e não é suficiente apenas obter uma res resposta de res e olhar para o vídeo antes e depois. Criamos um heurístico matching blue para obter as páginas anteriores e próximas. Desset Analysis. Em the end, sete mil duzentos repositórios e oitenta e dois mil páginas de dados foram corretos. Além disso, o número de release not tokens é sessenta e três, o que é bastante alto para a tarefa de summarização. Além disso, o número de tokens é bastante rangeado em oito mil oitocentos e trinta mil. Isso é due ao grande número de custo e nome de método. base e methods encontrados no repository. Next, eu explicarei o propósito. O modelo de summarização de crosswise, extractivos e abstractivos consiste em dois modules, um classificador usando bot ou bot e um gerador usando bot. Primeiro, GEAS usando um classificador para classificar cada comitê de mensagem em cinco classes, features, improvimentos, bug fixes, deprecations, press e outros. Os comitês de mensagens classificados como outros são descartados. Depois, GEAS. O GAS aplica o gerador aos quatro documentos de Rubio independentemente e gerar RISTNODE para cada classe. Em esta tarefa, as correspondências diretas entre comit mensagens e RISTNODE não são conhecidas. Portanto, para treinar o classe FIA, Classe FIA assignamos Rubio a cada input comit mensagem usando os primeiros dez caracteres de cada comitê. Modelamos o Crasse FIA's abstracto summarizing sua abordagem por dois métodos diferentes. O primeiro modelo, que chamamos GAS Single, Sigma consiste em uma síntese de segurança e gerar uma síntese de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança de segurança Metodos foram compurados GS, GS Single, GS Mouth, Rosselling, e previously Griff. Regarding aborreção, em alguns casos, essas notas são output em múltiplas sentenças. Since é difícil calcular o número de sentenças zero, elas são combinadas com espaços e tratadas como uma longa sentença. O brew é penalizado quando o sistema outputs uma sola sentença. Essa penalidade resultará em um lower brew value em experimentos descritos. Finalmente, também calculamos a especificidade porque Rouge e Bruce não podem ser calculados se os RISE nodes forem vazios. Uma alta especificidade significa que o modelo calculou a outputs em text em casos onde os RISE nodes assumem vazios. Aqui estão os resultados. Since o data set conta email addresses, hash values, etc, também aborrecemos o Crane data set, que exclui-los. CAS e CAS achievem Rouge error scores mais than ten points higher than the base lines. No entanto, no teste Green, a diferença de pontuação entre o método proposto e o base jumped para mais de vinte pontos. Esses resultados indicam que GAS e GAS são significativamente eficazes. GAS obteve uma melhor pontuação de Rouge LS do que GAS, sugerindo que combinar um classificador e um gerador é eficaz em treinar o classificador usando pseudobus. A alta cobertura do GAS pode ser alcançada adequadamente porque o classificador pode se concentrar em selecionar mensagens de comitê relevantes para cada classe. Xia's much tended to yield higher rules error than Xia's single, sugerindo que é also efetivo de depender de desenvolver diferentes modelos de resumir perspectivas para cada nó de graça. Hero e error analysis Xia's methods tend to output shorter sentences than human reference sentences. In the figure on the right, the reference sentence has three or four sentences, while Xia's has only one. The reason for this model reluctance is that que no training data, apenas trinta e três por cento das sentenças estão presentes no rebel features e quarenta por cento no rebel improvimentos. Furthermore, os métodos CS não geram acurate reels sem adicionar informações. O top example on the right é um exemplo de uma mensagem de comitamento e a complexa sentença não pode ser gerada sem referência ao correspondente request ou issue. O exemplo abaixo mostra que as duas mensagens de comitamento no input são related e devem ser combinadas em uma sentença. Finalmente, uma conclusão. Construímos um novo densidade para gerar notas de rede automaticamente. Também formalizamos a tarefa de entrar mensagens de comitê e resumir as notas para que seja aplicável a todos os projetos escritos em inglês. Nosso experimento mostrou que o método proposto gerou menos notas de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de rede de re"}
