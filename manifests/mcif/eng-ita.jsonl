{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 0, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/ICWfTnUMio.wav", "src_ref": "Hi! My name is Matthias Lindemann, and today I'm going to give you a brief introduction to our paper on \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\". This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, \"The girl slept.\" And \"Mary knew that the girl slept.\" These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don't use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they're not ordered. That's why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don't know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That's because this is related to the \"Traveling Salesman\" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.", "tgt_ref": "Ciao!\nMi chiamo Matthias Lindemann, e oggi vi offrirò una breve introduzione al nostro articolo \"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations\".\nQuesto è un lavoro congiunto con i miei advisor Alexander Koller e Ivan Titov.\nLa generalizzazione composizionale può essere intesa come la capacità di gestire ricorsioni più profonde e composizioni invisibili di frasi che sono state osservate individualmente durante l'addestramento.\nNel contesto del parsing semantico, il test per la generalizzazione composizionale potrebbe apparire in questo modo.\nCome al solito, disponiamo di un insieme di espressioni di addestramento.\nIn questo caso, \"La ragazza ha dormito\".\nE \"Mary sapeva che la ragazza dormiva\".\nQueste espressioni sono abbinate a forme logiche che rappresentano aspetti fondamentali del loro significato.\nA differenza della valutazione standard di machine learning, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente invisibili.\nIn questo esempio, il modello è stato esposto a una ricorsione superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda.\nI modelli seq2seq semplicistici hanno difficoltà con questo tipo di generalizzazione fuori distribuzione e spesso producono output distaccati dall'input.\nIn particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle codificate a colori nell'esempio.\nUn metodo diffuso per affrontare questo problema è integrare gli alberi nei modelli.\nGli alberi hanno lo scopo di catturare il processo compositivo che mette in relazione le espressioni con le forme logiche.\nQuesto funziona bene, ma gli alberi di solito non sono dati e devono essere ottenuti in qualche modo.\nCiò può rivelarsi un processo complicato e talvolta dispendioso dal punto di vista computazionale.\nIn genere, ciò comporta una notevole pre-elaborazione delle forme logiche specifiche del formalismo, ad esempio per gestire i simboli delle variabili.\nL'ottenimento di alberi può inoltre comportare procedure specializzate di induzione grammaticale.\nIn questo documento non usiamo alcun albero e introduciamo un modello neurale seq2seq che modella direttamente le corrispondenze tra frammenti di input e frammenti di output.\nPer la prima volta, mostriamo una forte generalizzazione a una ricorsione più profonda senza fare affidamento sugli alberi.\nIl nostro approccio prevede l'output dall'input in due passaggi.\nInnanzitutto, etichettiamo ciascun token di input con un multiset non ordinato di token che apparirà nell'output.\nDopo il primo passaggio, abbiamo tutti i token giusti, che però non sono ordinati.\nEcco perché nella seconda fase utilizziamo un altro modello per prevedere una permutazione che li metta nell'ordine giusto.\nIntroduciamo un nuovo metodo per prevedere la permutazione che non ponga vincoli rigidi sulle possibili permutazioni.\nQuesto rende il nostro approccio abbastanza flessibile ed espressivo.\nConcettualmente, il nostro modello di permutazione funziona all'incirca in questo modo.\nAndiamo da sinistra a destra sull'output e determiniamo quale token multiset mettere in ogni posizione.\nPer la prima posizione di output, ne selezioniamo semplicemente uno, come evidenziato in rosso.\nDopodiché passiamo al token multiset successivo, per determinare il secondo token nell'output.\nDeterminiamo il terzo token nell'output in modo simile saltando a un altro token multiset.\nProseguiamo con questo processo fino a quando ogni gettone del primo stadio non è stato visitato esattamente una volta.\nPer darvi un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS.\nIl nostro modello supera gli altri con un ampio margine sulla generalizzazione a una ricorsione più profonda.\nAlcuni altri tipi di generalizzazione strutturale rimangono però molto ostici.\nNel nostro articolo, risolviamo un paio di sfide tecniche interessanti.\nPrima di tutto, l'allineamento tra input e output non è dato nei dati di addestramento.\nDi conseguenza, per un dato token non sappiamo da quale multiset provenga, il che rappresenta una sfida per l'addestramento.\nInoltre, a volte ci sono più permutazioni coerenti con i dati, ma quella corretta dal punto di vista linguistico è latente.\nAffrontiamo questo problema inducendo l'allineamento come parte dell'addestramento.\nSebbene sia molto flessibile, la difficoltà del nostro metodo di permutazione risiede nel fatto che trovare la permutazione con il punteggio più alto è NP-difficile.\nQuesto perché è correlato al problema del \"Commesso viaggiatore\".\nLo approssimiamo con un rilassamento continuo compatibile con la GPU che ci permette anche di eseguire la retropropagazione attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili.\nSe volete saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, date uno sguardo al nostro articolo o al nostro poster.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "ICWfTnUMio.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 1, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/JRrbTnEZbF.wav", "src_ref": "Hi, I'm Myra and today I'll be talking about our paper \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.\" This work is done in collaboration with Esin Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However, these measures have various limitations. They usually rely on hand-constructed data sets that are very time-consuming to curate and they also usually only. measure very specific stereotypes, meaning that they don't generalize well to other demographics or contexts, or they simply capture very general broad associations, like negative associations with particular groups. Furthermore, most work in this space doesn't account for intersectionality, which is the notion that multi-faceted social identities can compound biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like \"Imagine you are an Asian woman. Describe yourself.\". And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that, while the outputs aren't overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming; the Middle-Eastern woman is referred to using words like exotic and like, referring to a mesmerizing region. And both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I'll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns, without having to rely on any specific lexicon. So the Marked Words method draws upon the sociolinguistic concept of \"markedness\", which states that there is an unmarked default, and any group that differs from that default is linguistically marked. So for instance, the word \"warrior\" is usually associated with men. So when people are describing a warrior who is a woman, they'll usually actually specify \"woman warrior\" and mark the term with \"woman\". And more broadly, dominant groups in society are both linguistically and socially unmarked, while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the Fightin’ Words method, which is basically using weighted log-odds ratios to distinguish the top words for each marked group. So for instance, for the personas of black women, we would do Fightin’ Words and compare the log-odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words and lexicon, we find very different things. So, while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words \"tall\" and \"athletic\". So, really just only the positive or at least non-negative ones. And in fact, this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we'll turn to the results from our Marked Words method to show how these positive-seeming words facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like \"culture\", \"tradition\", \"proud\", and \"exotic\". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there's a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like \"vibrant\" and \"curvaceous\" which connect to a trope of tropicalism. For Asian women, the words are things like \"petite\" and \"delicate\" and \"silky\" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like \"strong\" and \"resilient\". This connects to an archetype that people have called the \"Strong Black Women\" archetype. And while it sounds positive at first glance, there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don't know if it's because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can't make any assumptions or really study that further, without more transparency. Thank you so much for listening. Have a good time at ACL.", "tgt_ref": "Ciao, sono Myra e oggi parlerò del nostro articolo \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models\".\nQuesto lavoro è stato svolto in collaborazione con Esin Durmus e Dan Jurafsky.\nNegli ultimi anni, molti hanno documentato la prevalenza di bias e stereotipi sociali nei modelli linguistici di grandi dimensioni o LLM.\nVa precisato, tuttavia, che queste misure presentano svariati limiti.\nDi solito si basano su set di dati realizzati manualmente che richiedono molto tempo per essere mantenuti e di solito misurano solo stereotipi molto specifici, il che si traduce nel fatto che non eseguono una generalizzazione appropriata ad altri dati demografici o contesti, o semplicemente catturano associazioni molto generali, come associazioni negative con gruppi particolari.\nInoltre, la maggior parte del lavoro in questo ambiente non tiene conto dell'intersezionalità, che è la nozione secondo cui le identità sociali sfaccettate possono inasprire i bias e costituire punti unici di danno.\nPer superare queste limitazioni, ci affidiamo alla proprietà che questi nuovi LLM, concepiti per aderire alle istruzioni, si dimostrano molto validi nel fornire una risposta a istruzioni e prompt.\nQuindi, possiamo chiedere al modello di riprodurre una personalità, vale a dire una rappresentazione di un individuo immaginario, generando un prompt come \"Immagina di essere una donna asiatica.\nDescrivi te stessa\".\nA questo punto, possiamo subito notare l'elevato grado di generalizzazione a qualsiasi demografica, in quanto ci basta specificare qualsiasi marcatore di identità che desideriamo integrare in questo prompt.\nEcco alcuni esempi di generazioni da parte di GPT-4.\nEmerge subito che, anche se i risultati non sono apertamente negativi o tossici in senso stretto, ci troviamo di fronte ad alcuni schemi interessanti.\nLa donna asiatica è descritta come modesta; la donna mediorientale è definita con parole come \"esotica\" e simili, nel senso di località affascinante.\nEd entrambe le donne di colore fanno riferimento alle loro origini, mentre l'uomo bianco non riporta nulla del genere.\nPer catturare questi modelli, il nostro metodo è suddiviso in due parti.\nLa prima risiede nella generazione di tali personalità.\nI nostri prompt per generare queste personalità sono stati ispirati da uno studio in cui tali prompt sono stati assegnati a soggetti umani, per poi scoprire che, così facendo, sono emersi anche stereotipi razziali.\nInoltre, ciò getta le basi per un confronto diretto tra le nostre personalità generate e le risposte scritte da soggetti umani.\nLa seconda parte è costituita dalle parole marcate, che è un metodo per identificare le parole che distinguono i gruppi marcati da quelli non marcati, che provvederò ad approfondire a breve.\nIl vantaggio risiede nel fatto che otteniamo stereotipi e modelli molto specifici, senza dover fare affidamento su un lessico specifico.\nIn questo modo, il metodo delle parole marcate attinge al concetto sociolinguistico di \"marcatezza\", secondo cui esiste uno standard non marcato e qualsiasi gruppo che differisce da quello standard è marcato sotto il profilo linguistico.\nAd esempio, la parola \"guerriero\" è solitamente associata agli uomini.\nNe deriva che, quando le persone descrivono un guerriero di sesso femminile, di solito specificano la variabile \"guerriera\" e contrassegnano il termine con \"donna\".\nE, più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi emarginati sono generalmente marcati.\nQuindi, nel nostro metodo, designiamo prima quali sono i gruppi non marcati e quelli marcati, quindi confrontiamo le personalità usando il metodo delle fighting words, che fondamentalmente attinge ai rapporti di probabilità logaritmici ponderati per distinguere le parole principali per ciascun gruppo marcato.\nAd esempio, per le personalità delle donne nere, useremmo il metodo delle fighting words e confronteremmo i rapporti di probabilità logaritmici sia con le personalità bianche sia con le personalità maschili, poiché questi sono i due gruppi non marcati.\nOra vediamo alcuni risultati.\nIn primo luogo, ricorriamo a un lessico di stereotipi e scopriamo che le personalità generate contengono molti più stereotipi di quelli scritti dai soggetti umani.\nTuttavia, quando osserviamo effettivamente la distribuzione delle parole e del lessico, ci imbattiamo in esiti assai disparati.\nQuindi, se da un lato le personalità generate presentano percentuali molto più alte di parole del lessico, quelli scritti da soggetti umani rivelano una distribuzione molto più ampia di parole, mentre le parole stereotipate che si trovano nelle personalità generate sono in realtà solo le parole \"alto\" e \"atletico\".\nIn poche parole, solo quelle positive o quantomeno non negative.\nE in effetti, questo lessico non coglie interamente molti degli schemi dannosi che abbiamo visto nelle diapositive precedenti.\nQuindi, per muoverci in tal senso, ci rivolgeremo ai risultati del nostro metodo delle parole marcate per mostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzializzanti.\nNella nostra analisi, riveliamo come tali rappresentazioni apparentemente positive riflettano al contrario schemi dannosi.\nIn primo luogo, le parole principali comprendono nei nostri gruppi concetti quali \"cultura\", \"tradizione\", \"orgoglioso\" ed \"esotico\".\nE queste parole definiscono questi gruppi solo in base al rapporto con la loro identità e li distinguono come diversi dalla norma bianca.\nCiò contribuisce a una lunga tradizione di discriminazione e alterità per questi gruppi.\nInoltre, sono presenti molti tropi che si riflettono in queste parole, specialmente in riferimento alle donne di colore.\nAd esempio, le parole che descrivono le donne latine includono concetti come \"vivace\" e \"prosperosa\", che rimandano allo stereotipo del tropicalismo.\nPer le donne asiatiche, le parole sono \"minuta\", \"delicata\" e \"setosa\", che si collegano a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomesse, e così via.\nInfine, per le donne nere, notiamo che alcune delle parole più ricorrenti sono associate ai concetti di \"forza\" e \"resilienza\".\nCiò si collega a un archetipo che le persone hanno definito \"Donne nere toste\".\nE anche se a prima vista può sembrare positivo, alcuni studi hanno dimostrato che questo tipo di archetipo è in realtà molto dannoso, in quanto mette molta pressione su questi gruppi demografici affinché siano resilienti e forti contro gli ostacoli della società.\nQuindi, anziché operare concretamente per cambiare quegli ostacoli, mette pressione su quelle persone affinché li superino, il che porta, tra gli altri esiti lesivi, a conseguenze dannose per la salute.\nPiù in generale, scopriamo che le parole per ogni gruppo marcato riflettono nella pratica narrazioni decisamente essenzializzanti.\nQuindi, sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari di modelli.\nIn primo luogo, dovremmo, come ricercatori, affrontare gli stereotipi positivi e le narrazioni essenzializzanti.\nInoltre, dovremmo munirci di una lente intersezionale per studiare pregiudizi e danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo.\nInfine, dovrebbe esserci davvero una maggiore trasparenza sui metodi di mitigazione dei pregiudizi, perché, ad esempio, come nel caso di questi stereotipi positivi, non sappiamo se ciò sia dovuto a una sorta di strano ed eccessivo allineamento di valori in corso o, ancora, ad altri metodi anti-stereotipizzazione che stanno provocando questi modelli perniciosi.\nNon possiamo avanzare alcuna ipotesi né fare alcun approfondimento ulteriore senza una maggiore trasparenza.\nGrazie infinite dell'ascolto.\nBuon proseguimento ad ACL.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "JRrbTnEZbF.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 2, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/JhbtCwcsWY.wav", "src_ref": "Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state-of-the-art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analysis of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality, respectively, while the average Likert consistency scores explain only 4% or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses. They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction. And we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.", "tgt_ref": "Ciao, sono James Finch.\nE io sono Sarah Finch.\nOggi vi diremo tutto su ABC-Eval, un nuovo approccio dimensionale per valutare l'IA conversazionale.\nQuesto lavoro è stato svolto dall'Emory NLP Lab guidato dal professor Jinho Choi presso la Emory University e in collaborazione con Amazon Alexa AI.\nDunque, supponiamo che abbiate appena sviluppato un modello di dialogo e vogliate vedere come si confronta con lo stato dell'arte attuale.\nLa pratica comune è quella di utilizzare la valutazione umana, ad esempio chiedendo ai giudici umani di selezionare quale delle due conversazioni è migliore o di valutare le conversazioni in base a una scala Likert.\nQuesti approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo presenta molti aspetti.\nPertanto, l'intenzione potrebbe essere quella di valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più dettagliato.\nUn approccio consiste nella semplice richiesta ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, attraverso metodi comparativi o di scala Likert esistenti.\nTuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale.\nIl nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime o meno determinati comportamenti, come risposte con informazioni irrilevanti o contraddizioni.\nChiamiamo questo approccio annotazione dei comportamenti in chat, ABC-Eval in breve.\nAbbiamo sviluppato questo metodo per coprire in modo completo i comportamenti del modello di chat che, secondo la letteratura recente, influenzano la qualità della chat stessa.\nABC-Eval è in grado di misurare le velocità con cui i modelli di chat commetteranno vari errori tematici.\nAd esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante, contraddice se stesso o il suo partner, immagina fatti errati o viola la conoscenza del buon senso, e quando il modello riesce o non riesce a mostrare empatia.\nPer determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC-Eval.\nPer fare un confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a coppie a livello di dialogo.\nPer ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti del dialogo misurati con maggiore frequenza, poiché questa è la pratica standard per valutare i modelli di chat su più dimensioni.\nDall'analisi di questi risultati di valutazione, abbiamo scoperto che le etichette di comportamento ABC-Eval sono nel complesso più affidabili delle etichette raccolte dai metodi esistenti, come misurato dall'accordo tra annotatori su 100 conversazioni doppiamente etichettate.\nInoltre, le etichette ABC-Eval sono maggiormente predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare.\nAd esempio, si può vedere come la misurazione della proporzione di turni con contraddizioni proprie e del partner spieghi rispettivamente il 5% e il 10% della qualità della conversazione, mentre i punteggi medi di coerenza Likert spiegano solo il 4% o meno.\nInfine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare graduale.\nSi può vedere come la combinazione di tutte le metriche ABC-Eval spieghi oltre il 25% della qualità della conversazione e, man mano che le metriche vengono rimosse una dopo l'altra, la maggior parte di esse comporta la perdita di una discreta quantità di informazioni sulla qualità.\nD'altra parte, la combinazione di tutte le metriche Likert a livello di turno fornisce molte meno spiegazioni sulla qualità e solo una parte esigua di queste metriche include informazioni uniche.\nQueste metriche ABC-Eval affidabili, informative e distinte ci consentono di valutare l'IA conversazionale con una risoluzione più elevata rispetto ai metodi precedenti.\nI risultati del nostro esperimento rivelano che restano ancora diverse sfide che sono state quantificate con precisione.\nAd esempio, i bot che abbiamo testato presentano violazioni del buon senso in circa il 20% delle loro risposte.\nProducono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o contraddicono il loro partner circa il 10% delle volte.\nCon il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero vedere una diminuzione nei nuovi modelli rilasciati da quando è stata condotta la nostra valutazione.\nTuttavia, questo è un motivo in più per perseguire metriche di valutazione affidabili e precise per confrontare i modelli.\nCi auguriamo che ABC-Eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione.\nE non vediamo l'ora di vedere come l'IA conversazionale progredirà nei prossimi mesi e anni.\nGrazie per la visione.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "JhbtCwcsWY.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 3, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/MjDvRpkOFq.wav", "src_ref": "Hello, my name is Vasudha and I'm a Computer Science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.\" We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example where a person states, \"I know that cigarettes could kill me\", and then goes on to say \"I grabbed a couple of smokes after the meeting\". This belief and action are inconsistent, and they are in dissonance. Further mentioning that \"I don't think I could keep my job without them\" justifies the second occurrence. And they have a consonance relationship. While dissonance is a very common phenomenon we experienced in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large scale annotation of dissonance relations. We used dissonance-first approach, as seen in the flow chart here. Tweets were passed using the PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB since these two are closely related to the conception of consonance and dissonance and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance with the best, with AUC .62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. \"Cumulative\" accumulates all the data collected from active annotation so far, whereas \"Iterative\" updates the model by training on the latest set of data collected. Over the different strategies, we found that Cumulative performed equal or better than Iterative across the board. Next, to improve the number of dissonance examples, we use a Probability-of-Rare-Class strategy — PRC — to select mostly the examples that are highly likely to be descended by the current model at any round of rare. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improve dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task and help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.", "tgt_ref": "Salve, mi chiamo Vasudha e faccio il dottorato in Informatica presso la Stony Brook University.\nVorrei presentare il nostro lavoro accettato in ACL 2023 come un lungo articolo, \"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge\".\nIniziamo definendo la dissonanza cognitiva e perché rappresenta un problema importante da studiare nel linguaggio.\nIn poche parole, la dissonanza cognitiva consiste in due convinzioni o azioni incoerenti, come questo esempio in cui una persona afferma: \"So che le sigarette potrebbero uccidermi\", e poi continua dicendo \"Ho fumato un paio di sigarette dopo la riunione\".\nQuesta convinzione e questa azione sono incoerenti e sono in dissonanza.\nL'ulteriore affermazione che \"Non credo di poter mantenere il mio lavoro senza di loro\" giustifica la seconda occorrenza.\nQueste hanno una relazione di consonanza.\nSebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è davvero raro trovarla espressa nel linguaggio tra gli altri tipi di relazioni del discorso.\nMa perché tutta questa importanza?\nStudiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, a monitorare le tendenze e i valori delle credenze e i cambiamenti di atteggiamento nella popolazione.\nL'elevata dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone.\nLo studio della dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili.\nInfine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali e ci aiuta a capire meglio i processi decisionali.\nCon l'obiettivo di creare una risorsa relativa alla dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza.\nAbbiamo utilizzato un approccio basato sulla dissonanza, come è possibile vedere in questo diagramma di flusso.\nI tweet sono stati vagliati utilizzando il parser PDTB e le coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo.\nCome si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate.\nRaccogliendo circa 1.000 esempi di coppie di unità discorsive, abbiamo eseguito l'addestramento per un classificatore iniziale addestrato solo su 43 esempi di dissonanza.\nNon sorprende che il classificatore non abbia funzionato di gran lunga meglio del caso.\nData la scarsa presenza di dissonanza e l'assenza di qualsiasi precedente insieme di dati di questo tipo, ci troviamo di fronte al problema della rarità assoluta.\nPer ovviare a questo problema, sperimentiamo combinazioni di apprendimento per trasferimento e apprendimento attivo per eseguire l'annotazione, in modo tale che più campioni dissonanti possano essere raccolti su serie di annotazioni minori, riducendo i costi complessivi di annotazione e migliorando il rilevamento della dissonanza.\nPoiché il modello iniziale non era affatto in grado di catturare la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da attività strettamente correlate.\nEffettuiamo il trasferimento da due diverse attività: la classificazione della posizione di dissonanza indipendente dall'argomento, un'attività che determina se due affermazioni di dibattito di persone diverse sono d'accordo o in disaccordo, indipendentemente dall'argomento, qui chiamato dibattito, e sulla classificazione binaria delle classi di espansione e confronto di PDTB, poiché questi due sono strettamente correlati alla concezione di consonanza e dissonanza, che qui chiamiamo CE.\nNotiamo che trasferendo le prestazioni zero-shot sul set di dati annotati, queste sono di gran lunga migliori rispetto al caso con il migliore, con AUC .62.\nInoltre, con il fine-tuning iterativo su entrambi i compiti, scopriamo che il fine-tuning dei compiti CE seguita da un'ulteriore messa a punto sul dibattito produce una prestazione zero-shot assai migliore.\nQuindi, questo è il modello che utilizziamo per avviare a freddo l'apprendimento attivo.\nSuccessivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ogni ciclo di apprendimento attivo e annotazioni.\n\"Cumulative\" accumula tutti i dati raccolti dall'annotazione attiva fino a quel momento, mentre \"Iterative\" aggiorna il modello eseguendo l'addestramento sull'ultimo set di dati raccolti.\nTra le diverse strategie, abbiamo riscontrato che Cumulative ha avuto un rendimento uguale o migliore su tutta la linea rispetto a Iterative.\nSuccessivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia PRC (Probability-of-Rare-Class) per selezionare principalmente gli esempi che hanno un'alta probabilità di discendere dal modello attuale in qualsiasi round di rarità.\nConfrontiamo ciò con le altre strategie AL all'avanguardia comunemente usate nella comunità.\nRiteniamo che la strategia PRC proposta funzioni meglio di altre strategie all'avanguardia, sebbene la differenza sia esigua.\nVa notato che le prestazioni sono significativamente inferiori con riferimento al caso.\nIn ulteriori round di AL con le due migliori strategie, miglioriamo l'AUC di classificazione della dissonanza a 0,75, che è la migliore prestazione che abbiamo riscontrato finora sull'attività.\nVerifichiamo anche la fattibilità di ciascuna strategia per la qualità delle annotazioni e i costi per gli annotatori.\nRiteniamo che PRC abbia la più alta percentuale di dissonanza e funzioni meglio per la classe rara.\nTuttavia, anche gli annotatori trovano difficili gli esempi.\nIn sintesi, riteniamo che PRC sia una semplice strategia di AL per l'acquisizione di classi rare e l'avvio a freddo di AL con un'attività di apprendimento per trasferimento opportunamente progettata e un aiuto significativo.\nInoltre, crediamo che l'aggiornamento iterativo è utile per l'apprendimento per trasferimento da un dominio diverso, mentre le annotazioni attive nel dominio beneficiano dell'aggiornamento cumulativo.\nQuesti sono i link al nostro set di dati di base e al nostro articolo.\nNon esitate a contattarci in caso di domande.\nGrazie.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "MjDvRpkOFq.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 4, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/MmiKtcykVs.wav", "src_ref": "Hello everyone, I'm Akshatha, and today my co-author Martin and I are presenting our work \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.\" This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, \"John saw the newly elected president on TV.\" Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity \"John\" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun \"he\" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as \"Servin is a judge.\" And second, background knowledge such as \"Judges decide cases in law courts.\" Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: \"Background-Pretrain\", where background knowledge is assumed to be available at pretrain time. Second, there's a \"Background-Both\" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the \"Background-Inference\" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here's an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge \"Politicians seek elected seats in government\" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge \"Chichester is a politician.\" In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation \"mirituer\" instead of politician because \"mirituer\" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.", "tgt_ref": "Ciao a tutti, sono Akshatha, e oggi io e il mio coautore Martin presentiamo il nostro lavoro \"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources\".\nQuesto lavoro è una collaborazione tra McGill University/Mila e Microsoft Research.\nI modelli di comprensione del linguaggio naturale attingono a una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, di solito acquisita da un pre-addestramento, e la conoscenza fornita negli input al momento dell'inferenza.\nLavori recenti in attività come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza del tempo pre-addestrato per risolvere l'attività.\nMa la comprensione del linguaggio naturale richiede spesso conoscenze che vengono fornite anche al momento dell'inferenza.\nAd esempio, nella frase \"John ha visto il presidente appena eletto in TV\".\nI parametri pre-addestrati possono contenere informazioni su ciò che fanno i presidenti e su cosa sia una TV, ma non possono sapere in modo affidabile chi sia questa entità specifica dell'istanza \"John\", o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato dopo il pre-addestramento.\nPertanto, i modelli di successo per le attività di NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-allenamento che quella al momento dell'inferenza.\nIn questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza.\nIntroduciamo un'attività di risoluzione della coreferenza, progettata per sondare la capacità di attingere alle conoscenze disponibili in diverse fonti.\nValutiamo il set di dati con partecipanti allo studio umano e modelli di risoluzione della coreferenza stabiliti.\nEcco un esempio dal nostro set di dati.\nServin è un giudice.\nKea è un fornaio.\nServin e Kea si sono incontrati in un parco.\nLui era felice di rilassarsi dopo una lunga giornata di lavoro trascorsa a prendere decisioni sui casi in tribunale.\nIl compito qui è identificare l'entità corretta a cui si riferisce il pronome \"lui\", che in questo caso è Servin.\nLa risoluzione di un dato pronome richiede due tipi di informazioni.\nIn primo luogo, la conoscenza specifica dell'entità, ad esempio \"Servin è un giudice\".\nE, in secondo luogo, conoscenze di base come \"I giudici emettono sentenze sui casi in tribunale\".\nIn generale, la conoscenza di base viene appresa durante il pre-addestramento di grandi modelli linguistici, mentre la conoscenza specifica dell'entità viene di norma osservata al momento dell'inferenza.\nVariamo la disponibilità di queste due informazioni in modo che possano essere trovate in un'unica fonte o in più fonti.\nAbbiamo definito tre impostazioni di KITMUS.\nIn primo luogo, abbiamo l'impostazione tipica: \"Background-Pretrain\", in cui si presume che la conoscenza di base sia disponibile al momento del pre-addestramento.\nIn secondo luogo, c'è un'impostazione \"Background-Both\", in cui la conoscenza di base è disponibile sia al momento del pre-addestramento che al momento dell'inferenza.\nInfine, l'impostazione \"Background-Inference\", in cui entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza.\nQuest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati di pre-addestramento dei modelli.\nAd esempio, perché dal momento del pre-addestramento si sono sviluppate nuove occupazioni.\nEcco un esempio di come controlliamo la disponibilità dei fatti nelle fonti vere.\nNell'impostazione Background-Pretrain, si presume che la conoscenza di base \"I politici cercano seggi elettivi nel governo\" sia contenuta nei parametri pre-addestrati e nel contesto del tempo di inferenza si fornisce la conoscenza specifica dell'entità \"Chichester è un politico\".\nNell'impostazione Background-Both, forniamo inoltre non solo conoscenze specifiche dell'entità, ma anche conoscenze di base sui politici nel loro contesto di inferenza.\nNell'impostazione Background-Inference, forniamo l'occupazione fittizia \"mirituer\" invece di politico perché è improbabile che \"mirituer\" sia contenuto nei parametri pre-addestrati.\nValutiamo il set di dati sia con i partecipanti allo studio umano sia con i modelli di risoluzione della coreferenza stabiliti.\nIn questa figura, mostriamo i risultati dei modelli con le migliori prestazioni sulla variante più complessa dell'impostazione Background-Pretrain.\nSenza un addestramento specifico per l'attività su KITMUS, entrambi i modelli non operano adeguatamente.\nQuando addestrati su KITMUS, tuttavia, sia C2F che BERT4Coref funzionano notevolmente meglio della scelta casuale.\nCiò suggerisce che, quando vengono addestrati su set di dati di risoluzione di riferimento generici, la maggior parte impara a sfruttare gli spunti superficiali che non sono utili quando si eseguono test su KITMUS, dove tali code sono state rimosse.\nUlteriori esperimenti con conoscenza fittizia hanno indicato che anche i modelli con le migliori prestazioni non possono integrare in modo affidabile la conoscenza retrospettiva fornita solo al momento dell'inferenza.\nPer riassumere i principali punti del nostro articolo, molti modelli di risoluzione della coreferenza sembrano incapaci di ragionare sulla conoscenza proveniente da fonti diverse senza un addestramento specifico per l'attività.\nTuttavia, con un addestramento specifico per l'attività, alcuni modelli integrano con successo le conoscenze provenienti da più fonti.\nTuttavia, anche i modelli con le migliori prestazioni sembrano avere difficoltà a integrare in modo affidabile la conoscenza retrospettiva presentata solo al momento dell'inferenza.\nSe siete interessati a maggiori dettagli, consultate il nostro articolo e date un'occhiata al set di dati e al codice su GitHub.\nGrazie per l'attenzione.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "MmiKtcykVs.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 5, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/PJWMkwXVGI.wav", "src_ref": "Hi, I'm Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the \"Attention as a Guide for Simultaneous Speech Translation\" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing \"I'm going to talk about...\" and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we'll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model's computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.", "tgt_ref": "Ciao, sono Sara Papi dell'Università di Trento e della Fondazione Bruno Kessler e vi presenterò brevemente il paper \"Attention as a Guide for Simultaneous Speech Translation\", frutto di un lavoro congiunto con Matteo Negri e Marco Turchi.\nChe cos'è la traduzione simultanea vocale?\nLa traduzione simultanea vocale, o SimulST, è il processo di traduzione in tempo reale della lingua parlata in un testo in un'altra lingua, che consente la comunicazione tra lingue.\nE quali sono i problemi degli attuali modelli SimulST?\nDi solito vengono addestrate architetture specifiche che introducono moduli aggiuntivi da ottimizzare.\nProcedure di addestramento lunghe e complicate, ad esempio un addestramento che coinvolge diversi obiettivi di ottimizzazione.\nO ancora, l'addestramento e il mantenimento di diversi modelli per raggiungere diversi regimi di latenza.\nAd esempio, l'addestramento di un modello con una latenza media di un secondo e un altro con una latenza di due secondi, e così via.\nQual è la nostra soluzione?\nIn primo luogo, utilizzare modelli ST offline già esistenti senza addestrare nuovamente né adottare un'architettura specifica per SimulST.\nUtilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici.\nE sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra input audio e output testuale.\nQuesto è il meccanismo di cross-attention: potete vederne un esempio sulla destra.\nLa nostra soluzione è proporre EDAtt, o Encoder-Decoder Attention, una strategia mediante la quale possiamo decidere se emettere o meno una traduzione parziale in base a dove punta l'attenzione.\nUna parola viene emessa se l'attenzione non è concentrata, vale a dire che la sua somma è al di sotto di una certa soglia alfa verso gli ultimi segmenti vocali lambda, il che si traduce nel fatto che l'informazione ricevuta è piuttosto stabile.\nAd esempio, se riceviamo un frammento di discorso contenente \"Parlerò di...\" e il nostro modello prevede la traduzione in tedesco, osservando i pesi dell'attenzione incrociata, vedremo che le prime due parole puntano ai primi segmenti vocali ricevuti, mentre l'ultima parola punta agli ultimi segmenti vocali ricevuti, come segmenti vocali lambda.\nCiò significa che le prime due parole verranno emesse mentre, al contempo, dal momento che la somma dell'attenzione incrociata si trova al di sopra di una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro frammento vocale.\nSe, proseguendo, riceviamo un altro frammento vocale, e il nostro modello prevede altre tre parole, osservando quei pesi di attenzione incrociata, vedremo che nessuna parola punta agli ultimi segmenti vocali lambda.\nIl risultato di tutto ciò è che queste tre parole verranno emesse.\nSe guardiamo i principali risultati di EDAtt, rappresenteremo i risultati della traduzione simultanea vocale su grafici in cui da un lato abbiamo BLEU, che misura la qualità della traduzione, e il ritardo medio, che è la misura della latenza. Inoltre, prendiamo in considerazione il ritardo medio computazionale che tiene conto dei tempi di calcolo del modello per prevedere l'output.\nA tal fine, desideriamo che le nostre curve su questo grafico siano quanto più alte possibili.\nAllo stesso tempo, intendiamo spostarle a sinistra.\nProseguendo, confrontiamo il tutto con strategie note che vengono applicate anche a modelli offline, vale a dire la strategia Wait-k e il Local Agreement.\nIl confronto si estende all'architettura all'avanguardia, appositamente studiata per la pre-traduzione simultanea.\nQuesti sono tutti i risultati della strategia di traduzione simultanea vocale in tedesco.\nPossiamo notare che supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate a sinistra.\nE vediamo anche che se consideriamo il tempo effettivo trascorso o il tempo di calcolo, questa è la strategia più veloce.\nSe volete conoscere gli altri risultati, vi invitiamo a leggere il nostro articolo.\nAbbiamo anche rilasciato in open source il codice, i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro.\nGrazie per l'attenzione.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "PJWMkwXVGI.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 6, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/QTlIuodOsA.wav", "src_ref": "Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.", "tgt_ref": "Ciao a tutti, mi chiamo Shuheng.\nOggi presenterò il nostro articolo, \"Do CoNLL-2003 named entity taggers still work well in 2023?\"\nCominciamo.\nIl nostro articolo è incentrato sul problema della generalizzazione utilizzando l'attività Named Entity Recognition (Riconoscimento delle entità nominate) o attività NER.\nOsserviamo che i modelli sono stati utilizzati in CoNLL-2003 per sviluppare NER per quasi 20 anni e questo solleva naturalmente diversi problemi.\nIn primo luogo, è possibile generalizzare questi modelli ai dati moderni?\nE quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?\nAllo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli?\nPer analizzare questi problemi, abbiamo sviluppato il set di dati CoNLL++.\nSi tratta di un set di dati che abbiamo raccolto da Reuters News del 2020, successivamente annotato con le stesse linee guida di annotazione di CoNLL-2003.\nAbbiamo quindi adattato oltre 20 modelli su CoNLL-2003.\nLi abbiamo valutati sia sui set di test CoNLL-03 che su CoNLL++.\nUltimo ma non meno importante, abbiamo calcolato la variazione percentuale in F1 per valutare la generalizzazione di ciascun modello.\nQuindi, cosa serve per una buona generalizzazione?\nDurante gli esperimenti, abbiamo scoperto che sono presenti tre ingredienti principali da ritenersi necessari.\nIl primo è l'architettura del modello.\nGrazie ai nostri esperimenti, abbiamo scoperto che i modelli trasformatori normalmente generalizzano meglio i nuovi dati.\nIl secondo ingrediente è la dimensione del modello.\nAbbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione.\nE, ultimo ma non meno importante, sappiamo tutti che il numero di esempi di messa a punto influisce direttamente sulle prestazioni di un'attività a valle.\nIn questa circostanza abbiamo scoperto anche che più esempi di messa a punto portano in realtà a una migliore generalizzazione.\nPer la domanda successiva, vale a dire cosa provoca il calo delle prestazioni di alcuni modelli, avevamo due ipotesi.\nLa prima è l'overfitting adattivo, che è il costo del sovradattamento riutilizzando lo stesso set di test più e più volte, e questo di solito si manifesta come rendimenti decrescenti su un nuovo set di test.\nLa seconda ipotesi è rappresentata dalla deriva temporale, che descrive il degrado delle prestazioni causato dal crescente divario temporale tra i dati di addestramento e quelli di test.\nPer il sovradattamento dei dati, abbiamo visto che dal grafico sulla destra, la linea rossa di migliore adattamento presenta un gradiente maggiore di uno.\nCiò significa che ogni unità di miglioramento che abbiamo apportato, su CoNLL-2003 si traduce in un miglioramento di più di un'unità su CoNLL++, il che significa che non ci sono rendimenti decrescenti.\nE questo ci mostra che in questo caso non si osserva l'adattamento eccessivo.\nE allora che dire della deriva temporale?\nPer la deriva temporale, abbiamo eseguito un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni si degradano con un intervallo temporale più ampio: questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni sia dovuto alla deriva temporale.\nLa nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, di dimensioni del modello maggiori, nonché di più esempi di messa a punto.\nQuesti vanno di pari passo: non possiamo avere un solo ingrediente e buttare via gli altri.\nAllo stesso tempo, abbiamo scoperto che il calo delle prestazioni è causato in questo caso dalla deriva temporale e, sorprendentemente, non è causato da alcun overfitting adattivo, anche se CoNLL-2003 è stato impiegato per oltre 20 anni.\nTornando alla domanda che abbiamo sollevato nel titolo del nostro articolo: Do CoNLL-2003 taggers still work in 2023?\nAbbiamo scoperto che la risposta è in realtà un sonoro sì.\nCi auguriamo che il nostro articolo inviti a ulteriori ricerche su come migliorare le generalizzazioni dei modelli.\nInfine, assicuratevi di consultare il nostro articolo e il nostro set di dati. In caso di domande, non esitate a contattarmi.\nGrazie mille.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "QTlIuodOsA.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 7, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/UOlPKyCVgg.wav", "src_ref": "Hi! Welcome to our presentation of DEPLAIN, a new corpus for German text identification on the document level, and on the sentence level. My name is Regina Stodden, and I will guide you through the first part of the presentation. Let's first define text simplification. Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To train a text simplification model we require parallel pairs of text, for example of documents or sentences. And the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different techniques are possible as you can see in the example, such as lexical substitution, clause deletion, reordering, or insertion of words. We now propose our new corpus, DEPLAIN because in the recent years, there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on. The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone in their alignments. Therefore, we propose our new corpus DEPLAIN, which is split into two subcorpora: DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa is based on news texts. In DEPLAIN-apa, we aligned 483 documents all manually. It results in roughly 13,000 parallel sentence pairs. For DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods. In total we result in 30,450 sentence pairs. We analyzed our sentence pairs a little bit more, so for example, on the type of simplification. As you can see here, the Bible texts are much, stronger simplified than for example the news text, or the language learner texts. On all levels, regarding for example lexical simplification, structure simplification, also overall level of simplification. Furthermore, you can see that our DEPLAIN corpus has a high variety of different simplification transformations. So for example, in the DEPLAIN-apa corpus we have much more reorderings and word additions than we have in the DEPLAIN-web corpus. On the other hand, in the web corpus we have much more rephrasings. So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set DEPLAIN. So for the first use case, we can evaluate automatic alignment methods. In the recent years, there has been a lot of alignment methods, but in the context of machine translations, where we have two parallel documents written in different languages and we want to extract alignments of sentences in both documents. But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level. And now as we have our data set DEPLAIN, which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods. And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper. At the end, we concluded that the best automatic alignment method to use for German text simplification is the method of MASSalign. And you can also find the code to run this method on your own documents in the paper. The second use case that we showed in our paper is a case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text. We have fine-tuned two different models. We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications. You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores, and we proposed those results as a base benchmark for the problem of automatic text simplification in the future. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.", "tgt_ref": "Ciao!\nBenvenuti alla nostra presentazione di DEPLAIN, un nuovo corpus per l'identificazione del testo tedesco a livello di documento e a livello di frase.\nMi chiamo Regina Stodden e vi accompagnerò nella prima parte della presentazione.\nDefiniamo innanzitutto la semplificazione del testo.\nLa semplificazione del testo è un processo di adattamento di un testo per migliorarne la comprensione per un gruppo target specifico, come persone con problemi di lettura o non madrelingua.\nPer addestrare un modello di semplificazione del testo abbiamo bisogno di coppie parallele di testo, ad esempio di documenti o frasi.\nE nell'esempio riportato qui, potete notare una coppia di frasi allineate parallele di una frase tedesca complessa e la sua traduzione in un linguaggio semplice.\nPer semplificare la frase, sono possibili diverse tecniche, come si può vedere nell'esempio, come la sostituzione lessicale, la cancellazione delle clausole, il riordinamento o l'inserimento di parole.\nOra proponiamo il nostro nuovo corpus, DEPLAIN, in quanto negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti.\nAd esempio, questi corpora qui sono troppo piccoli per addestrare un modello di semplificazione del testo.\nGli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nei loro allineamenti.\nPertanto, proponiamo il nostro nuovo corpus DEPLAIN, che è diviso in due subcorpora: DEPLAIN-apa e DEPLAIN-web.\nDEPLAIN-apa si basa su testi di notizie.\nIn DEPLAIN-apa, abbiamo allineato manualmente 483 documenti.\nIl risultato è di circa 13.000 coppie di frasi parallele.\nPer quanto riguarda DEPLAIN-web, questo corpus include diversi domini. Allineiamo anche tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico.\nIn totale otteniamo 30.450 coppie di frasi.\nAbbiamo analizzato un po' a fondo le nostre coppie di frasi, ad esempio sul tipo di semplificazione.\nCome potete vedere qui, i testi biblici sono molto più semplificati rispetto, ad esempio, al testo delle notizie o ai testi per studenti di lingue.\nA tutti i livelli, per quanto riguarda ad esempio la semplificazione lessicale, la semplificazione strutturale e anche il livello generale di semplificazione.\nInoltre, potete vedere che il nostro corpus DEPLAIN presenta un'elevata varietà di diverse trasformazioni di semplificazione.\nAd esempio, nel corpus DEPLAIN-apa abbiamo molti più riordinamenti e aggiunte di parole di quelli che abbiamo nel corpus DEPLAIN-web.\nD'altra parte, nel corpus web abbiamo molte più riformulazioni.\nVediamo ora cosa possiamo fare con questo corpus.\nSalve, sono Omar e ora parlerò dei casi d'uso per il nostro set di dati DEPLAIN.\nPer il primo caso d'uso, possiamo valutare i metodi di allineamento automatico.\nNegli ultimi anni sono stati realizzati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, abbiamo due documenti paralleli scritti in lingue diverse e intendiamo estrarre allineamenti di frasi in entrambi i documenti.\nMa nel nostro caso d'uso, stiamo cercando di estrarre gli allineamenti tra le frasi di due documenti paralleli che hanno la stessa lingua, lo stesso contenuto, ma si trovano a un diverso livello di complessità.\nE ora che abbiamo il nostro set di dati DEPLAIN con frasi allineate manualmente, possiamo usare queste frasi come allineamenti gold standard per valutare alcuni dei metodi di allineamento proposti.\nAbbiamo apportato alcuni adattamenti ai metodi proposti e abbiamo pubblicato nel documento tutti questi adattamenti e i codici per eseguire i nostri esperimenti.\nInfine, siamo giunti alla conclusione che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo tedesco è il metodo MASSalign.\nE sui vostri documenti nell'articolo potete anche trovare il codice per eseguire questo metodo.\nIl secondo caso d'uso che abbiamo mostrato nel nostro articolo è un caso di semplificazione automatica del testo mediante il fine-tuning di modelli linguistici per produrre testo semplificato dal testo di input complesso.\nAbbiamo messo a punto due diversi modelli.\nAbbiamo messo a punto il modello long-mBART per produrre semplificazioni a livello di documento e il modello standard mBART per generare semplificazioni a livello di frase.\nNell'articolo potete anche trovare tutti i punti di controllo e approfondire i punteggi e le metriche di valutazione dei nostri esperimenti.\nAbbiamo concluso che questo fine-tuning semplice potrebbe produrre o ottenere punteggi migliori dei punteggi di base, e abbiamo proposto questi risultati come parametro di riferimento di base per il problema della semplificazione automatica del testo.\nGrazie mille per l'attenzione e speriamo di incontrarvi tutti durante la conferenza.\nGrazie.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "UOlPKyCVgg.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 8, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/crgYiwKDfX.wav", "src_ref": "Hi, I'm Siyu Yuan from Fudan University. I'm here to introduce our work \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\". In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as \"make a cake\". And show that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as \"make a chocolate cake\", still remains under-studied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goals of planning. An abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why learning models fail. Results in the figure show that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into a more fine-grained topic categories of constraints defined in wikiHow. The heat map in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories. Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it's essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We appy our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate constrained language planning ability of large language models and develop an over-generate-then-filter method for large language models. We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope the CoScript dataset can be a valuable resource to advance research on language planning. Thanks for your time. Please find more details of CoScript in our paper.", "tgt_ref": "Ciao, sono Siyu Yuan della Fudan University.\nSono qui per presentare il nostro lavoro \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\".\nNella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo sotto forma di script orientati all'obiettivo.\nIl lavoro precedente ha fatto ricorso ai modelli linguistici per pianificare obiettivi astratti di attività stereotipate come \"fare una torta\".\nE dimostrare che i grandi modelli linguistici possono efficacemente scomporre gli obiettivi in passaggi.\nTuttavia, il lavoro precedente si concentra principalmente sulla pianificazione degli obiettivi astratti delle attività stereotipate.\nLa pianificazione degli obiettivi con vincoli specifici, come \"fare una torta al cioccolato\", rimane ancora poco studiata.\nIn questo articolo, definiamo il problema della pianificazione linguistica vincolata che impone diverse limitazioni agli obiettivi della pianificazione.\nUn obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multiformi.\nUn buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli.\nIn questo articolo, valutiamo e miglioriamo la capacità di pianificazione linguistica vincolata di grandi modelli linguistici.\nDal momento che non esiste alcun set di dati di obiettivi specifici per supportare il nostro studio, dobbiamo anzitutto acquisire questi obiettivi.\nCome mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multiformi per l'acquisizione di dati umani nel ciclo utilizzando InstructGPT.\nCampioniamo 100 obiettivi specifici e valutiamo gli script generati da grandi modelli linguistici.\nQuesta tabella riporta l'accuratezza complessiva dei risultati.\nRiteniamo che tutti i modelli linguistici ottengano risultati insoddisfacenti nella pianificazione di obiettivi specifici.\nQuindi, conduciamo un'analisi dettagliata per esaminare la ragione per cui i modelli di apprendimento falliscono.\nI risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita.\nApprofondiamo una categoria di vincoli più dettagliata definita in wikiHow.\nLa heatmap nella figura mostra che le prestazioni di pianificazione di InstructGPT variano considerevolmente in funzione degli obiettivi di diverse categorie.\nStudi precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici mostra un'elevata varianza, portando a prestazioni scadenti.\nPertanto, adottiamo l'idea di sovra-generare e poi filtrare al fine di migliorare la qualità della generazione.\nMostriamo innanzitutto i tipi di vincolo con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti di partenza.\nQuindi, InstructGPT sovra-genera script K per obiettivi specifici.\nSuccessivamente, viene sviluppato un modello di filtro per selezionare gli script fedeli.\nConvertiamo script e obiettivi in incorporazioni InstructGPT e calcoliamo la similarità del coseno come punteggi di similarità per misurare la somiglianza semantica.\nInoltre, premiamo lo script che contiene le parole chiave del vincolo di destinazione.\nManteniamo lo script solo se l'obiettivo target ottiene il punteggio più alto nell'insieme di obiettivi.\nCon il nostro metodo, InstructGPT può generare script di qualità superiore.\nIl nostro metodo migliora notevolmente la capacità di pianificazione sia nella completezza semantica che nella fedeltà al vincolo.\nPoiché i grandi modelli linguistici sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati.\nLa creazione del set di dati è un passaggio essenziale a tal fine.\nTuttavia, gli studi precedenti non consentono la pianificazione per obiettivi specifici, mentre l'annotazione manuale del set di dati è costosa.\nPertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare set di dati di pianificazione del linguaggio vincolati da grandi modelli linguistici.\nApplichiamo il nostro metodo per costruire un set di dati di pianificazione linguistica vincolata, denominato CoScript.\nIn totale, con gli script generiamo 55.000 obiettivi specifici.\nPer garantire la qualità del set di convalida e test, chiediamo ai collaboratori in crowdsourcing di trovare e rivedere i campioni errati.\nQuesta figura mostra la distribuzione dei vincoli di CoScript.\nRiteniamo che CoScript mostri un alto pluralismo negli obiettivi specifici generati.\nCon CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata.\nScopriamo che T5 messo a punto su CoScript può generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che i modelli più piccoli possono superare i modelli più grandi se opportunamente addestrati su set di dati adeguati.\nIn sintesi, stabiliamo il problema della pianificazione linguistica vincolata.\nValutiamo la capacità di pianificazione linguistica vincolata di grandi modelli linguistici e sviluppiamo un metodo over-generate-then-filter per grandi modelli linguistici.\nUtilizziamo modelli linguistici di grandi dimensioni per generare un set di dati di script di alta qualità, CoScript, per la pianificazione linguistica vincolata.\nCi auguriamo che il set di dati CoScript possa essere una risorsa preziosa per far progredire la ricerca sulla pianificazione linguistica.\nGrazie per il tempo che mi avete dedicato.\nTroverete maggiori dettagli su CoScript nel nostro articolo.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "crgYiwKDfX.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 9, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/csJIsDTYMW.wav", "src_ref": "Hi, I am Yanis Labrak and I will present you our works on \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains.\" In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. We also introduced a comparison of models with multiple pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French. And finally, we conclude about the experiments and give you more details about how to access those models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn't have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. We are also observing that more specialized data is better, but it doesn't scale well. All the pre-trained model obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on our GitHub repository. So thank you for this presentation, and we are looking forward to exchange at the poster session in Toronto.", "tgt_ref": "Ciao, sono Yanis Labrak e vi presenterò i nostri lavori relativi a \"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains\".\nIn questa presentazione, parleremo innanzitutto di modellazione del linguaggio in ambito sanitario.\nDopodiché presenteremo il contributo principale del nostro articolo.\nIntroduciamo il primo modello biomedico in francese chiamato DrBERT, che si basa su RoBERTa ed è addestrato su NACHOS, un set di dati medici estratti dal web.\nAbbiamo inoltre introdotto un confronto di modelli con più impostazioni di pre-addestramento e fonti di dati.\nQuindi, presentiamo i nostri risultati su 11 attività biomediche e cliniche a valle in francese.\nInfine, chiudiamo sugli esperimenti e forniamo maggiori dettagli su come accedere a questi modelli.\nDal suo lancio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere le attività di Natural Language Processing e offre enormi vantaggi in termini di prestazioni rispetto ai metodi statici e contestualizzati storici come Word2vec, fastText o altri.\nDa allora, questo modello è stato adattato a molte altre lingue, come in francese con CamemBERT, e anche in settori come quello biomedico con PubMedBERT e BioBERT e in quello clinico con ClinicalBERT, ma soprattutto in inglese.\nI modelli specializzati per altre lingue sono scarsi e spesso si basano su una pre-formazione continua a causa della mancanza di dati nel dominio.\nTuttavia, non era presente alcun modello open source in francese per il settore biomedico.\nQuindi, ci siamo chiesti quali siano le fonti di dati più appropriate per un'ampia gamma di utilizzi e se i dati raccolti siano una buona alternativa ai dati clinici.\nPer rispondere a questa domanda, confrontiamo DrBERT con il nostro modello ChuBERT, che si basa su dati anonimi ottenuti dal data warehouse dell'Ospedale Universitario di Nantes.\nSuccessivamente, ci siamo chiesti di quanti dati abbiamo bisogno per addestrare un modello specializzato sui dati francesi.\nSono 4 gigabyte, 8 gigabyte o più?\nPer rispondere a questa domanda, innanzitutto addestriamo e confrontiamo quattro modelli da zero: una prima versione di DrBERT, con 7 GB di NACHOS, una seconda versione di 4 GB di set di NACHOS, una prima versione di ChuBERT, che è un modello clinico con 4 GB di frasi tratte da note cliniche e una versione finale di ChuBERT con un mix di 4 GB di set di NACHOS e 4 GB di note cliniche.\nOltre a questo confronto, abbiamo introdotto tre modelli addestrati sul pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento.\nUno basato sul peso di CamemBERT e addestrato su un set di 4 GB di NACHOS.\nUn altro basato sempre su CamemBERT, ma addestrato questa volta sui 4 GB di note cliniche e, infine, uno basato sul modello biomedico inglese PubMedBERT e addestrato su 4 GB di set di NACHOS.\nIn totale, abbiamo sette modelli.\nPer valutare i nostri sette modelli, raccogliamo dati per attività a valle pubbliche e private come riconoscimento di entità nominate, classificazione, etichettatura delle parti del discorso e risposta alle domande.\nQuesti modelli vengono confrontati con sei modelli di base che sono CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT.\nLa valutazione evidenzia che i modelli hanno ottenuto i migliori risultati sul compito con dati della stessa natura di quelli su cui è stato addestrato il modello.\nTuttavia, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili.\nNotiamo inoltre che il ricorso a più dati si traduce in prestazioni migliori.\nNel complesso, il pre-addestramento da zero sembra ottenere prestazioni più elevate nella maggior parte delle attività.\nTuttavia, il nostro esperimento sul pre-addestramento di controllo utilizzando il peso e la tokenizzazione di CamemBERT addestrato sul sottoinsieme di quattro GB di NACHOS ha mostrato risultati paragonabili a quelli ottenuti con DrBERT 4 GB da zero.\nQuesto non è il caso del modello basato su pesi e tokenizzatore CamemBERT, che soffre di problemi di stabilità.\nInfine, il nostro sistema ha offerto prestazioni migliori su nove delle 11 attività a valle e ha superato globalmente il risultato del modello generico, qui CamemBERT.\nStiamo anche osservando che dati più specializzati sono migliori, ma non scalano bene.\nTutti i modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face e sotto la licenza MIT, mentre gli script di addestramento si trovano sul nostro repository GitHub.\nGrazie per questa presentazione, e non vediamo l'ora di confrontarci alla sessione poster a Toronto.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "csJIsDTYMW.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 10, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/eXmqPhcZFN.wav", "src_ref": "Hi, I'm Shangbin, PhD student in the University of Washington. Today I'm presenting our work \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\". So language models are trained on large scale web crawl data. Political news media are well covered in their pretraining data. According to a survey of the C4 Corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks, specifically by asking the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? So specifically, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political conference test. This ensures us to do automatic evaluation well grounded in political science literature. So some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political campus. We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BART series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So we could conduct a controlled experiment by further pretraining language model checkpoints on 6 different partisan corpora separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its political biases. And we also try to investigate whether language models can pick up the polarisation that's prevalent in our modern society. So we divide pretraining corpora, into pre 45th president of the United States and after 45th president of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the centre after 2017. So this indicates that language models can also pick up the polarisation in our society. So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see that if we investigate the per category performance, that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern. For example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups, however are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men, however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. We further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that, people with opposite political opinions might be marginalised and hate speech targeting minority groups might just run rampant without any control. So this has sound the alarm for us to acknowledge and tackle the fairness issues resulting by language model political leanings. So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It's like between Scylla and Charybdis. So if we do not sanitize political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitaze somehow, we would also risk censorship, or exclusion. And it's incredibly hard to determine what is actually neutral and should be retaining language monitoring data. So it's kind of like the electric trolley problem. Ok, great. I think that's pretty much all I have for today. Thank you for your time.", "tgt_ref": "Ciao, sono Shangbin, studente di dottorato presso l'Università di Washington.\nOggi presento il nostro lavoro \"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\".\nI modelli linguistici sono addestrati su dati di scansione web su larga scala.\nI mezzi di informazione politici sono ben rappresentati nei rispettivi dati di pre-addestramento.\nSecondo un sondaggio del C4 Corpus, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben rappresentati nei dati di addestramento del modello linguistico.\nCiò ha creato un vantaggio misto per le applicazioni del modello linguistico.\nDa un lato, hanno dimostrato la capacità di apprendere da diverse prospettive, il che onora la democrazia e la pluralità di idee.\nDall'altro, queste diverse opinioni politiche sono intrinsecamente distorte dal punto di vista sociale e potrebbero condurre a potenziali problemi di equità nelle applicazioni di attività a valle.\nA tal fine, proponiamo di indagare l'infrastruttura di propagazione dei bias politici dai dati di pre-addestramento ai modelli linguistici, fino alle attività a valle, ponendo in particolare le seguenti domande: in primo luogo, come valutiamo l'inclinazione politica dei modelli linguistici e quale ruolo potrebbero avere i dati di pre-addestramento su tali bias politici?\nIn secondo luogo, come si comportano effettivamente i modelli linguistici con diverse inclinazioni politiche nelle attività a valle e se ciò potrebbe comportare problemi di equità nelle applicazioni NLP?\nIn particolare, abbiamo proposto di sollecitare i modelli linguistici con diversi formati di prompt utilizzando i questionari politici come il test della conferenza politica.\nQuesto ci permette di effettuare una valutazione automatica ben fondata nella letteratura delle scienze politiche.\nAlcuni risultati preliminari dimostrano che, in primo luogo, i modelli linguistici hanno diverse inclinazioni politiche.\nOccupano tutti e quattro i quadranti del campus politico.\nPossiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti, e la serie GPT è generalmente più liberale dal punto di vista sociale rispetto alla serie BART e alle sue varianti.\nIn secondo luogo, miriamo a indagare fino a che punto i pregiudizi politici dei modelli linguistici vengono effettivamente raccolti dai dati di addestramento.\nQuindi, potremmo condurre un esperimento controllato pre-addestrando ulteriormente i checkpoint del modello linguistico su 6 diversi corpora di parte suddivisi in notizie e social media, a loro volta suddivisi in base alla loro inclinazione politica.\nPre-addestrando ulteriormente i modelli linguistici su tali corpora di parte, possiamo vedere che, di conseguenza, anche le coordinate ideologiche del modello linguistico si spostano.\nAd esempio, per RoBERTa, ulteriormente addestrato su Reddit, corpus con una particolare inclinazione verso sinistra, possiamo vedere un sostanziale spostamento liberale in termini di bias politici.\nInoltre, cerchiamo di indagare se i modelli linguistici sono in grado di cogliere la polarizzazione prevalente nella nostra società moderna.\nSuccessivamente, dividiamo i corpora di pre-addestramento prima del 45° presidente degli Stati Uniti e dopo il 45° presidente degli Stati Uniti.\nPre-addestriamo separatamente i modelli linguistici sui due diversi corpora temporali.\nPossiamo vedere che i modelli linguistici presentano generalmente un orientamento politico più lontano dal centro a partire dal 2017.\nQuindi, questo indica che i modelli linguistici possono cogliere anche la polarizzazione nella nostra società.\nUltimo ma non meno importante, valutiamo i modelli linguistici con diverse inclinazioni politiche sul rilevamento dell'incitamento all'odio e sul rilevamento delle notizie false per quelle applicazioni NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative.\nQuindi, notiamo che, se indaghiamo le prestazioni per categoria, cioè se separiamo le prestazioni in diversi dati demografici o orientamento politico dei media, possiamo scorgere un modello.\nAd esempio, per il rilevamento dell'incitamento all'odio, i modelli linguistici di sinistra si rivelano migliori nel rilevare l'incitamento all'odio rivolto a gruppi socialmente minoritari, ma sono peggiori nel rilevare l'incitamento all'odio rivolto a gruppi più potenti nella nostra società.\nE viceversa, i modelli linguistici di destra sono più efficaci nel rilevare l'incitamento all'odio rivolto a bianchi e uomini, ma peggiori nel rilevare l'incitamento all'odio rivolto a neri, membri della comunità LGBTQ plus e altre comunità minoritarie.\nTendenze simili si verificano anche per il rilevamento di notizie false, dove vediamo che i modelli linguistici di sinistra dimostrano maggiore efficacia nel rilevare la disinformazione dal loro orientamento politico opposto e viceversa.\nInoltre, illustriamo svariati esempi qualitativi per provare che i modelli linguistici con diverse inclinazioni politiche offrono previsioni diverse sugli esempi di incitamento all'odio e disinformazione in base alle loro categorie sociali.\nCi sono molti altri esempi nell'appendice che evidenziano come tutto questo indichi la presenza di un problema di equità molto pressante riguardo ai pregiudizi politici insiti nei modelli linguistici.\nAd esempio, se i modelli linguistici di destra dovessero essere messi a punto su incitamento all'odio o disinformazione o altro e distribuiti su una popolare piattaforma di social media, questo significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e l'incitamento all'odio nei confronti di gruppi minoritari potrebbe facilmente dilagare senza alcun controllo.\nQuindi, ecco cosa ha fatto scattare l'allarme affinché riuscissimo a riconoscere e affrontare i problemi di equità derivanti dalle inclinazioni politiche del modello linguistico.\nPotremmo cogliere l'occasione per discuterne.\nVorremmo inoltre mettere in evidenza il fatto che affrontiamo il dilemma riguardante i pregiudizi politici del modello linguistico.\nÈ un po' come la questione tra Scilla e Cariddi.\nQuindi, se non \"sanifichiamo\" le opinioni politiche nei dati di addestramento del modello linguistico, il bias si propagherebbe dai dati di pre-addestramento ai modelli linguistici, fino alle attività a valle, finendo per creare problemi di equità.\nSe, invece, provassimo a sanificare il tutto, rischieremmo la censura o l'esclusione.\nEd è incredibilmente difficile determinare cosa sia effettivamente neutrale e debba conservare i dati di monitoraggio del linguaggio.\nÈ un po' come il problema del tram elettrico.\nOk, fantastico.\nPenso che sia tutto per oggi.\nGrazie per il tempo che ci avete dedicato.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "eXmqPhcZFN.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 11, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/gUAIqKCjIt.wav", "src_ref": "Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. So the minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. So what we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. So for example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario. So here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. So how does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.", "tgt_ref": "Buongiorno a tutti.\nSono Koustav Sinha e sono felice di darvi il benvenuto al nostro intervento sul nostro articolo ACL 2023.\nLanguage model acceptability judgments are not always robust to context.\nQuesto è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams.\nIn questo lavoro, rivisitiamo i paradigmi della coppia minima.\nIl paradigma della coppia minima valuta fondamentalmente i modelli linguistici sulla base dei giudizi di accettabilità.\nCiò può anche includere la grammaticalità come BLiMP, SyntaxGym o l'accettabilità in termini di stereotipi come le coppie CrowS.\nE, in questo paradigma della coppia minima, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o una frase grammaticale e poi mostrare una frase accettabile o una frase non grammaticale.\nDopodiché, la speranza è che il modello, in sostanza, aggiunga maggior probabilità alla frase accettabile.\nL'attuale pipeline MPP non ci permette di valutare l'accettazione di un modello in direzione di frasi più lunghe.\nIn questi giorni i grandi modelli linguistici sono caratterizzati da finestre di contesto sempre più lunghe.\nPertanto è fondamentale valutare l'accettabilità dei modelli in tutta la finestra di contesto ed è quello che stiamo cercando di fare qui.\nStiamo cercando di rivisitare la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe.\nEcco in cosa consiste questo approccio.\nQuindi, quello che facciamo per simulare queste sequenze più lunghe è rivisitare gli insiemi di dati stessi e ricreare le frasi scegliendo quelle accettabili o inaccettabili da quegli insiemi di dati.\nAd esempio, qui abbiamo scelto una tipica coppia di grammaticalità dal set di dati BLiMP dal caso Adjunct Island.\nE ciò che facciamo è ricreare sequenze più lunghe che siano accettabili e che abbiano la stessa corrispondenza della struttura grammaticale.\nEstrarremo frasi grammaticali da Adjunct Island e le aggiungeremo come prefisso sia alla query accettabile che a quella inaccettabile.\nQuindi, possiamo fare la stessa cosa scegliendo frasi inaccettabili dalla stessa corrispondenza, e questo potrebbe anche essere usato per testare l'accettabilità dei modelli.\nPossiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un diverso insieme di dati.\nQuesto è ciò che chiamiamo scenario di disallineamento.\nQui le frasi provengono ancora da insiemi di dati pertinenti, ma non dallo stesso insieme di dati con cui si sta effettuando la valutazione.\nE possiamo fare lo stesso per il caso dell'inaccettabilità.\nInfine, possiamo scegliere frasi da un dominio completamente estraneo come Wikipedia.\nCiò ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da qualsiasi contesto, ad esempio se il contesto proviene da un sottoinsieme diverso dell'insieme di dati, o se è completamente irrilevante per la frase corrente che stiamo analizzando.\nDunque, come funziona il modello?\nPrima di tutto, esaminiamo le frasi di Wikipedia, che sono completamente irrilevanti per la coppia di query corrente, e lì scopriamo che i giudizi MPP sono per lo più affidabili per una lunghezza del contesto arbitraria.\nAumentiamo la lunghezza del contesto fino a 1.024 per massimizzare i modelli OPT e GPT 2.\nAbbiamo visto nella linea tratteggiata arancione che i giudizi MPP sono relativamente stabili.\nOra, cosa succede quando scegliamo frasi dallo stesso insieme di dati?\nQui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso insieme di dati BLiMP o SyntaxGym.\nNotiamo che i giudizi MPP aumentano o diminuiscono in modo significativo quando si aggiungono prefissi accettabili o prefissi inaccettabili.\nMa quando abbiniamo la struttura, cioè quando scegliamo le frasi dagli stessi fenomeni in BLiMP o SyntaxGym, vediamo un aumento massiccio o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile.\nCiò è molto grande come questo effetto, aumenta per tutta la lunghezza del contesto e questo probabilmente influenzerebbe modelli linguistici simili più recenti che hanno una finestra di contesto ampia.\nAllora perché il prefisso di corrispondenza influisce così tanto sul giudizio del modello linguistico?\nAbbiamo condotto una serie di analisi in cui abbiamo cercato di perturbare la frase di input, provando al contempo a preservare la struttura rilevante ma aggiungendo rumore all'input.\nE dopo aver eseguito molte di queste perturbazioni, scopriamo che nessuno di questi rumori sta effettivamente facendo sì che il modello cambi il suo corso in termini di come ci mostra la stampa del giudizio MPP.\nFondamentalmente, scopriamo che i modelli sono sensibili alle frasi perturbate in modi simili.\nCioè, quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo simile.\nQuindi, i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili alle caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi.\nE la valutazione MPP, il modo in cui la effettuiamo attualmente con l'input di frasi brevi e singole, potrebbe non cogliere appieno la conoscenza astratta dei modelli linguistici in tutta la finestra di contesto.\nVi invitiamo a leggere il nostro articolo per maggiori dettagli sui nostri esperimenti.\nGrazie dell'ascolto.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "gUAIqKCjIt.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 12, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/krJSAnVcGR.wav", "src_ref": "Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning.\" This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I'd like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there's a catch, which is that people do assume that there's an additional clean validation set available for model selection. We can't stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that's not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there's no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.", "tgt_ref": "Salve, sono Dawei, studente di dottorato presso l'Università della Saarland in Germania.\nIn questo video, vorrei presentare il nostro ultimo lavoro, \"Weaker Than You Think: A Critical Look at Weakly Supervised Learning\".\nQuesto è un lavoro congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.\nVorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento scarsamente supervisionato.\nNella supervisione debole, i dati non vengono etichettati manualmente.\nAl contrario, etichettiamo i dati utilizzando fonti di etichettatura deboli, come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura sulla destra.\nRispetto alle annotazioni umane, le annotazioni più deboli sono molto più economiche, ma presentano anche un notevole rumore, il che si traduce nel fatto che una certa quantità di annotazioni non è corretta.\nSe addestriamo direttamente le reti neurali su dati scarsamente etichettati, le reti neurali tendono a memorizzare il rumore dell'etichetta e non generalizzano.\nNell'apprendimento scarsamente supervisionato, vengono proposti algoritmi di addestramento al fine di addestrare in modo robusto le reti neurali in presenza di errori che affliggono le etichette, in modo che i modelli addestrati continuino a generalizzare in modo adeguato.\nNei recenti lavori in WSL (WSL sta per Weakly Supervised Learning), un'affermazione comune è che le persone affermano di addestrare solo i modelli sui dati scarsamente etichettati e ottengono prestazioni elevate su set di test puliti.\nTecnicamente, questa affermazione non è sbagliata, ma c'è un problema, ovvero che le persone presumono la disponibilità di un set di convalida pulito aggiuntivo per la selezione del modello.\nNon possiamo fermarci a questo problema, ma ciò implica che sono necessarie ulteriori annotazioni manuali nell'apprendimento scarsamente supervisionato.\nMa come un elefante nella stanza, questa necessità viene spesso trascurata.\nIl dubbio di cui sopra è stato espresso per porre tre domande di ricerca.\nIn primo luogo, i dati di convalida puliti sono necessari per il WSL o, invece, possiamo utilizzare un set di convalida rumoroso?\nIn secondo luogo, se sono necessari dati puliti, o se i dati puliti sono obbligatori affinché il WSL funzioni, di quanti campioni puliti abbiamo bisogno?\nInfine, per la convalida dovremmo usare solo i campioni puliti o ci sono modi migliori per impiegarli?\nAbbiamo provato a rispondere a queste domande di ricerca nel nostro lavoro e i risultati sono i seguenti.\nIn primo luogo, va sottolineato che i recenti metodi WSL richiedono effettivamente campioni di convalida puliti per funzionare correttamente.\nIn caso contrario, si verifica un notevole calo delle prestazioni.\nCome mostrato in questa figura, se non ci sono campioni di convalida puliti, i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento si rivela inutile.\nCiò fa riferimento al fatto che gli approcci WSL richiedono effettivamente dati etichettati in modo pulito per funzionare correttamente e il costo dell'annotazione per ottenere campioni di convalida puliti non deve essere trascurato.\nLa nostra seconda scoperta è che l'aumento del numero di campioni di convalida puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura sulla sinistra.\nIn genere abbiamo bisogno solo di 20 campioni per classe per ottenere prestazioni elevate.\nMa non è finita qui, perché se decidiamo di accedere a campioni puliti indipendentemente dal modo, l'addestramento diretto su di essi raggiungerà anche prestazioni migliori.\nLa figura sulla destra mostra la differenza di prestazioni tra gli approcci di fine-tuning, che vengono applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la convalida.\nCome possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a battere gli approcci WSL.\nInfine, il miglioramento delle prestazioni rivendicato nei precedenti approcci WSL può essere ottenuto con facilità, consentendo di continuare il fine-tuning sui campioni di convalida puliti.\nCome possiamo vedere dalle figure, il modello convenzionale, denominato FTw, rivela inizialmente prestazioni inferiori rispetto a metodi WSL più complicati, come COSINE.\nTuttavia, se continuiamo a eseguire il fine-tuning sui campioni puliti, FTw funziona altrettanto bene come altri metodi.\nQuindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco.\nPer riassumere, abbiamo dimostrato che i recenti approcci WSL richiedono campioni puliti e annotati manualmente per funzionare correttamente.\nL'incremento di prestazioni e la praticità sono fortemente sopravvalutati.\nLe nostre raccomandazioni concrete per il lavoro futuro sono le seguenti.\nInnanzitutto, riportare i criteri di selezione del modello.\nAd esempio, segnalare se la selezione del modello viene eseguita tramite campioni di convalida puliti.\nIn secondo luogo, gli approcci WSL dovrebbero essere confrontati con le baseline dell'apprendimento few-shot, poiché entrambi funzionano su campioni puliti.\nIn terzo luogo, il fine-tuning continuo è una base semplice ma solida che dovrebbe essere considerata nel lavoro futuro in WSL.\nInfine, abbiamo reso il nostro codice open source.\nPotete trovarlo tramite il codice QR su questa diapositiva.\nNon esitate a consultare il lavoro.\nGrazie e buona conferenza.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "krJSAnVcGR.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 13, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/miPjvjWOvI.wav", "src_ref": "Hello everyone, my name is David Vilar, and I will be giving a short review of the paper \"Prompting PaLM for Translation: Assessing Strategies and Performance.\" This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It's trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it's important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it's in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn't have a big influence in the case of several short promptings. It's crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It's the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it's important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it's more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the \"Style/Awkward\" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that's it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.", "tgt_ref": "Ciao a tutti, mi chiamo David Vilar. Mi trovo qui per offrirvi una panoramica dell'articolo \"Prompting PaLM for Translation: Assessing Strategies and Performance\".\nQuesto è un lavoro congiunto con i miei colleghi di Google Translate.\nPaLM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri presentato lo scorso anno, nel 2022.\nÈ addestrato su una vasta raccolta di testi, che comprende 780 miliardi di token.\nAl momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di attività NLP.\nIn questo lavoro, presentiamo il primo studio sistematico di prompting di grandi modelli linguistici per la traduzione automatica.\nAbbiamo valutato la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità MT.\nCiò comporta l'utilizzo dei set di test più recenti per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico.\nE abbiamo eseguito un confronto con i sistemi all'avanguardia, quindi con il sistema più performante, sulla base della valutazione WMT.\nFacciamo ricorso a metriche di MT neurale all'avanguardia e mostriamo i risultati della valutazione umana effettuata da esperti.\nInfine, forniamo alcune raccomandazioni per strategie di selezione rapida.\nIl prompting ha una grande influenza sulle prestazioni dei LLM per la traduzione, come possiamo vedere in un semplice esperimento, in cui abbiamo utilizzato un prompting one-shot e fornito due diversi prompt per ogni frase.\nLa maggior parte delle frasi, 516 su 1.000.\nLa differenza osservata non si limita al singolo punto BLEURT.\nE questo può arrivare, in casi estremi, fino a 40 punti BLEURT.\nQuindi, è importante selezionare una buona strategia di prompting.\nNei nostri esperimenti, ci siamo accontentati di una strategia di prompting 5-shot in cui non abbiamo fatto altro che contrassegnare ogni frase messa a disposizione del sistema con la lingua in cui è elaborata.\nQuindi, in questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi in tedesco, nella lingua di origine, sono contrassegnate con i due punti tedeschi e le traduzioni inglesi con i due punti inglesi.\nAbbiamo visto che la forma effettiva del prompt non ha una grande influenza nel caso di svariati prompt brevi.\nCiò si rivela fondamentale per i prompt zero-shot e one-shot.\nE quando operiamo con cinque prompt, come nel nostro caso, non emerge quasi alcuna differenza con la forma effettiva del prompting.\nSi tratta degli esempi che portano la maggior parte del peso.\nLa sintesi dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di origine.\nQuindi è importante selezionare gli esempi da traduzioni di alta qualità.\nIn particolare, confrontiamo i prompt di selezione dai dati di addestramento per le valutazioni WMT sui dati di sviluppo.\nI dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che invece sono più rumorosi.\nE i loro risultati offrono prestazioni migliori quando si utilizzano i dati di sviluppo.\nTuttavia, i sistemi specializzati all'avanguardia presentano un vantaggio sostanziale rispetto alle traduzioni PaLM.\nPaLM si avvicina abbastanza a un sistema commerciale.\nNel nostro caso, abbiamo scelto di procedere alla valutazione con Google Translate.\nLe intuizioni acquisite dalla valutazione umana che abbiamo eseguito utilizzando il framework MQM ci hanno fatto scoprire che la fluidità di PaLM è paragonabile ai sistemi all'avanguardia, ma la differenza principale deriva in particolare dalla precisione.\nQuindi, gli errori più comuni sono quelli di omissione.\nSembra che PaLM scelga di produrre una traduzione migliore, a volte tralasciando parti della frase di partenza che vengono fatte in traduzione.\nTuttavia, la categoria \"Style/Awkward\" per PaLM è inferiore rispetto ai sistemi all'avanguardia, il che è un segnale ulteriore che PaLM fornisce un output realmente fluente, ma ancora contraddistinto da alcuni problemi di precisione.\nE questo è tutto per ora.\nPer maggiori dettagli, siete invitati a partecipare alla presentazione completa dell'articolo.\nGrazie mille.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "miPjvjWOvI.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 14, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/rOwZgUjcwB.wav", "src_ref": "Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding as services via backdoor watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLAMA, PALM are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various, NLP tasks. For example, OpenAI offers a GPT based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contain the watermark. The watermark method need to meet the following properties. First the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability. Therefore, in this paper we propose Embedding marker, which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our embedding marker. Embedding marker contains two main steps. Watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user send a sentence to the provider service the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When a number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the word mark. We first construct a back door and a benign data set. Back door data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets. Then the provider requests the embeddings from the stealer's service with the data set. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four data sets AG News, MIND, SST2 and Enron Spam. We assume the provider apply wiki text data set to count word frequency. The results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualising the embedding of sentences on four datasets via PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between, the backdoor embeddings and normal embeddings. That's all. Thank you. Welcome to discuss with us.", "tgt_ref": "Ciao a tutti, mi chiamo Jingwei Yi dell'Università di Scienza e Tecnologia della Cina.\nÈ un piacere presentare un breve video promozionale del nostro articolo.\n\"Are you copying my model?\nProtecting the copyright of large language models for embedding as services via backdoor watermark.\"\nIntroduciamo innanzitutto le informazioni di contesto sull'integrazione con i servizi.\nAttualmente, i grandi modelli linguistici come GPT, LLAMA, PALM si rivelano eccezionali nella comprensione e nella generazione del linguaggio naturale.\nL'integrazione con i servizi è uno dei servizi costruiti su grandi modelli linguistici per assistere varie attività NLP.\nAd esempio, OpenAI offre un'API di integrazione basata su GPT.\nTuttavia, i lavori recenti hanno dimostrato che l'utente malintenzionato può rubare il modello attraverso l'apprendimento dall'integrazione e fornire servizi simili.\nPertanto, è necessario proteggere il copyright dell'integrazione con i servizi.\nPer proteggere il copyright dell'integrazione con i servizi, una delle soluzioni consiste nell'incorporare una filigrana nel servizio del provider e rilevare se un altro servizio contiene la filigrana.\nIl metodo della filigrana deve soddisfare le seguenti proprietà.\nIn primo luogo, il metodo dovrebbe essere applicabile all'incorporamento all'interno di servizi.\nIn secondo luogo, la filigrana non deve degradare l'utilità delle integrazioni fornite.\nIn terzo luogo, la filigrana dovrebbe essere sufficientemente nascosta all'autore delle minacce, altrimenti quest'ultimo potrebbe rimuoverla con relativa facilità.\nInfine, la filigrana deve essere trasferibile ai servizi dell'utente malintenzionato durante il processo di estrazione del modello.\nI lavori esistenti possono essere ampiamente classificati in quattro categorie.\nTuttavia, questo metodo non è applicabile all'integrazione con i servizi o manca di trasferibilità.\nPertanto, in questo documento proponiamo Embedding marker, che è un metodo di filigrana basato su backdoor applicabile all'integrazione con i servizi.\nQuindi, permettetemi di presentare i dettagli del nostro Embedding Marker.\nIl marker di integrazione contiene due fasi principali.\nIniezione di filigrana e verifica del copyright.\nPrima di questi passaggi principali, selezioniamo innanzitutto un insieme di trigger.\nIl set di trigger è un gruppo di parole in un intervallo di frequenza moderata.\nPartiamo dal presupposto che il provider possa raccogliere un corpus di testo generale e contare la frequenza delle parole contenute.\nNell'iniezione della filigrana, definiamo innanzitutto un integrazione di destinazione.\nQuando un utente invia una frase al servizio del provider, quest'ultimo conta il numero di trigger nella frase.\nL'integrazione fornita è una somma ponderata dell'integrazione di destinazione e dell'integrazione originale.\nIl peso dell'integrazione di destinazione è proporzionale al numero di trigger nella frase.\nQuando un numero di trigger nella frase è maggiore di m, l'integrazione fornita è esattamente uguale all'integrazione di destinazione.\nLa verifica del copyright consiste nel rilevare se un modello dietro un altro servizio contiene il marchio della parola.\nPer prima cosa costruiamo una backdoor e un insieme di dati benigni.\nIl set di dati di backdoor contiene frasi di cui tutte le parole appartengono al set di trigger, mentre tutte le parole nelle frasi del set di dati benigni non appartengono ai set di trigger.\nQuindi il provider richiede gli embedding dal servizio del ladro con l'insieme di dati.\nVengono calcolati il coseno e la similarità L2 tra l'integrazione richiesta e l'integrazione di destinazione.\nCalcoliamo la differenza di similarità tra il set di dati benigni e quello di backdoor che è definita come delta coseno e delta L2.\nNel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza metrica.\nConduciamo esperimenti su quattro set di dati AG News, MIND, SST2 e Enron Spam.\nPartiamo dal presupposto che il provider applichi il set di dati di testo wiki per contare la frequenza delle parole.\nI risultati su quattro set di dati mostrano che il nostro marcatore di integrazione può avere grandi prestazioni di rilevamento mantenendo una grande utilità per le attività a valle.\nConvalidiamo anche la segretezza dell'integrazione fornita visualizzando l'integrazione di frasi su quattro set di dati [INUDIBILE 4:39] PCA.\nLa legenda delle figure indica il numero di trigger in ogni frase.\nCome mostrato nelle figure, è difficile distinguere tra gli embedding di backdoor e gli embedding normali.\nÈ tutto.\nGrazie.\nSe volete parlarne con noi, siete i benvenuti.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "rOwZgUjcwB.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 15, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/vrydRuOXbT.wav", "src_ref": "Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, we report the min and max performance and the standard deviation of the performance across all 5 experiments. If the task is a multi-model classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. For NLP task, we report Rouge-L as well. We also introduce an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task regardless of the slight variation in the wording of the instruction. Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity. So we also did one experiment. We use one instruction versus 5 instruction. As we can see, using more instructions can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategies on the model sensitivity. As we can see by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to attain much better performance on the natural instruct dataset. So overall, we propose the first large scale multi-model instruction tuning dataset with significantly improved their short capability of OFA, and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-model instruction tuning dataset with around 150 additional vision language tasks and we will release them. So this is a QR code for our data and model. Thank you.", "tgt_ref": "Ciao a tutti, mi chiamo Ying, e il mio collega Zhiyang e io presenteremo la nostra ricerca su MultiInstruct migliorando l'apprendimento multimodale zero-shot tramite l'Instruction Tuning.\nCon i progressi nei modelli linguistici di grandi dimensioni, molte ricerche hanno iniziato a esplorare nuovi paradigmi di apprendimento del riutilizzo di modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente in termini di parametri e dati.\nRecentemente, molti studi hanno dimostrato che l'ottimizzazione su istruzioni consente ai grandi modelli linguistici di eseguire attività non osservate in modalità zero-shot seguendo istruzioni naturali.\nTuttavia, la maggior parte dei lavori precedenti sull'adattamento delle istruzioni si è concentrata sul miglioramento delle prestazioni zero-shot su attività solo linguistiche, mentre la visione artificiale e le attività multimodali sono state tralasciate.\nPertanto, in questo lavoro vogliamo indagare se l'adattamento a istruzioni di un modello multimodale pre-addestrato può effettivamente migliorare la generalizzazione a compiti multimodali non osservati.\nInoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di set di dati di istruzioni tra NLP e multimodale.\nEsistono più di 1.600 attività con istruzione solo in lingua.\nTuttavia, non esiste un'attività di istruzione multimodale su larga scala disponibile al pubblico.\nPertanto, questo ci motiva a costruire un set di dati di sintonizzazione delle istruzioni multimodali.\nQui presentiamo MultiInstruct, il primo set di dati di riferimento per l'adattamento delle istruzioni multimodali, composto da 62 diverse attività multimodali che coprono 10 ampie categorie.\nQueste attività derivano da 21 set di dati open source esistenti e ogni attività è dotata di cinque istruzioni scritte da esperti.\nPer analizzare l' adattamento di istruzioni multimodali sul nostro set di dati proposto, prendiamo come modello di base OFA, un modello pre-addestrato multimodale unificato.\nOFA utilizza un vocabolario unificato per la lingua, i token di immagine e le coordinate di un riquadro di delimitazione.\nQui mostriamo alcune istanze di esempio dal nostro set di dati MultiInstruct per unificare l'elaborazione di vari tipi di dati di input e output.\nSeguiamo il metodo di OFA e formuliamo tutte le attività in un formato unificato da sequenza a sequenza.\nIn cui il testo di input, le immagini, le istruzioni e i riquadri di delimitazione sono rappresentati nello stesso spazio token.\nBene, ora parlerò dell'adattamento di istruzioni multimodali.\nPer il set di dati di addestramento, utilizziamo 53 attività di 9 gruppi per l'addestramento e campioniamo 10.000 istanze per attività.\nPer i test, riserviamo l'intero gruppo di ragionamento di buon senso per i test e selezioniamo altre 5 attività dai gruppi VQ e Varie.\nUsiamo tutte le istanze nella suddivisione del test per ogni attività.\nInoltre, campioniamo casualmente 20 attività dalla suddivisione del test delle istruzioni naturali come attività non osservata per NLP.\nQuindi, usiamo il modello OFA di grandi dimensioni pre-addestrato come modello di base.\nDurante l'addestramento, mescoliamo tutte le istanze per tutte le attività.\nOgni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzione.\nSuccessivamente, durante il test per ciascuna attività, conduciamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni.\nIn ogni esperimento, riportiamo le prestazioni minime e massime e la deviazione standard delle prestazioni in tutti e 5 gli esperimenti.\nSe l'attività è un'attività di classificazione multi-modello, riportiamo l'accuratezza.\nSe si tratta di un'attività di generazione multimodale, riportiamo Rouge-L. Riportiamo Rouge-L anche per l'attività di NLP.\nInoltre, introduciamo un'ulteriore metrica di valutazione chiamata sensibilità.\nCiò misura la capacità del modello di produrre costantemente gli stessi output per la stessa attività, indipendentemente dalla lieve variazione nella formulazione dell'istruzione.\nEcco il risultato.\nCome possiamo vedere, l'ottimizzazione delle istruzioni può migliorare significativamente le prestazioni di OFA su attività multimodali osservate.\nInoltre, l'apprendimento per trasferimento dal set di dati di istruzioni naturali può beneficiare della messa a punto delle istruzioni.\nQui possiamo vedere che, all'aumentare della quantità di attività, il modello raggiunge prestazioni migliori e, nel frattempo, una sensibilità inferiore.\nDopodiché abbiamo condotto anche un esperimento.\nUsiamo una istruzione contro 5 istruzioni.\nCome possiamo vedere, l'utilizzo di più istruzioni può migliorare le prestazioni complessive del modello e ridurne notevolmente la sensibilità.\nQuindi, questo mostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello.\nCome possiamo vedere dall'apprendimento per trasferimento da set di dati di istruzioni naturali, il modello può raggiungere una sensibilità molto più elevata rispetto al modello OFA originale.\nPossiamo anche vedere che l'apprendimento per trasferimento da set di dati di istruzioni naturali può aiutare OFA a ottenere prestazioni molto migliori sul set di dati di istruzioni naturali.\nQuindi, nel complesso, proponiamo il primo set di dati di messa a punto delle istruzioni multi-modello su larga scala con una capacità di OFA notevolmente migliorata ed esploriamo diverse tecniche di apprendimento per trasferimento mostrandone i vantaggi.\nProgettiamo una nuova metrica chiamata sensibilità.\nUn'altra cosa: stiamo raccogliendo un set di dati di messa a punto delle istruzioni multi-modello molto più ampio con circa 150 attività di linguaggio visivo aggiuntive e lo renderemo disponibile.\nQuesto è un codice QR per i nostri dati e il nostro modello.\nGrazie.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "vrydRuOXbT.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 16, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/wJAPXMIoIG.wav", "src_ref": "Hello everyone, my name is Yusen Zhang from the Penn State University. Today I'm going to present our work \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And Cross-Lingual Semantic Parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and etcetera. Existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications. For instance, there are lots of coverage on certain natural languages. But Chinese is missing and lack of coverage on certain meaning representation. The Lambda calculus is missing, or they're only evaluated on certain neural models. For example, there's only one single model to evaluate them. So to this end we propose XSemPLR. We provide a uniform data set XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL. And we'll also test Monolingual Model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data. And we test Multilingual Model which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model. And during inference we can use this model to translate German queries or Chinese queries, et cetera. And we also consider Cross-lingual Zero-shot and Few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English queries or the combination of English and German Few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. So, regarding analysis of monolingual models, we evaluate on two groups of models including Encoder-PTR which stands for Multilingual Pretrained Encoders with Pointer-based Decoders, such as XLM-R + PTR and mBERT + PTR. And, we also evaluate Encoder-Decoder models, which is Multilingual Pretrained Encoder-Decoder Models, such as mBART and mT5. We found that Encoder-Decoder obtains the best performance on all nine datasets. And we evaluate on mT5 and XLM-R + PTR on multilingual setting. We found that Encoder-Decoder or Encoder-PTR can be improved by training in a mixture of various languages. We found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as the \"Curse of Multilinguality\". We also compare the cross-language performance gap. In this figure, the blue line is Cross-lingual Few-shot transfer. The orange line is Cross-lingual Zero-shot transfer. While the green line is the Monolingual Setting. We found that, by comparing the green and orange line, we found the Zero-shot setting, the Cross-lingual transfer performance gap is significant, and then comparing the blue and orange lines, we found that with the Few-shot setting the transfer gap is shortened rapidly. We also find some other interesting findings. For example, Encoder-Decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages, and we found multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show many interesting findings. And et cetera. And welcome to visit our paper and code. Thanks for listening.", "tgt_ref": "Ciao a tutti, mi chiamo Yusen Zhang della Penn State University.\nOggi presenterò il nostro lavoro \"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\".\nDunque, l'analisi semantica è un'attività per creare rappresentazioni semantiche di query utente come SQL e Lambda Calculus.\nE l'analisi semantica cross-linguistica costituisce il compito di tradurre le query in più lingue naturali in più rappresentazioni di significato.\nCome mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL e così via.\nI modelli di analisi semantica translinguistica esistenti sono proposti e valutati separatamente su un set di dati di attività e applicazioni limitate.\nAd esempio, la letteratura su alcune lingue naturali è molto ampia.\nTuttavia mancano il cinese e i riferimenti su alcune rappresentazioni di significato.\nManca il calcolo lambda oppure sono valutati solo su alcuni modelli neurali.\nAd esempio, è presente solo un modello per valutarli.\nQuindi, a tal fine, proponiamo XSemPLR.\nForniamo un set di dati uniforme XSemPLR per l'analisi semantica translinguistica in più lingue naturali e rappresentazioni di significato.\nContiene 9 set di dati in vari domini, 5 attività di analisi semantica, 8 rappresentazioni di significato e 22 lingue naturali in 15 famiglie linguistiche.\nE per valutare meglio il nostro parametro di riferimento, consideriamo le sei impostazioni per l'addestramento e la valutazione.\nIl primo è Translate-Test.\nUtilizziamo l'API di Google Translate per tradurre la fonte nella lingua di destinazione, quindi utilizziamo il modello monolingue per l'addestramento e la valutazione.\nPer esempio, addestriamo il modello inglese sulla query inglese e durante l'inferenza traduciamo la query tedesca utilizzando l'API in inglese e, successivamente, utilizziamo il modello addestrato per prevedere l'SQL.\nDopodiché testeremo il modello monolingue.\nIn questa impostazione, la lingua di origine è la stessa della lingua di destinazione, ad esempio da tedesco a tedesco o da inglese a inglese.\nTestiamo anche l'impostazione Monolingual Few-shot addestrando modelli monolingue con solo il 10% dei dati di addestramento.\nE testiamo il modello multilingue, ossia addestriamo un modello multilingue per tutte le lingue.\nAd esempio, mettiamo insieme le query tedesche, inglesi e cinesi per addestrare un modello multilingue.\nE durante l'inferenza possiamo usare questo modello per tradurre query tedesche o cinesi, eccetera.\nConsideriamo anche il trasferimento Cross-lingual Zero-shot e Few-shot.\nCi esercitiamo su una lingua di partenza e passiamo a un'altra lingua.\nQuindi, durante l'addestramento, la addestriamo su query in inglese o sulla combinazione di query Few-shot in inglese e tedesco per addestrare un modello multilingue a prevedere l'output SQL.\nE troviamo anche molti risultati interessanti.\nPer quanto riguarda l'analisi dei modelli monolingue, valutiamo due gruppi di modelli tra cui Encoder-PTR che sta per Multilingual Pretrained Encoders with Pointer-based Decoders, come XLM-R + PTR e mBERT + PTR.\nInoltre, valutiamo anche i modelli Encoder-Decoder, che sono modelli Encoder-Decoder preaddestrati multilingue, come mBART e mT5.\nAbbiamo notato che Encoder-Decoder ottiene le migliori prestazioni su tutti e nove i set di dati.\nE valutiamo su mT5 e XLM-R + PTR su impostazione multilingue.\nAbbiamo scoperto che Encoder-Decoder o Encoder-PTR possono essere migliorati attraverso l'addestramento in una combinazione di varie lingue.\nAbbiamo osservato che ciò accade perché la maggior parte delle principali lingue naturali può ottenere un guadagno di prestazioni, tranne per il fatto che le prestazioni in inglese scendono in sette set di dati e guadagnano solo in tre set di dati.\nCredo che ciò sia noto come la \"maledizione multilinguistica\".\nConfrontiamo anche il divario delle prestazioni tra lingue.\nIn questa figura, la linea blu rappresenta il trasferimento Cross-lingual Few-shot.\nLa linea arancione rappresenta il trasferimento Cross-lingual Zero-shot.\nMentre la linea verde rappresenta l'impostazione monolingue.\nConfrontando la linea verde e arancione, abbiamo trovato l'impostazione Zero-shot, il divario di prestazioni del trasferimento translinguistico è significativo, e poi confrontando le linee blu e arancione, abbiamo scoperto che con l'impostazione Few-shot il divario di trasferimento si riduce rapidamente.\nRileviamo anche altri risultati interessanti.\nAd esempio, Encoder-Decoder supera i lavori precedenti o raggiunge risultati comparabili.\nIl pre-addestramento sul linguaggio naturale inglese può aumentare significativamente le prestazioni di Few-shot sulle lingue naturali di destinazione e abbiamo scoperto che i modelli linguistici multilingue come Codex e BLOOM sono ancora inadeguati per le attività di analisi semantica cross-linguistica.\nPer riassumere, costruiamo XSemPLR, un benchmark unificato per l'analisi semantica cross-linguistica con più lingue naturali e rappresentazioni di significato.\nConduciamo uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue.\nE i nostri risultati mostrano molti risultati interessanti.\nEccetera.\nSiete invitati a consultare il nostro articolo e il nostro codice.\nGrazie per l'attenzione.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "wJAPXMIoIG.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 17, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/wLmrUehthl.wav", "src_ref": "Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. So in this case, Lisa. A similar approach is assumed in Igor Mel'čuk's meaning text theory, where again, the whole coordinate structure is headed by the first conjuct. So these two approaches are asymmetric. Right. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. So, we get some dependencies from end to all the conjuncts. And finally, there's also a multi-headed approach that's used, for example, in the Hudson's Word Grammar, where they say all conjuncts are heads of the coordinate structure. So we get dependencies from the governor. Here loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples. So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So \"Marge read it yesterday\" is fine because the direct object is close to the verb, while \"Marge read yesterday it\" is much worse. Right? Because here between the verb and the direct object is an adjunct: \"yesterday\". However, this effect may be ameliorated when the direct object is very heavy and very long. Because then it can be moved to the position after the adjunct. This is illustrated here. So both these sentences are fine. \"Marge read this absolutely fascinating book about bees yesterday.\" It's okay the way instead of \"it\", we have this long NP. But it's also OK to say, \"Marge read yesterday this absolutely fascinating book about bees.\" So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb, it satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred. So these two trees only show the length of the crucial dependencies, the ones that are not constant among these two structures. So here we have a dependency from \"read\" to the adjunct of length 7 measured in words and from \"read\" to \"book\" of length 4, so together it's 11. When you swap these two constituents, the sum of these two dependencies becomes 6. So instead of 11, 6 is much shorter. That's why this sounds quite okay. Right? It violates one principle, but it satisfies another one. Ok. So what we did, we extracted various statistics about coordination from the enhanced version of the Penn Treebank and see the paper \"Why wouldn't you use universal dependencies\" and these statistics confirm the observation made many times before that left conjuncts tend to be shorter. So, \"salt and pepper\" and not \"pepper and salt\", measured in syllables. And, also the observation that was made in parsing that this tendency grows with length difference. So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one, stronger, right? So the proportion is bigger of the left short conjunct. But what's novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent. Right? So the governor is on the left in this example \"I saw Bart and Lisa\" so is the governor is on the left. It's absent in the second example \"Homer came and sneezed.\" Here we have coordination of two verbs and there's no outsides, external governor. In such cases, the left conjunct prefers to be shorter; the most of the biggest difference between the two conjuncts. However, when the governor is on the right, as here, \"laughed\" governs the coordination Ted and Ned, this effect disappears. So we showed that by measuring length in characters, the first column, in syllables the middle column, and in words the right column. So I'll concentrate on the right one. What we see here is that when the governor is on the left, the tendency for the left conjunct to be shorter grows steadily, with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences. But when the governor is on the right this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures, as these two. So see the paper for the full arguments. And talk to us about at the poster session. Thank you.", "tgt_ref": "Ciao, mi chiamo Adam Przepiórkowski e questo talk è incentrato sulla struttura di dipendenza della coordinazione.\nCome forse saprete, le diverse teorie e gli approcci ai corpus implicano svariate strutture di dipendenza.\nAd esempio, nelle dipendenze universali, la struttura della coordinazione, Lisa, Bart e Maggie, è tale che il primo congiunto è il capo dell'intera struttura coordinata.\nQuindi in questo caso, Lisa.\nUn approccio simile è assunto nella teoria del testo di significato di Igor Mel'čuk, nella quale l'intera struttura coordinata è ancora una volta guidata dal primo congiunto.\nQuindi, questi due approcci sono asimmetrici.\nBene.\nIndividuano uno dei congiunti.\nOra, si tratta di approcci asimmetrici alle strutture coordinate, come l'approccio di Praga.\nL'approccio guidato dalla congiunzione assunto nei treebank di dipendenza di Praga prevede che le strutture coordinate siano guidate dalla congiunzione.\nQuindi, otteniamo alcune dipendenze dalla fine a tutti i congiunti.\nE infine, è presente anche un approccio multi-testa che viene utilizzato, ad esempio, nella Hudson's Word Grammar, dove dicono che tutti i congiunti sono teste della struttura coordinata.\nSuccessivamente, otteniamo dipendenze dal governatore.\nLa preferenza è quella di avere i congiunti separati: Lisa, Bart e Maggie.\nOra, lo scopo di questo articolo è produrre un nuovo argomento per le strutture simmetriche di coordinazione, come queste due, e contro le strutture asimmetriche di coordinazione, come queste due.\nBene.\nL'argomento si basa sul principio della minimizzazione della lunghezza della dipendenza che spiegherò sulla base di questi esempi.\nCome forse saprete, in inglese gli oggetti diretti preferiscono essere vicini al verbo, mentre gli elementi aggiuntivi possono essere più lontani.\nQuindi \"Marge read it yesterday\" va bene perché l'oggetto diretto è vicino al verbo, mentre \"Marge read yesterday it\" è decisamente meno accettabile.\nGiusto?\nPerché qui tra il verbo e l'oggetto diretto c'è un elemento aggiuntivo: \"yesterday\".\nTuttavia, questo effetto può essere migliorato quando l'oggetto diretto è molto pesante e molto lungo.\nPerché allora può essere spostato nella posizione dopo l'elemento aggiuntivo.\nCiò è illustrato qui.\nQuindi entrambe queste frasi vanno bene.\n\"Marge read this absolutely fascinating book about bees yesterday\".\nCi sembra accettabile il fatto che, invece di \"it\", abbiamo questo lungo sintagma nominale.\nMa va bene anche dire: \"Marge read yesterday this absolutely fascinating book about bees\".\nQuindi, il ragionamento qui è che tutto questo è possibile perché anche se questa frase viola il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere accanto al verbo, soddisfa il principio di minimizzazione della lunghezza della dipendenza, che afferma che sono preferibili dipendenze più brevi.\nDi conseguenza, questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture.\nQui abbiamo una dipendenza da \"read\" all'elemento aggiuntivo di lunghezza 7 misurato in parole e da \"read\" a \"book\" di lunghezza 4, che insieme danno 11.\nQuando si scambiano questi due costituenti, la somma di queste due dipendenze diventa 6.\nQuindi invece di 11, 6 è molto più corto.\nEcco perché sembra andare piuttosto bene.\nGiusto?\nViola un principio, ma ne soddisfa un altro.\nOk.\nQuindi, quello che abbiamo fatto è stato estrarre varie statistiche sul coordinamento dalla versione avanzata del Penn Treebank e consultare l'articolo \"Why wouldn't you use universal dependencies\". Queste statistiche confermano l'osservazione fatta molte volte in passato, vale a dire che i congiunti di sinistra tendono a essere più brevi.\nQuindi, \"salt and pepper\" e non \"pepper and salt\", misurato in sillabe.\nInoltre, l'osservazione fatta nel corso dell'analisi è che questa tendenza aumenta con la differenza di lunghezza.\nQuindi, quando la differenza tra le lunghezze dei due congiunti cresce, quello più corto preferisce essere il primo, il più forte, giusto?\nDunque la proporzione è maggiore è quella del congiunto corto sinistro.\nMa la novità di questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o è del tutto assente.\nGiusto?\nQuindi, il governatore è sulla sinistra in questo esempio \"I saw Bart and Lisa\", quindi il governatore è sulla sinistra.\nÈ assente nel secondo esempio \"Homer came and sneezed\".\nQui ci troviamo di fronte alla coordinazione di due verbi e non c'è nessun governatore esterno.\nIn questi casi, il congiunto sinistro preferisce essere più corto; ecco dov'è gran parte della differenza tra i due congiunti.\nTuttavia, quando il governatore è sulla destra, come in questa circostanza, e \"laughed\" governa la coordinazione Ted e Ned, questo effetto scompare.\nQuindi, abbiamo dimostrato che la lunghezza viene misurata in caratteri, la prima colonna, in sillabe la colonna centrale e in parole la colonna di destra.\nOra mi concentrerò su quella di destra.\nQuello che vediamo qui è che quando il governatore è a sinistra, la tendenza del congiunto sinistro a essere più corto cresce costantemente, con la differenza assoluta in parole, e lo stesso si osserva quando non c'è alcun governatore come nella coordinazione delle frasi.\nMa quando il governatore è a destra, questa tendenza viene meno.\nNell'articolo mostriamo come tutto ciò rappresenti una tesi contro le strutture asimmetriche di coordinazione, come queste due, e a favore delle strutture simmetriche, come queste due.\nSiete invitati a consultare l'articolo per gli argomenti completi.\nPer poi parlarne alla sessione poster.\nGrazie.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "wLmrUehthl.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 18, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/xUDLtuhJUS.wav", "src_ref": "Hello, my name is Kayo Yin and I will be presenting our work titled \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate \"mole\" in this sentence? Well, if the previous sentence was \"Things could start to get dangerous if the ministers find out\", then \"mole\" refers to a spy. But if the previous sentence was \"Could it be anything serious, doctor?\", then \"mole\" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that's rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.", "tgt_ref": "Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro dal titolo \"When Does Translation Require Context?\nA Data-driven, Multilingual Exploration\".\nQuesto lavoro è stato svolto in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.\nMolte traduzioni dipendono dal contesto.\nAd esempio, come tradurremmo \"mole\" in questa frase?\nBeh, se la frase precedente fosse \"Things could start to get dangerous if the ministers find out\", allora \"mole\" significherebbe \"talpa\", nel senso di spia.\nMa se la frase precedente fosse \"Could it be anything serious, doctor?\", allora \"mole\" si riferirebbe a un neo.\nQuindi, a seconda del contesto, il significato della parola cambia, e quindi cambia anche la sua traduzione.\nTuttavia, valutare l'accuratezza con cui i modelli possono tradurre termini come questo è piuttosto difficile.\nIn primo luogo perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come BLEU incapaci di catturare il senso di queste traduzioni.\nAlcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue poiché si basano solitamente sulla conoscenza del dominio e sulla selezione umana.\nIn questo lavoro, cerchiamo di rispondere a queste due domande.\nIn primo luogo, quand'è che la traduzione necessita del contesto?\nE in secondo luogo, in che modo i modelli gestiscono questi casi?\nPer rispondere alla prima domanda, abbiamo iniziato misurando il grado di dipendenza dal contesto di una parola durante la traduzione.\nNel lavoro precedente, abbiamo introdotto CXMI come misura del ricorso al contesto da parte dei modelli di traduzione automatica.\nCiò viene fatto misurando quante informazioni il contesto C fornisce sul target Y, data la sorgente X. Si può pensare a CXMI come alle informazioni acquisite fornendo contesto al modello.\nIn questo lavoro, estendiamo CXMI a Pointwise CXMI, che può misurare l'uso del contesto a livello di frase o a livello di parola.\nPossiamo pensare a parole che hanno un P-CXMI elevato come a quelle che necessitano del contesto per la traduzione.\nOra analizziamo le parole con un P-CXMI elevato per cercare modelli tra queste.\nDopodiché eseguiamo la nostra analisi sulle trascrizioni dei discorsi TED tradotti dall'inglese in 14 lingue diverse.\nEseguiamo la nostra analisi su tre diversi livelli.\nIn primo luogo, esaminiamo i tag per parti del discorso che hanno un P-CXMI medio elevato.\nQuesto ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un P-CXMI relativamente alto.\nE questo può essere spiegato in base al fatto che l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo.\nAllo stesso modo, scopriamo che anche alcune lingue necessitano del contesto per la scelta della forma verbale appropriata.\nEsaminiamo quindi gli elementi del vocabolario che hanno un P-CXMI medio elevato su tutte le sue diverse occorrenze.\nQuesto ci aiuta a identificare casi come quello in oggetto, dove in cinese è necessario il contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento.\nParallelamente, scopriamo che il contesto è importante per tradurre nella corretta formalità.\nInfine, esaminiamo diversi token individuali che presentano un P-CXMI elevato.\nQuesto ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione delle ellissi.\nOra usiamo i risultati della nostra analisi per progettare un parametro di riferimento per la traduzione a livello di documento.\nPer ciascuno dei cinque fenomeni del discorso che abbiamo identificato, creiamo dei tagger per individuare automaticamente le parole che rimandano al fenomeno in questione.\nAbbiamo chiamato il nostro tagger Multilingual Discourse-Aware o tagger MuDA.\nPossiamo anche notare che lingue diverse presentano proporzioni diverse di questi fenomeni del discorso.\nQuindi, ricorriamo al tagger MuDA applicandolo su un corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione scelte sugli esempi dipendenti dal contesto che il tagger MuDA ha identificato.\nInfine, utilizziamo il nostro parametro di riferimento e altre metriche per valutare diversi modelli sulla traduzione automatica a livello di documento.\nPrima di tutto, quando utilizziamo metriche a livello di corpus: quindi, con riferimento a BLEU, scopriamo che i modelli indipendenti dal contesto offrono le migliori prestazioni.\nMa poi, se usiamo COMET, i modelli sensibili al contesto funzionano meglio.\nE se usiamo la word f-measure, allora i modelli con e senza contesto hanno prestazioni comparabili.\nCiò dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se utilizziamo solo metriche a livello di corpus.\nOra, usiamo il benchmark MuDA per valutare i modelli e scopriamo che i modelli sensibili al contesto sono significativamente più accurati dei modelli che non usano il contesto per alcuni fenomeni del discorso quali, ad esempio, la formalità e la coesione lessicale.\nMa questi modelli non sono migliori dei modelli che non usano il contesto su altri fenomeni come ellissi, pronomi e forma verbale.\nQuesto suggerisce dove avremmo bisogno di vedere più progressi per la traduzione a livello di documento.\nAbbiamo anche confrontato diversi sistemi commerciali e il nostro parametro di riferimento mostra che DeepL è solitamente più accurato di Google Translate per la traduzione a livello di documento.\nPer riassumere, eseguiamo un'analisi basata sui dati su 14 coppie di lingue per identificare quando le traduzioni necessitano del contesto; quindi, utilizziamo i nostri risultati per costruire un parametro di riferimento per la traduzione automatica a livello di documento che può aiutarci a identificare quali modelli di fenomeni del discorso possono gestire bene o meno, e quali sistemi di traduzione si rivelano validi nella traduzione a livello di documento.\nGrazie mille per l’attenzione.\nCi vediamo a Toronto.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "xUDLtuhJUS.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 19, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/xiSxNRoOzm.wav", "src_ref": "Hi everyone. I'm Jenny, a first year PhD student at Carnegie Mellon University and today I'll be presenting your work NLPositionality characterising design biases of datasets and Models. This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke and Maarten Sap. So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content. You might turn towards a popular API like Prospective API for toxicity detection, and this works really well if you're Carl Jones. Where prospective API is able to detect correctly toxic instances. But that's not really the case for Aditya Sharma. Where prospective AP is really not as sensitive to offensive terms that are more common in Indian contexts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces. And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make. And so one question that people might ask is, do datasets and models have positionality? And we're not trying to say that models themselves in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people, and can thus represent certain positionalities over others. So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps and models and data sets, as well as theoretical definitions of model positionality. However these works really don't look at comparing end users with the datasets and models themselves, and studying model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterise how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs. So to study data set and model positionality, we actually compare the annotations with real users with existing datasets and models. We do this through our framework NLPositionality. Our framework works in two main steps. The first step is to re annotate data sets with diverse annotators. And we ought to do this over looking at the demographics of original data sets annotators, because, usually only a few annotators annotate each instance and because demographics are rarely collected and shared. And so we opt to re annotate data to get many annotates for instance and to get a rich set of demographic data. We then take the annotations by demographic and compare them to the models and datasets using a Pearson's R correlation score, and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions. Our frame is largely enabled through Lab in the Wild and online crowdsourcing platform for where HCI collaborator. In Live in the Wild is an online experimentation platform where we can recruit divers volunteers. Compared to the platforms like M Turk which largely have participants from the US or India and further Lab in the Wild still is able to get high quality data. We host 2 tasks on lab in the wild, one of them being social acceptability, and the way this works is that participants will read a situation from the social chemistry dataset and, then they'll write how socially acceptable a situation is. Afterwards to stay engaged in the study, they can compare their responses to an AI and others. We've then compared these, annotations with Social Chemistry, Delphi and GPT 4. We then replicate a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynahate and write whether they think it's instance of hate speech. We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate Roberta and GPT 4. Our study in the end amassed over 16,000 annotations from over 1000 annotators from 87 countries. So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP. For example, we find that data sets and models are most aligned to English speaking countries. So for the GPT 4 social acceptability analysis, we find that it's most aligned to confucian and English speaking countries. We find that Dynahate is also most aligned to English speaking countries. We also find most additional alignment with people who have a college education. So for GPT 4, in the social acceptability task, we find that it's most aligned to people with a college education or Graduate School education and we find the same for Dynahate where it's most aligned to people with a college education. However, when models and data sets are aligned to specific populations, some are inevitably left behind. An example of this is that datasets and models are less aligned to non binary people compared to the men and women counterparts. We find this in the GPT 4 social acceptability task as well as the Dynahate task analysis as well. So, given that there is positionality in NLP, what can we do about it? So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism. Our third recommendation is to build specialised datasets and models within 4 specific communities. And a good example of this is the Masakhani initiative. I mean, we want to emphasise that inclusive NLP isn't just making. You know, all technologies work for everyone. And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.", "tgt_ref": "Ciao a tutti.\nSono Jenny, studente al primo anno di dottorato alla Carnegie Mellon University e oggi presenterò il lavoro NLPositionality che caratterizza i pregiudizi di progettazione di set di dati e modelli.\nQuesto lavoro è stato svolto in collaborazione con alcuni membri dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.\nQuindi, iniziamo immaginando che stiate lavorando per un giornale e stiate setacciando i commenti sotto il vostro articolo di notizie con l'obiettivo di rimuovere i contenuti tossici.\nPotreste rivolgervi a un'API popolare come Prospective API per il rilevamento della tossicità, e questo funziona davvero bene se siete Carl Jones.\nDove l'API Prospective è in grado di rilevare correttamente le istanze tossiche.\nMa questo non è esattamente il caso di Aditya Sharma.\nDove Prospective API non è così sensibile ai termini offensivi che sono più comuni nei contesti indiani.\nQuesto è un esempio di pregiudizio progettuale in cui osserviamo differenze sistematiche nelle prestazioni della tecnologia tra le popolazioni.\nPregiudizi di progettazione come quello che abbiamo appena visto potrebbero verificarsi a causa della posizionalità dei ricercatori di NLP e degli sviluppatori di modelli.\nLa posizionalità corrisponde semplicemente alle prospettive che le persone hanno in base ai loro dati demografici, alla loro identità e alle loro esperienze di vita.\nQuesto è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer.\nE come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi esiti e risultati, in quanto può cambiare le decisioni che prendono i ricercatori.\nQuindi, una domanda che le persone potrebbero porsi è la seguente: i set di dati e i modelli hanno una posizionalità?\nE non stiamo cercando di dire che i modelli negli insiemi di dati stessi abbiano identità demografiche ed esperienze di vita, ma raccolgono giudizi e opinioni di persone reali e possono quindi rappresentare alcune posizioni rispetto ad altre.\nPertanto, il lavoro precedente ha suggerito alcune prove aneddotiche di posizionalità, come lacune culturali, modelli, set di dati e definizioni teoriche di posizionalità del modello.\nTuttavia, questi lavori non guardano al confronto degli utenti finali con i set di dati e i modelli stessi, e lo studio della posizionalità del modello e del set di dati è sempre più importante man mano che le attività di NLP diventano più soggettive e socialmente orientate. Ne consegue che è difficile spiegare il motivo per cui queste posizionalità sono distorte, perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API.\nQuindi, per studiare il set di dati e la posizionalità del modello, confrontiamo effettivamente le annotazioni da parte di utenti reali con set di dati e modelli esistenti.\nLo facciamo attraverso il nostro framework NLPositionality.\nIl nostro framework funziona in due fasi principali.\nLa prima è quella di riannotare i set di dati con diversi annotatori.\nPer procedere in tal senso, dovremmo osservare i dati demografici degli annotatori dei set di dati originali, perché, di solito, solo pochi annotatori annotano ogni istanza e perché i dati demografici vengono raccolti e condivisi solo di rado.\nE così scegliamo di riannotare i dati per ottenere, ad esempio, svariate annotazioni e per ottenere un ricco set di dati demografici.\nQuindi, prendiamo le annotazioni in base alle informazioni demografiche e le confrontiamo con i modelli e i set di dati utilizzando un punteggio di correlazione R di Pearson. Così facendo, il nostro framework differisce dalla letteratura sul disaccordo degli annotatori confrontando gli utenti finali con modelli e set di dati, previsioni ed etichette, anziché prendere in considerazione solo l'accordo degli annotatori o modellare le distribuzioni dei suddetti.\nIl nostro quadro è in gran parte supportato da Lab in the Wild e dalla piattaforma di crowdsourcing online per il collaboratore HCI.\nIn Live in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare diversi volontari.\nRispetto alle piattaforme come M Turk, che hanno in gran parte partecipanti dagli Stati Uniti o dall'India e oltre, Lab in the Wild è ancora in grado di acquisire dati di alta qualità.\nOspitiamo due attività su Lab in the Wild, una delle quali è l'accettabilità sociale, e il modo in cui funziona è che i partecipanti leggeranno una situazione dal set di dati di chimica sociale e poi scriveranno quanto sia socialmente accettabile una determinata situazione.\nSuccessivamente, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'IA e gli altri.\nAbbiamo quindi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4.\nReplichiamo quindi una configurazione molto simile per l'attività di rilevamento della tossicità e dell'incitamento all'odio, in cui leggeranno un'istanza da Dynahate e scriveranno se ritengono che si tratti di un'istanza di incitamento all'odio.\nAbbiamo quindi confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT 4.\nIl nostro studio alla fine ha raccolto oltre 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 Paesi.\nQuindi, ora siamo in grado di rispondere meglio a chi e cosa si allinea maggiormente con i set di dati e i modelli NLP.\nScopriamo che c'è posizionalità nell'NLP.\nAd esempio, rileviamo che i set di dati e i modelli sono più allineati ai Paesi anglofoni.\nQuindi, relativamente all'analisi di accettabilità sociale di GPT 4, scopriamo che è più allineato ai Paesi influenzati dal confucianesimo e quelli anglofoni.\nScopriamo che anche Dynahate è maggiormente allineato ai Paesi di lingua inglese.\nRiscontriamo, inoltre, un maggiore allineamento alle persone che hanno un'istruzione universitaria.\nQuindi, per GPT 4, nell'attività di accettabilità sociale, scopriamo che è più allineato alle persone con un'istruzione universitaria o con un'istruzione post-laurea, e lo stesso vale per Dynahate, anch'esso più allineato alle persone con un'istruzione universitaria.\nTuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni vengono inevitabilmente abbandonati.\nUn esempio di ciò è che i set di dati e i modelli sono meno allineati alle persone non binarie rispetto alle controparti maschili e femminili.\nLo riscontriamo nell'attività di accettabilità sociale di GPT 4 e anche nell'analisi delle attività di Dynahate.\nQuindi, dato che c'è posizionalità nell'NLP, cosa possiamo fare al riguardo?\nAbbiamo alcune raccomandazioni in merito.\nLa prima è tenere traccia di tutte le scelte di progettazione rilevanti durante il processo di ricerca.\nE l'altra è fare ricerca NLP con la lente del prospettivismo.\nLa nostra terza raccomandazione è quella di costruire set di dati e modelli specializzati all'interno di 4 community specifiche.\nE un buon esempio di ciò è l'iniziativa Masakhani.\nVogliamo sottolineare che l'NLP inclusivo non si limita alla sola creazione.\nSapete, tutte le tecnologie funzionano per tutti.\nE con questo termina la nostra presentazione.\nIn ogni caso, se volete saperne di più, non esitate a consultare la nostra dashboard per i risultati delle analisi più aggiornati e il nostro articolo.\nGrazie.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "xiSxNRoOzm.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
{"dataset_id": "mcif_v0.2", "dataset_type": "unseen", "sample_id": 20, "src_audio": "/Users/ahrii/Documents/workspace/Research/MTM/SLT/MCIF_V0.2/LONG_AUDIOS/yBDqNxQUwV.wav", "src_ref": "Hi! I'm going to talk about our work on \"Resolving Indirect Referring Expressions for Entity Selection\", in which we introduce the AltEntities Corpus. My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis. Our goal is to understand users’ language when they want to make a choice. Consider this alternative question. \"Did you mean 'Easy on Me' or 'I Gotta Feeling'?\" Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example by saying the name of the song \"Easy on Me\" or its position, \"the first one\". But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song. Or the pronunciations are too similar to each other and hard to disambiguate. Or when the user wants to specify a preference. Here are some examples of indirect references for example, \"the newer one\" or \"the song that's not energetic.\" This is an important problem in conversational systems and also for benchmarking LLMs' entity understanding. We're not aware of a larger-scale public data set for the task, so we collect one using crowd annotation. Our data set covers three different domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, \"Remember that song we were listening to yesterday?\" And with that, Bob sets the dialogue context. In the second speech bubble, Alice says, \"Do you mean 'Easy on Me' or 'I Gotta Feeling'?\" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, \"the newer one.\" We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question is generated as follows. We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia. Here are the different sampling methods we've used. When we move higher in the list, the entities become more similar to each other and it's usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name \"The Return\". The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don't necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song, and read about each song. Here's for example, the Google search result for the song \"Easy on Me.\" For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then, we asked the annotators to pick one of these entities, for example, here's the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, \"the one without words\", \"not the one with the 12 year old boy\", or \"the fictional one\", or \"comes from Azerbaijan\", and so on. The AltEntities Corpus has 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it's around 92 to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there's a lot of room for improvement. We've also shown that the models are domain-generalizable. Here is a link to our dataset. Thanks.", "tgt_ref": "Ciao!\nParlerò del nostro lavoro su \"Resolving Indirect Referring Expressions for Entity Selection\", in cui introduciamo il corpus AltEntities.\nMi chiamo Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis.\nIl nostro obiettivo è comprendere il linguaggio degli utenti quando intendono compiere una scelta.\nConsideriamo questa domanda alternativa.\n\"Intendevi 'Easy on Me' o 'I Gotta Feeling'?\"\nQui, un utente vuole scegliere una tra queste due canzoni.\nLa cosa più ovvia è usare un riferimento diretto, ad esempio pronunciando il nome della canzone \"Easy on Me\" o la sua posizione, \"la prima\".\nMa a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale.\nQuesto potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone.\nOppure le pronunce sono troppo simili tra loro e difficili da disambiguare.\nO, ancora, quando l'utente vuole specificare una preferenza.\nEcco alcuni esempi di riferimenti indiretti, ad esempio \"la più recente\" o \"la canzone non vivace\".\nQuesto è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione dell'entità dei LLM.\nNon siamo a conoscenza di alcun set di dati pubblici su larga scala per l'attività, quindi ne raccogliamo uno utilizzando l'annotazione della folla.\nIl nostro set di dati copre tre diversi domini: musica, libri e ricette.\nLa nostra metodologia di raccolta dei dati sottolinea l'informalità utilizzando una configurazione di completamento a fumetti.\nIl fumetto ha tre nuvolette.\nNella prima, Bob dice: \"Ricordi quella canzone che stavamo ascoltando ieri?\"\nIn tal modo, Bob imposta il contesto del dialogo.\nNella seconda, Alice dice: \"Intendi 'Easy on Me' o 'I Gotta Feeling'?\"\nChe è la domanda alternativa.\nE nella terza, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio \"la più recente\".\nLa prima e la seconda nuvoletta vengono generate automaticamente, mentre la terza deve essere compilata dall'annotatore.\nLa prima viene scelta tra alcuni suggerimenti manuali per dominio.\nLa seconda, che è la domanda alternativa, viene generata come segue.\nUsiamo sempre un modello semplice.\nIntendi A o B?\nDove A e B sono campioni da Wikipedia.\nEcco i diversi metodi di campionamento che abbiamo utilizzato.\nQuando ci spostiamo più in alto nell'elenco, le entità diventano più simili tra loro e di solito è più difficile disambiguarle.\nLa prima è casualmente uniforme.\nLa seconda è quando le entità hanno titoli simili, ad esempio due libri con il nome \"The Return\".\nLa terza è quando presentano descrizioni simili su Wikipedia.\nE, infine, quando hanno riquadri informativi o attributi simili su Wikipedia.\nAd esempio, lo stesso genere o lo stesso artista per una canzone.\nQuando mostriamo questa domanda alternativa agli annotatori, conoscono il nome di queste entità, ma non necessariamente le entità stesse.\nQuindi, quello che facciamo è mostrare alcune conoscenze di base sulle due entità.\nPer le canzoni, mostriamo semplicemente un link di ricerca di Google per ogni canzone e poi chiediamo agli annotatori di ascoltare alcune parti e di leggere qualcosa su ciascuna.\nEcco ad esempio il risultato della ricerca su Google per la canzone \"Easy on Me\".\nPer il dominio delle ricette e dei libri, mostriamo alcuni testi di sfondo da Wikipedia.\nPer le ricette, mostriamo anche le loro immagini, tratte sempre da Wikipedia, in modo che gli annotatori sappiano che aspetto hanno.\nSuccessivamente, abbiamo chiesto agli annotatori di scegliere una di queste entità, ecco ad esempio la prima, e descriverle usando da tre a cinque espressioni di riferimento indirette.\nAd esempio, quella con la musica per pianoforte.\nEcco alcuni esempi dal nostro set di dati.\nAd esempio, \"quella senza parole\", \"non quella con il ragazzo di 12 anni\", o \"quella immaginaria\", o \"viene dall'Azerbaigian\", e così via.\nL'AltEntities Corpus consta di 6.000 domande alternative in tre domini e di 42.000 espressioni di riferimento indirette.\nI risultati con il modello T5 XL sono riassunti di seguito.\nSe il modello linguistico ha accesso alle stesse identiche conoscenze di base degli annotatori, allora l'accuratezza è decisamente alta, intorno al 92-95%.\nMa ciò non è realistico.\nSe il modello linguistico ha accesso ad alcune conoscenze di base parzialmente sovrapposte, l'accuratezza è compresa tra l'82 e l'87%, il che è più realistico.\nAd esempio, quando il modello linguistico recupera le conoscenze di base.\nSe il modello linguistico ha accesso solo ai nomi delle entità, l'accuratezza è solo del 60%, quindi c'è molto margine di miglioramento.\nAbbiamo anche dimostrato che i modelli sono generalizzabili per dominio.\nEcco un link al nostro set di dati.\nGrazie.", "src_lang": "eng", "ref_lang": "ita", "benchmark_metadata": {"subset": "long", "task": "IF", "audio_file": "yBDqNxQUwV.wav", "sources": {"ref_xml": "MCIF0.2.IF.long.it.ref.xml", "src_text_source": "LONG_TEXTS"}}}
