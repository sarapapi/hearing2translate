{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫阿萨·法拉里，我将介绍我们的论文《利用微调转换器架构进行少量表格数据增强》。数据科学家分析数据，主要集中在操作数据的现有特征。但有时这些特征是有限的。使用另一个数据源生成特征可能会增加大量信息。我们的研究目标是自动使用外部来源的自由文本来增强表格数据。假设我们有一个表格数据集和一个知识库。我们需要一个自动过程，涉及实体链接和文本分析，以从知识库的自由文本中提取新特征。我们的框架FEST就是这个自动过程。让我们看一个例子。在输入到FEST的数据集中，这个例子中的数据集是大学数据集，其目标是将大学分类为低排名大学和高排名大学。作为知识库，我们使用维基百科。FEST的第一阶段是实体链接，每个实体，在这个例子中是大学名称，与知识库中的实体链接。知识库实体的文本被提取并添加到数据集中。在这个例子中，文本是维基百科页面的摘要。现在我们需要从检索的文本中生成或提取特征。因此，我们需要一个包括文本分析的特征提取阶段。这是本文的主要贡献，我将在接下来的幻灯片中深入探讨。在特征提取阶段之后，有一个特征生成阶段，我们使用提取的特征生成少量新特征。首先，生成与原始数据集类别数量相同的特征。在这个例子中，原始数据集有两个类别，所以首先生成两个新特征。但如果数据集有五个类别，首先生成五个新特征。每个特征代表每个类别的可能性。为了分析文本，我们使用当前的文本分析最新技术，即基于转换器的语言模型，如BERT、GPT、XLNet等。但我们不太可能使用输入数据集训练语言模型。因此，一个天真的方法是目标任务微调。因此，在特征提取阶段，我们可以下载预训练的语言模型，在目标数据集上微调语言模型。在这个例子中，微调语言模型以分类文本为类别，摘要为类别，低或高，接收语言模型输出，即每个类别的可能性，并将其用作新特征。这种方法的问题是数据集可能有少量不同的实体文本。在我们的实验中，几乎一半的数据集包含少于400个样本，最小的数据集包含35个样本。因此，在该数据集上微调语言模型将是无效的。但我们可以使用对预分析数据集的先验知识，因为我们在多个数据集上应用FEST，我们可以使用N-1个数据集来收集有关N-1个数据集的信息，并在分析N个数据集时使用这些信息。我们建议添加另一个微调阶段，即预备多任务微调阶段，在N-1个数据集上微调语言模型，然后执行另一个微调阶段，即目标任务微调，在N个目标数据集上微调语言模型。多任务微调的最新技术称为MT-DNN。在MT-DNN中，MT-DNN在训练集中维护与任务数量相同的头。因此，在这个例子中，训练集中有四个任务，所以MT-DNN维护四个头，如图所示。它从训练集中采样一个随机批次。如果随机批次属于，例如，单句分类任务，它通过第一个头执行前向和后向传递。如果随机批次属于成对排名任务，它通过最后一个头执行前向和后向传递。在我们的场景中，表格数据集的类别数量各不相同。因此，有许多任务。MT-DNN维护与类别数量相同的头，输出层。此外，MT-DNN需要为新数据集和新任务初始化新头。我们的方法称为任务重新表述微调。在我们的方法中，任务重新表述微调，我们不维护多个头，而是将每个数据集重新表述为句子对分类问题，即两个类别任务。让我们看一个例子。这是我们的输入数据集，由实体、特征、文本和类别组成。我们将任务从分类文本为低或高重新表述为分类文本、摘要和类别为真或假。换句话说，我们训练语言模型以分类摘要和类别，以确定摘要是否属于该类别。因此，标签向量在Zix的情况下始终由两个类别组成。这是我们重新表述微调方法的算法。让我们看看完整的框架。数据集输入到FEST，然后FEST执行实体链接阶段。它从知识库中提取文本，在这个例子中是维基百科页面的摘要。然后它将任务重新表述为句子对分类任务。将语言模型应用于新任务并输出每个类别的可能性。请注意，语言模型已经在N-1个数据集上进行了预备多任务微调。然后我们将语言模型的输出向量用作新生成的特征，其数量与类别数量相同。为了评估我们的框架，我们使用17个表格分类数据集，其大小、特征、平衡、领域和初始性能各不相同。作为知识库，我们使用维基百科。我们设计实验为留一法评估，在16个数据集上训练FEST，并将其应用于第17个数据集。我们还将每个数据集分成四个折叠，并应用四折交叉验证。然后我们生成新特征，并使用五个评估分类器对其进行评估。我们在实验中使用基于BERT的架构。这是我们实验的结果。您可以看到我们将我们的框架与目标数据集微调、目标任务微调和MT-DNN预备微调进行比较，我们的重新表述微调实现了最佳结果，最佳性能，而MT-DNN在目标数据集微调上实现了2%的改进，我们的方法实现了6%的改进。当我们查看小数据集时，我们可以看到MT-DNN的性能下降，预备多任务微调阶段的改进下降到1.5%，但我们的性能增加到11%，与目标任务微调相比。总之，FEST使得从35个样本开始的少量增强成为可能。它对所有任务数据集使用一种架构，并保持模型的头。但它添加了重新表述阶段。它增强了训练集，并需要一个具有语义意义的目标值，以便我们可以将其输入到语言模型中，并在句子对分类问题中使用它。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。今天我要介绍我们的研究工作，学习推理，将复杂的推理提取作为数学问题解决方案。我是来自拜登人工智能实验室的艾伦，这是与德克萨斯大学奥斯汀分校的杰瑞和苏丹大学的韦鲁合作的成果。首先，我想谈谈我们推理的动机。这里我们展示了多步推理有助的例子。这个图表来自Pound论文，他们在少量学习场景中进行提示以解决数学问题。在左侧，我们可以看到如果我们只给出一些示例和答案，我们可能无法得到正确的答案。但如果我们给出更多的推理描述，模型能够预测推理描述，并做出正确的预测。因此，具有可解释的多步推理作为输出是有益的。我们也认为数学问题解决是评估这种推理能力的直接应用。在我们的问题设置中，给定问题，我们需要解决这个问题并获得数值答案。在我们的数据集中，我们还给出了导致这个特定答案的数学表达式。某些假设也适用于之前的工作。我们假设数量的精度是已知的，我们只考虑基本运算符，如加法、减法、乘法、除法和指数。此外，复杂的运算符实际上可以分解为这些基本运算符。因此，之前的数学问题解决工作实际上可以分类为序列到序列和序列到树模型。传统的序列到序列模型将表达式转换为特定的序列进行生成，实现起来非常容易，并且可以推广到许多不同的复杂问题。但其缺点是性能通常不如结构模型，并且缺乏预测的可解释性。但由于变压器模型，这个方向仍然非常受欢迎。在基于树的模型中，我们实际上将这些表达式结构化为树形，并遵循树生成中的前序遍历。在这里，我们不断生成运算符，直到我们到达叶子，即数量。这里的好处是它实际上给了我们这个二叉树结构，但它实际上是相当不直观的，因为我们首先生成运算符，然后在最后生成数量。第二个问题是它也包含一些重复的计算。如果我们看这个表达式，8乘以3加3实际上生成了两次，但实际上我们应该重用结果。在我们的提议方法中，我们希望以逐步和可解释的方式解决这些问题。例如，在第二步，我们可以获得这个除数，即27。我们还可以参考原始问题以找到相关内容。在这些步骤中，我们获得了除数。然后在第三步，我们实际上得到了商。在这三步之后，我们实际上可以重用第二步的结果，然后得到第四步的结果。最后，我们可以获得被除数。在这里，我们实际上直接生成整个表达式，而不是生成单个运算符或数量。这使得过程更加准确。在我们的演绎系统中，我们首先从问题中呈现的一堆数量开始，并包括一些常数作为我们的初始状态。表达式由EIJOP表示，我们从QI到QJ执行运算符，这种表达式实际上是有方向的。我们还在这里有减法反向来表示相反的方向。这与关系提取非常相似。在形式化的演绎系统中，在时间步T，我们在QI和QJ对之间应用运算符，然后我们获得这个新表达式。我们将其添加到下一个状态，成为一个新数量。这张幻灯片实际上可视化了状态的演变，我们不断将表达式添加到当前状态。在我们的模型实现中，我们首先使用预训练的语言模型，可以是BERT或Roberta，然后我们编码句子，然后我们获得这些数量表示。一旦我们获得数量表示，我们就可以开始进行推理。这里我们展示了一个Q1的示例，以获得Q1除以Q2然后乘以Q3的表示。首先，我们获得对表示，基本上是Q1和Q2之间的连接。然后我们应用一个前向网络，该网络由运算符参数化。最后，我们获得表达式表示Q1除以Q2。但在实践中，在推理阶段，我们可能会获得不正确的表达式。因此，所有可能的表达式等于运算符数量的三倍。这里的好处是我们可以轻松添加约束来控制这个搜索空间。例如，如果这个表达式不允许，我们可以简单地从我们的搜索空间中删除这个表达式。因此，在第二步，我们做同样的事情，但唯一的区别是一个数量。这个数量来自之前计算的表达式。最后，我们可以获得这个最终表达式Q3乘以Q4。我们还可以看到所有可能表达式的数量与前一步不同。这种差异使得应用束搜索变得困难，因为这两步之间的概率分布是不平衡的。因此，训练过程类似于训练序列到序列模型，我们在每个时间步优化损失。在这里，我们还使用这个tau来表示我们应该何时终止这个生成过程。这里的空间与序列到序列不同，因为每个时间步的空间不同，而在传统的序列到序列模型中，它是词汇量的数量。它还允许我们从先验知识中施加某些约束。我们在常用的数学问题数据集上进行实验，包括MAWPS、Math23K、MathQA和SWAMP。这里我们简要展示了与之前最佳方法的结果比较。我们的最佳性能网络是Roberta演绎推理器，实际上我们没有使用束搜索，而其他方法使用束搜索。最佳方法通常是基于树的模型。因此，总体而言，我们的推理器能够显著超越这个基于树的模型，但我们可以看到MathQA或SWAMP上的绝对数字并不高。因此，我们进一步调查SWAMP上的结果。这个数据集具有挑战性，因为作者试图手动添加一些内容来混淆NLP模型，例如添加环境信息和额外的数量。在我们的预测中，我们发现一些中间值实际上是负数。例如，在这些问题中，我们问杰克有多少苹果，但我们有一些额外的信息，如17个桃子，史蒂芬有8个桃子，这完全是环境。因此，我们的模型做出了一些预测，产生了负值。我们观察到这两个表达式实际上有相似的得分。因此，我们实际上可以通过删除这些结果是负数来限制这个搜索空间，以便我们可以使答案正确。因此，我们进一步发现这种约束实际上对某些模型改进了很多。例如，对于BERT，我们改进了7点，然后对于基于Roberta的模型，我们实际上改进了2点。因此，更好的语言模型具有更好的语言理解能力，因此这里的数字对于Roberta更高，对于BERT更低。我们还试图分析所有这些数据集背后的难度。我们假设未使用数量的数量可以被视为这里的无关信息。因此，我们可以看到我们有未使用数量的样本百分比，SWAMP数据集有最大比例。我们还展示了整体性能。对于那些没有未使用数量的样本，整体性能实际上高于整体性能。但对于那些有未使用数量的样本，性能实际上远低于整体性能。对于MAWPS，我们没有太多这些情况。因此，我忽略了这一部分。最后，我们想通过一个崩溃和预测的例子来展示可解释性。在这里，我们的模型实际上在第一步做出了错误的预测。因此，我们实际上可以将这个表达式与这个句子相关联。因此，我们认为这个句子可能误导模型做出不正确的预测。因此，在这里，种植另一个35使模型认为它应该是一个加法运算符。因此，我们试图修改句子，使其成为类似于梨树的数量比苹果树少55的东西。因此，我们使其传达更准确的语义，以便模型能够做出正确的预测。因此，这项研究展示了可解释的预测如何帮助我们理解模型行为。因此，总结我们的工作。首先，我们的模型实际上非常高效，我们能够提供可解释的解决方案过程。我们可以轻松地将一些先验知识作为约束，这可以帮助改进性能。最后，底层机制不仅适用于数学问题解决任务，还适用于涉及多步推理的其他任务。但我们也有一些限制。如果我们有大量的运算符或常数，内存消耗可能会很高。第二个问题是，如前所述，由于不同时间步之间的概率分布不平衡，因此应用束搜索策略也非常具有挑战性。这是演讲的结束，欢迎提问。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是安托万，来自马斯特里赫特大学。我将与杰里一起介绍我们的联合工作，主题是新的法规条文检索数据集。法律问题在许多人生活中占有重要地位。然而，大多数公民对他们的权利和基本法律程序知之甚少。结果，许多无法负担昂贵法律专家帮助的弱势公民得不到保护，甚至被剥削。我们的工作旨在通过开发有效的法规条文检索系统，弥合人与法律之间的差距。这样的系统可以为非专业人士提供免费的专业法律帮助服务。在深入探讨这项工作的主要贡献之前，让我们首先描述法规条文检索的问题。给定一个关于法律问题的简单问题，例如“如果我违反职业保密义务，我会面临什么风险？”，模型需要从大量的立法中检索出所有相关的法规条文。这种信息检索任务带来了自己的挑战。首先，它涉及两种语言：问题使用的是普通自然语言，而法规使用的是复杂的法律语言。这种语言分布的差异使得系统难以检索相关的候选条文，因为它间接需要一个内在的解释系统，能够将自然问题翻译成与法规术语匹配的法律问题。此外，法规不是一堆可以独立处理的条文，就像新闻或食谱一样。相反，它是一个结构化的法律条文集合，只有在整体上下文中才有完整的意义，即与其相邻条文、所属的领域和子领域以及在法律结构中的位置一起考虑。最后，法规条文不是小段落，通常是大多数检索工作中的典型检索单元。在这里，它们是长达6000个单词的文档。自然语言处理的最新进展激发了许多法律任务的巨大兴趣，例如法律判决预测或自动合同审查，但法规条文检索主要未受触动，原因是缺乏大量高质量的标记数据集。在这项工作中，我们介绍了一个新的、本土化的、以公民为中心的数据集，以研究检索模型是否能够接近法律专家在法规条文检索任务中的效率和可靠性。我们的比利时法规条文检索数据集（SARTS）包含超过1100个由比利时公民提出的法律问题。这些问题涵盖了从家庭、住房、金钱到工作和社会保障等广泛主题。每个问题都由经验丰富的法律专家标记，并附有来自超过22,600个比利时法律条文的相关条文引用。现在让我们谈谈如何收集这个数据集。首先，我们编制了一个大型的法律条文语料库。我们考虑了32个公开可用的比利时法典，并提取了所有条文及其相应的章节标题。然后，我们收集了带有相关法规引用的法律问题。为此，我们与一家每年收到约4000封来自比利时公民的电子邮件的比利时律师事务所合作，这些公民寻求个人法律问题的建议。我们很幸运地获得了他们的网站访问权限，他们的法律专家团队在那里解答比利时最常见的法律问题。我们收集了成千上万个带有类别、子类别和相关法规引用的问题。最后，我们解析了法律引用，并过滤掉引用不在我们考虑的法典中的问题。剩下的引用被匹配并转换为我们语料库中相应的条文ID。最终，我们得到了1,108个问题，每个问题都仔细标记了我们大型语料库中相关条文的ID。此外，每个问题都有一个主类别和子类别的串联，每个条文都有一个在法律结构中的后续标题的串联。这些额外信息在当前工作中没有使用，但可能对未来的法律信息检索或法律文本分类研究有兴趣。让我们看看我们数据集的一些特征。问题的长度在5到44个单词之间，中位数为40个单词。条文要长得多，中位数长度为77个单词，其中142个超过1000个单词，最长的达5,790个单词。如前所述，问题涵盖了广泛的主题，其中约85%涉及家庭、住房、金钱或司法，而剩下的15%涉及社会保障、外国人或工作。条文也非常多样化，因为它们来自32个不同的比利时法典，涵盖了大量的法律主题。以下是从每个比利时法典收集的条文总数。在22,633个条文中，只有1,612个被认为与数据集中的至少一个问题相关。其中约80%的引用条文来自民法典、司法法典、刑事调查法典或刑法典。与此同时，32个法典中的18个在至少一个问题中被提及的条文少于5个，这可以解释为这些法典更少关注个人及其关切。总的来说，这些引用条文的中位引用次数为2，不到25%的条文被引用超过5次。使用我们的数据集，我们对几种检索方法进行了基准测试，包括词汇和密集架构。给定一个查询和一个条文，词汇模型通过计算查询术语在该条文中的权重之和，为查询-条文对分配一个分数。我们实验了标准的TF-IDF和BM25排名函数。这些方法的主要问题是它们只能检索包含查询中关键词的条文。为了克服这一限制，我们实验了一种基于神经网络的架构，可以捕捉查询和条文之间的语义关系。我们使用了一个B-encoder模型，将查询和条文映射到密集向量表示，并通过它们的嵌入的相似性计算查询-条文对的相关分数。这些嵌入通常是对词嵌入模型输出的池化操作的结果。首先，我们在零次评估设置中研究了Siamese B-encoders的有效性，这意味着预训练的词嵌入模型是直接使用的，而不进行任何额外的微调。我们实验了上下文无关的文本编码器，即Word2Vec和FastText，以及上下文相关的嵌入模型，即Roberta，特别是Camembert，这是一个法语Roberta模型。此外，我们在我们的数据集上训练了基于Camembert的B-encoder模型。请注意，在训练中，我们实验了B-encoder架构的两种风格。Siamese使用一个唯一的词嵌入模型，将查询和条文一起映射到共享的密集向量空间。Two Tower使用两个独立的词嵌入模型，分别将查询和条文编码到不同的嵌入空间。我们实验了均值、最大值和CLS池化，以及点积和余弦计算相似性。以下是我们在测试集上的基准结果，词汇方法在上方，在零次评估设置中评估的Siamese B-encoders在中间，微调的B-encoders在下方。总的来说，微调的B-encoders显著优于所有其他基准。Two Tower模型在100次召回率上优于其Siamese变体，但在其他指标上表现相似。尽管BM25在训练的B-encoder上表现不佳，但其性能表明它仍然是领域特定检索的强基准。关于Siamese B-encoder的零次评估，我们发现直接使用预训练的Camembert模型的嵌入，而不优化信息检索任务，会产生不佳的结果，这与以前的发现一致。此外，我们观察到基于Word2Vec的B-encoder显著优于基于FastText和Bird的模型，这表明预训练的词级嵌入可能比字符级或子词级嵌入更适合该任务，当直接使用时。尽管这些结果令人鼓舞，但它们表明与能够检索到任何问题的所有相关条文的法律专家相比，仍有大量改进的机会，从而获得完美的分数。让我们通过讨论数据集的两个限制来总结。首先，条文语料库仅限于从32个考虑的比利时法典中收集的条文，这并未涵盖整个比利时法律，因为法令、指令和法规的条文被忽略了。在数据集构建过程中，所有对这些未收集条文的引用都被忽略，导致一些问题最终只剩下其初始相关条文数量的一部分。这种信息损失意味着剩下的相关条文中的答案可能是不完整的，尽管仍然是完全适当的。其次，我们应该注意到，并非所有法律问题都能通过法规本身来回答。例如，问题“如果我的租户噪音太大，我可以驱逐他们吗？”可能在法规中没有详细的答案，量化允许驱逐的特定噪音阈值。相反，房东可能需要更多地依赖案例法，并找到类似当前情况的先例。例如，租户每周举办两次派对，直到凌晨2点。因此，一些问题比其他问题更适合法规条文检索任务，而不太适合的领域仍需确定。我们希望我们的工作能激发开发实用且可靠的法规条文检索模型的兴趣，以帮助改善所有人的司法公正。您可以在以下链接查看我们的论文、数据集和代码。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我们很高兴向大家介绍我们的工作，即 VALSE，这是一个任务独立的基准测试，旨在测试视觉和语言模型的特定语言现象。我们为什么要费力设置这个基准测试呢？在过去几年里，我们看到了基于 Transformer 的视觉和语言模型的爆炸式增长，这些模型在大量图像文本对上进行了预训练。每一个模型都在视觉和语言任务上推动了最新的技术，例如视觉问答、视觉常识推理、图像检索、短语定位。因此，我们得出结论，这些任务特定基准测试的准确性在稳步提高。但我们真的知道这些模型到底学到了什么吗？当视觉和语言 Transformer 为这张图像和这句句子分配高分，而为这张图像分配低分时，它到底理解了什么？视觉和语言模型是专注于正确的东西，还是像之前的工作所示，专注于偏见？为了更好地了解这一点，我们提出了一个更加任务无关的方向，并引入了 VALSE，它测试了视觉和语言模型对影响语言和视觉模态的特定语言现象的敏感性。我们的目标是存在、多重性、计数、空间关系、动作和实体共指。但我们如何测试视觉和语言模型是否捕捉到了这些现象呢？通过欺骗，这是一种方法，以前只应用于视觉和语言模型的非短语，由拉维·谢卡尔和他的合作者，以及我们在之前的工作中应用于计数。欺骗基本上意味着我们取图像的标题并通过改变标题使其不再描述图像来生成一个欺骗。我们通过专注于六个特定部分来进行这些短语改变，例如存在、多重性、计数、空间关系、动作和实体共指，每个部分可以由一个或多个工具组成，如果我们发现了多种有趣的方法来创建欺骗实例。例如，在动作部分，我们有两个工具，一个是将动作动词改为不同的动作，另一个是交换动作。计数和共指也是有多个工具的部分。我们通过确保这些欺骗不描述图像、语法正确且其他有效句子来创建这些欺骗。这并不容易做到，因为欺骗标题可能比原始标题不太可能。例如，虽然这并非不可能，但从统计上讲，植物切割人类的可能性比人类切割植物的可能性小，大型视觉和语言模型可能会注意到这一点。因此，为了获得有效的欺骗，我们必须采取行动。首先，我们利用强大的语言模型来提出欺骗。其次，我们使用自然语言推理或简称 NLI 来过滤可能仍然描述图像的欺骗，因为在构建欺骗时，我们需要确保它们不描述图像。为了自动测试这一点，我们应用自然语言推理，理由如下。我们将图像视为前提，其标题为其蕴含的假设。此外，我们将标题视为前提，欺骗为其假设。如果 NLI 模型预测欺骗与标题相矛盾或中立，我们将其视为有效欺骗的指示器。如果 NLI 预测欺骗由标题蕴含，它就不能是一个好的欺骗，因为通过传递性，它将给出图像的真实描述，我们将这些欺骗过滤掉。但这个过程并不完美。它只是有效欺骗的指示器。因此，作为生成有效欺骗的第三种措施，我们雇佣人类标注员来验证 VALSE 中使用的数据。因此，在过滤和人类评估后，我们有与本表中描述的测试实例一样多的测试实例。请注意，VALSE 不提供任何训练数据，只提供测试数据，因为它是一个零次测试基准测试。它旨在利用视觉和语言模型在预训练后的现有能力。微调只会使模型利用数据中的伪影或统计偏差。我们都知道这些模型喜欢作弊和走捷径。正如我们所说，我们有兴趣评估视觉和语言模型在预训练后的能力。我们在 VALSE 上对五个视觉和语言模型进行了实验，即 Clip、AlexMert、Wilbert、Wilbert 12 in 1 和 VisualBert。我们最重要的评估指标之一是模型在将图像句子对分类为标题和欺骗方面的准确性。也许对这个视频更相关的是，我们将展示我们更宽松的指标，即成对准确性，它衡量图像句子对齐分数是否大于正确的图像文本对，而不是其欺骗对。有关更多指标及其结果，请查看我们的论文。成对准确性的结果如下，与我们从其他指标获得的结果一致。最佳零次性能由 Wilbert 12 in 1 实现，其次是 Wilbert、AlexMert、Clip，最后是 VisualBert。值得注意的是，以个别对象为中心的工具，如存在和名词短语，几乎被 Wilbert 12 in 1 解决，突显了模型能够识别图像中命名对象及其存在。然而，在我们的对抗欺骗设置中，没有其他部分可以可靠地解决。我们从多重性和计数工具中看到，视觉和语言模型在区分单个对象与多个对象的引用或在图像中计数它们方面存在困难。关系部分表明，它们在正确分类图像中命名对象之间的空间关系方面存在困难。即使受到可信偏见的支持，它们也难以区分动作并识别其参与者，如我们在动作部分中看到的那样。从共指部分，我们发现使用代词在图像中跟踪多个引用到同一对象也是视觉和语言模型的难点。作为一个健全性检查，因为这是一个有趣的实验，我们还对两个仅文本模型 GPT-1 和 GPT-2 进行了基准测试，以评估 VALSE 是否可以由这些单模型解决，通过计算正确和欺骗标题的困惑度（没有图像），并预测困惑度最低的条目。如果欺骗的困惑度更高，我们将其视为欺骗标题可能受到可信偏见或其他语言偏见的影响。有趣的是，在某些情况下，仅文本 GPT 模型比视觉和语言模型更好地捕捉到了世界的可信性。总之，VALSE 是一个使用语言结构的镜头来帮助社区通过严格测试其视觉定位能力来改进视觉和语言模型的基准测试。我们的实验表明，视觉和语言模型能够很好地识别图像中命名对象及其存在，如存在部分所示，但当被迫尊重语言指示器时，它们在定位其相互依赖性和视觉场景中的关系方面存在困难。我们真的希望鼓励社区使用 VALSE 来衡量视觉和语言模型的语言定位进展。此外，VALSE 可以用作数据集的间接评估，因为模型可以在训练或微调前后进行评估，以查看数据集是否有助于模型在 VALSE 测试的任何方面取得进展。如果你感兴趣，请查看 GitHub 上的 VALSE 数据，如果你有任何问题，请随时联系我们。"}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自东京大学的Kamizawa。我将介绍一篇题为《R&Sum：基于提交日志总结的大规模自动发布说明数据集》的论文。我将按照以下顺序进行介绍。首先，我将介绍我们在这项研究中正在进行的自动发布说明生成。发布说明是一种技术文档，总结了每次软件产品发布的变更。图像显示了BuildJS库版本2.6.4的发布说明。发布说明在开源开发中起着重要作用，但手动准备需要大量时间。因此，能够自动生成高质量的发布说明将非常有用。我将参考两项关于自动发布说明生成的先前研究。第一项是2014年发布的名为Arena的系统。它采用基于规则的方法，例如使用变更提取器从发布之间的差异中提取核心差异、库变更和文档变更，并最终将它们结合在一起。该系统最显著的特点是右上角的问题提取器，它必须链接到Jira问题跟踪系统，并且只能应用于使用Jira的项目。换句话说，它不能用于GitHub上的许多项目。第二项是2020年宣布的Griff。它可以在互联网上使用，并且可以通过pip存储。该系统具有简单的基于学习的文本分类模型，并为每个输入的提交消息输出五个标签之一，例如功能或修复。图像是一个示例用法，返回一个集体或修复标签。Griff的训练数据相当小，大约5000个，并且在下面描述的实验中显示，文本分类模型的性能不高。我介绍了两项相关研究，但它们存在适用性有限和数据资源稀缺的问题。我们的论文解决了这两个问题，并自动生成高质量的发布说明。对于适用性有限的问题，我们提出了一种仅使用提交消息作为输入的高质量分类总结方法。该方法可以用于所有英语存储库。对于数据资源稀缺的问题，我们通过使用GitHub API从公共GitHub存储库收集数据，构建了一个包含约82,000个数据的R&Sum数据集。接下来，我将描述我们的数据集。这是一个数据示例。左侧是提交消息，右侧是发布说明。发布说明被标记为改进、修复等。我们设置了一个任务，将提交消息作为输入，并输出标记的发布说明。这可以被视为一个总结任务。我们预先定义了所有标签，包括功能、改进、修复、删除和重大变更。这些标签是基于先前研究和其他事实设置的。右下角的发布说明是从左下角显示的发布说明中提取的。此时，需要检测预先设置的四个标签。但标签并不总是与每个存储库一致。例如，改进标签包括改进、增强、优化等。我们为每个这些符号变体准备了一个词汇表，并使用它来检测发布说明类，并纠正类后面的文本作为发布说明句子。接下来是提交消息。提交消息不是与每个发布绑定的，如下图所示。如果当前发布是版本2.5.19，我们需要识别前一个发布版本2.5.18，并获取其差异。这有点棘手，仅获取发布列表并查看前后是不够的。我们创建了一个启发式匹配规则来获取前一个和下一个版本。数据集分析。最终，收集了7,200个存储库和82,000个数据。此外，发布说明令牌的平均数量为63，这对于总结任务来说相当高。此外，唯一令牌的数量相当大，为8,830,000。这是由于在存储库中发现了大量唯一的类和方法名称。接下来，我将解释所提出的方法。分类提取和抽象总结模型由两个神经模块组成，一个使用BART或CODBART的分类器，一个使用BART的生成器。首先，CAS使用分类器将每个提交消息分类为五个发布说明类，我们选择改进、修复、删除和其他。将提交消息分类为其他或丢弃。然后，CAS将生成器应用于四个标签文档，并为每个类生成发布说明。在这个任务中，提交消息和发布说明之间的直接对应关系是未知的。因此，为了训练分类器，我们使用每个提交消息的前10个字符为每个输入提交消息分配两个标签。我们通过两种不同的方法对分类抽象总结进行建模。第一个模型，我们称之为CAS单个，由一个单一的Sec2Sec网络组成，并生成一个单一的长发布说明文本，给出输入提交消息的串联。输出文本可以根据特殊的类特定端点符号分为分类段。第二种方法，我们称之为CAS多个，由四个不同的Sec2Sec网络组成，每个网络对应于发布说明类之一。好的，让我解释实验。比较了五种方法：CAS、CAS单个、CAS多个、Plustering和先前研究Griff。关于评估，在某些情况下，发布说明是以多个句子输出的。由于难以计算句子数量为零，因此它们与空格结合在一起，并被视为一个长句子。当系统输出短句子时，蓝色会受到惩罚。这种惩罚导致实验结果中蓝色值较低。最后，我们还计算了特异性，因为如果发布说明为空，则无法计算Rouge和Blue。高特异性意味着模型在发布说明为空的情况下正确输出空文本。以下是结果。由于数据集包含电子邮件地址、哈希值等，我们还评估了清理后的数据集，排除了它们。CAS和CAS的Rouge-L分数比基线高出10分以上。特别是在清理后的测试集上，所提出方法与基线之间的分数差距跳至20分以上。这些结果表明CAS和CAS显著有效。CAS的Rouge-L分数比CAS更好，表明将分类器和生成器结合在一起是有效的，并且使用两个标签训练分类器。CAS的高覆盖率可能是因为分类器可以专注于为每个类选择相关的提交消息。CAS多个倾向于比CAS单个具有更高的Rouge-L，表明为每个发布说明类独立开发不同的抽象总结模型也是有效的。以下是错误分析。CAS方法倾向于输出比人类参考句子更短的句子。右侧图像中的参考句子有三或四个句子，而CAS只有一个。这种模型倾向的原因是，在训练数据中，只有33%的句子出现在功能标签中，40%出现在改进标签中。此外，CAS方法无法在没有额外信息的情况下生成准确的发布说明。右上角的示例是一个非常混乱的提交消息的示例，完整的句子无法在没有参考相应的拉取请求或问题的情况下生成。下面的示例显示输入中的两个提交消息是相关的，并且应该合并为一个句子，但它无法做到这一点。最后，结论。我们构建了一个新的数据集，用于自动发布说明生成。我们还将任务定义为输入提交消息并总结它们，使其适用于所有用英语编写的项目。我们的实验表明，所提出的方法生成的发布说明噪声较少，覆盖率较高。请查看我们的数据集。谢谢。"}
