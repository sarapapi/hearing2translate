<?xml version="1.0" encoding="UTF-8"?>
<mteval>
<srcset setid="iwslt-ACLtest2023" srclang="English">
<doc docid="2022.acl-long.410" genre="presentations">
<talkid>2022.acl-long.410</talkid>
<abstract>Solving math word problems requires deductive reasoning over the quantities in the text. Various recent research efforts mostly relied on sequence-to-sequence or sequence-to-tree models to generate mathematical expressions without explicitly performing relational reasoning between quantities in the given context. While empirically effective, such approaches typically do not provide explanations for the generated expressions. In this work, we view the task as a complex relation extraction problem, proposing a novel approach that presents explainable deductive reasoning steps to iteratively construct target expressions, where each step involves a primitive operation over two quantities defining their relation. Through extensive experiments on four benchmark datasets, we show that the proposed model significantly outperforms existing strong baselines. We further demonstrate that the deductive procedure not only presents more explainable steps but also enables us to make more accurate predictions on questions that require more complex reasoning.</abstract>
<seg id="1">大家好。今天我将介绍我们的研究工作：《学习演绎推理：作为复杂关系提取的数学文字问题解决方法》。</seg>
<seg id="2">我是ByteDance 人工智能实验室的Allan，以下是我与德克萨斯大学奥斯汀分校的Jierui Li和SUTD的Wei Lu的合作成果。</seg>
<seg id="3">首先，我想谈谈我们对于推理的动机。</seg>
<seg id="4">在这里，我们展示了一个多步骤推理有帮助的例子。</seg>
<seg id="5">这个数字取自PaLM的论文，他们在这个论文中进行了提示，以解决少样本学习情况下的网络问题。</seg>
<seg id="6">在左侧我们可以看到，如果我们给出一些只有问题和答案的例子，我们可能无法获得正确的答案。</seg>
<seg id="7">但是，如果我们给出更多的推理描述，那么这里的模型将能够预测推理描述，同样也会做出正确的预测。</seg>
<seg id="8">所以，最好将可解释的多步骤推理作为输出。</seg>
<seg id="9">我们还认为，数学文字问题是一个用来评估这种推理能力的简明应用。</seg>
<seg id="10">在我们的问题设置中，围绕疑问，我们需要解决这个疑问，并获得数字答案。</seg>
<seg id="11">在我们的数据集中，还向我们提供了数学表达式，该表达式也导向这个特定的答案。</seg>
<seg id="12">某些假设也适用于之前的工作。</seg>
<seg id="13">我们假设数量的精确度是已知的。</seg>
<seg id="14">并且我们只考虑基本运算符，如加法、减法、乘法、除法和指数。</seg>
<seg id="15">此外，复杂的运算符实际上可以分解成这些基本运算符。</seg>
<seg id="16">之前解决数学文字问题的工作实际上可以分为序列到序列和序列到树模型。</seg>
<seg id="17">传统的序列到序列模型将表达式转换为特定的序列以进行生成。</seg>
<seg id="18">这很容易实现，可以概括成许多不同的复杂问题。</seg>
<seg id="19">但缺点是，其表现实际上并不比结构化模型好，并且缺乏用于预测的可解释性。</seg>
<seg id="20">但因为转换器模型的原因，实际上这个方向仍然很受欢迎。</seg>
<seg id="21">因此，在基于树的模型中，我们实际上以树的形式进行这些表达式的结构化，并在树代中遵循预先排序的遍历。</seg>
<seg id="22">所以在这里，我们继续生成运算符，直到我们到达叶子，也就是数量。</seg>
<seg id="23">这里的好处是，它实际上给了我们这个二进制树结构，但实际上它非常违反直觉，因为我们首先生成运算符，然后在最后生成数量。</seg>
<seg id="24">其次，其中还包含一些重复的计算。</seg>
<seg id="25">如果我们看一下这个表达式，八乘三加三实际上生成两次，但实际上我们应该重复使用结果。</seg>
<seg id="26">在我们提出的方法中，我们希望一步一步地以可解释的方式解决这些问题。</seg>
<seg id="27">例如，在第二步中，我们可以得到这些除数，即27。</seg>
<seg id="28">我们也可以回头参考原始问题来查找相关内容。</seg>
<seg id="29">在这些步骤中，我们得到了除数。</seg>
<seg id="30">然后在第三步，我们实际上得到了商。</seg>
<seg id="31">好的。经过这三个步骤，我们实际上可以重复使用第二个步骤的结果，然后得到第四个步骤的结果，最后可以得到被除数。</seg>
<seg id="32">在这里我们实际上直接生成整个表达式，而不是生成单个运算符或数量。</seg>
<seg id="33">这使得这个过程更加准确。</seg>
<seg id="34">在我们的演绎系统中，我们首先从问题中提供的一堆量开始，并且还包括一些常数作为我们的初始状态。</seg>
<seg id="35">表达式由e i j o p表示。</seg>
<seg id="36">我们执行从q_i到q_j的运算符，这样的表达式实际上是定向的。</seg>
<seg id="37">我们在这里也有减法与单词代表相反的方向。</seg>
<seg id="38">这与关系提取 非常相似。</seg>
<seg id="39">在形式演绎系统中，在时间步骤t ，我们在q_i和q_j对之间应用运算符，然后我们得到这个新表达式。</seg>
<seg id="40">我们把它添加到下一个状态，成为一个新的数量。</seg>
<seg id="41">这些幻灯片实际上可视化了我们不断向当前状态添加表达式的状态的演变。</seg>
<seg id="42">在我们的模型实现中，我们首先使用预训练语言模型，它可以是BERTs或Robertas，然后我们编码句子，然后我们得到这些数量陈述。</seg>
<seg id="43">一旦我们得到数量陈述，我们就可以开始做推理。</seg>
<seg id="44">这里我们展示了一个q_1的例子，以获得针对q_2除以q_2再乘以q_3的表达。</seg>
<seg id="45">首先，我们得到配对表达，它基本上只是q_1和q_2之间的联结，然后我们应用一个由运算符参数化的前馈网络。</seg>
<seg id="46">最后，我们得到表达式表达q_1除以q_2。</seg>
<seg id="47">但实际上，在实践中，在推理阶段，我们也可能会得到错误的表达式。</seg>
<seg id="48">这里所有可能的表达式等于运算符数量的三倍。</seg>
<seg id="49">这里的好处是我们可以轻松地添加约束条件，来控制这个搜索这个搜索空间。</seg>
<seg id="50">例如，如果此表达式不被允许，那么我们可以简单地在我们的搜索空间中删除此表达式。</seg>
<seg id="51">所以在第二步中，我们做同样的事情，但唯一的区别是多了一个数量。</seg>
<seg id="52">所以，这个数量来自之前计算的表达式。</seg>
<seg id="53">我们最终可以得到最后这个表达式q_3乘以q_4。</seg>
<seg id="54">我们还可以看到，所有可能的表达式的数字与之前的步骤不同。</seg>
<seg id="55">这种差异使得应用波束搜索变得困难，因为这两个步骤之间的概率分布是不平衡的。</seg>
<seg id="56">训练过程类似于训练 序列到序列 模型，我们在每个时间步骤中优化损失。</seg>
<seg id="57">在这里，我们也使用这个tau来表示我们何时应该终止这个生成过程。</seg>
<seg id="58">这里的空间从序列到序列是不同的，因为空间在每个时间步不同，而在传统的序列到序列 模型中，这是词汇的数字。</seg>
<seg id="59">它还允许我们根据先前的知识施加某些约束。</seg>
<seg id="60">我们对常用的数学文字问题 数据集、MAWPS、Math23K、MathQA和SVAMP进行实验。</seg>
<seg id="61">在这里，我们简要地展示了与之前最佳方法相比的结果。</seg>
<seg id="62">我们表现最好的变体是Roberta-DeductiveReasoner。</seg>
<seg id="63">事实上，我们不使用波束搜索。相反，所有之前的方法都使用波束搜索。</seg>
<seg id="64">没错。最好的方法通常是基于树的模型。</seg>
<seg id="65">总的来说，我们的推理能够显著优于这个基于树的模型。</seg>
<seg id="66">但我们可以看到，MathQA或SVAMP上的绝对数字并不高。</seg>
<seg id="67">我们进一步研究SVAMP的结果。</seg>
<seg id="68">这个数据集具有挑战性，因为作者试图手动添加一些东西来混淆NLP模型，例如添加不相关的信息和额外的数量。</seg>
<seg id="69">在我们的预测中，我们发现一些中间值实际上是负数。</seg>
<seg id="70">例如，在这些问题中，我们问Jake有多少个苹果？</seg>
<seg id="71">但我们有一些额外的信息，例如照片少了17张，Seventeen有8张照片，但这些信息完全无关紧要。</seg>
<seg id="72">我们的模型做出了一些这样的预测，产生了负值。</seg>
<seg id="73">我们观察到这两个表达式实际上有相似的分数。</seg>
<seg id="74">我们实际上可以通过删除负数的结果来限制这个搜索空间，这样我们就可以得出正确的答案。</seg>
<seg id="75">我们进一步发现，针对某些模型，这种约束实际上改善了很多。</seg>
<seg id="76">例如，对于BERT，我们提高了7分。然后，对于Roberta基础模型，我们实际上提高了两分。</seg>
<seg id="77">因此，更好的语言模型具有更好的语言理解能力，因此，这里的数字对于Roberta来说更高，对于BERT来说更低。</seg>
<seg id="78">我们还试图分析所有这些数据集背后的困难。</seg>
<seg id="79">我们假设这里的未使用数量的数量可以被视为不相关的信息。</seg>
<seg id="80">在这里我们可以看到，我们有未使用数量的样本的百分比，其中SVAMP数据集占了最大的部分。</seg>
<seg id="81">在这里，我们还展示了整体表现。</seg>
<seg id="82">对于那些没有未使用数量的样本，其整体表现实际上高于整体表现。</seg>
<seg id="83">但是，对于具有未使用数量的样本，其表现实际上比整体表现差得多。</seg>
<seg id="84">对于MAWPS，我们其实没有太多的测试例子，所以我会忽略掉这一部分。</seg>
<seg id="85">最后，我们想通过一个问题扰动的例子来展示可解释性。</seg>
<seg id="86">在这里，我们的模型实际上在第一步就做出了错误的预测。</seg>
<seg id="87">我们实际上可以将这个表达式与这里的句子相关联。好。</seg>
<seg id="88">我们认为这个句子可能会误导模型做出错误的预测。</seg>
<seg id="89">在这里再植入一个35，会使得模型以为它是一个加法运算符。</seg>
<seg id="90">我们尝试将句子修改为类似梨树的数量比苹果树少35棵。</seg>
<seg id="91">我们使其传达更准确的语义，以便模型能够使预测正确。</seg>
<seg id="92">这项研究展示了可解释预测如何帮助我们理解模型行为。</seg>
<seg id="93">对我们的工作做个总结，首先我们的模型实际上是非常有效的。</seg>
<seg id="94">我们能够提供可解释的解决程序。</seg>
<seg id="95">我们可以简单地将一些先前的知识作为约束，这样就可以帮助提高表现。</seg>
<seg id="96">最后一点是，底层机制不仅适用于解决任务的网络问题，也适用于涉及多步骤推理的其他任务。</seg>
<seg id="97">我们也有一定的局限性。</seg>
<seg id="98">如果我们有大量 数量的运算符或常量，内存消耗可能相当高。</seg>
<seg id="99">第二件事是，如前所述，由于不同时间步骤之间的概率分布不平衡，因此应用波束搜索策略也非常具有挑战性。</seg>
<seg id="100">我的演讲到此结束，欢迎各位提出问题。谢谢。</seg>
</doc>
<doc docid="2022.acl-long.468" genre="presentations">
<talkid>2022.acl-long.468</talkid>
<abstract>Statutory article retrieval is the task of automatically retrieving law articles relevant to a legal question. While recent advances in natural language processing have sparked considerable interest in many legal tasks, statutory article retrieval remains primarily untouched due to the scarcity of large-scale and high-quality annotated datasets. To address this bottleneck, we introduce the Belgian Statutory Article Retrieval Dataset (BSARD), which consists of 1,100+ French native legal questions labeled by experienced jurists with relevant articles from a corpus of 22,600+ Belgian law articles. Using BSARD, we benchmark several state-of-the-art retrieval approaches, including lexical and dense architectures, both in zero-shot and supervised setups. We find that fine-tuned dense retrieval models significantly outperform other systems. Our best performing baseline achieves 74.8% R@100, which is promising for the feasibility of the task and indicates there is still room for improvement. By the specificity of the domain and addressed task, BSARD presents a unique challenge problem for future research on legal information retrieval. Our dataset and source code are publicly available.</abstract>
<seg id="101">大家好，我叫Antoine，来自马斯特里赫特大学。</seg>
<seg id="102">我将展示我与Jerry的合作结果，它是针对法定条款检索的新数据集。</seg>
<seg id="103">法律问题是许多人生活中不可或缺的一部分。</seg>
<seg id="104">但大多数公民对他们的权利和基本法律程序知之甚少。</seg>
<seg id="105">结果，许多无法承担法律专家昂贵费用的弱势公民得不到保护，或者（更糟糕的是）受到剥削。</seg>
<seg id="106">所有工作旨在通过开发针对法定条款的高效检索系统，来弥合公民与法律之间的鸿沟。</seg>
<seg id="107">这样一个系统可以为非技术人员提供免费的专业法律援助服务。</seg>
<seg id="108">在深入探讨这项工作的主要贡献之前，让我们首先描述法定条款检索的问题。</seg>
<seg id="109">给出一个关于法律问题的简单问题，例如，如果我违反专业保密规定，我会承担怎么风险？</seg>
<seg id="110">需要一个模型来从大量立法中检索所有相关的法定条款。</seg>
<seg id="111">这种信息检索 任务有其自身的一系列挑战。</seg>
<seg id="112">首先，它涉及两种类型的语言。</seg>
<seg id="113">针对问题本身的通用自然语言，以及针对法规使用的复杂法律语言。</seg>
<seg id="114">这种语言分布的差异使得系统更难检索到相关候选信息，因为它间接需要一个固有的解释系统，可以将自然问题翻译成与法规术语相匹配的法律问题。</seg>
<seg id="115">此外，成文法不是一堆可以自身作为完整信息来源的独立条款，举例来说，不像新闻或食谱那样。</seg>
<seg id="116">相反，它是法律条款的结构化集合，只有在整体上下文中考虑时才具有完整的意义，也就是说，连同相邻条款的补充信息，它们所属的字段和子字段，以及它们在法律结构中的位置。</seg>
<seg id="117">最后，法定条款不是小段落，而小段落通常是大多数检索作品中的典型检索单元。</seg>
<seg id="118">在这里，有一些有可能是长达六千字的长文档。</seg>
<seg id="119">自然语言处理中的最新进展引发了对许多法律任务的巨大兴趣，例如法律判断预测或自动联系人合同审查。</seg>
<seg id="120">但由于缺乏较大的高质量标签化数据集，法定条款检索基本上没有受到影响。</seg>
<seg id="121">在这项工作中，我们提出了一个新的以法国本土公民为中心的数据集，以研究检索模型是否可以接近法律专家执行法定条款检索任务的效率和可靠性。</seg>
<seg id="122">我们的比利时法定条款检索数据集BSARD由比利时公民提出的1100多个法律问题组成。</seg>
<seg id="123">这些问题涵盖了从家庭、住房、金钱到工作和社会安全等一系列主题。</seg>
<seg id="124">每个问题都由经验丰富的法学家标记，并参考了比利时法典中超过22,600篇法律条款的语料库中的相关条款。</seg>
<seg id="125">现在，让我们来谈谈我们是如何收集这个数据集的。</seg>
<seg id="126">首先，我们从汇编大量语料库的法律条款开始。</seg>
<seg id="127">我们考虑了32个公开可用的比利时法典，并提取了所有条款以及相应的章节标题。</seg>
<seg id="128">然后，我们参考相关法规收集了法律问题。</seg>
<seg id="129">为此，我们与比利时律师事务所合作，该事务所每年收到约4000封来自比利时公民的电子邮件，他们就个人法律问题征求意见。</seg>
<seg id="130">我们很幸运能够访问他们的网站，在这个网站上他们经验丰富的法学家团队解决比利时人最常见的法律问题。</seg>
<seg id="131">我们收集了数以千计的问题，并用类别、子类别和相关法规的法律参考进行了注释。</seg>
<seg id="132">最后，我们浏览了法律参考文献，并过滤掉参考文献不包含在我们所考虑的其中任何一部法律条款的问题。</seg>
<seg id="133">其余的参考文献被匹配，并转换为我们语料库中相应的条款ID。</seg>
<seg id="134">我们最终得到了 10108 个问题，每个问题都仔细地标记了我们大型语料库的22633个法定条款中的相关条款的ID。</seg>
<seg id="135">此外，每个问题都具有主类别和一系列子类别。</seg>
<seg id="136">每个条款都带有法律结构中子序列标题的联结。</seg>
<seg id="137">这些额外的信息未在当前工作中使用，但可能对今后关于法律信息检索或法律文本分类的研究有所帮助。</seg>
<seg id="138">让我们来看看我们的数据集的一些特征。</seg>
<seg id="139">这些问题的长度在5到44个单词之间，中位数为14个单词。</seg>
<seg id="140">条款则要长得多，中位数为77个单词 ，其中有142条超过1000单词。</seg>
<seg id="141">最长的一条有5790个单词。</seg>
<seg id="142">如前所述，这些问题涵盖了广泛的主题，其中约85%的主题是关于家庭、住房、金钱或正义。</seg>
<seg id="143">则其余的15%则涉及社会保障、外国人或工作。</seg>
<seg id="144">这些法律条款也非常多样化，因为它们来自32个不同的比利时法典，涵盖了大量 数量的法律主题。</seg>
<seg id="145">以下是从这些比利时法典中收集的条款总数。</seg>
<seg id="146">在22633个条款中，只有1612个与数据集中的至少一个问题相关。</seg>
<seg id="147">这些被引用的条款中约有80%来自民法典、司法法典、刑事调查法典或刑法典。</seg>
<seg id="148">与此同时，32个法典中有18个中提到的与至少一个问题相关的条款少于5个。</seg>
<seg id="149">这可以解释为，这些法典较少关注个人及个人关心的问题。</seg>
<seg id="150">总体而言，这些被引用条款的引用次数中位数为2次，其中引用次数超过5次的不到25%。</seg>
<seg id="151">使用所有数据集，我们对几种检索方法进行了基准测试，包括词汇和密集架构。</seg>
<seg id="152">给定一个查询和一个条款，词汇模型通过计算该条款中每个术语的权重总和来为查询条款对分配一个分数。</seg>
<seg id="153">我们使用标准的 TF-IDF和BM25排序函数进行实验。</seg>
<seg id="154">这些方法的主要问题是，它们只能检索包含有查询中存在的关键字的条款。</seg>
<seg id="155">为了克服这个限制，我们尝试了一种基于神经的架构，它可以捕获查询和条款之间的语义关系。</seg>
<seg id="156">我们使用双编码器模型将查询和条款映射到密集的向量陈述中，并通过查询条款对嵌入的相似度计算它们之间的相关性分数。</seg>
<seg id="157">这些嵌入通常来自于对单词嵌入模型输出的池化操作。</seg>
<seg id="158">首先，我们研究了Siamese双编码器在零样本评估设置中的有效性，这意味着预训练的单词嵌入模型可以直接拿来使用，无需进行任何额外的微调。</seg>
<seg id="159">我们尝试了与上下文无关的文本编码器，即word2vec和fastText，以及上下文相关的嵌入模型，即Roberta，更具体地说是CamemBERT，它是个法语Roberta模型。</seg>
<seg id="160">此外，我们在数据集上训练我们自己的基于CamemBERT的 模型双编码器。</seg>
<seg id="161">请注意，针对训练，我们尝试使用了双编码器架构的两种风格。</seg>
<seg id="162">第一种是siamese，它使用一个独特的单词嵌入模型，将查询和条款一起映射到一个共享的密集向量空间；另一种是双塔，它使用两个独立的单词嵌入模型，将查询和条款分别编码到不同的嵌入空间。</seg>
<seg id="163">我们试验了均值、最大值和CLS集合，以及计算相似性的乘积和余弦。</seg>
<seg id="164">以下是我们在测试集上的基线结果。</seg>
<seg id="165">上面是词汇方法，中间是在零样本设定中评估的siamese双编码器，下面是微调的双编码器。</seg>
<seg id="166">总体而言，微调的双编码器明显优于所有其他基线。</seg>
<seg id="167">双塔模型在召回率上比其siamese模型高出了100，但在其他指标上表现相似。</seg>
<seg id="168">虽然BM25的表现明显低于训练好的双编码器，但它的表现表明，它仍然是特定领域检索的一个强大基线。</seg>
<seg id="169">关于siamese双编码器的零样本评估，我们发现直接使用预训练的CamemBERT模型的嵌入，而不对信息检索任务进行优化，结果很差，这与之前的发现一致。</seg>
<seg id="170">此外，我们观察到，基于word2vec的双编码器的表现明显优于基于fastText和BERT的模型，这表明在直接使用时，也许预训练的词级嵌入比字符级或子词级嵌入对这项任务更适合。</seg>
<seg id="171">虽然有希望，但这些结果表明，与一个熟练的法律专家相比，还有很多改进的机会，因为他最终可以检索到任何问题的所有相关条款，从而获得满分。</seg>
<seg id="172">最后，让我们讨论一下数据集的两个局限性。</seg>
<seg id="173">首先，条款的内容仅限于从比利时32部法典中收集的条款，这并不包括整个比利时的法律，因为法令、指令和条例中的条款都没有。</seg>
<seg id="174">在构建数据集的过程中，所有对这些未收集的条款的引用都被忽略，这导致一些问题最终只有最初相关条款数量的一小部分。</seg>
<seg id="175">因此，这一信息意味着其余相关条款中包含的答案可能是不完整的，尽管它仍然完全合适。</seg>
<seg id="176">其次，我们应该注意，并非所有的法律问题都可以仅通过法规来回答。</seg>
<seg id="177">例如这个问题：如果我的租户制造太多噪音，我可以驱逐他们吗？</seg>
<seg id="178">在成文法中可能没有详细的答案来量化允许驱逐的特定噪音阈值。</seg>
<seg id="179">相反，房东可能应该更多地依靠判例法，找到与他们目前情况相似的先例。</seg>
<seg id="180">例如，租户每周举行两次派对，直到凌晨2点。</seg>
<seg id="181">因此，有些问题比其他问题更适合于法定条款检索任务，而不太适合的问题所在的领域还有待确定。</seg>
<seg id="182">我们希望我们的工作能够激发在开发实用可靠的法定条款检索模型方面的兴趣。</seg>
<seg id="183">这可以帮助改善所有人对司法的利用。</seg>
<seg id="184">您可以在以下链接查看我们的论文、数据集和法典。谢谢。</seg>
</doc>
<doc docid="2022.acl-long.567" genre="presentations">
<talkid>2022.acl-long.567</talkid>
<abstract>We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V&L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&L models from a linguistic perspective, complementing the canonical task-centred V&L evaluations.</abstract>
<seg id="185">大家好，很高兴向你们介绍我们的工作成果——VALSE，这是一个独立于任务的基准，旨在用特定的语言现象测试视觉和语言模型。</seg>
<seg id="186">我们为什么要费尽心思设立这个基准呢？</seg>
<seg id="187">那是因为，在过去的几年里，我们看到了基于转换器的视觉和语言模型在大量的图像文本对上进行预训练的爆炸性增长。</seg>
<seg id="188">这些模型中的每一个都在视觉和语言任务上推动了最先进的技术，如视觉问题回答、视觉常识推理、图像检索，以及短语领域。</seg>
<seg id="189">因此，我们得到了一个信息，这些任务的准确性和特定基准正在稳步提升。</seg>
<seg id="190">但我们是否知道模型实际上学到了什么？</seg>
<seg id="191">视觉和语言转换器在为这个图像和这个句子分配高分时，所理解的是什么呢？</seg>
<seg id="192">而这一个的低分呢？</seg>
<seg id="193">视觉和语言模型关注的是正确的事情吗？</seg>
<seg id="194">还是像之前的工作所显示的那样，他们专注于偏差？</seg>
<seg id="195">为了进一步阐明这方面的问题，我们提出了一个与任务更加无关的方向，并引入VALSE，对于影响语言和视觉形态的特定语言现象，其测试视觉和语言模型的敏感性。</seg>
<seg id="196">我们的目标是存在性、复数、计数、空间关系、动作和实体共指。</seg>
<seg id="197">但是，我们如何测试视觉和语言模型是否捕获了这种现象？</seg>
<seg id="198">通过干扰Ravi Shekhar和合作者以前只应用于视觉和语言模型的名词短语的方法，以及我们在之前工作中对计数的方法。</seg>
<seg id="199">干扰的意思是，我们获取一个图像的标题，通过改变标题，使其不再描述图像中的物体，从而造成干扰。</seg>
<seg id="200">当我们在做这些短语改动时，重点关注了六个具体的方面，如存在性、复数、计数、空间关系、动作和实体共指，其中每个方面都可能包含一种或多种工具，以备我们发现不止一个有趣的方式来创造干扰实例。</seg>
<seg id="201">例如，在动作方面，我们有两种工具，在一种当中是用不同的动作改变动作动词，在另一种当中是动作被交换。</seg>
<seg id="202">计数和共指也是具有多种工具的方面。</seg>
<seg id="203">当我们创造这些干扰时，要确保它们不能描述图像，它们符合语法，且仍然是有效的句子。</seg>
<seg id="204">这并不容易，因为被干扰的标题可能比原始标题更不可能。</seg>
<seg id="205">例如，虽然这不是不可能的，但从统计学上来说，植物砍人的可能性比人砍植物的可能性要小，较大的视觉和语言模型可以发现这一点。</seg>
<seg id="206">所以，要制造有效的干扰，我们必须想方设法。</seg>
<seg id="207">首先，我们利用强大的语言模型来提出干扰。</seg>
<seg id="208">其次，我们使用自然语言推断或简短NLI来过滤可能仍在描述图像的干扰词，因为在构建干扰词时，我们需要确保它们无法描述图像。</seg>
<seg id="209">为了自动检验这一点，我们应用了自然语言推理，其基本原理如下。</seg>
<seg id="210">我们认为图像是前提，其标题是其附带的假设。</seg>
<seg id="211">此外，我们认为标题是前提，而干扰词是其假设。</seg>
<seg id="212">如果NLI 模型预测干扰词与标题相矛盾或保持中立，我们将其作为有效干扰词的指标。</seg>
<seg id="213">如果NLI预测干扰词是标题中所包含的，那么它就不可能是一个好的干扰词，因为根据反证法，它将给出图像的真实描述，我们将这些干扰词过滤掉。</seg>
<seg id="214">但这个过程并不完美，它只是一个有效干扰词的指标。</seg>
<seg id="215">所以，作为生成有效干扰词的第三项措施，我们使用人类 注释着来验证VALSE中使用的数据。</seg>
<seg id="216">因此，经过过滤和人工评估后，我们拥有与本表中所述的测试实例一样多的测试实例。</seg>
<seg id="217">请注意，VALSE不提供任何训练数据，而仅提供测试数据。</seg>
<seg id="218">由于它仅是一个零样本测试基准，因此它旨在利用预训练后的视觉和语言模型的现存功能。</seg>
<seg id="219">微调只会使模型能够利用数据中的工件或统计 偏差。</seg>
<seg id="220">我们都知道，这些模型喜欢作弊和走捷径。</seg>
<seg id="221">正如我们所说，我们有兴趣评估视觉和语言模型在预训练后具有哪些能力。</seg>
<seg id="222">我们在VALSE上尝试了五种视觉和语言模型，即使用CLIP、LXMert、ViLBERT、ViLBERT十二合一和VisualBERT。</seg>
<seg id="223">我们最重要的两个评估指标是模型在将图像句子对分类为标题和干扰词的准确性。</seg>
<seg id="224">也许与这段视频更相关的是，我们将展示我们更宽容的指标，即成对的准确性，它衡量的是正确的图像文本对的图像句子对齐得分是否大于其受干扰的对。</seg>
<seg id="225">有关更多指标及其结果，请查看我们的论文。</seg>
<seg id="226">这里显示了成对的准确度的结果，它们与我们从其他指标中得到的结果一致，即ViLBERT十二分之一取得了最佳的零样本表现，其次是ViLBERT、LXMert、CLIP，最后是VisualBERT。</seg>
<seg id="227">值得注意的是，以个别对象（如存在性和名词短语）为中心的工具几乎被ViLBERT十二分之一解决了，这突出表明模型能够识别已命名的对象和它们在图片中的存在。</seg>
<seg id="228">但是，在我们的对抗性干扰设置中，其余的部分都不能被可靠地解决。</seg>
<seg id="229">我们从复数和计数工具中看到，视觉和语言模型难以区分对单个和多个对象的引用，或在图像中计算它们。</seg>
<seg id="230">关系部分表明，它们难以正确地分类图像中对象之间的命名空间关系。</seg>
<seg id="231">他们也很难区分动作和识别动作的参与者，即使像我们在动作这块看到的那样有合理性偏差的支持。</seg>
<seg id="232">从核心推理这一块，我们发现通过使用代词来追踪对图像中同一对象的多个引用，对于视觉和语言模型来说也是困难的。</seg>
<seg id="233">作为理智的检查，同时也因为这是一个有趣的实验，我们还对两个纯文本模型（GPT one和GPT two）进行了基准测试，以评估VALSE是否可由这些单模态模型解决，方法是抛开图像不管，计算正确标题和受干扰标题的困惑度，并预测具有最低困惑度的条目。</seg>
<seg id="234">如果受干扰标题的困惑度更高，我们认为这表明受干扰的标题可能存在合理性偏差或其他语言偏差。</seg>
<seg id="235">有趣的是，在某些情况下，纯文本的GPT模型比视觉和语言模型更好地捕捉了世界的合理性。</seg>
<seg id="236">因此，总的来说，VALSE是一个基准，它使用语言构造的镜头，通过硬性测试社区的视觉接地能力，来帮助其改善视觉和语言模型。</seg>
<seg id="237">我们的实验表明，视觉和语言模型能很好地识别命名对象及其在图片中的存在（如“存在性”部分所示），但在被迫尊重语言指标时，却很难在视觉场景中建立它们的相互依存性和关系。</seg>
<seg id="238">我们非常希望鼓励社区使用VALSE来衡量用视觉和语言模型实现语言接地的进展。</seg>
<seg id="239">更重要的是，VALSE可以作为数据集的间接评估，因为可以在训练或微调前后对模型进行评估，以了解数据集是否有助于模型在VALSE测试的任何方面得到改善。</seg>
<seg id="240">如果您有兴趣，请查看GitHub上的VALSE数据。如果您有任何疑问，请随时与我们联系。</seg>
</doc>
<doc docid="2022.acl-long.597" genre="presentations">
<talkid>2022.acl-long.597</talkid>
<abstract>A release note is a technical document that describes the latest changes to a software product and is crucial in open source software development. However, it still remains challenging to generate release notes automatically. In this paper, we present a new dataset called RNSum, which contains approximately 82,000 English release notes and the associated commit messages derived from the online repositories in GitHub. Then, we propose classwise extractive-then-abstractive/abstractive summarization approaches to this task, which can employ a modern transformer-based seq2seq network like BART and can be applied to various repositories without specific constraints. The experimental results on the RNSum dataset show that the proposed methods can generate less noisy release notes at higher coverage than the baselines. We also observe that there is a significant gap in the coverage of essential information when compared to human references. Our dataset and the code are publicly available.</abstract>
<seg id="241">大家好，我是东京大学的Kamezawa。</seg>
<seg id="242">我将发表一篇论文，题目为《RNSum：通过提交日志总结自动生成发行说明的大规模数据集》。</seg>
<seg id="243">我将按照这个顺序解释。</seg>
<seg id="244">首先，我将介绍我们在这项研究中正在进行的自动发行说明生成。</seg>
<seg id="245">发行说明是一个技术文档，它总结了软件产品的每个版本所分发的更改。</seg>
<seg id="246">这个图像显示的是vuejs库的2.6.4版本的发行说明。</seg>
<seg id="247">发行说明在开源开发中起着重要作用，但手动准备它们是很耗时的。</seg>
<seg id="248">因此，如果能够自动生成高质量的发行说明，那将是非常有用的。</seg>
<seg id="249">我将遵从之前的两项关于自动生成发行说明的研究。</seg>
<seg id="250">第一个是被称为ARENA的系统 ，发布于2014年。</seg>
<seg id="251">它采取了一种基于规则的方法，例如使用变化提取器从不同版本的差异中提取所有的差异、库的变化和文件的变化，最后再将它们结合起来。</seg>
<seg id="252">这个系统最显著的特征是右上角的问题提取器。</seg>
<seg id="253">这必须留给问题跟踪器系统 JIRA ，并且只能应用于使用JIRA的项目。</seg>
<seg id="254">换句话说，它不能用于GitHub上的许多项目。</seg>
<seg id="255">第二个是Glyph ，最近在2020年宣布。</seg>
<seg id="256">它可以在互联网上下载，并可以通过pip安装。</seg>
<seg id="257">这个系统有一个简单的基于学习的文本分类模型，并为每个输入的提交信息输出五个标签之一，如特征或错误修复。</seg>
<seg id="258">此图像是一个返回纠正或错误修复标签的示例用法。</seg>
<seg id="259">Glyph的训练数据相当小，约为五千，并将在下面描述的实验中显示。</seg>
<seg id="260">文本分类模型的表现不佳。</seg>
<seg id="261">我提出了两个相关的研究，但它们的问题是适用性有限和数据资源稀缺。</seg>
<seg id="262">我们的论文解决了这两个问题，并自动生成了高质量的发行说明。</seg>
<seg id="263">面对适用性有限的问题，我们提出了一种只使用提交信息作为输入的高质量的分类总结方法。</seg>
<seg id="264">这个提议的方法可以用于所有英语存储库。</seg>
<seg id="265">对于第二个数据资源稀缺的问题，我们通过使用GitHub API从公共GitHub存储库收集数据，建立了由大约八万两千条数据组成的RNSum数据集。</seg>
<seg id="266">接下来，我将介绍我们的数据集。</seg>
<seg id="267">以下是数据的示例。</seg>
<seg id="268">左侧是提交消息，右侧是发行说明。</seg>
<seg id="269">发行说明被标记为优化或修复等。</seg>
<seg id="270">我们设置了一个任务，将提交信息作为输入，并输出一个标签化的发行说明。</seg>
<seg id="271">这可以看作是一项总结 任务。</seg>
<seg id="272">我们预先定义了四个标签：特征、优化、错误修复、弃用删除和重大更改。</seg>
<seg id="273">这些都是基于之前的研究和其他因素设定的。</seg>
<seg id="274">右下角的发行说明是从左下角的发行说明中提取的。</seg>
<seg id="275">现在，有必要检测事先设置好的四个标签。</seg>
<seg id="276">但是，标签并不总是与每个存储库一致。</seg>
<seg id="277">例如，改进标签包括改进、增强、优化等。</seg>
<seg id="278">我们为这些符号变体中的每一个准备了一个大约30个标签的词汇列表。</seg>
<seg id="279">这是为了检测发行说明类，并收集后面的发行文本作为该类的发行说明句子。</seg>
<seg id="280">接下来是提交消息。</seg>
<seg id="281">提交信息并不与每个版本相联系。</seg>
<seg id="282">如下面的图像所示，如果当前的版本是2.5219的版本，那么我们需要识别之前的版本2.5218，并得到一个差异。</seg>
<seg id="283">这有点繁琐，而且仅仅得到一个发布列表并查看前后的情况是不够的。</seg>
<seg id="284">我们创建了一个启发式匹配规则来获取上一个和下一个版本。</seg>
<seg id="285">数据集分析。</seg>
<seg id="286">最后，收集到了7200个存储库和82000份数据。</seg>
<seg id="287">此外，发行说明令牌的平均数量为63，这对于一个总结任务来说是相当高的。</seg>
<seg id="288">此外，独特的令牌数量也相当是大的，有883万个。</seg>
<seg id="289">这是由于在资源库中发现了大量独特的类或方法名称。</seg>
<seg id="290">接下来，我将解释所提议的方法。</seg>
<seg id="291">这个先按类别抽取然后抽象总结的模型是由两个神经模块组成的。</seg>
<seg id="292">一个是使用BERT或CodeBERT的分类器，另一个是使用BART的生成器。</seg>
<seg id="293">首先，CEAS使用分类器将每条提交信息分为五个发行说明类别，其中使用了“改进”、“错误修复”、“弃用”，以及“其他”。</seg>
<seg id="294">被归类为“其他”的提交消息将被丢弃。</seg>
<seg id="295">然后CEAS将生成器独立地应用于四个标签化 文档，并为每个类别生成发行说明。</seg>
<seg id="296">在这项任务中，提交信息和发行说明之间的直接对应关系并不清楚。</seg>
<seg id="297">所以，为了训练分类器，这就是为什么我们使用每条提交信息的前十个字符，来对每个输入的提交信息重新分配调查。</seg>
<seg id="298">我们通过两种不同的方法对分类抽象总结方法进行建模。</seg>
<seg id="299">第一种方法是我们称为CAS-Single的模型，它由一个单一的六对六网络组成，并生成一个单一的发行说明文本，给出输入提交信息的串联。</seg>
<seg id="300">输出的文本可以根据特殊的类特定端点符号分为分类段落。</seg>
<seg id="301">第二种方法，方法，我们称之为CAS-Multi，由四个不同的seq2seq网络组成，每个网络都对应于一个固定的发行说明类别。</seg>
<seg id="302">好，让我解释一下实验。</seg>
<seg id="303">我们比较了五种方法：CEAS、CAS-Single、CAS-Multi、Clustering，以及之前的研究“Glyph”。</seg>
<seg id="304">关于评估，在某些情况下，发行说明是以多个句子形式输出的。</seg>
<seg id="305">由于很难计算出这些句子的数量，所以用空格合并，作为一个长句处理。</seg>
<seg id="306">当系统输出一个短句时，BLEU会受到惩罚。</seg>
<seg id="307">在接下来描述的实验结果中，这种惩罚导致了较低的BLEU值。</seg>
<seg id="308">最后，我们还计算了特异性，因为如果发行说明是空的，就无法计算ROUG和BLEU。</seg>
<seg id="309">更高的特异性意味着，在发行说明假定为空的情况下，模型会正确地输出一个空文本。</seg>
<seg id="310">结果如下。</seg>
<seg id="311">由于该数据集包含电子邮件地址、哈希值等内容，所以我们还评估了经过清理的数据集，其中不包括这些内容。</seg>
<seg id="312">CEAS和CAS的ROUGE-L得分比基线高10分以上。</seg>
<seg id="313">特别是在干净的测试集上，建议的方法和基线之间的分数差距跃升到20分以上。</seg>
<seg id="314">这些结果表明CEAS和CAS受到严重影响。</seg>
<seg id="315">CEAS比CAS得到了更好的ROUGE-L分数，这表明将分类器和生成器结合在一起对使用伪标签训练分类器是有效的。</seg>
<seg id="316">CEAS的高覆盖率之所以能够实现，可能是因为分类器可以专注于为每个类别选择相关的提交信息。</seg>
<seg id="317">CAS -Multi倾向于比CAS -Single产生更高的ROUGE -L。</seg>
<seg id="318">这代表着，为每个发行说明类别独立开发不同的抽象总结模型也是有效的。</seg>
<seg id="319">这里有一个错误分析。</seg>
<seg id="320">CAS方法倾向于输出比人类参考句子更短的句子。</seg>
<seg id="321">在右图中，引用句子有3或4个句子 ，而CAS只有1个。</seg>
<seg id="322">这个模型不愿输出的原因是，在训练数据中，只有33%的句子出现在特征标签中，40%出现在改进标签中。</seg>
<seg id="323">此外，如果没有额外的信息，CAS方法就无法生成准确的发行说明。</seg>
<seg id="324">右边最上面的例子是一个非常混乱的提交消息的例子，如果不参考相应的进度或问题，就无法生成完整的句子。</seg>
<seg id="325">下面的例子显示，输入中的两个提交信息是相关的，应该合并成一个句子，但它没有这么做。</seg>
<seg id="326">最后，一个结论。</seg>
<seg id="327">我们已经建立了一个新的数据集，用于自动生成发行说明。</seg>
<seg id="328">我们还制定了一项输入提交信息并对其进行总结的任务，以便适用于所有用英语写的项目。</seg>
<seg id="329">我们的实验表明，与基线相比，所提出的方法在更高的覆盖率下产生了更少干扰的发行说明。</seg>
<seg id="330">请查看我们在GitHub上的数据集。</seg>
<seg id="331">谢谢。</seg>
</doc>
<doc docid="2022.acl-long.111" genre="presentations">
<talkid>2022.acl-long.111</talkid>
<abstract>The enrichment of tabular datasets using external sources has gained significant attention in recent years. Existing solutions, however, either ignore external unstructured data completely or devise dataset-specific solutions. In this study we proposed Few-Shot Transformer based Enrichment (FeSTE), a generic and robust framework for the enrichment of tabular datasets using unstructured data. By training over multiple datasets, our approach is able to develop generic models that can be applied to additional datasets with minimal training (i.e., few-shot). Our approach is based on an adaptation of BERT, for which we present a novel fine-tuning approach that reformulates the tuples of the datasets as sentences. Our evaluation, conducted on 17 datasets, shows that FeSTE is able to generate high quality features and significantly outperform existing fine-tuning solutions.</abstract>
<seg id="332">大家好。我的名字是Asaf Harari。</seg>
<seg id="333">我将介绍我们的论文《使用微调转换器架构的少样本表格式数据充实》。</seg>
<seg id="334">数据科学家对数据进行分析，主要侧重于对数据的现有特征进行操作。</seg>
<seg id="335">但有时，这些特征是有限的。</seg>
<seg id="336">使用另一个数据来源生成特征可能会增加大量的信息。</seg>
<seg id="337">我们的研究目标是，利用外部来源的自由文本自动丰富表格数据。</seg>
<seg id="338">假设我们有一个表格数据集和一个知识库。</seg>
<seg id="339">我们需要一个自动过程，其中包括实体链接和文本分析，以从知识库的自由文本中提取新的特征。</seg>
<seg id="340">我们的框架FeSTE正是这个自动过程。</seg>
<seg id="341">我们来看看在一个数据集中输入FeSTE的例子。</seg>
<seg id="342">在这个例子中，数据集是大学数据集。</seg>
<seg id="343">当它的目标是将大学分为低排名的大学和高排名的大学时。</seg>
<seg id="344">我们使用维基百科作为知识库。</seg>
<seg id="345">FeSTE的第一阶段是实体链接。</seg>
<seg id="346">当每个实体，在这个例子中是指大学名称，被链接到知识库中的一个实体。</seg>
<seg id="347">并且知识库的实体文本被提取出来，并添加到数据集中。</seg>
<seg id="348">在这个例子中，文本是维基百科页面的摘要。</seg>
<seg id="349">现在，我们需要从检索 文本中生成或提取特征。</seg>
<seg id="350">因此，我们需要进行特征提取阶段，其中包括文本分析。</seg>
<seg id="351">这是本论文的主要新颖之处，我将在接下来的幻灯片中深入探讨。</seg>
<seg id="352">在特征提取阶段之后，还有一个特征生成阶段，我们使用提取的特征来生成少量的新特征。</seg>
<seg id="353">首先，在原始数据集的类别数量中生成特征。</seg>
<seg id="354">在这个例子中，原始数据集有两个类别。</seg>
<seg id="355">所以，FeSTE生成两个特征。</seg>
<seg id="356">但如果数据集有五个类，FeSTE就会生成五个新的特征。</seg>
<seg id="357">每个特征表示每个类的可能性。</seg>
<seg id="358">为了分析文本，我们使用了目前最先进的文本分析方法，即基于转换器的语言模型，如BERT、GPT、XLNet等。</seg>
<seg id="359">但是，我们不可能用输入数据集来训练语言模型。</seg>
<seg id="360">因此，有一个朴素的方法是：目标任务微调。</seg>
<seg id="361">因此，在特征提取阶段，我们可以下载预训练的语言模型，在目标数据集上微调语言模型。</seg>
<seg id="362">在这个例子中，要对语言模型进行微调，将文本分类，抽象成类，低或高。</seg>
<seg id="363">接收语言模型的输出，也就是每个类别的可能性，并作为新的特征使用。</seg>
<seg id="364">这种方法的问题是，数据集可能只有几个不同的实体/文本。</seg>
<seg id="365">在我们的实验中，几乎有一半的数据集包含少于400个样本，最小的数据集包含35个样本，在一个训练集中。</seg>
<seg id="366">因此，在这个数据集上微调语言模型将是无效的。</seg>
<seg id="367">但我们可以使用关于预先分析的数据集的先验知识。</seg>
<seg id="368">因为FeSTE，我们在多个数据集上应用FeSTE，我们可以使用n减1的数据集来收集n减1的数据集的信息，并在分析第n个数据集时使用这些信息。</seg>
<seg id="369">我们的建议是，添加另一个微调阶段。</seg>
<seg id="370">一个初步的多任务微调阶段。</seg>
<seg id="371">当你在n减1数据集上微调语言模型时。</seg>
<seg id="372">然后，我们执行另一个微调阶段，即目标任务微调，此时我们在第n个目标数据集上微调语言模型。</seg>
<seg id="373">最先进的多任务多任务微调技术称为MTDNN。</seg>
<seg id="374">在MTDNN中，MTDNN在训练集的任务数量上保持着头部。</seg>
<seg id="375">因此，在这个例子中，训练集里有四个任务，所以MTDNN维持四个头，正如在图像上看到的那样。</seg>
<seg id="376">它从训练集中随机抽取一批。</seg>
<seg id="377">而如果他们的随机批属于一个，例如单句分类任务，它就会通过第一个头执行前向和后向路径。</seg>
<seg id="378">而如果随机批属于成对排名任务，它就会通过最后一个头执行前向和后向路径。</seg>
<seg id="379">在我们的场景中，表格数据集在类的数量上有所不同。</seg>
<seg id="380">存在很多的任务。</seg>
<seg id="381">MTDNN保持了类、头、输出层的数量。</seg>
<seg id="382">另外，MTDNN需要为一个新的数据集和一个新的任务初始化新的头。</seg>
<seg id="383">我们的方法，称为“任务重构微调”，它是在我们的方法任务重构微调，而不是维护多个头，我们把每个数据集重构为每个分类问题的一个句子，也就是两个类的任务。</seg>
<seg id="384">让我们来看一个例子。</seg>
<seg id="385">这里是我们的输入数据集，由实体、特征、文本和类组成。</seg>
<seg id="386">而且，我们将任务从对文本进行低级或高级分类，重新表述为对文本、抽象和类别进行真或假的分类。</seg>
<seg id="387">或者换句话说，我们训练了语言模型，将一个抽象和类划分为抽象和类，无论抽象属不属于类。</seg>
<seg id="388">因此，标签向量在这种情况下始终保持……它总是由两个类组成。</seg>
<seg id="389">而这就是我们精细的、重新制定的微调方法的算法。</seg>
<seg id="390">让我们来看看完整的框架。</seg>
<seg id="391">数据集送入FeSTE。</seg>
<seg id="392">然后，FeSTE执行实体链接阶段。</seg>
<seg id="393">它从知识库中提取文本，在本例子中，它是维基百科页面的摘要。</seg>
<seg id="394">然后，它将任务重新表述为一个成对的句子分类任务。</seg>
<seg id="395">将语言模型应用于新的任务，并对每一类的输出可能性进行分析。</seg>
<seg id="396">现在，语言模型已经用初步的多任务微调在n减1的数据集上进行了微调。</seg>
<seg id="397">然后，在类的数量上，我们用语言模型的输出向量作为新生成的特征。</seg>
<seg id="398">为了评估我们的框架，我们使用了17个表格分类数据集，这些数据集在大小、特征、平衡、域和初始表现上都有所不同。</seg>
<seg id="399">我们还是使用维基百科来作为知识库。</seg>
<seg id="400">我们将实验设计为留出一个评估，在16个数据集上训练FeSTe，并将其应用于第17个数据集。</seg>
<seg id="401">我们还将每个数据集分成四折，并应用四折交叉验证。</seg>
<seg id="402">然后，我们生成新的特征，并使用五个评估分类器对其进行评估。</seg>
<seg id="403">在我们的实验中，我们使用基础BERT基础架构。</seg>
<seg id="404">以下是我们实验的结果。</seg>
<seg id="405">你可以看到，我们将我们的框架与目标数据集微调……目标数据集微调和MTDNN初步微调进行了比较。</seg>
<seg id="406">而我们重新制定的微调实现了最好的结果，最好的表现。</seg>
<seg id="407">而MTDNN比目标 数据集 微调提高了2%。</seg>
<seg id="408">我们的方法实现了6%的改进。</seg>
<seg id="409">当我们在小数据集上看时，我们可以看到MTDNN的表现下降了，初步的多任务微调阶段的改进下降到1.5%。</seg>
<seg id="410">但与单独的目标任务微调相比，我们的表现提高到了11%。</seg>
<seg id="411">总而言之，在我们的实验中，FeSTE可以从35个样品中进行少量的富集。</seg>
<seg id="412">它使用一个架构用于所有任务和数据集。</seg>
<seg id="413">而且它保留了模型的头部。</seg>
<seg id="414">但它增加了重新制定阶段。</seg>
<seg id="415">它增加了训练集，它需要一个具有语义的目标值，这样我们就可以把它输入到语言模型中，并在句对分类问题中使用它。</seg>
<seg id="416">谢谢。</seg>
</doc>
</srcset>
</mteval>
