<?xml version="1.0" encoding="UTF-8"?>
<mteval>
<srcset setid="iwslt-ACLtest2023" srclang="English">
<doc docid="2022.acl-long.410" genre="presentations">
<talkid>2022.acl-long.410</talkid>
<abstract>Solving math word problems requires deductive reasoning over the quantities in the text. Various recent research efforts mostly relied on sequence-to-sequence or sequence-to-tree models to generate mathematical expressions without explicitly performing relational reasoning between quantities in the given context. While empirically effective, such approaches typically do not provide explanations for the generated expressions. In this work, we view the task as a complex relation extraction problem, proposing a novel approach that presents explainable deductive reasoning steps to iteratively construct target expressions, where each step involves a primitive operation over two quantities defining their relation. Through extensive experiments on four benchmark datasets, we show that the proposed model significantly outperforms existing strong baselines. We further demonstrate that the deductive procedure not only presents more explainable steps but also enables us to make more accurate predictions on questions that require more complex reasoning.</abstract>
<seg id="1">Hallo zusammen. Heute werde ich unsere Forschungsarbeit Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction vorstellen.</seg>
<seg id="2">Ich bin Allan vom ByteDance AI Lab, und dies ist eine gemeinsame Arbeit mit Jierui Li von der University of Texas in Austin und Wei Lu von SUTD.</seg>
<seg id="3">Zunächst möchte ich über unsere Motivation für das Argumentieren sprechen.</seg>
<seg id="4">Deshalb zeigen wir hier ein Beispiel, bei dem mehrstufiges Argumentieren hilfreich ist.</seg>
<seg id="5">Diese Abbildung stammt aus dem PaLM-Paper, in dem sie Prompting durchführen, um das Netzwerkproblem in einem Few-Shot-Lernszenario zu lösen.</seg>
<seg id="6">Auf der linken Seite können wir also sehen, dass wenn einige Beispiele nur aus Frage und Antworten bestehen, dann bekommen wir nicht die richtigen Antworten.</seg>
<seg id="7">Wenn wir jedoch eine weitere Argumentationsbeschreibung hinzufügen, ist das Modell in der Lage, die Argumentationsbeschreibung vorherzusagen und hier auch eine korrekte Vorhersage zu treffen.</seg>
<seg id="8">Es ist also gut, eine interpretierbare mehrstufige Argumentation als Ergebnis zu haben.</seg>
<seg id="9">Wir denken auch, dass mathematische Wortprobleme eine einfache Anwendung sind, um solche Argumentationsfähigkeiten zu bewerten.</seg>
<seg id="10">In unserem Problem-Setup müssen wir also angesichts der Fragen diese Frage lösen und die numerischen Antworten erhalten.</seg>
<seg id="11">In unseren Datensätzen finden wir auch den mathematischen Ausdruck, der uns zu dieser bestimmten Antwort führt.</seg>
<seg id="12">Es gelten also auch hier bestimmte Annahmen wie in früheren Arbeiten.</seg>
<seg id="13">Wir gehen davon aus, dass die Genauigkeit der Mengen bekannt ist.</seg>
<seg id="14">Und wir betrachten nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponentialrechnung.</seg>
<seg id="15">Darüber hinaus können komplizierte Operatoren in diese Grundoperatoren zerlegt werden.</seg>
<seg id="16">Die früheren Arbeiten zum Lösen von mathematischen Wortproblemen lassen sich also in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modell einteilen.</seg>
<seg id="17">Das traditionelle Sequenz-zu-Sequenz-Modell wandelt also den Ausdruck in eine spezifische Sequenz für die Generierung um.</seg>
<seg id="18">Es ist ziemlich einfach zu implementieren und kann für viele verschiedene komplizierte Probleme verallgemeinert werden.</seg>
<seg id="19">Die Nachteile sind jedoch, dass die Leistung im Allgemeinen nicht besser ist als die des strukturierten Modells und dass es an Interpretierbarkeit für Vorhersagen mangelt.</seg>
<seg id="20">Aber eigentlich ist diese Richtung immer noch recht beliebt wegen des Transformermodells.</seg>
<seg id="21">In baumbasierten Modellen strukturieren wir diese Ausdrücke in Baumform und folgen einem geordneten Traversal in Baumgenerationen.</seg>
<seg id="22">Hier fahren wir also fort, die Operatoren zu generieren, bis wir die Blätter erreichen, die die Mengen darstellen.</seg>
<seg id="23">Das Gute daran ist, dass wir dadurch diese binäre Baumstruktur erhalten. Sie ist eigentlich ziemlich kontraintuitiv, weil wir zuerst den Operator und dann am Ende die Mengen generieren müssen.</seg>
<seg id="24">Zudem enthält sie auch einige sich wiederholende Berechnungen.</seg>
<seg id="25">Wenn wir uns also diesen Ausdruck ansehen, wird 8 x 3 + 3 eigentlich zweimal generiert, aber eigentlich sollten wir die Ergebnisse wiederverwenden.</seg>
<seg id="26">In unserem vorgeschlagenen Ansatz wollen wir diese Probleme schrittweise und auf interpretierbare Art und Weise lösen.</seg>
<seg id="27">Hier im zweiten Schritt zum Beispiel können wir diese Teiler erhalten, was 27 ergibt.</seg>
<seg id="28">Wir können auch auf die ursprünglichen Fragen zurückgreifen, um die relevanten Inhalte zu finden.</seg>
<seg id="29">In diesen Schritten erhalten wir die Teiler.</seg>
<seg id="30">Und in diesem dritten Schritt erhalten wir dann den Quotienten.</seg>
<seg id="31">Okay. Nach diesen drei Schritten können wir die Ergebnisse des zweiten Schritts wiederverwenden, dann die Ergebnisse des vierten Schritts erhalten und schließlich die Dividenden erhalten.</seg>
<seg id="32">Hier wird also der gesamte Ausdruck direkt generiert und nicht nur ein einzelner Operator oder eine Menge.</seg>
<seg id="33">Dadurch wird der Prozess genauer.</seg>
<seg id="34">In unserem deduktiven System beginnen wir also zunächst mit einer Reihe von Mengen, die in den Fragen vorgestellt werden, und schließen auch mit einer Konstante als Anfangszustand.</seg>
<seg id="35">Der Ausdruck wird also durch e i j o p dargestellt.</seg>
<seg id="36">Wir führen den Operator von q_i nach q_j aus, und dieser Ausdruck ist tatsächlich geregelt.</seg>
<seg id="37">Wir haben hier also auch eine Subtraktion mit Wörtern, um die umgekehrte Richtung darzustellen.</seg>
<seg id="38">Dies ist der Beziehungsextraktion recht ähnlich.</seg>
<seg id="39">In einem formalen deduktiven System wenden wir also in einem Zeitschritt t den Operator zwischen dem Paar q_i und q_j an und erhalten dann diesen neuen Ausdruck.</seg>
<seg id="40">Wir fügen ihn dem nächsten Zustand hinzu, um eine neue Menge zu erhalten.</seg>
<seg id="41">Diese Folien veranschaulichen also die Entwicklung des Zustands, wobei wir dem aktuellen Zustand immer neue Ausdrücke hinzufügen.</seg>
<seg id="42">In unseren Modellimplementierungen verwenden wir also zunächst ein vortrainiertes Sprachmodell, bei dem es sich um BERTs oder Robertas handeln kann, und dann kodieren wir den Satz und erhalten diese Mengendarstellungen.</seg>
<seg id="43">Sobald wir also die Mengendarstellungen haben, können wir mit der Inferenz beginnen.</seg>
<seg id="44">Hier zeigen wir ein Beispiel für q_1, um die Darstellung für q_2 geteilt durch q_2 und dann mal q_3 zu erhalten.</seg>
<seg id="45">Zunächst erhalten wir die Paardarstellung, die im Grunde nur die Verkettung zwischen q_1 und q_2 ist, und dann wenden wir ein Feedforward-Netzwerk an, das durch den Operator parametrisiert ist.</seg>
<seg id="46">Und dann erhalten wir schließlich die Ausdrucksdarstellung q_1 geteilt durch q_2.</seg>
<seg id="47">Aber in der Praxis könnten wir in der Inferenzphase auch den falschen Ausdruck erhalten.</seg>
<seg id="48">Hier sind also alle möglichen Ausdrücke gleich dem Dreifachen der Anzahl der Operatoren.</seg>
<seg id="49">Das Schöne daran ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu kontrollieren.</seg>
<seg id="50">Wenn dieser Ausdruck zum Beispiel nicht erlaubt ist, können wir ihn einfach aus unserem Suchraum entfernen.</seg>
<seg id="51">Im zweiten Schritt machen wir das Gleiche, aber der einzige Unterschied ist, dass wir nur eine Menge mehr haben.</seg>
<seg id="52">Diese Menge stammt also aus dem vorherigen berechneten Ausdruck.</seg>
<seg id="53">So können wir schließlich diesen endgültigen Ausdruck q_3 mal q_4 erhalten.</seg>
<seg id="54">Wir können auch sehen, dass die Anzahl aller möglichen Ausdrücke anders ist als beim vorherigen Schritt.</seg>
<seg id="55">Ein solcher Unterschied erschwert die Anwendung von einer Beam Search, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgewogen ist.</seg>
<seg id="56">Das Trainingsverfahren ist also ähnlich dem Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust bei jedem Zeitschritt optimieren.</seg>
<seg id="57">Auch hier verwenden wir dieses Tau, um darzustellen, wann wir diesen Generierungsprozess beenden sollten.</seg>
<seg id="58">Hier unterscheidet sich der Raum von Sequenz zu Sequenz, weil der Raum bei jedem Zeitschritt anders ist, während im traditionellen Sequenz-zu-Sequenz-Modell dies die Anzahl des Vokabulars ist.</seg>
<seg id="59">Es erlaubt uns auch, bestimmte Beschränkungen aus dem Vorwissen aufzuerlegen.</seg>
<seg id="60">Daher führen wir Experimente mit den häufig verwendeten Datensätzen der mathematischen Wortprobleme durch, MAWPS, Math23K, MathQA und SVAMP.</seg>
<seg id="61">Hier zeigen wir kurz die Ergebnisse im Vergleich zu den bisherigen besten Ansätzen.</seg>
<seg id="62">Unsere leistungsstärkste Variante ist also Roberta-DeductiveReasoner.</seg>
<seg id="63">Tatsächlich verwenden wir keine Beam Search, im Gegensatz zu allen früheren Ansätzen, die Beam Search verwendet haben.</seg>
<seg id="64">Okay. Daher sind die besten Ansätze oft baumbasierte Modelle.</seg>
<seg id="65">Insgesamt ist unser Reasoner also in der Lage, dieses baumbasierte Modell signifikant zu übertreffen.</seg>
<seg id="66">Aber wir können sehen, dass die absoluten Zahlen bei MathQA oder SVAMP nicht wirklich hoch sind.</seg>
<seg id="67">Deshalb untersuchen wir die Ergebnisse bei SVAMP weiter.</seg>
<seg id="68">Dieser Datensatz ist eine Herausforderung, weil der Autor versucht hat, manuell etwas hinzuzufügen, um das NLP-Modell zu verwirren, wie z. B. das Hinzufügen irrelevanter Informationen und zusätzlicher Mengen.</seg>
<seg id="69">In unserer Vorhersage stellen wir also fest, dass einige der Zwischenwerte eigentlich negativ sind.</seg>
<seg id="70">Zum Beispiel geht es in diesen Fragen darum, wie viele Äpfel Jake hat.</seg>
<seg id="71">Aber wir haben einige zusätzliche Informationen wie 17 Bilder weniger, und Steven hat acht Bilder, was völlig irrelevant ist.</seg>
<seg id="72">Unser Modell macht also eine Vorhersage wie diese, die negative Werte ergibt.</seg>
<seg id="73">Wir stellen fest, dass diese beiden Ausdrücke tatsächlich ähnliche Werte haben.</seg>
<seg id="74">Wir können also diesen Suchraum eingrenzen, indem wir die negativen Ergebnisse entfernen, damit die Antwort richtig ist.</seg>
<seg id="75">Wir stellen also fest, dass eine solche Bedingung für einige Modelle tatsächlich eine erhebliche Verbesserung darstellt.</seg>
<seg id="76">Bei BERT haben wir uns zum Beispiel um sieben Punkte verbessert und beim Roberta-Basismodell haben wir uns sogar um zwei Punkte verbessert.</seg>
<seg id="77">Ein besseres Sprachmodell hat also bessere Sprachverständnisfähigkeiten, sodass die Zahl hier höher bei Roberta und niedriger bei BERT ist.</seg>
<seg id="78">Wir versuchen auch, die Schwierigkeiten hinter all diesen Datensätzen zu analysieren.</seg>
<seg id="79">Wir gehen davon aus, dass die Anzahl der ungenutzten Mengen hier als irrelevante Informationen betrachtet werden kann.</seg>
<seg id="80">Hier können wir also sehen, dass wir den Prozentsatz der Proben mit ungenutzten Mengen haben, und der SVAMP-Datensatz hat den größten Anteil.</seg>
<seg id="81">Hier zeigen wir auch die Gesamtleistung.</seg>
<seg id="82">Bei den Proben ohne ungenutzte Mengen ist die Leistung sogar höher als die Gesamtleistung.</seg>
<seg id="83">Aber bei diesen Proben mit ungenutzten Mengen ist sie eigentlich viel schlechter als die Gesamtleistung.</seg>
<seg id="84">Bei MAWPS haben wir nicht wirklich viele Testfälle, also ignoriere ich diesen Teil einfach.</seg>
<seg id="85">Schließlich wollen wir die Interpretierbarkeit anhand eines Frage-Störungsbeispiels zeigen.</seg>
<seg id="86">Hier macht unser Modell also tatsächlich eine falsche Vorhersage im ersten Schritt.</seg>
<seg id="87">Wir können diesen Ausdruck also tatsächlich mit dem Satz hier korrelieren. Okay.</seg>
<seg id="88">Wir denken also, dass dieser Satz das Modell zu falschen Vorhersagen verleiten könnte.</seg>
<seg id="89">Hier führt eine weitere 35 dazu, dass das Modell denkt, dass es ein Additionsoperator sein sollte.</seg>
<seg id="90">Wir versuchen also den Satz so umzuformulieren, dass die Anzahl der Birnbäume um 35 weniger als die der Apfelbäume ist.</seg>
<seg id="91">Wir sorgen also dafür, dass eine genauere Semantik vermittelt wird, sodass das Modell in der Lage ist, die Vorhersage korrekt zu treffen.</seg>
<seg id="92">Diese Studie zeigt also, wie uns die interpretierbaren Vorhersagen helfen, das Modellverhalten zu verstehen.</seg>
<seg id="93">Um unsere Arbeit abzuschließen: Zunächst ist unser Modell tatsächlich ziemlich effizient.</seg>
<seg id="94">Wir sind in der Lage, ein interpretierbares Lösungsverfahren anzubieten.</seg>
<seg id="95">Wir können einfach einiges Vorwissen als Bedingung einbeziehen, was zur Verbesserung der Leistung beitragen kann.</seg>
<seg id="96">Schließlich gilt der zugrundeliegende Mechanismus nicht nur für das Lösen von Netzwerkproblemen, sondern auch für andere Aufgaben, die mehrstufiges Argumentieren erfordern.</seg>
<seg id="97">Auch sind uns gewisse Grenzen gesetzt.</seg>
<seg id="98">Wenn wir eine große Anzahl von Operatoren oder Konstanten haben, könnte der Speicherverbrauch ziemlich hoch sein.</seg>
<seg id="99">Wie bereits erwähnt ist es zudem aufgrund der unausgewogenen Wahrscheinlichkeitsverteilung zwischen den verschiedenen Zeitschritten eine ziemliche Herausforderung, die Beam Search-Strategie anzuwenden.</seg>
<seg id="100">Wir sind am Ende des Vortrags angekommen und freuen uns auf etwaige Fragen. Vielen Dank!</seg>
</doc>
<doc docid="2022.acl-long.468" genre="presentations">
<talkid>2022.acl-long.468</talkid>
<abstract>Statutory article retrieval is the task of automatically retrieving law articles relevant to a legal question. While recent advances in natural language processing have sparked considerable interest in many legal tasks, statutory article retrieval remains primarily untouched due to the scarcity of large-scale and high-quality annotated datasets. To address this bottleneck, we introduce the Belgian Statutory Article Retrieval Dataset (BSARD), which consists of 1,100+ French native legal questions labeled by experienced jurists with relevant articles from a corpus of 22,600+ Belgian law articles. Using BSARD, we benchmark several state-of-the-art retrieval approaches, including lexical and dense architectures, both in zero-shot and supervised setups. We find that fine-tuned dense retrieval models significantly outperform other systems. Our best performing baseline achieves 74.8% R@100, which is promising for the feasibility of the task and indicates there is still room for improvement. By the specificity of the domain and addressed task, BSARD presents a unique challenge problem for future research on legal information retrieval. Our dataset and source code are publicly available.</abstract>
<seg id="101">Hallo, mein Name ist Antoine und ich bin von der Universität Maastricht.</seg>
<seg id="102">Ich werde meine gemeinsame Arbeit mit Jerry vorstellen, bei der es um einen neuen Datensatz für das Retrieval von Gesetzesartikeln geht.</seg>
<seg id="103">Rechtsfragen sind ein wesentlicher Bestandteil des Lebens vieler Menschen.</seg>
<seg id="104">Aber die Mehrheit der Bürger verfügt nur über wenig Wissen über ihre Rechte und grundlegende rechtliche Verfahren.</seg>
<seg id="105">Dies hat zur Folge, dass viele schutzbedürftige Bürger, die sich die kostspielige Hilfe eines Rechtsexperten nicht leisten können, schutzlos bleiben oder schlimmstenfalls ausgebeutet werden.</seg>
<seg id="106">Unsere Arbeit zielt darauf ab, die Kluft zwischen den Menschen und dem Gesetz zu überbrücken, indem ein effektives Retrieval-System für Gesetzesartikel entwickelt wird.</seg>
<seg id="107">Ein solches System könnte einen kostenlosen professionellen Rechtshilfedienst für unerfahrene Menschen anbieten.</seg>
<seg id="108">Bevor wir uns dem Hauptbeitrag dieser Arbeit widmen, wollen wir zunächst das Problem des Retrievals von Gesetzesartikeln beschreiben.</seg>
<seg id="109">Eine einfache Frage zu einer Rechtsangelegenheit wäre: „Was riskiere ich, wenn ich das Berufsgeheimnis verletze?“</seg>
<seg id="110">Hier wird ein Modell benötigt, um alle relevanten Gesetzesartikel aus einem großen Bestand an Rechtsvorschriften abzurufen.</seg>
<seg id="111">Diese Informationsbeschaffungsaufgabe bringt eine Reihe von Herausforderungen mit sich.</seg>
<seg id="112">Erstens: Es geht um zwei Arten von Sprache.</seg>
<seg id="113">Die gemeinsame natürliche Sprache für die Fragen und die komplexe juristische Sprache für die Gesetze.</seg>
<seg id="114">Dieser Unterschied bei den Sprachverteilungen macht es für ein System schwieriger, relevante Kandidaten zu finden, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine rechtliche Frage übersetzen kann, die der Terminologie der Gesetze entspricht.</seg>
<seg id="115">Außerdem ist das Recht keine Ansammlung unabhängiger Artikel, die für sich allein als vollständige Informationsquelle betrachtet werden können, anders als beispielsweise Nachrichten oder Rezepte.</seg>
<seg id="116">Vielmehr handelt es sich um eine strukturierte Sammlung von Rechtsvorschriften, die nur im Gesamtkontext, d. h. zusammen mit den ergänzenden Informationen aus den Nachbarartikeln, den Bereichen und Teilbereichen, denen sie angehören, und ihrer Stellung im Aufbau des Gesetzes, eine vollständige Bedeutung haben.</seg>
<seg id="117">Schließlich handelt es sich bei den Gesetzesartikeln nicht um kleine Absätze, die in den meisten Retrieval-Arbeiten die typische Retrieval-Einheit darstellen.</seg>
<seg id="118">Hier gibt es lange Dokumente, die bis zu 6.000 Wörter umfassen können.</seg>
<seg id="119">Die jüngsten Fortschritte im Bereich NLP haben großes Interesse an vielen juristischen Aufgaben geweckt, z. B. an der Vorhersage juristischer Urteile oder der automatisierten Prüfung von Verträgen.</seg>
<seg id="120">Doch das Retrieval von Gesetzesartikeln ist aufgrund des Mangels an großen und qualitativ hochwertig markierten Datensätzen weitgehend unberührt geblieben.</seg>
<seg id="121">In dieser Arbeit stellen wir einen neuen Datensatz in der französischen Muttersprache vor, mit dem wir Retrievalmodelle untersuchen und analysieren, ob sie bei der Aufgabe der Suche nach Gesetzesartikeln an die Effizienz und Zuverlässigkeit eines Rechtsexperten rankommen können.</seg>
<seg id="122">Unser belgischer Datensatz für das Retrieval von Gesetzesartikeln, BSARD, besteht aus mehr als 1.100 juristischen Fragen, die von belgischen Bürgern gestellt wurden.</seg>
<seg id="123">Diese Fragen decken ein breites Spektrum an Themen ab, von Familie, Wohnen, Geld bis hin zu Arbeit und sozialer Sicherheit.</seg>
<seg id="124">Jede von ihnen wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als 22.600 Rechtsartikeln aus belgischen Gesetzbüchern markiert.</seg>
<seg id="125">Lassen Sie uns nun darüber sprechen, wie wir diesen Datensatz gesammelt haben.</seg>
<seg id="126">Zunächst haben wir einen großen Korpus von Rechtsartikeln zusammengestellt.</seg>
<seg id="127">Wir haben 32 öffentlich zugängliche belgische Gesetzbücher berücksichtigt und alle Artikel sowie die entsprechenden Abschnittsüberschriften extrahiert.</seg>
<seg id="128">Dann haben wir rechtliche Fragen mit Verweisen auf einschlägige Gesetze zusammengestellt.</seg>
<seg id="129">Zu diesem Zweck arbeiten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr etwa 4.000 E-Mails von belgischen Bürgern erhält, die um Rat in einer persönlichen Rechtsangelegenheit bitten.</seg>
<seg id="130">Wir hatten das Glück, Zugang zu ihren Websites zu erhalten, auf denen ihr Team aus erfahrenen Juristen die häufigsten Rechtsangelegenheiten der Belgier behandelt.</seg>
<seg id="131">Wir haben Tausende von Fragen gesammelt und mit Kategorien, Unterkategorien und Verweisen auf einschlägige Gesetze annotiert.</seg>
<seg id="132">Schließlich haben wir die rechtlichen Verweise überprüft und die Fragen herausgefiltert, deren Verweise nicht in einem der von uns berücksichtigten Gesetzbücher enthalten waren.</seg>
<seg id="133">Die übrigen Referenzen wurden zusammengeführt und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt.</seg>
<seg id="134">Am Ende hatten wir 1.108 Fragen, jede sorgfältig markiert mit den IDs der relevanten Artikel aus unserem großen Korpus von 22.630 Gesetzesartikeln.</seg>
<seg id="135">Darüber hinaus enthält jede Frage eine Hauptkategorie und eine Verkettung von Unterkategorien.</seg>
<seg id="136">Jeder Artikel hat eine Verkettung mit der anschließenden Überschrift im Aufbau des Gesetzes.</seg>
<seg id="137">Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für künftige Forschungen zum juristischen Informationen-Retrieval oder zur juristischen Textklassifikation von Interesse sein.</seg>
<seg id="138">Schauen wir uns einige Merkmale unseres Datensatzes an.</seg>
<seg id="139">Die Fragen sind zwischen fünf und 24 Wörter lang, mit einem Mittelwert von 14 Wörtern.</seg>
<seg id="140">Die Artikel sind mit einer durchschnittlichen Länge von 77 Wörtern sehr viel länger, wobei 142 von ihnen mehr als 1.000 Wörter umfassen.</seg>
<seg id="141">Der längste von ihnen hat bis zu 5.790 Wörter.</seg>
<seg id="142">Wie bereits erwähnt decken die Fragen ein breites Spektrum an Themen ab, wobei etwa 85 Prozent der Fragen entweder die Familie, das Wohnen, Geld oder die Gerechtigkeit betreffen.</seg>
<seg id="143">Die restlichen 15 Prozent betreffen entweder soziale Sicherheit, Ausländer oder Arbeit.</seg>
<seg id="144">Die Artikel sind auch sehr vielfältig, da sie aus 32 verschiedenen belgischen Gesetzbüchern stammen, die eine große Reihe von Rechtsthemen abdecken.</seg>
<seg id="145">Hier ist die Gesamtzahl der Artikel, die aus diesen belgischen Gesetzbüchern gesammelt wurden.</seg>
<seg id="146">Von den 22.633 Artikeln werden nur 1.612 als relevant für mindestens eine Frage im Datensatz genannt.</seg>
<seg id="147">Etwa 80 Prozent dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, dem Gerichtsgesetzbuch, dem Gesetzbuch für strafrechtliche Ermittlungen oder dem Strafgesetzbuch.</seg>
<seg id="148">Bei 18 von 32 Gesetzbüchern werden weniger als fünf Artikel als relevant für mindestens eine Frage bestimmt.</seg>
<seg id="149">Dies lässt sich dadurch erklären, dass diese Gesetzbücher weniger auf den Einzelnen und seine Anliegen ausgerichtet waren.</seg>
<seg id="150">Insgesamt liegt der Durchschnitt der Anzahl der Zitate bei zwei, und weniger als 25 Prozent der zitierten Artikel werden mehr als fünfmal zitiert.</seg>
<seg id="151">Unter Verwendung aller Datensätze haben wir verschiedene Retrieval-Ansätze, einschließlich lexikalischer und dichter Architektur, einem Benchmarking unterzogen.</seg>
<seg id="152">Bei einer Abfrage und einem Artikel weist ein lexikalisches Modell dem Abfrage-Artikel-Paar eine Punktzahl zu, indem es die Summe der Gewichtungen von jedem der Begriffe in diesem Artikel über die Abfrage-Termini berechnet.</seg>
<seg id="153">Wir experimentieren mit den Standard-Ranking-Funktionen TF-IDF und BM25.</seg>
<seg id="154">Das Hauptproblem bei diesen Ansätzen ist, dass sie nur Artikel finden können, die Schlüsselwörter enthalten, die in der Anfrage vorkommen.</seg>
<seg id="155">Um diese Einschränkung zu überwinden, experimentieren wir mit einer neuronal-basierten Architektur, die semantische Beziehungen zwischen Abfragen und Artikeln erfassen kann.</seg>
<seg id="156">Wir verwenden ein Bi-Encoder-Modell, das Abfragen und Artikel in dichten Vektor-Darstellungen abbildet und einen Relevanzwert zwischen einem Abfrage-Artikel-Paar anhand der Ähnlichkeit ihrer Einbettungen berechnet.</seg>
<seg id="157">Diese Einbettungen resultieren in der Regel aus einer Pooling-Operation auf der Ausgabe eines Worteinbettungsmodells.</seg>
<seg id="158">Zunächst untersuchen wir die Effektivität von Siamesischen Bi-Encodern in einem Zero-Shot-Evaluation-Setup, d. h. dass vortrainierte Worteinbettungsmodelle ohne zusätzliche Feinabstimmung sofort angewendet werden.</seg>
<seg id="159">Wir experimentieren mit kontextunabhängigen Textencodern, wie word2vec und fastText, und kontextabhängigen Einbettungsmodellen, wie Roberta und insbesondere CamemBERT, einem französischen Roberta-Modell.</seg>
<seg id="160">Darüber hinaus trainieren wir unser eigenes, auf CamemBERT basierendes Modell, Bi-Encoder, auf unserem Datensatz.</seg>
<seg id="161">Beachten Sie, dass wir beim Training mit den beiden Varianten der Bi-Encoder-Architektur experimentieren.</seg>
<seg id="162">Siamese, das ein einzigartiges Worteinbettungsmodell verwendet, das die Abfrage und den Artikel zusammen in einem geteilten dichten Vektorraum abbildet, und Two-Tower, das zwei unabhängige Worteinbettungsmodelle verwendet, die die Abfrage und den Artikel getrennt in verschiedenen Einbettungsräumen kodieren.</seg>
<seg id="163">Wir experimentieren mit Mittelwert, Maximalwert und CLS-Pooling sowie mit Produkt und Kosinus für die Berechnung von Ähnlichkeiten.</seg>
<seg id="164">Hier sind die Ergebnisse unserer Baseline auf den Testsätzen.</seg>
<seg id="165">Die lexikalischen Methoden werden oben, die Siamesischen Bi-Encoder in einem Zero-Shot-Setup in der Mitte und die fein abgestimmten Bi-Encoder werden unten ausgewertet.</seg>
<seg id="166">Insgesamt übertrifft der fein abgestimmte Bi-Encoder alle anderen Baselines deutlich.</seg>
<seg id="167">Das Two-Tower-Modell verbessert sich gegenüber seinen Siamesischen Varianten beim Recall um 100, schneidet aber bei den anderen Metriken ähnlich gut ab.</seg>
<seg id="168">Obwohl BM25 deutlich schlechter abschnitt als der trainierte Bi-Encoder, zeigte seine Leistung, dass er immer noch eine gute Basis für ein domänenspezifisches Retrieval ist.</seg>
<seg id="169">Bezüglich der Zero-Shot-Evaluation des Siamesischen Bi-Encoders stellen wir fest, dass die direkte Verwendung der Einbettungen eines vortrainierten CamemBERT-Modells ohne Optimierung für die Informationen-Retrieval-Aufgabe schlechte Ergebnisse liefert, was mit früheren Erkenntnissen übereinstimmt.</seg>
<seg id="170">Darüber hinaus stellen wir fest, dass der word2vec-basierte Bi-Encoder die fastText- und BERT-basierten Modelle signifikant übertrifft, was darauf hindeutet, dass vortrainierte Wort-Ebenen-Einbettungen vielleicht besser für die Aufgabe geeignet sind als Zeichen- oder Unterwort-Ebenen-Einbettungen, wenn sie direkt verwendet werden.</seg>
<seg id="171">Obwohl diese Ergebnisse vielversprechend sind, lassen sie im Vergleich zu einem erfahrenen Rechtsexperten auf ein großes Verbesserungspotenzial schließen, der früher oder später alle relevanten Artikel zu jeder Frage abrufen kann und somit perfekte Ergebnisse erzielt.</seg>
<seg id="172">Abschließend möchte ich auf zwei Einschränkungen unseres Datensatzes eingehen.</seg>
<seg id="173">Erstens ist der Artikelkorpus auf die Artikel der 32 herangezogenen belgischen Gesetzbücher beschränkt, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Dekreten, Richtlinien und Verordnungen fehlen.</seg>
<seg id="174">Bei der Erstellung des Datensatzes werden alle Verweise auf diese nicht erfassten Artikel ignoriert, was dazu führt, dass sich einige Fragen am Ende nur auf einen Bruchteil der ursprünglichen Anzahl der relevanten Artikel beziehen.</seg>
<seg id="175">Diese Information impliziert also, dass die Antwort, die in den übrigen relevanten Artikeln enthalten ist, unvollständig sein könnte, obwohl sie immer noch völlig angemessen ist.</seg>
<seg id="176">Zweitens sollten wir beachten, dass nicht alle rechtlichen Fragen allein mit Gesetzen beantwortet werden können.</seg>
<seg id="177">Zum Beispiel die Frage: „Kann ich meinen Mietern kündigen, wenn sie zu viel Lärm machen?“</seg>
<seg id="178">Möglicherweise gibt es im Gesetz keine detaillierte Antwort, die eine bestimmte Lärmschwelle festlegt, ab der eine Räumung zulässig ist.</seg>
<seg id="179">Stattdessen sollte sich der Vermieter wahrscheinlich eher auf das Fallrecht stützen und Präzedenzfälle finden, die seiner aktuellen Situation ähnlich sind.</seg>
<seg id="180">Zum Beispiel veranstalten die Mieter zwei Partys pro Woche bis zwei Uhr morgens.</seg>
<seg id="181">Einige Fragen eignen sich daher besser als andere für die Aufgabe des Retrievals von Gesetzesartikeln. Die Domäne der weniger geeigneten Fragen muss noch bestimmt werden.</seg>
<seg id="182">Wir hoffen, dass unsere Arbeit das Interesse an der Entwicklung praktischer und zuverlässiger Modelle für das Retrieval von Gesetzesartikeln weckt.</seg>
<seg id="183">Sie kann dazu beitragen, den Zugang zur Justiz für alle zu verbessern.</seg>
<seg id="184">Sie können unser Paper, den Datensatz und die Gesetzbücher unter den folgenden Links einsehen. Vielen Dank!</seg>
</doc>
<doc docid="2022.acl-long.567" genre="presentations">
<talkid>2022.acl-long.567</talkid>
<abstract>We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V&L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&L models from a linguistic perspective, complementing the canonical task-centred V&L evaluations.</abstract>
<seg id="185">Hallo, wir freuen uns, Ihnen unsere Arbeit über VALSE vorstellen zu können. Das ist ein Aufgaben-unabhängiger Benchmark, der für das Testen von Seh- und Sprachmodellen mit spezifischen sprachlichen Phänomenen konzipiert ist.</seg>
<seg id="186">Warum haben wir uns die Mühe gemacht, diesen Benchmark einzurichten?</seg>
<seg id="187">In den letzten Jahren haben wir eine explosionsartige Zunahme von Transformer-basierten Seh- und Sprachmodellen erlebt, die mit großen Mengen an Bild-Text-Paaren vortrainiert wurden.</seg>
<seg id="188">Jedes dieser Modelle treibt den Fortschritt bei Seh- und Sprachaufgaben voran, wie z. B. die visuelle Fragenbeantwortung, das Argumentieren mit visuellem Menschenverstand, Bild-Retrieval, Phrasen-Erdung.</seg>
<seg id="189">Wir haben also die Nachricht erhalten, dass die Genauigkeiten bei diesen Aufgaben und spezifischen Benchmarks stetig steigen.</seg>
<seg id="190">Aber wissen wir, was die Modelle tatsächlich gelernt haben?</seg>
<seg id="191">Was hat ein Seh- und Sprach-Transformer verstanden, wenn er für dieses Bild und diesen Satz eine hohe Punktzahl zuweist?</seg>
<seg id="192">Und hier eine niedrige Punktzahl?</seg>
<seg id="193">Konzentrieren sich Seh- und Sprachmodelle auf die wesentlichen Punkte?</seg>
<seg id="194">Oder konzentrieren sie sich auf Verzerrungen, wie in früheren Arbeiten aufgezeigt wurde?</seg>
<seg id="195">Um diesen Aspekt näher zu beleuchten, schlagen wir eine eher aufgabenbezogene agnostische Richtung vor und führen VALSE ein, das die Empfindlichkeit von Seh- und Sprachmodellen für spezifische sprachliche Phänomene testet, die sowohl die sprachlichen als auch die visuellen Modalitäten betreffen.</seg>
<seg id="196">Wir zielen auf Existenz, Pluralität, Zählung, räumliche Beziehungen, Handlungen und Entity-Koreferenz ab.</seg>
<seg id="197">Doch wie lässt sich überprüfen, ob die Seh- und Sprachmodelle dieses Phänomen erfasst haben?</seg>
<seg id="198">Das Verfälschen einer Methode wurde zuvor von Ravi Shekhar und Mitarbeitern nur für Substantivphrasen in den Seh- und Sprachmodellen und beim Zählen von uns in früheren Arbeiten angewandt.</seg>
<seg id="199">Verfälschen bedeutet im Grunde, dass wir die Beschriftung eines Bildes nehmen und eine Verfälschung erzeugen, indem wir die Beschriftung so verändern, dass sie das Bild nicht mehr beschreibt.</seg>
<seg id="200">Wir führen diese Veränderungen der Phrase durch, indem wir uns auf sechs spezifische Teile konzentrieren, wie Existenz, Pluralität, Zählung, räumliche Relationen, Handlungen und Entität-Koreferenz, wobei jeder Teil aus einem oder mehreren Instrumenten bestehen kann, falls wir mehr als einen interessanten Weg gefunden haben, um Verfälschungsinstanzen zu erzeugen.</seg>
<seg id="201">Zum Beispiel haben wir im Fall des Handlungsteils zwei Instrumente, eines, bei dem das Handlungsverb durch eine andere Handlung ersetzt wird, und eines, bei dem die Aktanten ausgetauscht werden.</seg>
<seg id="202">Zählung und Koreferenz sind auch Teile, die mehr als ein Instrument haben.</seg>
<seg id="203">Wir erstellen diese Verfälschungen, indem wir sicherstellen, dass sie das Bild nicht beschreiben, dass sie grammatikalisch richtig sind und auch korrekte Sätze bilden.</seg>
<seg id="204">Das ist nicht einfach zu bewerkstelligen, denn eine verfälschte Beschriftung ist weniger wahrscheinlich als die ursprüngliche Beschriftung.</seg>
<seg id="205">Es ist zwar zum Beispiel nicht unmöglich, aber statistisch gesehen ist es unwahrscheinlicher, dass Pflanzen einen Menschen schneiden, als dass ein Mensch Pflanzen schneidet. Große Seh- und Sprachmodelle könnten dies erkennen.</seg>
<seg id="206">Daher müssen wir handeln, wenn wir gültige Verfälschungen erhalten wollen.</seg>
<seg id="207">Erstens nutzen wir starke Sprachmodelle, um Verfälschungen vorzuschlagen.</seg>
<seg id="208">Zweitens verwenden wir die Inferenz der natürlichen Sprache oder kurz NLI, um Verfälschungen herauszufiltern, die das Bild noch beschreiben könnten. Bei der Konstruktion von Verfälschungen müssen wir sicherstellen, dass sie das Bild nicht beschreiben.</seg>
<seg id="209">Um dies automatisch zu testen, wenden wir die Inferenz der natürlichen Sprache mit der folgenden Logik an.</seg>
<seg id="210">Wir betrachten ein Bild als die Prämisse und seine Beschriftung als die daraus abgeleitete Hypothese.</seg>
<seg id="211">Darüber hinaus betrachten wir die Beschriftung als Prämisse und die Verfälschung als ihre Hypothese.</seg>
<seg id="212">Wenn ein NLI-Modell vorhersagt, dass die Verfälschung der Beschriftung widerspricht oder neutral ist, ist das für uns ein Indikator für eine gültige Verfälschung.</seg>
<seg id="213">Wenn ein NLI vorhersagt, dass die Verfälschung die Beschriftung beinhaltet, kann es keine gute Verfälschung sein, da sie durch Transitivität eine wahrheitsgemäße Beschreibung des Bildes liefert. Wir filtern diese Verfälschungen heraus.</seg>
<seg id="214">Dieses Verfahren ist aber nicht perfekt, sondern nur ein Indikator für gültige Verfälschungen.</seg>
<seg id="215">Daher setzen wir als dritte Maßnahme zur Generierung gültiger Verfälschungen menschliche Annotatoren ein, um die in VALSE verwendeten Daten zu validieren.</seg>
<seg id="216">Nach dem Filtern und der menschlichen Evaluation haben wir also so viele Testinstanzen, wie in dieser Tabelle beschrieben.</seg>
<seg id="217">Beachten Sie hier, dass VALSE keine Trainingsdaten, sondern nur Testdaten liefert.</seg>
<seg id="218">Da es sich um einen reinen Zero-Shot-Test-Benchmark handelt, ist er so konzipiert, dass er die bestehenden Fähigkeiten der Seh- und Sprachmodelle nach dem Vortraining nutzt.</seg>
<seg id="219">Die Feinabstimmung würde es den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen.</seg>
<seg id="220">Wir alle wissen, dass diese Modells gerne schummeln und Abkürzungen nehmen.</seg>
<seg id="221">Wie wir schon erwähnten, sind wir an der Bewertung interessiert, welche Fähigkeiten die Seh- und Sprachmodelle nach dem Vortraining haben.</seg>
<seg id="222">Wir experimentieren mit fünf Seh- und Sprachmodellen auf VALSE, nämlich mit CLIP, LXMert, ViLBERT, ViLBERT zwölf in einem und VisualBERT.</seg>
<seg id="223">Zwei unserer wichtigsten Evaluationsmetriken sind die Genauigkeit der Modelle bei der Klassifikation von Bild-Satz-Paaren in Bildbeschreibungen und Verfälschungen.</seg>
<seg id="224">Vielleicht ist es für dieses Video relevanter, unsere tolerantere Metrik vorzustellen, die paarweise Genauigkeit. Diese misst, ob die Bild-Satz-Ausrichtung für das korrekte Bild-Text-Paar größer ist als für sein Verfälschungspaar.</seg>
<seg id="225">Weitere Metriken und Ergebnisse dazu finden Sie in unserem Paper.</seg>
<seg id="226">Die Ergebnisse mit paarweiser Genauigkeit werden hier gezeigt und stimmen mit den Ergebnissen überein, die wir von den anderen Metriken erhalten haben, nämlich dass die beste Zero-Shot-Leistung von ViLBERT zwölf zu eins erreicht wird, gefolgt von ViLBERT, LXMert, CLIP und schließlich VisualBERT.</seg>
<seg id="227">Es ist bemerkenswert, wie Instrumente, die sich auf die einzelnen Objekte konzentrieren, wie Existenz und Substantivphrasen, von ViLBERT zwölf zu eins fast gelöst werden. Das unterstreicht, dass Modelle in der Lage sind, benannte Objekte und ihre Existenz in Bildern zu identifizieren.</seg>
<seg id="228">Keines der verbleibenden Teile kann jedoch in unseren gegnerischen Verfälschungseinstellungen zuverlässig gelöst werden.</seg>
<seg id="229">Wir sehen an den Instrumenten der Pluralität und der Zählung, dass Seh- und Sprachmodelle Schwierigkeiten haben, Verweise auf einzelne und mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen.</seg>
<seg id="230">Der Relationen-Teil zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Relation zwischen Objekten in einem Bild richtig zu klassifizieren.</seg>
<seg id="231">Sie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie durch Plausibilitätsbias unterstützt werden, wie wir bei dem Handlung-Teil sehen.</seg>
<seg id="232">Beim Koreferenz-Teil geht hervor, dass die Verfolgung mehrerer Verweise auf dasselbe Objekt in einem Bild unter Verwendung von Pronomen auch für die Seh- und Sprachmodelle schwierig ist.</seg>
<seg id="233">Zur Überprüfung der Korrektheit und weil es ein interessantes Experiment ist, vergleichen wir auch zwei reine Textmodelle, GPT 1 und GPT 2. So wollen wir feststellen, ob VALSE durch diese unimodalen Modelle lösbar ist. Wir berechnen die Perplexität der korrekten und der verfälschten Beschriftung, ohne Bild, um den Eintrag mit der niedrigsten Perplexität vorherzusagen.</seg>
<seg id="234">Wenn die Perplexität bei der Verfälschung höher ist, ist das für uns ein Hinweis, dass die verfälschte Beschriftung möglicherweise unter Plausibilitätsverzerrungen oder anderen sprachlichen Verzerrungen leidet.</seg>
<seg id="235">Es ist interessant zu sehen, dass in einigen Fällen die GPT-Modelle mit reinem Text die Plausibilität der Welt besser erfasst haben als die Seh- und Sprachmodelle.</seg>
<seg id="236">Zusammenfassend lässt sich sagen, dass VALSE ein Benchmark ist, der durch das Objektiv von linguistischen Konstrukten schaut, um die Gemeinschaft bei der Verbesserung von Seh- und Sprachmodellen zu unterstützen, indem er ihre visuelle Erdung hart testet.</seg>
<seg id="237">Unsere Experimente zeigen, dass Seh- und Sprachmodelle benannte Objekte und ihre Existenz in Bildern gut erkennen, wie der Existenzteil zeigt. Sie haben aber Schwierigkeiten damit, ihre gegenseitige Abhängigkeit und Beziehungen in visuellen Szenen zu begründen, wenn sie gezwungen sind, sprachliche Indikatoren zu berücksichtigen.</seg>
<seg id="238">Wir möchten die Community wirklich dazu ermutigen, VALSE für die Messung des Fortschritts von Sprach- Erdung mit Seh- und Sprachmodellen zu nutzen.</seg>
<seg id="239">Zudem könnte VALSE als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten. So kann kontrolliert werden, ob ein Datensatz dazu beiträgt, dass sich Modelle in einem der von VALSE getesteten Aspekte verbessern.</seg>
<seg id="240">Wenn Sie Interesse haben, schauen Sie sich die VALSE-Daten auf GitHub an. Wenn Sie irgendwelche Fragen haben, zögern Sie nicht, uns zu kontaktieren.</seg>
</doc>
<doc docid="2022.acl-long.597" genre="presentations">
<talkid>2022.acl-long.597</talkid>
<abstract>A release note is a technical document that describes the latest changes to a software product and is crucial in open source software development. However, it still remains challenging to generate release notes automatically. In this paper, we present a new dataset called RNSum, which contains approximately 82,000 English release notes and the associated commit messages derived from the online repositories in GitHub. Then, we propose classwise extractive-then-abstractive/abstractive summarization approaches to this task, which can employ a modern transformer-based seq2seq network like BART and can be applied to various repositories without specific constraints. The experimental results on the RNSum dataset show that the proposed methods can generate less noisy release notes at higher coverage than the baselines. We also observe that there is a significant gap in the coverage of essential information when compared to human references. Our dataset and the code are publicly available.</abstract>
<seg id="241">Hallo, mein Name ist Kamezawa von der Universität Tokio.</seg>
<seg id="242">Ich werde folgendes Paper vorstellen: RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.</seg>
<seg id="243">Ich werde es in dieser Reihenfolge erklären.</seg>
<seg id="244">Zunächst werde ich die automatische Generierung von Versionshinweisen vorstellen, an der wir in dieser Forschung arbeiten.</seg>
<seg id="245">Ein Versionshinweis ist ein technisches Dokument, das die Änderungen zusammenfasst, die mit jeder Version eines Softwareprodukts vertrieben werden.</seg>
<seg id="246">Das Bild zeigt einen Versionshinweis für Version 2.6.4 der Vuejs-Bibliothek.</seg>
<seg id="247">Versionshinweise spielen bei der Open-Source-Entwicklung eine wichtige Rolle, aber ihre Erstellung ist manuell zeitaufwändig.</seg>
<seg id="248">Daher wäre es sehr nützlich, wenn man automatisch qualitativ hochwertige Versionshinweise generieren könnte.</seg>
<seg id="249">Ich werde zwei frühere Untersuchungen zur automatischen Generierung von Versionshinweisen erwähnen.</seg>
<seg id="250">Zuerst gibt es ein System mit dem Namen ARENA, das im Jahr 2014 veröffentlicht wurde.</seg>
<seg id="251">Es verfolgt einen regelbasierten Ansatz, indem es zum Beispiel den Change-Extractor verwendet, um alle Unterschiede, Bibliotheksänderungen und Dokumentänderungen aus den verschiedenen Versionen zu extrahieren und sie schließlich zu kombinieren.</seg>
<seg id="252">Die auffälligste Funktion dieses Systems ist der Ausgabe-Extraktor in der oberen rechten Ecke.</seg>
<seg id="253">Dies muss Jira, dem Fehlerverwaltungssystem, überlassen werden und kann nur auf Projekte angewendet werden, die Jira verwenden.</seg>
<seg id="254">Mit anderen Worten: Es kann nicht für viele Projekte auf GitHub verwendet werden.</seg>
<seg id="255">Das zweite ist Glyph, das kürzlich in 2020 angekündigt wurde.</seg>
<seg id="256">Es ist im Internet verfügbar und kann über pip installiert werden.</seg>
<seg id="257">Dieses System verfügt über ein einfaches, lernbasiertes Textklassifikationsmodell und gibt eine von fünf Markierung wie Funktionen oder Fehlerbehebungen für jede eingegebene Commit-Nachricht aus.</seg>
<seg id="258">Dieses Bild ist eine Stichprobe, das eine Markierung für eine Korrektur oder Fehlerbehebung zurückgibt.</seg>
<seg id="259">Die Trainingsdaten von Glyph sind ziemlich klein, etwa 5.000, und werden in den unten beschriebenen Experimenten gezeigt.</seg>
<seg id="260">Die Leistung des Textklassifikationsmodells ist nicht hoch.</seg>
<seg id="261">Ich stelle zwei verwandte Forschungsarbeiten vor, deren Probleme jedoch die begrenzte Anwendbarkeit und die knappen Datenressourcen sind.</seg>
<seg id="262">Unser Paper löst diese zwei Probleme und erzeugt automatisch qualitativ hochwertige Versionshinweise.</seg>
<seg id="263">Aufgrund des Problems der begrenzten Anwendbarkeit schlagen wir eine qualitativ hochwertige, klassenweise Zusammenfassungsmethode vor, die nur Commit-Nachrichten als Eingabe verwendet.</seg>
<seg id="264">Diese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden.</seg>
<seg id="265">Für das zweite Problem der knappen Datenressourcen haben wir unseren RNSum-Datensatz erstellt, der aus etwa 82.000 Daten besteht. Diese wurden aus den Daten der öffentlichen GitHub-Repositories mithilfe der GitHub-API gesammelt.</seg>
<seg id="266">Als Nächstes werde ich unseren Datensatz beschreiben.</seg>
<seg id="267">Hier ist ein Beispiel für die Daten.</seg>
<seg id="268">Auf der linken Seite ist eine Commit-Nachricht und auf der rechten Seite sind die Versionshinweise.</seg>
<seg id="269">Versionshinweise sind als Verbesserungen, Korrekturen usw. markiert.</seg>
<seg id="270">Wir haben eine Aufgabe eingerichtet, die die Commit-Nachricht als Eingabe aufnimmt und die Ausgabe ist ein markierter Versionshinweis.</seg>
<seg id="271">Dies kann als eine Zusammenfassungsaufgabe betrachtet werden.</seg>
<seg id="272">Wir haben vier Markierungen vordefiniert: Funktionen, Verbesserungen, Fehlerbehebungen, Beseitigung von Verwerfungen und bahnbrechende Änderungen.</seg>
<seg id="273">Diese wurden auf der Grundlage von früheren Forschungen und anderen Faktoren festgelegt.</seg>
<seg id="274">Die Versionshinweise unten rechts wurde aus den Versionshinweisen unten links extrahiert.</seg>
<seg id="275">Zu diesem Zeitpunkt müssen die vier Markierungen, die im Voraus erstellt wurden, erkannt werden.</seg>
<seg id="276">Die Markierungen sind jedoch nicht immer mit jedem Repository konsistent.</seg>
<seg id="277">Die Markierung „Verbesserungen“ umfasst beispielsweise Verbesserungen, Erweiterungen, Optimierungen und so weiter.</seg>
<seg id="278">Wir haben eine Wortschatzliste mit etwa 30 Markierungen für jede dieser Notationsvarianten erstellt.</seg>
<seg id="279">Diese dient dazu, die Klasse des Versionshinweises zu erkennen. Sie sammelt den Text des Versionshinweises, der als Versionshinweissatz für die Klasse folgt.</seg>
<seg id="280">Als Nächstes folgt eine Commit-Nachricht.</seg>
<seg id="281">Commit-Nachrichten sind nicht an die einzelnen Versionen gebunden.</seg>
<seg id="282">Wenn die aktuelle Version 2.5 bis 19 ist, müssen wir, wie im Bild unten gezeigt, die frühere Version 2.5 bis 18 identifizieren und eine Diff erhalten.</seg>
<seg id="283">Dies ist etwas mühsam und es reicht nicht aus, nur eine Liste von Versionen zu erstellen und die Vorher-Nachher-Bilanz zu betrachten.</seg>
<seg id="284">Wir haben eine heuristische Abgleichsregel erstellt, um die vorherige und die nächste Version zu bekommen.</seg>
<seg id="285">Die Analyse des Datensatzes.</seg>
<seg id="286">Am Ende wurden 7.200 Repositories und 82 Daten gesammelt.</seg>
<seg id="287">Außerdem liegt die durchschnittliche Anzahl der Versionshinweis-Token bei 63, was für eine Zusammenfassungsaufgabe recht hoch ist.</seg>
<seg id="288">Auch die Anzahl der einzigartigen Token ist mit 830.000 recht groß.</seg>
<seg id="289">Dies ist auf die große Anzahl von eindeutigen Klassen- oder Methodennamen im Repository zurückzuführen.</seg>
<seg id="290">Als Nächstes werde ich die vorgeschlagene Methode erläutern.</seg>
<seg id="291">Das klassenweise extraktive und dann abstrahierende Zusammenfassungsmodell besteht aus zwei neuronalen Modulen.</seg>
<seg id="292">Ein Klassifikator verwendet BERT oder CodeBERT und ein Generator verwendet BART.</seg>
<seg id="293">Zuerst verwendet CEAS einen Klassifikator, um jede Commit-Nachricht in fünf Klassen von Versionshinweisen zu klassifizieren, die Verbesserungen, Fehlerbehebungen, Verwerfungen und eine weitere Klasse enthalten.</seg>
<seg id="294">Die als „Sonstiges“ eingestuften Commit-Nachrichten werden verworfen.</seg>
<seg id="295">Dann wendet CEAS den Generator auf die vier markierten Dokumente unabhängig voneinander an und erstellt Versionshinweise für jede Klasse.</seg>
<seg id="296">In dieser Aufgabe sind die direkten Korrespondenzen zwischen Commit-Nachrichten und Versionshinweisen nicht bekannt.</seg>
<seg id="297">Um den Klassifikator zu trainieren, haben wir daher jeder eingegebenen Commit-Nachricht eine Umfrage zugewiesen und dabei die ersten zehn Zeichen jeder Commit-Nachricht verwendet.</seg>
<seg id="298">Wir haben den klassenweisen abstrakten Zusammenfassungsansatz durch zwei verschiedene Methoden modelliert.</seg>
<seg id="299">Das erste Modell, das wir CAS-Single nennen, besteht aus einem einzigen 6-zu-6-Netzwerk und generiert einen einzigen Versionshinweistext, der eine Verkettung von eingegebenen Commit-Nachrichten ergibt.</seg>
<seg id="300">Die ausgegebenen Texte können auf der Grundlage spezieller klassenspezifischer Endpunktsymbole in klassenweise Segmente unterteilt werden.</seg>
<seg id="301">Die zweite Methode, die wir CAS-Multi nennen, besteht aus vier verschiedenen seq2seq-Netzwerken, von denen jedes einer der festen Versionshinweisklassen entspricht.</seg>
<seg id="302">Ich werde nun die Experimente erklären.</seg>
<seg id="303">Fünf Methoden wurden verglichen: CEAS, CAS-Single, CAS-Multi, Clustering und Glyph aus einer früheren Studie.</seg>
<seg id="304">Was die Evaluation betrifft, so werden in einigen Fällen die Versionshinweise in mehreren Sätzen ausgegeben.</seg>
<seg id="305">Da es schwierig ist, die Anzahl von Sätzen zu berechnen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt.</seg>
<seg id="306">Der BLEU wird benachteiligt, wenn das System einen kurzen Satz ausgibt.</seg>
<seg id="307">Dieser Abzug führt zu einem niedrigeren BLEU-Wert in den nachfolgend beschriebenen Versuchsergebnissen.</seg>
<seg id="308">Schließlich berechnen wir auch die Spezifität, da ROUGE und BLEU nicht berechnet werden können, wenn die Versionshinweise leer sind.</seg>
<seg id="309">Eine höhere Spezifität bedeutet, dass das Modell in Fällen, in denen die Versionshinweise leer sind, einen leeren Text korrekt ausgibt.</seg>
<seg id="310">Hier sind die Resultate.</seg>
<seg id="311">Da der Datensatz E-Mail-Adressen, Hash-Werte usw. enthält, haben wir auch den bereinigten Datensatz ausgewertet, der diese ausschließt.</seg>
<seg id="312">CEAS und CAS erreichten ROUGE-L-Werte, die mehr als zehn Punkte über den Baselines lagen.</seg>
<seg id="313">Insbesondere beim bereinigten Testsatz ist der Abstand zwischen der vorgeschlagenen Methode und den Baselines auf mehr als zwanzig Punkte angestiegen.</seg>
<seg id="314">Diese Ergebnisse zeigen, dass CEAS und CAS signifikant betroffen sind.</seg>
<seg id="315">CEAS erzielte bessere ROUGE-L-Werte als CAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators beim Training des Klassifikators mit Pseudo-Markierungen effektiv ist.</seg>
<seg id="316">Eine hohe Abdeckung von CEAS kann wahrscheinlich erreicht werden, weil sich der Klassifikator auf die Auswahl relevanter Commit-Nachrichten für jede Klasse konzentrieren kann.</seg>
<seg id="317">CAS-Multi ergab tendenziell mehr ROUGE-L-Werte als CAS-Single.</seg>
<seg id="318">Wir schlagen vor, dass es auch effektiv ist, unterschiedliche abstrakte Zusammenfassungsmodelle, die unabhängig voneinander sind, für jede Klasse von den Versionshinweisen zu entwickeln.</seg>
<seg id="319">Hier ist eine Fehleranalyse.</seg>
<seg id="320">CAS-Methoden geben tendenziell kürzere Sätze aus als menschliche Referenzsätze.</seg>
<seg id="321">In der Abbildung rechts besteht der Referenzsatz aus drei oder vier Sätzen, während CAS aus nur einem besteht.</seg>
<seg id="322">Der Grund für die Reluktanz dieses Modells ist, dass in den Trainingsdaten nur 33 Prozent der Sätze in der Markierung Funktionen und 40 Prozent in der Markierung Verbesserung zu finden sind.</seg>
<seg id="323">Darüber hinaus können CAS-Methoden ohne zusätzliche Informationen keine genauen Versionshinweise erstellen.</seg>
<seg id="324">Oben auf der rechten Seite ist ein Beispiel für eine sehr unübersichtliche Commit-Nachricht. Der vollständige Satz kann nicht ohne Bezug auf den entsprechenden Fortschritt oder das Problem generiert werden.</seg>
<seg id="325">Das folgende Beispiel zeigt, dass die beiden Commit-Nachrichten in der Eingabe zusammenhängen und zu einem Satz zusammengefasst werden sollten, was jedoch nicht gelingt.</seg>
<seg id="326">Nun kommen wir zum Fazit.</seg>
<seg id="327">Wir haben einen neuen Datensatz für die automatische Generierung von Versionshinweisen erstellt.</seg>
<seg id="328">Wir haben auch eine Aufgabe für die Eingabe von Commit-Nachrichten und deren Zusammenfassung formuliert, sodass sie für alle auf Englisch geschriebene Projekte anwendbar ist.</seg>
<seg id="329">Unsere Experimente zeigen, dass die vorgeschlagene Methode weniger verrauschte Versionshinweise mit höherem Abdeckungsgrad als die Baselines erzeugt.</seg>
<seg id="330">Bitte sehen Sie sich unseren Datensatz auf GitHub an.</seg>
<seg id="331">Vielen Dank!</seg>
</doc>
<doc docid="2022.acl-long.111" genre="presentations">
<talkid>2022.acl-long.111</talkid>
<abstract>The enrichment of tabular datasets using external sources has gained significant attention in recent years. Existing solutions, however, either ignore external unstructured data completely or devise dataset-specific solutions. In this study we proposed Few-Shot Transformer based Enrichment (FeSTE), a generic and robust framework for the enrichment of tabular datasets using unstructured data. By training over multiple datasets, our approach is able to develop generic models that can be applied to additional datasets with minimal training (i.e., few-shot). Our approach is based on an adaptation of BERT, for which we present a novel fine-tuning approach that reformulates the tuples of the datasets as sentences. Our evaluation, conducted on 17 datasets, shows that FeSTE is able to generate high quality features and significantly outperform existing fine-tuning solutions.</abstract>
<seg id="332">Hallo! Mein Name ist Asaf Harari.</seg>
<seg id="333">Ich werde unser Paper vorstellen: Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.</seg>
<seg id="334">Datenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der bestehenden Funktionen von Daten.</seg>
<seg id="335">Aber manchmal sind diese Funktionen begrenzt.</seg>
<seg id="336">Die Generierung von Funktionen unter Verwendung einer anderen Datenquelle kann wesentliche Informationen hinzufügen.</seg>
<seg id="337">Unser Forschungsziel ist die automatische Anreicherung tabellarischer Daten mit freiem Text aus externen Quellen.</seg>
<seg id="338">Angenommen, wir haben einen tabellarischen Datensatz und eine Wissensbasis.</seg>
<seg id="339">Wir brauchen einen automatischen Prozess, der Entity Linking und eine Textanalyse umfasst, um neue Funktionen aus dem freien Text der Wissensbasis zu extrahieren.</seg>
<seg id="340">Unser Rahmen FeSTE ist genau dieser automatische Prozess.</seg>
<seg id="341">Sehen wir uns also ein Beispiel in einem Datensatz an, der in FeSTE eingespeist wird.</seg>
<seg id="342">In diesem Beispiel ist der Datensatz der Universitätsdatensatz.</seg>
<seg id="343">Das Ziel besteht darin, die Universitäten nach niedrigem und hohem Rang einzuteilen.</seg>
<seg id="344">Als Wissensbasis verwenden wir Wikipedia.</seg>
<seg id="345">Die erste Phase von FeSTE ist Entity Linking.</seg>
<seg id="346">Jede Entität, in diesem Beispiel der Name der Universität, ist mit einer Entität innerhalb der Wissensbasis verknüpft.</seg>
<seg id="347">Der Text der Entitäten aus der Wissensbasis wird extrahiert und dem Datensatz hinzugefügt.</seg>
<seg id="348">In diesem Beispiel ist der Text das Abstract der Wikipedia-Seite.</seg>
<seg id="349">Nun müssen wir Funktionen aus dem abgerufenen Text generieren oder extrahieren.</seg>
<seg id="350">Wir müssen also eine Phase der Funktionsextraktion einleiten, zu der die Textanalyse gehört.</seg>
<seg id="351">Das ist die wichtigste Neuerung dieses Papers, auf die ich auf den nächsten Folien näher eingehen werde.</seg>
<seg id="352">Nach der Phase der Funktionsextraktion folgt eine Phase der Funktionsgenerierung, in der wir die extrahierten Funktionen verwenden, um eine kleine Anzahl neuer Funktionen zu erzeugen.</seg>
<seg id="353">Zunächst werden Funktionen in der Anzahl der Klassen des ursprünglichen Datensatzes generiert.</seg>
<seg id="354">In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen.</seg>
<seg id="355">FeSTE generiert also zwei neue Funktionen.</seg>
<seg id="356">Wenn der Datensatz jedoch fünf Klassen hat, generiert FeSTE fünf neue Funktionen.</seg>
<seg id="357">Jede Funktion stellt die Wahrscheinlichkeit für jede Klasse dar.</seg>
<seg id="358">Um den Text zu analysieren, verwenden wir die aktuellste Textanalyse, die auf Transformer-basierten Sprachmodellen wie BERT, GPT, XLNet und anderen beruht.</seg>
<seg id="359">Es ist aber unwahrscheinlich, dass wir Sprachmodelle mit den eingegebenen Datensätzen trainieren können.</seg>
<seg id="360">Ein naiver Ansatz ist also eine Feinabstimmung des Zieldatensatzes.</seg>
<seg id="361">In der Phase der Funktionsextraktion können wir also vortrainierte Sprachmodelle herunterladen und das Sprachmodell über den Zieldatensatz feinabstimmen.</seg>
<seg id="362">In diesem Beispiel zur Feinabstimmung des Sprachmodells klassifizieren wir den Text in Klassen, d. h. wir klassifizieren das Abstract in die Klassen niedrig oder hoch.</seg>
<seg id="363">Wir erhalten die Ausgabe des Sprachmodells, die die Wahrscheinlichkeit für jede Klasse darstellt, und verwenden sie als neue Funktionen.</seg>
<seg id="364">Das Problem bei diesem Ansatz ist, dass Datensätze möglicherweise nur wenige unterschiedliche Entitäten/Texte beinhalten.</seg>
<seg id="365">In unserem Experiment enthält fast die Hälfte der Datensätze weniger als 400 Stichproben und der kleinste Datensatz enthält 35 Stichproben in einem Trainingssatz.</seg>
<seg id="366">Die Feinabstimmung eines Sprachmodells über diesen Datensatz ist also unwirksam.</seg>
<seg id="367">Aber wir können vorherige Kenntnisse über bereits analysierte Datensätze nutzen.</seg>
<seg id="368">Da wir FeSTE auf mehrere Datensätze anwenden, können wir die „n minus 1“-Datensätze verwenden, um Informationen über die „n minus 1“-Datensätze zu sammeln, und diese Informationen verwenden, wenn wir den n-ten Datensatz analysieren.</seg>
<seg id="369">Wir schlagen vor, eine weitere Feinabstimmungsphase hinzuzufügen.</seg>
<seg id="370">Das war eine vorläufige Multitask-Feinabstimmungsphase.</seg>
<seg id="371">Die Feinabstimmung des Sprachmodells geht über die „n minus 1“-Datensätze.</seg>
<seg id="372">Dann führen wir eine weitere Feinabstimmungsphase durch, die eine Feinabstimmung des Zieldatensatzes ist, wenn wir das Sprachmodell über den n-ten Zieldatensatz feinabstimmen.</seg>
<seg id="373">Die modernste Multitask-Feinabstimmung wird MTDNN genannt.</seg>
<seg id="374">Bei der MTDNN bleiben die Überschriften in der Reihe der Aufgaben im Trainingssatz.</seg>
<seg id="375">In diesem Beispiel gibt es also vier Aufgaben im Trainingssatz. Das MTDNN enthält also vier Überschriften, wie Sie auf dem Bild sehen können.</seg>
<seg id="376">Es nimmt sich eine Zufallsstichprobe aus dem Trainingssatz heraus.</seg>
<seg id="377">Wenn die Zufallsstichprobe zum Beispiel zu einer einzelnen Satzklassifikationsaufgabe gehört, führt sie Vorwärts- und Rückwärtspfade durch die erste Überschrift aus.</seg>
<seg id="378">Wenn die Zufallsstichprobe zur paarweisen Ranking-Aufgabe gehört, führt sie den Vorwärts- und Rückwärtspfad über die letzte Überschrift aus.</seg>
<seg id="379">In unserem Szenario variieren tabellarische Datensätze in der Anzahl der Klassen.</seg>
<seg id="380">Es gibt also viele Aufgaben.</seg>
<seg id="381">MTDNN enthält eine Anzahl von Klassen, Überschriften und ausgegebenen Ebenen.</seg>
<seg id="382">Und zusätzlich muss MTDNN neue Überschriften für einen neuen Datensatz mit einer neuen Aufgabe initialisieren.</seg>
<seg id="383">Unser Ansatz, genannt Feinabstimmungsreformulierungsaufgabe, besteht darin, dass wir, anstatt mehrere Überschriften beizubehalten, jeden Datensatz in einen Satz pro Klassifikationssproblem umformulieren, was zwei Klassenaufgaben entspricht.</seg>
<seg id="384">Schauen wir uns ein Beispiel an.</seg>
<seg id="385">Hier ist unser eingegebener Datensatz, der aus Entitäten, Funktionen, Text und Klassen besteht.</seg>
<seg id="386">Wir formulieren die Aufgabe von einer Klassifizierung des Textes in niedrig oder hoch zu einer Klassifizierung des Textes, des Abstracts und der Klasse in wahr oder falsch um.</seg>
<seg id="387">Mit anderen Worten: Wir haben das Sprachmodell so trainiert, dass es ein Abstract und die Klasse als Abstract und die Klasse klassifiziert, wenn das Abstract zur Klasse gehört oder nicht.</seg>
<seg id="388">So bleibt die Vektor-Markierung in diesem Fall bestehen, die immer aus zwei Klassen besteht.</seg>
<seg id="389">Das ist der Algorithmus für unseren neu formulierten Finabstimmungsansatz.</seg>
<seg id="390">Schauen wir uns also den gesamten Rahmen an.</seg>
<seg id="391">Der Datensatz wird in FeSTE eingespeist.</seg>
<seg id="392">Dann führt FeSTE die Entity-Linking-Phase aus.</seg>
<seg id="393">Es extrahiert den Text aus der Wissensbasis, die in diesem Beispiel das Abstract der Wikipedia-Seite ist.</seg>
<seg id="394">Dann formulierte es die Aufgabe in eine paarweise Satzklassifikationsaufgabe um.</seg>
<seg id="395">Das Sprachmodell wird auf die neue Aufgabe und die Ausgabewahrscheinlichkeit für jede Klasse angewandt.</seg>
<seg id="396">Nun ist das Sprachmodell bereits über den „n minus 1“-Datensatz mithilfe einer vorläufigen Multitask-Feinabstimmung feinabgestimmt.</seg>
<seg id="397">Dann verwenden wir den Ausgabe-Vektor des Sprachmodells als eine neu generierte Funktion in der Anzahl der Klassen.</seg>
<seg id="398">Zur Evaluation unseres Rahmens verwenden wir 17 tabellarische Klassifikationsdatensätze, die sich in Größe, Funktionen, Ausgewogenheit, Domäne und anfänglicher Leistung unterscheiden.</seg>
<seg id="399">Als Wissensbasis verwenden wir Wikipedia.</seg>
<seg id="400">Wir konzipieren unser Experiment als Leave-One-Out-Evaluation, bei dem wir FeSTe mit 16 Datensätzen trainieren und auf den 17. Datensatz anwenden.</seg>
<seg id="401">Außerdem teilen wir jeden Datensatz in vier Brüche auf und wenden eine vierfache Kreuzvalidierung an.</seg>
<seg id="402">Dann generieren wir die neuen Funktionen und bewerten sie mit fünf Evaluationssklassifikatoren.</seg>
<seg id="403">Wir verwenden in unseren Experimenten die Basisarchitektur BERT.</seg>
<seg id="404">Hier sind die Ergebnisse unserer Experimente.</seg>
<seg id="405">Sie können sehen, dass wir unseren Rahmen mit der Feinabstimmung des Zieldatensatzes, mit der Feinabstimmung der Zielaufgabe und einer vorläufigen MTDNN-Feinabstimmung vergleichen.</seg>
<seg id="406">Unsere neu formulierte Feinabstimmung erreicht das beste Ergebnis und die beste Leistung.</seg>
<seg id="407">MTDNN erreichte zwei Prozent Verbesserung gegenüber der Feinabstimmung des Zieldatensatzes.</seg>
<seg id="408">Mit unserem Ansatz haben wir eine Verbesserung von sechs Prozent erreicht.</seg>
<seg id="409">Wenn wir uns den kleinen Datensatz anschauen, sehen wir, dass die Leistung von MTDNN abnimmt und die Verbesserung der vorläufigen Multitask-Feinabstimmungsphase auf 1,5 Prozent sinkt.</seg>
<seg id="410">Aber unsere Leistung stieg auf elf Prozent im Vergleich zur Feinabstimmung des Zieldatensatzes allein.</seg>
<seg id="411">Für die Summierung ermöglicht FeSTE in unseren Experimenten eine Few-Shot-Anreicherung aus 35 Proben.</seg>
<seg id="412">Es verwendet eine Architektur für alle Aufgaben und Datensätze.</seg>
<seg id="413">Und es behält den Hauptteil des Modells.</seg>
<seg id="414">Aber es fügt eine Reformulierungsphase hinzu.</seg>
<seg id="415">Es erweitert den Trainingssatz und benötigt einen Zielwert mit semantischer Bedeutung, damit wir ihn in das Sprachmodell einspeisen und für das Satzpaar-Klassifikationsproblem verwenden können.</seg>
<seg id="416">Vielen Dank!</seg>
</doc>
</srcset>
</mteval>
