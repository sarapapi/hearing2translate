{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Asaf Ferrari e vou apresentar nosso artigo, \"Enriquecimento de dados tabulares com poucos exemplos usando arquiteturas de transformadores ajustadas\". Os cientistas de dados analisam dados e se concentram principalmente na manipulação dos recursos existentes. Mas, às vezes, esses recursos são limitados. A geração de recursos usando outra fonte de dados pode adicionar informações substanciais. Nosso objetivo de pesquisa é o enriquecimento automático de dados tabulares usando fontes externas de texto livre. Suponha que temos um conjunto de dados tabular e uma base de conhecimento. Precisamos de um processo automático que envolva a ligação de entidades e a análise de texto para extrair novos recursos do texto livre da base de conhecimento. Nosso framework, FEAST, é exatamente esse processo automático. Vamos ver um exemplo. Em um conjunto de dados alimentado no FEAST. Neste exemplo, o conjunto de dados é um conjunto de dados de universidades, cujo objetivo é classificar universidades em universidades de baixo e alto ranking. Como base de conhecimento, usamos a Wikipedia. A primeira fase do FEAST é a ligação de entidades, quando cada entidade, neste exemplo, o nome da universidade, é vinculada a uma entidade dentro da base de conhecimento. E o texto das entidades da base de conhecimento é extraído e adicionado ao conjunto de dados. Neste exemplo, o texto é o resumo da página da Wikipedia. Agora, precisamos gerar ou extrair recursos do texto recuperado. Precisamos de uma fase de extração de recursos que inclua análise de texto. E esta é a principal novidade deste artigo, e vou aprofundar isso nas próximas slides. Após a fase de extração de recursos, há uma fase de geração de recursos, quando usamos os recursos extraídos para gerar um pequeno número de novos recursos. Primeiro, geramos recursos no número de classes do conjunto de dados original. Neste exemplo, o conjunto de dados original tem duas classes, então o FEAST gera dois novos recursos. Mas se o conjunto de dados tiver cinco classes, o FEAST gera cinco novos recursos. Cada recurso representa a probabilidade de cada classe. Para analisar o texto, usamos o estado da arte da análise de texto, que são modelos de linguagem baseados em transformadores, como BERT, GPT, XLNet, etc. Mas não é provável que possamos treinar um modelo de linguagem usando os conjuntos de dados de entrada. Portanto, uma abordagem ingênua seria um ajuste fino de tarefa-alvo. Portanto, na fase de extração de recursos, podemos baixar um modelo de linguagem pré-treinado, ajustar o modelo de linguagem sobre o conjunto de dados-alvo. Neste exemplo, para ajustar o modelo de linguagem para classificar o texto em classes, resumo em classes, baixo ou alto, receber a saída do modelo de linguagem, que é a probabilidade de cada classe, e usá-la como novos recursos. O problema com essa abordagem é que os conjuntos de dados podem ter poucos textos de entidades distintas. Em nosso experimento, quase metade dos conjuntos de dados contém menos de 400 amostras, e o menor conjunto de dados contém 35 amostras em seu conjunto de treinamento. Portanto, ajustar um modelo de linguagem sobre esse conjunto de dados seria ineficaz. Mas podemos usar o conhecimento prévio sobre conjuntos de dados pré-analisados porque aplicamos o FEAST em vários conjuntos de dados. Podemos usar os N-1 conjuntos de dados para coletar informações sobre os N-1 conjuntos de dados e usar essas informações quando analisamos o N-ésimo conjunto de dados. O que sugerimos é adicionar outra fase de ajuste fino, uma fase de ajuste fino multitarefa preliminar, quando ajustamos o modelo de linguagem sobre N-1 conjuntos de dados e, em seguida, executamos outra fase de ajuste fino, que é um ajuste fino de tarefa-alvo, quando ajustamos o modelo de linguagem sobre o N-ésimo conjunto de dados-alvo. O estado da arte no ajuste fino multitarefa é chamado de MT-DNN. No MT-DNN, o MT-DNN mantém cabeças no número de tarefas no conjunto de treinamento. Portanto, neste exemplo, há quatro tarefas no conjunto de treinamento, então o MT-DNN mantém quatro cabeças, como você pode ver na imagem, e ele amostra um lote aleatório do conjunto de treinamento. E se o lote aleatório pertence, por exemplo, a tarefas de classificação de sentenças únicas, ele executa passagens diretas e inversas através da primeira cabeça. E se o lote aleatório pertence a tarefas de classificação de pares, ele executa passagens diretas e inversas através da última cabeça. Em nosso cenário, os conjuntos de dados tabulares variam no número de classes. Portanto, há muitas tarefas. O MT-DNN mantém o número de cabeças de classes, camadas de saída. E, adicionalmente, o MT-DNN precisa inicializar novas cabeças para um novo conjunto de dados com uma nova tarefa. Nossa abordagem, chamada de ajuste fino de reformulação de tarefa, em nossa abordagem, ajuste fino de reformulação de tarefa, em vez de manter múltiplas cabeças, reformulamos cada conjunto de dados em um problema de classificação de sentenças por classificação, que é uma tarefa de duas classes. Vamos ver um exemplo. Aqui está nosso conjunto de dados de entrada, que consiste em recursos de entidades, texto e classes. E reformulamos a tarefa de classificar o texto em baixo e alto para classificar o texto, o resumo e a classe em verdadeiro ou falso. Ou, em outras palavras, treinamos o modelo de linguagem para classificar o resumo e a classe, se o resumo pertence à classe ou não. Portanto, o vetor de rótulo no caso de Zix permanece sempre, que consiste sempre em duas classes. E este é o algoritmo para nossa abordagem de ajuste fino reformulada. Vamos ver o framework completo. Conjunto de dados alimentado no FEAST. E então o FEAST executa a fase de ligação de entidades. Ele extrai o texto da base de conhecimento, que neste exemplo é o resumo da página da Wikipedia. Em seguida, reformula a tarefa em uma tarefa de classificação de sentenças por classificação. Aplica o modelo de linguagem à nova tarefa e a saída de probabilidade para cada classe. Observe que o modelo de linguagem já foi ajustado sobre N-1 conjuntos de dados usando um ajuste fino multitarefa preliminar. Em seguida, usamos o vetor de saída do modelo de linguagem como um novo recurso gerado no número de classes. Para avaliar nosso framework, usamos 17 conjuntos de dados de classificação tabular, que variam em tamanho, recursos, equilíbrio, domínio e desempenho inicial. E como base de conhecimento, usamos a Wikipedia. Projetamos nosso experimento como uma avaliação de exclusão de um, quando treinamos o FEAST sobre 16 conjuntos de dados e o aplicamos ao 17º conjunto de dados. Também dividimos cada conjunto de dados em quatro dobras e aplicamos uma validação cruzada de quatro dobras. Em seguida, geramos o novo recurso e os avaliamos usando cinco classificadores de avaliação. Usamos em nosso experimento uma arquitetura baseada em BERT. Aqui estão os resultados de nosso experimento. Você pode ver que comparamos nosso framework com o ajuste fino do conjunto de dados-alvo, ajuste fino de tarefa-alvo e ajuste fino preliminar do MT-DNN, e nosso ajuste fino reformulado alcança o melhor resultado, o melhor desempenho, enquanto o MT-DNN alcança uma melhoria de 2% sobre o ajuste fino do conjunto de dados-alvo, nossa abordagem alcança uma melhoria de 6%. Quando olhamos para o pequeno conjunto de dados, podemos ver que o desempenho do MT-DNN diminui e a melhoria da fase de ajuste fino multitarefa preliminar diminui para 1,5%, mas nosso desempenho aumenta para 11% em comparação com o ajuste fino de tarefa-alvo. Para resumir, o FEAST permite o enriquecimento com poucos exemplos a partir de 35 amostras em nosso experimento. Ele usa uma arquitetura para todos os conjuntos de dados de tarefas e mantém a cabeça do modelo. Mas ele adiciona uma fase de reformulação. Ele aumenta o conjunto de treinamento e precisa de um valor-alvo com significado semântico para que possamos alimentá-lo no modelo de linguagem e usá-lo no problema de classificação de sentenças por classificação. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "pt", "output": "Olá a todos. Hoje vou apresentar nosso trabalho de pesquisa, aprendendo a raciocinar dedutivamente, resolvendo problemas matemáticos como extração de regiões complexas. Sou Alan do ByteDance AI Lab e este é um trabalho conjunto com Jerry da Universidade do Texas em Austin e Weilu da SUTD. Primeiro, gostaria de falar sobre nossa motivação para o raciocínio. Então, aqui mostramos exemplos onde o raciocínio em várias etapas é útil. Então, esta figura é tirada do artigo Pound, onde eles realizam o prompt para resolver o problema de problema de método em um cenário de aprendizado de poucos tiros. Então, no lado esquerdo, podemos ver que, se dermos algumas amostras com apenas correção e respostas, talvez não consigamos obter as respostas corretas. Mas se dermos mais descrição de raciocínio, o modelo é capaz de prever a descrição do raciocínio e também fazer uma previsão correta aqui. Então, é bom ter raciocínio interpretável em várias etapas como saída. E também achamos que o problema do método é uma aplicação direta para avaliar tais habilidades de raciocínio. Então, aqui em nosso problema, dado as perguntas, precisamos resolver esta pergunta e obter as respostas numéricas. Então, em nossos conjuntos de dados, também nos são dadas as expressões matemáticas que levam a esta resposta em particular. Então, certas suposições também se aplicam como no trabalho anterior. Suponhamos que a precisão das quantidades não é conhecida e consideramos apenas operadores básicos como adição, subtração, multiplicação, divisão e exponencial. Além disso, os operadores complicados podem ser realmente decompostos nesses operadores básicos. Então, o trabalho anterior na resolução de problemas de método pode ser realmente categorizado em modelo de sequência para sequência e sequência para árvore. Então, o modelo de sequência para sequência tradicional converte a expressão em uma sequência específica para geração e é bastante fácil de implementar e pode generalizar para muitos problemas complicados diferentes. Mas o desempenho é geralmente não melhor do que o modelo estruturado e falta a interpretabilidade para a previsão. Mas, na verdade, esta direção ainda é bastante popular por causa do modelo de transformador. Então, nos modelos baseados em árvore, realmente estruturamos essas expressões na forma de árvore e seguimos uma travessia pré-ordem na geração de árvore. Então, aqui continuamos gerando os operadores até alcançarmos as folhas, que são as quantidades. Então, aqui a coisa boa é que realmente nos dá essa estrutura de árvore binária e é, mas na verdade é bastante contra-intuitivo porque geramos o operador primeiro e depois, no final, geramos as quantidades. E a segunda coisa é que também contém alguns cálculos repetitivos. Então, aqui, se olharmos para esta expressão, 8 vezes 3 mais 3 é realmente gerado duas vezes. Mas, na verdade, deveríamos reutilizar os resultados. Então, em nossa abordagem proposta, queremos resolver esses problemas de maneira passo a passo e interpretável. Então, por exemplo, aqui na segunda etapa, podemos obter este divisor, que é 27. E também podemos nos referir às perguntas originais para encontrar os conteúdos relevantes. E nessas etapas, obtemos os divisores. Então, e então nesta terceira etapa, realmente obtemos o quociente. Tudo bem. E depois dessas três etapas, podemos realmente reutilizar os resultados da segunda etapa e, em seguida, obter os resultados da quarta etapa. E, finalmente, podemos obter os dividendos. Então, aqui realmente geramos a expressão completa diretamente, em vez de gerar um único operador ou quantidades. Então, isso torna o processo mais preciso. Então, em nosso sistema dedutivo, começamos com um monte de quantidades apresentadas nas perguntas e também incluindo algumas constantes como nosso estado inicial. Então, a expressão é representada por EIJOP, onde realizamos o operador de QI para QJ, e tal expressão é realmente direcionada. Então, também temos subtração reversa aqui para representar a direção oposta. Isso é bastante semelhante à extração de relação. Então, em um sistema dedutivo formal, em um instante t, aplicamos o operador entre o par QI e QJ, e então obtemos essa nova expressão. Adicionamos ao próximo estado para se tornar uma nova quantidade. Então, este slide realmente visualiza a evolução dos estados, onde continuamos adicionando expressão ao estado atual. Então, em nossas implementações de modelo, primeiro usamos um modelo de linguagem pré-treinado que pode ser BERTs ou Roberta, e então codificamos a sentença e, em seguida, obtemos essas representações de quantidade. Então, uma vez que obtemos as representações de quantidade, podemos começar a fazer inferência. Aqui mostramos um exemplo de Q1 para obter a representação para Q1 dividido por Q2 e, em seguida, vezes Q3. Primeiro, obtemos a representação do par, que é basicamente apenas a concatenação entre Q1 e Q2, e então aplicamos uma rede feedforward, que é parametrizada pelo operador. E, finalmente, obtemos a representação da expressão Q1 dividido por Q2. Mas, na verdade, na prática, na fase de inferência, podemos obter a expressão incorreta também. Então, aqui todas as expressões possíveis são iguais a três vezes o número de operadores. Então, a coisa boa aqui é que podemos facilmente adicionar restrições para controlar este espaço de busca. Por exemplo, se esta expressão não for permitida, podemos simplesmente remover esta expressão em nosso espaço de busca. Então, na segunda etapa, fazemos a mesma coisa, mas a única diferença é que temos uma quantidade a mais. Então, esta quantidade vem da expressão calculada anteriormente. Então, finalmente, podemos obter esta expressão final Q3 vezes Q4. E também podemos ver que o número de todas as expressões possíveis é diferente da etapa anterior. Então, tal diferença torna difícil aplicar a busca de feixe porque a distribuição de probabilidade entre essas duas etapas está desequilibrada. Então, o procedimento de treinamento é semelhante ao treinamento de um modelo de sequência para sequência, onde otimizamos a perda em cada instante. E aqui também usamos este tau para representar quando devemos terminar este processo de geração. E aqui o espaço é diferente de sequência para sequência porque o espaço é diferente em cada instante, enquanto no modelo de sequência para sequência tradicional, é o número de vocabulário. E também nos permite impor certas restrições a partir do conhecimento prévio. Então, realizamos experimentos nos conjuntos de dados de problemas matemáticos comumente usados, MAWPS, Math 23K, MathQA e SWAMP. E aqui mostramos brevemente os resultados comparados com as melhores abordagens anteriores. Então, nossa melhor versão de desempenho é Roberta Deductive Reasoner. E, na verdade, não usamos a busca de feixe em contraste com as abordagens óbvias usando a busca de feixe. Tudo bem. Então, as melhores abordagens são geralmente um modelo baseado em árvore. Então, no geral, nosso raciocinador é capaz de superar significativamente este modelo baseado em árvore, mas podemos ver que o número absoluto em MathQA ou SWAMP não é realmente alto. Então, investigamos mais os resultados no SWAMP. E este conjunto de dados é desafiador porque o autor tenta adicionar manualmente algo para confundir o modelo NLP, como adicionar informações irrelevantes e quantidades extras. Então, em nossa previsão, encontramos alguns dos valores intermediários que são realmente negativos. Por exemplo, nessas perguntas, estamos perguntando quantas maçãs Jake tem, mas temos algumas informações extras como 17 figuras e Steven tem oito figuras, o que é totalmente irrelevante. Então, nosso modelo faz algumas previsões como esta, que produz valores negativos. E observamos que essas duas expressões realmente têm pontuações semelhantes. Então, podemos realmente limitar este espaço de busca removendo, como, esses resultados são negativos para que possamos fazer a resposta correta. Então, descobrimos que tal restrição realmente melhora bastante para alguns modelos. Por exemplo, para BERTs, melhoramos sete pontos. E então, para o modelo baseado em Roberta, realmente melhoramos dois pontos. Então, um melhor modelo de linguagem tem uma melhor capacidade de compreensão de linguagem, para que o número aqui seja maior para Roberta e menor para BERTs. E também tentamos analisar a dificuldade por trás de todos esses conjuntos de dados. Suponhamos que o número de quantidade não usada pode ser considerado como informação irrelevante aqui. Então, aqui podemos ver que temos a porcentagem de amostras com quantidades não usadas, e o conjunto de dados SWAMP tem a maior proporção. E aqui também mostramos o desempenho geral. Para aquelas amostras sem quantidades não usadas. Então, o desempenho geral é realmente maior do que o desempenho geral. Mas com aquelas amostras que têm quantidade não usada é realmente muito pior do que o desempenho geral. Para MAWPS, não temos muitos desses casos. Então, ignoro esta parte. Então, finalmente, queremos mostrar a interpretabilidade por meio de um exemplo de previsão de previsão. Então, aqui nosso modelo realmente faz uma previsão errada na primeira etapa. Então, podemos realmente correlacionar esta expressão com a sentença aqui. Tudo bem. Então, achamos que esta sentença pode estar enganando o modelo para previsões incorretas. Então, aqui plantar outra 35 faz o modelo pensar que deveria ser um operador de adição. Então, tentamos revisar a sentença para algo como o número de árvores de peras é 55 menor do que as árvores de maçãs. Então, fazemos com que transmita uma semântica mais precisa, de modo que o modelo seja capaz de fazer a previsão correta. Então, este estudo mostra como as previsões interpretáveis nos ajudam a entender o comportamento do modelo. Então, para concluir nosso trabalho, primeiro nosso modelo é realmente bastante eficiente e somos capazes de fornecer um procedimento de resolução interpretável. E podemos facilmente incorporar algum conhecimento prévio como restrição, o que pode ajudar a melhorar o desempenho. E a última coisa é que o mecanismo subjacente não se aplica apenas a tarefas de resolução de problemas de método, mas também a outras tarefas que envolvem raciocínio em várias etapas. Mas também temos certas limitações. Se tivermos um grande número de operadores ou constantes, o consumo de memória pode ser bastante alto. E a segunda coisa é que, como mencionado, porque a distribuição de probabilidade está desequilibrada em diferentes instantes, também é bastante desafiador aplicar a estratégia de busca de feixe. Então, este é o fim da palestra e as perguntas são bem-vindas. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Antoine e sou da Universidade de Maastricht. Vou apresentar meu trabalho conjunto com Jerry, que trata de um novo conjunto de dados para a recuperação de artigos estatutários. Questões legais são uma parte integrante da vida de muitas pessoas. No entanto, a maioria dos cidadãos tem pouco ou nenhum conhecimento sobre seus direitos e processos legais fundamentais. Como resultado, muitos cidadãos vulneráveis que não podem pagar a assistência cara de um especialista legal ficam desprotegidos ou, pior ainda, explorados. Nosso trabalho visa preencher a lacuna entre as pessoas e a lei, desenvolvendo um sistema de recuperação eficaz para artigos estatutários. Um sistema como esse poderia fornecer um serviço de ajuda legal profissional gratuito para humanos não qualificados. Antes de mergulharmos na principal contribuição deste trabalho, vamos primeiro descrever o problema da recuperação de artigos estatutários. Dada uma pergunta simples sobre uma questão legal, como \"O que arrisco se violar a confidencialidade profissional?\", um modelo é necessário para recuperar todos os artigos estatutários relevantes de um grande corpo de legislação. Esta tarefa de recuperação de informações vem com seu próprio conjunto de desafios. Primeiro, lida com dois tipos de linguagem: linguagem natural comum para as perguntas e linguagem legal complexa para os estatutos. Essa diferença nas distribuições de linguagem torna mais difícil para um sistema recuperar candidatos relevantes, pois indiretamente requer um sistema de interpretação inerente que possa traduzir uma pergunta natural em uma pergunta legal que corresponda à terminologia dos estatutos. Além disso, a lei estatutária não é uma pilha de artigos independentes que podem ser tratados como uma fonte completa de informações por si só, como notícias ou receitas, por exemplo. Em vez disso, é uma coleção estruturada de disposições legais que têm um significado completo apenas quando consideradas em seu contexto geral, ou seja, juntamente com as informações suplementares de seus artigos vizinhos, os campos e subcampos aos quais pertencem e seu lugar na estrutura da lei. Por fim, os artigos estatutários não são pequenos parágrafos, que geralmente são a unidade de recuperação típica na maioria dos trabalhos de recuperação. Aqui, são documentos longos que podem ter até 6.000 palavras. Os avanços recentes no PLN despertaram enorme interesse em muitas tarefas legais, como previsão de julgamento legal ou revisão automática de contrato, mas a recuperação de artigos estatutários permaneceu principalmente intocada devido à falta de grandes e conjuntos de dados de alta qualidade rotulados. Neste trabalho, apresentamos um novo conjunto de dados nativo francês e centrado no cidadão para estudar se um modelo de recuperação pode aproximar a eficiência e a confiabilidade de um especialista legal para a tarefa de recuperação de artigos estatutários. Nosso conjunto de dados de recuperação de artigos estatutários belgas consiste em mais de 1.100 perguntas legais feitas por cidadãos belgas. Essas perguntas abrangem uma ampla gama de tópicos, desde família, habitação, dinheiro até trabalho e segurança social. Cada uma delas foi rotulada por juristas experientes com referências a artigos relevantes de um corpus de mais de 22.600 artigos legais do Código Belga. Vamos agora falar sobre como coletamos esse conjunto de dados. Primeiro, começamos compilando um grande corpus de artigos legais. Consideramos 32 códigos belgas disponíveis publicamente e extraímos todos os seus artigos, bem como os títulos das seções correspondentes. Em seguida, reunimos perguntas legais com referências a estatutos relevantes. Para isso, nos associamos a um escritório de advocacia belga que recebe anualmente cerca de 4.000 e-mails de cidadãos belgas que pedem conselhos sobre uma questão pessoal ou legal. Tivemos a sorte de ter acesso aos sites deles, onde sua equipe de juristas experientes aborda as questões legais mais comuns dos belgas. Coletamos milhares de perguntas, anotadas com categorias, subcategorias e referências legais a estatutos relevantes. Por fim, analisamos as referências legais e filtramos as perguntas cujas referências não eram artigos em um dos códigos de lei que consideramos. As referências restantes foram correspondidas e convertidas nos IDs de artigos correspondentes do nosso corpus. Eventualmente, chegamos a 1.108 perguntas, cada uma cuidadosamente rotulada com os IDs dos artigos relevantes do nosso grande corpus de 22.633 artigos estatutários. Além disso, cada pergunta vem com uma categoria principal e uma concatenação de subcategorias, e cada artigo vem com uma concatenação de seus títulos subsequentes na estrutura da lei. Essas informações extras não são usadas no presente trabalho, mas podem ser de interesse para pesquisas futuras sobre recuperação de informações legais ou classificação de texto legal. Vamos ver algumas características do nosso conjunto de dados. As perguntas têm entre 5 e 44 palavras, com uma mediana de 40 palavras. Os artigos são muito mais longos, com um comprimento mediano de 77 palavras, com 142 deles excedendo 1.000 palavras. O mais longo tem até 5.790 palavras. Como mencionado anteriormente, as perguntas abrangem uma ampla gama de tópicos, com cerca de 85% delas sendo sobre família, habitação, dinheiro ou justiça, enquanto os 15% restantes se referem a segurança social, estrangeiros ou trabalho. Os artigos também são muito diversos, pois vêm de 32 códigos belgas diferentes que cobrem um grande número de tópicos legais. Aqui está o número total de artigos coletados de cada um desses códigos belgas. Dos 22.633 artigos, apenas 1.612 são referidos como relevantes para pelo menos uma pergunta no conjunto de dados. E cerca de 80% desses artigos citados vêm do Código Civil, Código Judicial, Código de Investigação Criminal ou Código Penal. Enquanto isso, 18 dos 32 códigos têm menos de cinco artigos mencionados como relevantes para pelo menos uma pergunta, o que pode ser explicado pelo fato de que esses códigos se concentram menos em indivíduos e suas preocupações. No geral, o número mediano de citações para esses artigos citados é dois, e menos de 25% deles são citados mais de cinco vezes. Usando nosso conjunto de dados, fazemos o benchmark de várias abordagens de recuperação, incluindo arquitetura lexical e densa. Dada uma consulta e um artigo, um modelo lexical atribui uma pontuação à consulta do artigo calculando a soma, sobre os termos da consulta, dos pesos de cada um desses termos naquele artigo. Experimentamos com as funções de classificação TF-IDF e BM25 padrão. O principal problema com essas abordagens é que elas só podem recuperar artigos que contenham palavras-chave presentes na consulta. Para superar essa limitação, experimentamos com uma arquitetura baseada em neurônios que pode capturar a relação semântica entre consultas e artigos. Usamos um modelo B-encoder que mapeia consultas e artigos em representações vetoriais densas e calcula uma pontuação de relevância entre uma consulta e um artigo pela similaridade de seus embeddings. Esses embeddings geralmente resultam de uma operação de pooling na saída de um modelo de embedding de palavras. Primeiro, estudamos a eficácia dos B-encoders siameses em um cenário de avaliação zero-shot, o que significa que os modelos de embedding de palavras pré-treinados são aplicados fora da caixa sem nenhum ajuste fino adicional. Experimentamos com codificadores de texto independentes de contexto, nomeadamente Word2Vec e FastText, e modelos de embedding dependentes de contexto, nomeadamente Roberta e, mais especificamente, Camembert, que é um modelo Roberta francês. Além disso, treinamos nosso próprio modelo B-encoder baseado em Camembert em nosso conjunto de dados. Observe que, para o treinamento, experimentamos com as duas variantes da arquitetura B-encoder. Siamese, que usa um único modelo de embedding de palavras que mapeia a consulta e o artigo juntos em um espaço vetorial denso compartilhado, e Two Tower, que usa dois modelos de embedding de palavras independentes que codificam a consulta e o artigo separadamente em espaços de embedding diferentes. Experimentamos com pooling de média, max e CLS, bem como produto escalar e cosseno para calcular similaridades. Aqui estão os resultados do nosso baseline no conjunto de teste, com os métodos lexicais acima, os B-encoders siameses avaliados no cenário zero-shot no meio e os B-encoders ajustados finamente abaixo. No geral, os B-encoders ajustados finamente superam significativamente todos os outros baselines. O modelo Two Tower melhora sua variante siamesa na recuperação em 100, mas se comporta de maneira semelhante nas outras métricas. Embora o BM25 tenha se saído pior que o B-encoder treinado significativamente, seu desempenho indica que ainda é um baseline forte para a recuperação específica do domínio. Quanto à avaliação zero-shot do B-encoder siamês, descobrimos que usar diretamente os embeddings de um modelo Camembert pré-treinado sem otimizar para a tarefa de recuperação de informações resulta em resultados pobres, o que é consistente com achados anteriores. Além disso, observamos que o B-encoder baseado em Word2Vec supera significativamente o modelo baseado em FastText e Bird, sugerindo que talvez os embeddings de nível de palavra pré-treinados sejam mais apropriados para a tarefa do que os embeddings de nível de caractere ou subpalavra quando usados fora da caixa. Embora promissores, esses resultados sugerem ampla oportunidade de melhoria em comparação com um especialista legal qualificado que eventualmente pode recuperar todos os artigos relevantes para qualquer pergunta e, portanto, obter pontuações perfeitas. Vamos concluir discutindo duas limitações do nosso conjunto de dados. Primeiro, o corpus de artigos é limitado àqueles coletados dos 32 códigos belgas considerados, o que não cobre toda a lei belga, pois faltam artigos de decretos, diretivas e ordenanças. Durante a construção do conjunto de dados, todas as referências a esses artigos não coletados são ignoradas, o que faz com que algumas perguntas terminem com apenas uma fração do número inicial de artigos relevantes. Essa perda de informações implica que a resposta contida nos artigos relevantes restantes pode ser incompleta, embora ainda seja completamente apropriada. Em segundo lugar, devemos observar que nem todas as perguntas legais podem ser respondidas apenas com estatutos. Por exemplo, a pergunta \"Posso expulsar meus inquilinos se eles fizerem muito barulho?\" pode não ter uma resposta detalhada na lei estatutária que quantifique um limite de barulho específico no qual a expulsão é permitida. Em vez disso, o proprietário provavelmente deve confiar mais na jurisprudência e encontrar precedentes semelhantes à situação atual. Por exemplo, o inquilino faz duas festas por semana até as 2h. Portanto, algumas perguntas são mais adequadas do que outras para a tarefa de recuperação de artigos estatutários, e o domínio das menos adequadas ainda precisa ser determinado. Esperamos que nosso trabalho desperte interesse no desenvolvimento de modelos de recuperação de artigos estatutários práticos e confiáveis que possam ajudar a melhorar o acesso à justiça para todos. Você pode conferir nosso artigo, conjunto de dados e código nos seguintes links. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, estamos felizes em apresentar nosso trabalho sobre VALSE, um benchmark independente de tarefa, destinado a testar modelos de visão e linguagem com fenômenos linguísticos específicos. Por que nos demos ao trabalho de configurar este benchmark? Bem, nos últimos anos, vimos uma explosão de modelos de visão e linguagem baseados em transformadores, pré-treinados em grandes quantidades de pares de imagem-texto. Cada um desses modelos avança o estado da arte em tarefas de visão e linguagem, como resposta a perguntas visuais, raciocínio de senso comum visual, recuperação de imagens, ancoragem de frases. Então, recebemos uma mensagem, as acurácias nesses benchmarks específicos de tarefa estão aumentando constantemente. Mas sabemos o que os modelos realmente aprenderam? O que um transformador de visão e linguagem entendeu ao atribuir uma pontuação alta para esta imagem e esta frase corresponderem e uma pontuação baixa para esta? Os modelos de visão e linguagem focam na coisa certa ou focam em vieses, como mostrado por trabalhos anteriores? Para lançar mais luz sobre este aspecto, propomos uma direção mais agnóstica de tarefa e introduzimos VALSE, que testa a sensibilidade dos modelos de visão e linguagem a fenômenos linguísticos específicos que afetam tanto as modalidades linguísticas quanto visuais. Nosso alvo é a existência, a pluralidade, a contagem, as relações espaciais, as ações e a correferência de entidades. Mas como testamos se os modelos de visão e linguagem capturaram esses fenômenos? Por meio de foiling, um método anteriormente aplicado para modelos de visão e linguagem apenas para frases não por Ravi Shekhar e colaboradores e na contagem por nós em trabalhos anteriores. Foiling basicamente significa que pegamos a legenda de uma imagem e produzimos um foil alterando a legenda de modo que ela não descreva mais a imagem. E fazemos essas alterações de frases focando em seis peças específicas, como existência, pluralidade, contagem, relações espaciais, ações e correferência de entidades, onde cada peça pode consistir em um ou mais instrumentos, caso tenhamos encontrado mais de uma maneira interessante de criar instâncias de foil. Por exemplo, no caso da peça de ações, temos dois instrumentos, um em que o verbo de ação é alterado para uma ação diferente e outro em que os agentes são trocados. Contagem e correferência também são peças que têm mais de um instrumento. E criamos esses foils garantindo que eles falhem em descrever a imagem, que sejam gramaticais e, de outra forma, sentenças válidas. Isso não é fácil de fazer porque uma legenda foil pode ser menos provável do que a legenda original. Por exemplo, embora não seja impossível, é estatisticamente menos provável que plantas cortem um homem do que um homem corte plantas, e grandes modelos de visão e linguagem poderiam perceber isso. Portanto, para obter foils válidos, devemos tomar uma atitude. Primeiro, fazemos uso de modelos de linguagem fortes para propor foils. Em segundo lugar, usamos inferência de linguagem natural, ou NLI, para filtrar foils que ainda poderiam descrever a imagem, pois, ao construir foils, precisamos garantir que eles falhem em descrever a imagem. Para testar isso automaticamente, aplicamos inferência de linguagem natural com a seguinte racionalidade. Consideramos uma imagem como a premissa e sua legenda como a hipótese implicada. Além disso, consideramos a legenda como a premissa e o foil como sua hipótese. Se um modelo NLI prevê que o foil contradiz ou é neutro em relação à legenda, consideramos isso um indicador de um foil válido. Se um NLI prevê que o foil é implicado pela legenda, não pode ser um bom foil, pois, por transitividade, dará uma descrição verdadeira da imagem, e filtramos esses foils. Mas este procedimento não é perfeito. É apenas um indicador para foils válidos. Portanto, como terceira medida para gerar foils válidos, empregamos anotadores humanos para validar os dados usados em VALSE. Então, após a filtragem e a avaliação humana, temos tantas instâncias de teste quanto descritas nesta tabela. Observe que VALSE não fornece nenhum dado de treinamento, apenas dados de teste, pois é um benchmark de teste de tiro zero. É projetado para aproveitar as capacidades existentes dos modelos de visão e linguagem após o pré-treinamento. O ajuste fino permitiria apenas que os modelos explorassem artefatos ou vieses estatísticos nos dados. E todos nós sabemos que esses modelos gostam de trapacear e tomar atalhos. E, como dissemos, estamos interessados em avaliar quais capacidades os modelos de visão e linguagem têm após o pré-treinamento. Experimentamos com cinco modelos de visão e linguagem em VALSE, a saber, com Clip, AlexMert, Wilbert, Wilbert 12 em 1 e VisualBird. Duas de nossas métricas de avaliação mais importantes são a acurácia dos modelos na classificação de pares de imagem-sentença em legendas e foils. Talvez mais relevante para este vídeo, mostraremos nossa métrica mais permissiva, a acurácia por pares, que mede se a pontuação de alinhamento de imagem-sentença é maior para o par de imagem-texto correto do que para seu par foil. Para mais métricas e resultados sobre elas, consulte nosso artigo. Os resultados com acurácia por pares são mostrados aqui e são consistentes com os resultados que obtivemos das outras métricas. É que o melhor desempenho de tiro zero é alcançado por Wilbert 12 em 1, seguido por Wilbert, AlexMert, Clip e, finalmente, VisualBird. É notável como os instrumentos centrados em objetos individuais, como existência e frases nominais, são quase resolvidos por Wilbert 12 em 1, destacando que os modelos são capazes de identificar objetos nomeados e sua presença em imagens. No entanto, nenhuma das peças restantes pode ser resolvida de forma confiável em nossos cenários de foiling adversários. Vemos dos instrumentos de pluralidade e contagem que os modelos de visão e linguagem têm dificuldade em distinguir referências a objetos únicos em comparação com múltiplos ou contá-los em uma imagem. A peça de relação mostra que eles têm dificuldade em classificar corretamente uma relação espacial nomeada entre objetos em uma imagem. Eles também têm dificuldade em distinguir ações e identificar seus participantes, mesmo que apoiados por vieses de plausibilidade, como vemos na peça de ações. Da peça de correferência, descobrimos que rastrear múltiplas referências ao mesmo objeto em uma imagem usando pronomes também é difícil para os modelos de visão e linguagem. Como verificação de sanidade e porque é um experimento interessante, também benchmarkamos dois modelos de texto apenas, GPT-1 e GPT-2, para avaliar se VALSE é solucionável por esses modelos unimodais calculando a perplexidade da legenda correta e da legenda foil, sem imagem aqui, e prevendo a entrada com a menor perplexidade. Se a perplexidade for maior para o foil, consideramos isso um indicador de que a legenda foil pode sofrer de viés de plausibilidade ou outros vieses linguísticos. E é interessante ver que, em alguns casos, os modelos GPT de texto apenas capturaram a plausibilidade do mundo melhor do que os modelos de visão e linguagem. Então, para resumir, VALSE é um benchmark que usa a lente de construções linguísticas para ajudar a comunidade a melhorar os modelos de visão e linguagem testando rigorosamente suas capacidades de ancoragem visual. Nossos experimentos mostram que os modelos de visão e linguagem identificam objetos nomeados e sua presença em imagens bem, como mostrado pela peça de existência, mas lutam para ancorar sua interdependência e relações em cenas visuais quando forçados a respeitar indicadores linguísticos. Gostaríamos muito de encorajar a comunidade a usar VALSE para medir o progresso em direção à ancoragem de linguagem com modelos de visão e linguagem. E ainda mais, VALSE poderia ser usado como uma avaliação indireta de conjuntos de dados, pois os modelos poderiam ser avaliados antes e depois do treinamento ou ajuste fino para ver se um conjunto de dados ajuda os modelos a melhorar em algum dos aspectos testados por VALSE. Se você estiver interessado, confira os dados VALSE no GitHub e, se tiver alguma dúvida, não hesite em nos contatar."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Kamizawa, da Universidade de Tóquio. Vou apresentar um artigo intitulado \"R&Sum: um grande conjunto de dados para a geração automática de notas de versão por meio da sumarização de logs de commit\". Vou explicar na seguinte ordem. Primeiro, vou introduzir a geração automática de notas de versão em que estamos trabalhando nesta pesquisa. Uma nota de versão é um documento técnico que resume as mudanças distribuídas com cada versão de um produto de software. A imagem mostra as notas de versão da versão 2.6.4 da biblioteca JQuery. As notas de versão desempenham um papel importante no desenvolvimento de código aberto, mas são demoradas para preparar manualmente. Portanto, seria muito útil poder gerar automaticamente notas de versão de alta qualidade. Vou me referir a duas pesquisas anteriores sobre a geração automática de notas de versão. A primeira é um sistema chamado Arena, lançado em 2014. Ele adota uma abordagem baseada em regras, por exemplo, usando o extrator de mudanças para extrair diferenças principais, mudanças de biblioteca e mudanças de documentação das diferenças entre versões e, finalmente, combinando-as. A característica mais notável desse sistema é o extrator de problemas no canto superior direito, que deve estar vinculado ao sistema de rastreamento de problemas Jira e só pode ser aplicado a projetos que usam Jira. Em outras palavras, não pode ser usado para muitos projetos no GitHub. A segunda é o Grief, anunciado recentemente em 2020. Está disponível na internet e pode ser instalado via pip. Esse sistema tem um modelo de classificação de texto simples baseado em aprendizado e gera uma das cinco etiquetas, como recursos ou correções de bugs, para cada mensagem de commit de entrada. A imagem é um exemplo de uso que retorna uma etiqueta de correção de bugs. Os dados de treinamento do Grief são bastante pequenos, cerca de 5.000, e a revisão é mostrada nos experimentos descritos abaixo. O desempenho do modelo de classificação de texto não é alto. Apresento duas pesquisas relacionadas, mas há problemas de aplicabilidade limitada e recursos de dados escassos. Nosso artigo resolve esses dois problemas e gera automaticamente notas de versão de alta qualidade. Para o problema de aplicabilidade limitada, propomos um método de sumarização de classificação de alta qualidade usando apenas mensagens de commit como entrada. Esse método proposto pode ser usado para todos os repositórios em inglês. Para o segundo problema de recursos de dados escassos, construímos um conjunto de dados R&Sum, composto por cerca de 82.000 peças de dados, coletando dados de repositórios públicos do GitHub usando a API do GitHub. A seguir, descrevo nosso conjunto de dados. Aqui está um exemplo de dados. O lado esquerdo é a mensagem de commit e o lado direito é a nota de versão. As notas de versão são rotuladas como implementações, correções de bugs, etc. Configuramos uma tarefa que usa as mensagens de commit como entrada e gera as notas de versão rotuladas. Isso pode ser considerado uma tarefa de sumarização. Definimos previamente todas as etiquetas, recursos, implementações, correções de bugs, duplicações, remoções e mudanças de interrupção. Essas foram definidas com base em pesquisas anteriores e outros fatos. As notas de versão no canto inferior direito são extraídas da nota de versão mostrada no canto inferior esquerdo. Nesse momento, é necessário detectar as quatro etiquetas que foram definidas antecipadamente. Mas as etiquetas não são sempre consistentes com cada repositório. Por exemplo, a etiqueta de implementações inclui implementações, melhorias, otimizações e assim por diante. Preparamos uma lista de vocabulário ou etiquetas de estudo para cada uma dessas variações de notação, usamos para detectar a classe de notas de versão e corrigimos o texto da nota que segue como a frase de nota de versão para a classe. A seguir, a mensagem de commit. As mensagens de commit não estão vinculadas a cada versão. Como mostrado na imagem abaixo, se a versão atual for a versão 2.5.19, precisamos identificar a versão anterior, 2.5.18, e obter sua diferença. Isso é um pouco tedioso e não é suficiente apenas obter uma lista de versões e olhar antes e depois. Criamos uma regra de correspondência heurística para obter as versões anterior e seguinte. Análise do conjunto de dados. No final, 7.200 repositórios e 82.000 peças de dados foram coletados. Além disso, o número médio de tokens de notas de versão é 63, o que é bastante alto para uma tarefa de sumarização. Além disso, o número de tokens exclusivos é bastante grande, 8.830.000. Isso se deve ao grande número de nomes de classes e métodos exclusivos encontrados na biblioteca. A seguir, explicarei o método proposto. O modelo de sumarização extrativa e abstrativa por classe consiste em dois módulos neurais, um classificador usando BART ou CodBART e um gerador usando BART. Primeiro, o CEAS usa um classificador para classificar cada mensagem de commit em cinco classes de notas de versão, que são implementações, correções de bugs, duplicações e outras. As mensagens de commit classificadas como outras são descartadas. Em seguida, o CEAS aplica o gerador aos quatro documentos rotulados independentemente e gera notas de versão para cada classe. Nessa tarefa, as correspondências diretas entre mensagens de commit e notas de versão não são conhecidas. Portanto, para treinar o classificador, atribuímos duas etiquetas a cada mensagem de commit de entrada usando os primeiros 10 caracteres de cada mensagem de commit. Modelamos a sumarização abstrativa por classe por meio de dois métodos diferentes. O primeiro modelo, que chamamos de CAS single, consiste em uma única rede seq2seq e gera um único texto de nota de versão longa, dado uma concatenação de mensagens de commit de entrada. O texto de saída pode ser dividido em segmentos classificados com base em símbolos de ponto final específicos da classe. O segundo método, que chamamos de CAS match, consiste em quatro redes seq2seq diferentes, cada uma correspondendo a uma das classes de notas de versão. Ok, vamos explicar o experimento. Cinco métodos foram comparados: CAS, CAS single, CAS match, Plustering e o estudo anterior Grief. Quanto à avaliação, em alguns casos, as notas de versão são geradas em múltiplas frases. Como é difícil calcular o número de frases como zero, elas são combinadas com espaços e tratadas como uma única frase longa. O BLEU é penalizado quando o sistema gera uma frase curta. Essa penalidade resulta em um valor BLEU mais baixo nos resultados do experimento descritos a seguir. Finalmente, também calculamos a especificidade porque Rouge e BLEU não podem ser calculados se as notas de versão estiverem vazias. Uma alta especificidade significa que o modelo gera corretamente um texto vazio em casos em que as notas de versão são assumidas como vazias. Aqui estão os resultados. Como o conjunto de dados contém endereços de e-mail, valores de hash, etc., também avaliamos o conjunto de dados limpo, que os exclui. CEAS e CEAS obtiveram pontuações Rouge-L mais de 10 pontos mais altas do que as linhas de base. Em particular, no conjunto de dados de teste limpo, a lacuna de pontuação entre o método proposto e a linha de base saltou para mais de 20 pontos. Esses resultados indicam que CEAS e CEAS são significativamente eficazes. CEAS obteve uma pontuação Rouge-L melhor do que CEAS, sugerindo que combinar um classificador e um gerador é eficaz e treinar o classificador usando duas etiquetas. A alta cobertura do CEAS pode ser um desafio, provavelmente porque o classificador pode se concentrar em selecionar mensagens de commit relevantes para cada classe. CEAS match tende a ter um Rouge-L mais alto do que CEAS single, sugerindo que também é eficaz desenvolver independentemente diferentes modelos de sumarização abstrativa para cada classe de notas de versão. Aqui está uma análise de erros. Os métodos CEAS tendem a gerar frases mais curtas do que as frases de referência humanas. Na figura à direita, a frase de referência tem três ou quatro frases, enquanto CEAS tem apenas uma. A razão para essa relutância do modelo é que, nos dados de treinamento, apenas 33% das frases estão presentes na etiqueta de recursos e 40% na etiqueta de implementações. Além disso, os métodos CEAS não conseguem gerar notas de versão precisas sem informações adicionais. O exemplo superior à direita é um exemplo de uma mensagem de commit muito confusa e a frase completa não pode ser gerada sem referência ao pull request ou problema correspondente. O exemplo abaixo mostra que as duas mensagens de commit na entrada estão relacionadas e devem ser combinadas em uma frase, mas ele hesita em fazer isso. Finalmente, uma conclusão. Construímos um novo conjunto de dados para a geração automática de notas de versão. Também formulamos a tarefa de inserir mensagens de commit e sumarizá-las para que seja aplicável a todos os projetos escritos em inglês. Nosso experimento mostra que o método proposto gera notas de versão menos ruidosas com maior cobertura do que as linhas de base. Por favor, confira nosso conjunto de dados no GitHub. Obrigado."}
