{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Asaf Ferrari et je vais présenter notre article intitulé « Enrichissement de données tabulaires à partir de quelques exemples en utilisant des architectures de transformateurs finement ajustées ». Les scientifiques des données analysent les données et se concentrent principalement sur la manipulation des caractéristiques existantes des données. Mais parfois, ces caractéristiques sont limitées. La génération de caractéristiques à partir d'une autre source de données peut ajouter des informations substantielles. Notre objectif de recherche est l'enrichissement automatique des données tabulaires à l'aide de sources externes de texte libre. Supposons que nous ayons un ensemble de données tabulaires et une base de connaissances. Nous avons besoin d'un processus automatique qui implique le lien d'entités et l'analyse de texte pour extraire de nouvelles caractéristiques de la base de connaissances de texte libre. Notre cadre, FEAST, est exactement ce processus automatique. Voyons un exemple. Dans les ensembles de données fournis à FEAST. Dans cet exemple, l'ensemble de données est un ensemble de données universitaires dont l'objectif est de classer les universités en universités de faible rang et en universités de haut rang. En tant que base de connaissances, nous utilisons Wikipédia. La première phase de FEAST est le lien d'entités, où chaque entité, dans cet exemple le nom de l'université, est liée à une entité au sein de la base de connaissances. Et le texte des entités de la base de connaissances est extrait et ajouté à l'ensemble de données. Dans cet exemple, le texte est le résumé de la page Wikipédia. Maintenant, nous devons générer ou extraire des caractéristiques du texte récupéré. Nous avons donc besoin d'une phase d'extraction de caractéristiques qui inclut l'analyse de texte. Et c'est la principale contribution de cet article, et je vais approfondir ce point dans les diapositives suivantes. Après la phase d'extraction de caractéristiques, il y a une phase de génération de caractéristiques où nous utilisons les caractéristiques extraites pour générer un petit nombre de nouvelles caractéristiques. FEAST génère d'abord des caractéristiques dans le nombre de classes de l'ensemble de données d'origine. Dans cet exemple, l'ensemble de données d'origine a deux classes, donc FEAST génère deux nouvelles caractéristiques. Mais si l'ensemble de données a cinq classes, FEAST génère cinq nouvelles caractéristiques. Chaque caractéristique représente la probabilité pour chaque classe. Pour analyser le texte, nous utilisons l'état actuel de l'art de l'analyse de texte, qui sont des modèles de langage basés sur des transformateurs, tels que BERT, GPT, XLNet, etc. Mais il est peu probable que nous puissions former un modèle de langage à l'aide des ensembles de données d'entrée. Une approche naïve serait donc un ajustement fin de la tâche cible. Dans la phase d'extraction de caractéristiques, nous pouvons télécharger un modèle de langage pré-entraîné, ajuster finement le modèle de langage sur l'ensemble de données cible. Dans cet exemple, pour ajuster finement le modèle de langage afin de classer le texte en classes, résumé en classes, faible ou élevé, recevoir la sortie du modèle de langage, qui est la probabilité pour chaque classe, et l'utiliser comme nouvelles caractéristiques. Le problème avec cette approche est que les ensembles de données peuvent avoir peu de textes d'entités distincts. Dans notre expérience, près de la moitié des ensembles de données contiennent moins de 400 échantillons, et le plus petit ensemble de données contient 35 échantillons dans son ensemble d'entraînement. Ajuster finement un modèle de langage sur cet ensemble de données serait donc inefficace. Mais nous pouvons utiliser des connaissances préalables sur des ensembles de données pré-analysés car FEAST est appliqué sur plusieurs ensembles de données. Nous pouvons utiliser les ensembles de données N-1 pour recueillir des informations sur les ensembles de données N-1 et utiliser ces informations lorsque nous analysons l'ensemble de données N. Ce que nous suggérons, c'est d'ajouter une autre phase d'ajustement fin, une phase d'ajustement fin préliminaire multitâche, où nous ajustons finement le modèle de langage sur les ensembles de données N-1, puis nous exécutons une autre phase d'ajustement fin, qui est un ajustement fin de la tâche cible lorsque nous ajustons finement le modèle de langage sur l'ensemble de données cible N. L'état de l'art en ajustement fin multitâche s'appelle MT-DNN. Dans MT-DNN, MT-DNN maintient des têtes dans le nombre de tâches dans l'ensemble d'entraînement. Dans cet exemple, il y a quatre tâches dans l'ensemble d'entraînement, donc MT-DNN maintient quatre têtes, comme vous pouvez le voir sur l'image, et il échantillonne un lot aléatoire de l'ensemble d'entraînement. Et si le lot aléatoire appartient, par exemple, à des tâches de classification de phrases, il exécute un passage avant et arrière à travers la première tête. Et si le lot aléatoire appartient à une tâche de classement par paires, il exécute un passage avant et arrière à travers la dernière tête. Dans notre scénario, les ensembles de données tabulaires varient en fonction du nombre de classes. Il y a donc de nombreuses tâches. MT-DNN maintient un nombre de têtes de classes, des couches de sortie, et en plus, MT-DNN doit initialiser de nouvelles têtes pour un nouvel ensemble de données avec une nouvelle tâche. Notre approche s'appelle l'ajustement fin de reformulation de tâche. Dans notre approche, l'ajustement fin de reformulation de tâche, au lieu de maintenir plusieurs têtes, nous reformulons chaque ensemble de données en un problème de classification de phrases, qui est une tâche à deux classes. Voyons un exemple. Voici notre ensemble de données d'entrée, qui se compose d'entités, de caractéristiques, de texte et de classes, et nous reformulons la tâche de classer le texte en faible et élevé pour classer le texte, le résumé et la classe en vrai ou faux. Ou en d'autres termes, nous formons le modèle de langage pour classer un résumé et une classe, pour classer un résumé et une classe, si le résumé appartient à la classe ou non. Le vecteur d'étiquette dans ce cas reste toujours, qui se compose toujours de deux classes. Et voici l'algorithme pour notre approche d'ajustement fin reformulée. Voyons le cadre complet. Un ensemble de données est fourni à FEAST, puis FEAST exécute la phase de lien d'entités. Il extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia. Ensuite, il reformule la tâche en une tâche de classification de phrases. Il applique le modèle de langage à la nouvelle tâche et produit la probabilité pour chaque classe. Notez que le modèle de langage est déjà ajusté finement sur l'ensemble de données N-1 à l'aide d'un ajustement fin préliminaire multitâche. Ensuite, nous utilisons le vecteur de sortie du modèle de langage comme nouvelle caractéristique générée dans le nombre de classes. Pour évaluer notre cadre, nous utilisons 17 ensembles de données de classification tabulaires, qui varient en taille, caractéristiques, équilibre, domaine et performance initiale. Et en tant que base de connaissances, nous utilisons Wikipédia. Nous concevons notre expérience comme une évaluation de type « leave one out », où nous formons FEAST sur 16 ensembles de données et l'appliquons au 17e ensemble de données. Nous divisons également chaque ensemble de données en quatre plis et appliquons une validation croisée à quatre plis. Ensuite, nous générons la nouvelle caractéristique et les évaluons à l'aide de cinq classificateurs d'évaluation. Nous utilisons dans notre expérience une architecture basée sur BERT. Voici les résultats de notre expérience. Vous pouvez voir que nous comparons notre cadre à l'ajustement fin de l'ensemble de données cible, à l'ajustement fin de la tâche cible et à l'ajustement fin préliminaire MT-DNN, et notre ajustement fin reformulé obtient le meilleur résultat, la meilleure performance, tandis que MT-DNN obtient une amélioration de 2 % par rapport à l'ajustement fin de l'ensemble de données cible, notre approche obtient une amélioration de 6 %. Lorsque nous regardons les petits ensembles de données, nous pouvons voir que la performance de MT-DNN diminue et que l'amélioration de la phase d'ajustement fin préliminaire multitâche diminue à 1,5 %, mais notre performance augmente à 11 % par rapport à l'ajustement fin de la tâche cible seul. En résumé, FEAST permet un enrichissement à partir de quelques exemples à partir de 35 échantillons dans notre expérience. Il utilise une architecture pour tous les ensembles de données de tâches et conserve la tête du modèle. Mais il ajoute une phase de reformulation. Il augmente l'ensemble de données et a besoin d'une valeur cible avec une signification sémantique afin que nous puissions l'alimenter dans le modèle de langage et l'utiliser dans le problème de classification de phrases. Merci."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour à tous. Aujourd'hui, je vais présenter notre travail de recherche, Apprendre à raisonner déductivement, la résolution de problèmes mathématiques comme une extraction de raisonnement complexe. Je suis Alan du laboratoire d'IA de Baidu, et ceci est un travail conjoint avec Jerry de l'Université du Texas à Austin et Weilu de SUDD. Tout d'abord, j'aimerais parler de notre motivation pour le raisonnement. Ici, nous montrons des exemples où le raisonnement en plusieurs étapes est utile. Cette figure est tirée de l'article de Pound, où ils effectuent un prompt pour résoudre le problème de la méthode dans un scénario d'apprentissage à quelques coups. Donc, sur le côté gauche, nous pouvons voir que si nous donnons quelques échantillons avec juste des questions et des réponses, nous ne pourrons peut-être pas obtenir les bonnes réponses. Mais si nous donnons une description de raisonnement plus détaillée, le modèle est capable de prédire la description de raisonnement et de faire une prédiction correcte ici. Il est donc bon d'avoir un raisonnement en plusieurs étapes interprétable comme sortie. Et nous pensons également que le problème de la méthode est une application directe pour évaluer de telles capacités de raisonnement. Donc, dans notre configuration de problème, étant donné les questions, nous devons résoudre cette question et obtenir les réponses numériques. Donc, dans nos ensembles de données, nous recevons également l'expression mathématique, qui conduit à cette réponse particulière. Donc, certaines hypothèses s'appliquent également comme dans les travaux précédents. Nous supposons que la précision des quantités n'est pas connue, et nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle. De plus, les opérateurs complexes peuvent en fait être décomposés en ces opérateurs de base. Donc, les travaux précédents sur la résolution de problèmes de méthode peuvent en fait être catégorisés en modèle de séquence à séquence et de séquence à arbre. Donc, les modèles de séquence à séquence traditionnels convertissent l'expression en une séquence spécifique pour la génération, et il est assez facile à mettre en œuvre, et il peut généraliser à de nombreux problèmes différents et compliqués. Mais les inconvénients sont que les performances ne sont généralement pas meilleures que celles du modèle structuré, et il manque d'interprétabilité pour la prédiction. Mais en fait, cette direction est encore assez populaire à cause du modèle de transformateur. Donc, dans les modèles basés sur les arbres, nous structurons en fait ces expressions sous forme d'arbre et suivons un parcours préfixe dans la génération d'arbres. Donc, ici, nous continuons à générer les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités. Donc, ici, la bonne chose est qu'il nous donne en fait cette structure d'arbre binaire, et c'est, mais en fait, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur, et puis à la fin, nous générons les quantités. Et la deuxième chose est qu'il contient également certains calculs répétitifs. Donc, ici, si nous regardons cette expression, 8 fois 3 plus 3 est en fait généré deux fois. Mais en fait, nous devrions réutiliser les résultats. Donc, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable. Donc, par exemple, ici, à la deuxième étape, nous pouvons obtenir ce diviseur, qui est 27. Et nous pouvons également nous référer aux questions originales pour trouver les contenus pertinents. Et dans ces étapes, nous obtenons les diviseurs. Donc, et puis à cette troisième étape, nous obtenons en fait le quotient. D'accord. Et après ces trois étapes, nous pouvons en fait réutiliser les résultats de la deuxième étape et puis obtenir les résultats de la quatrième étape. Et puis finalement, nous pouvons obtenir les dividendes. Donc, ici, nous générons en fait toute l'expression directement plutôt que de générer un seul opérateur ou des quantités. Donc, cela rend le processus plus précis. Donc, dans notre système déductif, nous commençons d'abord par un ensemble de quantités présentées dans les questions et incluant également certaines constantes comme notre état initial. Donc, l'expression est représentée par EIJOP, où nous effectuons l'opérateur de QI à QJ, et une telle expression est en fait dirigée. Donc, nous avons également une soustraction inverse ici pour représenter la direction opposée. C'est assez similaire à l'extraction de relations. Donc, dans un système déductif formel, à un instant t, nous appliquons l'opérateur entre la paire QI et QJ, et puis nous obtenons cette nouvelle expression. Nous l'ajoutons aux états suivants pour devenir une nouvelle quantité. Donc, cette diapositive visualise en fait l'évolution des états où nous continuons à ajouter l'expression aux états actuels. Donc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langage pré-entraîné qui peut être BERT ou Roberta, et puis nous encodons la phrase, et puis nous obtenons ces représentations de quantité. Donc, une fois que nous avons les représentations de quantité, nous pouvons commencer à faire des inférences. Ici, nous montrons un exemple de Q1 pour obtenir la représentation de Q1 divisé par Q2 et puis multiplié par Q3. Tout d'abord, nous obtenons la représentation de paire, qui est essentiellement juste la concaténation entre Q1 et Q2, et puis nous appliquons un réseau feedforward, qui est paramétré par l'opérateur. Et puis finalement, nous obtenons la représentation de l'expression Q1 divisé par Q2. Mais en fait, en pratique, à l'étape d'inférence, nous pourrions obtenir l'expression incorrecte également. Donc, ici, toutes les expressions possibles sont égales à trois fois le nombre d'opérateurs. Donc, la bonne chose ici est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche. Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche. Donc, à la deuxième étape, nous faisons la même chose, mais la seule différence est que nous, la seule différence est une quantité de plus. Donc, cette quantité provient de l'expression calculée précédemment. Donc, finalement, nous pouvons obtenir cette expression finale Q3 fois Q4. Et nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape précédente. Donc, une telle différence rend difficile l'application de la recherche de faisceau parce que la distribution de probabilité entre ces deux étapes est déséquilibrée. Donc, la procédure d'entraînement est similaire à l'entraînement d'un modèle de séquence à séquence, où nous optimisons la perte à chaque instant. Et ici, nous utilisons également ce tau pour représenter quand nous devrions terminer ce processus de génération. Et ici, l'espace est différent de la séquence à séquence parce que l'espace est différent à chaque instant, tandis que dans le modèle de séquence à séquence traditionnel, c'est le nombre de vocabulaire. Et cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures. Donc, nous menons des expériences sur les ensembles de données de problèmes de méthode couramment utilisés, MAWPS, Math 23K, MathQA, et SWAMP. Et ici, nous montrons brièvement les résultats comparés aux meilleures approches précédentes. Donc, notre version la mieux performante est Roberta, le raisonneur déductif. Et en fait, nous n'utilisons pas la recherche de faisceau contrairement aux approches objectives utilisant la recherche de faisceau. D'accord. Donc, les meilleures approches sont souvent un modèle basé sur les arbres. Donc, dans l'ensemble, notre raisonneur est capable de surperformer de manière significative ce modèle basé sur les arbres, mais nous pouvons voir que le nombre absolu sur MathQA ou SWAMP n'est pas vraiment élevé. Donc, nous examinons davantage les résultats sur SWAMP, et cet ensemble de données est difficile parce que l'auteur essaie d'ajouter manuellement quelque chose pour confondre le modèle NLP, comme ajouter des informations environnementales et des quantités supplémentaires. Donc, dans notre prédiction, nous trouvons que certaines des valeurs intermédiaires sont en fait négatives. Par exemple, dans ces questions, nous demandons combien de pommes Jake a, mais nous avons des informations supplémentaires comme 17 pêches et Steven en a huit, ce qui est totalement environnemental. Donc, notre modèle fait une prédiction comme celle-ci, qui produit des valeurs négatives. Et nous observons que ces deux expressions ont en fait des scores similaires. Donc, nous pouvons en fait limiter cet espace de recherche en supprimant, comme, ces résultats sont négatifs afin que nous puissions rendre la réponse correcte. Donc, nous trouvons en fait que cette contrainte améliore beaucoup pour certains modèles. Par exemple, pour BERT, nous améliorons de sept points, et puis pour le modèle basé sur Roberta, nous améliorons en fait de deux points. Donc, un meilleur modèle de langage a une meilleure capacité de compréhension du langage, donc le nombre ici est plus élevé pour Roberta et plus bas pour BERT. Et nous essayons également d'analyser la difficulté derrière tous ces ensembles de données. Nous supposons que le nombre de quantités inutilisées peut être considéré comme des informations environnementales ici. Donc, ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et l'ensemble de données SWAMP a la plus grande proportion. Et ici, nous montrons également les performances globales. Pour ces échantillons sans quantités inutilisées. Donc, les performances globales sont en fait plus élevées que les performances globales. Mais avec ces échantillons avec des quantités inutilisées, c'est en fait bien pire que les performances globales. Pour MAWPS, nous n'avons pas vraiment beaucoup de ces cas. Donc, j'ignore juste cette partie. Donc, enfin, nous voulons montrer l'interprétabilité à travers un exemple de prédiction de crash. Donc, ici, notre modèle fait en fait une mauvaise prédiction à la première étape. Donc, nous pouvons en fait corréler cette expression avec la phrase ici. D'accord. Donc, nous pensons que cette phrase pourrait induire le modèle en erreur et en prédictions incorrectes. Donc, ici, planter un autre 35 fait penser au modèle qu'il devrait être un opérateur d'addition. Donc, nous essayons de réviser la phrase pour qu'elle soit quelque chose comme le nombre d'arbres à poires est de 55 de moins que les pommiers. Donc, nous la faisons transmettre une sémantique plus précise, de sorte que le modèle est capable de faire la prédiction correcte. Donc, cette étude montre comment les prédictions interprétables nous aident à comprendre le comportement du modèle. Donc, pour conclure notre travail, tout d'abord, notre modèle est en fait assez efficace, et nous sommes capables de fournir une procédure de résolution interprétable. Et nous pouvons facilement incorporer certaines connaissances antérieures en tant que contraintes, ce qui peut aider à améliorer les performances. Et la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution de problèmes de méthode, mais aussi à d'autres tâches qui impliquent un raisonnement en plusieurs étapes. Mais nous avons également certaines limitations. Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée. Et la deuxième chose est que, comme mentionné, parce que la distribution de probabilité est déséquilibrée à différents instants, il est également assez difficile d'appliquer la stratégie de recherche de faisceau. Donc, c'est la fin de la présentation, et les questions sont les bienvenues. Merci."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht. Je vais présenter mon travail conjoint avec Jerry, qui porte sur un nouveau jeu de données pour la récupération d'articles législatifs. Les questions juridiques font partie intégrante de la vie de nombreuses personnes. Mais la majorité des citoyens ont peu ou pas de connaissances sur leurs droits et les processus juridiques fondamentaux. En conséquence, de nombreux citoyens vulnérables qui ne peuvent pas se permettre l'assistance coûteuse d'un expert juridique se retrouvent sans protection ou, pire encore, exploités. Notre travail vise à combler le fossé entre les personnes et la loi en développant un système de récupération efficace pour les articles législatifs. Un tel système pourrait fournir un service d'aide juridique professionnel gratuit pour les personnes non qualifiées. Avant de plonger dans la contribution principale de ce travail, décrivons d'abord le problème de la récupération d'articles législatifs. Étant donné une question simple sur une question juridique, comme \"que risque-t-on si l'on viole le secret professionnel ?\", un modèle est requis pour récupérer tous les articles législatifs pertinents à partir d'un grand corpus de législation. Cette tâche de récupération d'informations comporte son propre ensemble de défis. Premièrement, elle traite de deux types de langage : le langage naturel commun pour les questions et le langage juridique complexe pour les statuts. Cette différence dans les distributions linguistiques rend plus difficile pour un système de récupérer des candidats pertinents, car elle nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique qui correspond à la terminologie des statuts. De plus, la loi statutaire n'est pas un ensemble d'articles indépendants qui peuvent être traités comme une source d'information complète en eux-mêmes, comme les nouvelles ou les recettes, par exemple. Au lieu de cela, c'est une collection structurée de dispositions juridiques qui n'ont un sens complet que lorsqu'elles sont considérées dans leur contexte global, c'est-à-dire avec les informations supplémentaires de leurs articles voisins, les domaines et sous-domaines auxquels ils appartiennent, et leur place dans la structure de la loi. Enfin, les articles législatifs ne sont pas de petits paragraphes, qui sont généralement l'unité de récupération typique dans la plupart des travaux de récupération. Ici, ce sont de longs documents qui peuvent atteindre 6 000 mots. Les récentes avancées en TAL ont suscité un grand intérêt pour de nombreuses tâches juridiques, telles que la prédiction de jugements juridiques ou la révision automatique de contrats, mais la récupération d'articles législatifs est restée principalement intouchée en raison du manque de grands jeux de données étiquetés de haute qualité. Dans ce travail, nous présentons un nouveau jeu de données natif français centré sur le citoyen pour étudier si un modèle de récupération peut approcher l'efficacité et la fiabilité d'un expert juridique pour la tâche de récupération d'articles législatifs. Notre jeu de données de récupération d'articles législatifs belges, Sorts, se compose de plus de 1 100 questions juridiques posées par des citoyens belges. Ces questions couvrent une large gamme de sujets, allant de la famille, du logement, de l'argent, au travail et à la sécurité sociale. Chacune d'elles a été étiquetée par des juristes expérimentés avec des références aux articles pertinents d'un corpus de plus de 22 600 articles juridiques du Code belge. Parlons maintenant de la manière dont nous avons collecté ce jeu de données. Tout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques. Nous avons considéré 32 codes belges disponibles publiquement et extrait tous leurs articles ainsi que les en-têtes de section correspondants. Ensuite, nous avons rassemblé des questions juridiques avec des références aux statuts pertinents. Pour ce faire, nous avons collaboré avec un cabinet d'avocats belge qui reçoit chaque année environ 4 000 e-mails de citoyens belges demandant des conseils sur une question juridique personnelle. Nous avons eu la chance d'accéder à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges. Nous avons collecté des milliers de questions, annotées avec des catégories, des sous-catégories et des références juridiques aux statuts pertinents. Enfin, nous avons analysé les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de loi que nous avons considérés. Les références restantes ont été mises en correspondance et converties en identifiants d'articles correspondants de notre corpus. Nous avons finalement abouti à 1 108 questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de 22 633 articles législatifs. De plus, chaque question est accompagnée d'une catégorie principale et d'une concaténation de sous-catégories, et chaque article est accompagné d'une concaténation de leurs en-têtes subséquents dans la structure de la loi. Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient être d'intérêt pour des recherches futures sur la récupération d'informations juridiques ou la classification de textes juridiques. Examinons quelques caractéristiques de notre jeu de données. Les questions font entre 5 et 44 mots de long, avec une médiane de 40 mots. Les articles sont beaucoup plus longs, avec une longueur médiane de 77 mots, dont 142 dépassent 1 000 mots. Le plus long fait jusqu'à 5 790 mots. Comme mentionné précédemment, les questions couvrent une large gamme de sujets, environ 85 % d'entre elles portant soit sur la famille, le logement, l'argent ou la justice, tandis que les 15 % restants concernent soit la sécurité sociale, les étrangers ou le travail. Les articles sont également très diversifiés, car ils proviennent de 32 codes belges différents qui couvrent un grand nombre de sujets juridiques. Voici le nombre total d'articles collectés à partir de chacun de ces codes belges. Sur les 22 633 articles, seuls 1 612 sont cités comme pertinents pour au moins une question dans le jeu de données. Et environ 80 % de ces articles cités proviennent soit du Code civil, du Code judiciaire, du Code d'enquête pénale ou du Code pénal. Pendant ce temps, 18 des 32 codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question, ce qui peut s'expliquer par le fait que ces codes se concentrent moins sur les individus et leurs préoccupations. Globalement, le nombre médian de citations pour ces articles cités est de deux, et moins de 25 % d'entre eux sont cités plus de cinq fois. En utilisant notre jeu de données, nous avons évalué plusieurs approches de récupération, y compris les architectures lexicales et denses. Étant donné une requête et un article, un modèle lexical attribue un score à la paire requête-article en calculant la somme, sur les termes de la requête, des poids de chacun de ces termes dans cet article. Nous avons expérimenté avec les fonctions de classement standard TF-IDF et BM25. Le principal problème avec ces approches est qu'elles ne peuvent récupérer que des articles contenant des mots-clés présents dans la requête. Pour surmonter cette limitation, nous avons expérimenté avec une architecture basée sur le neurone qui peut capturer les relations sémantiques entre les requêtes et les articles. Nous utilisons un modèle B-encoder qui mappe les requêtes et les articles dans des représentations vectorielles denses et calcule un score de pertinence entre une paire requête-article par la similarité de leurs embeddings. Ces embeddings résultent généralement d'une opération de pooling sur la sortie d'un modèle d'embedding de mots. Tout d'abord, nous étudions l'efficacité des B-encoders Siamese dans un cadre d'évaluation zéro-shot, ce qui signifie que les modèles d'embedding de mots pré-entraînés sont appliqués directement sans aucun ajustement fin supplémentaire. Nous avons expérimenté avec des encodeurs de texte indépendants du contexte, à savoir Word2Vec et FastText, et des modèles d'embedding dépendants du contexte, à savoir Roberta et plus spécifiquement Camembert, qui est un modèle Roberta français. De plus, nous avons formé notre propre modèle B-encoder basé sur Camembert sur notre jeu de données. Notez que pour l'entraînement, nous avons expérimenté avec les deux saveurs de l'architecture B-encoder. Siamese, qui utilise un modèle d'embedding de mots unique qui mappe la requête et l'article ensemble dans un espace vectoriel dense partagé, et Two Tower, qui utilise deux modèles d'embedding de mots indépendants qui encodent la requête et l'article séparément dans des espaces d'embedding différents. Nous avons expérimenté avec le pooling moyen, max et CLS, ainsi qu'avec le produit scalaire et le cosinus pour calculer les similarités. Voici les résultats de notre ligne de base sur l'ensemble de test, avec les méthodes lexicales ci-dessus, les B-encoders Siamese évalués dans un cadre zéro-shot au milieu, et les B-encoders ajustés en dessous. Globalement, les B-encoders ajustés surpassent de manière significative toutes les autres lignes de base. Le modèle Two Tower s'améliore par rapport à sa variante Siamese sur le rappel à 100, mais se comporte de manière similaire sur les autres métriques. Bien que BM25 ait sous-performé les B-encoders entraînés de manière significative, ses performances indiquent qu'il reste une ligne de base solide pour la récupération spécifique au domaine. En ce qui concerne l'évaluation zéro-shot des B-encoders Siamese, nous constatons que l'utilisation directe des embeddings d'un modèle Camembert pré-entraîné sans optimisation pour la tâche de récupération d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats précédents. De plus, nous observons que le B-encoder basé sur Word2Vec surpasse de manière significative le modèle basé sur FastText et Bird, suggérant que peut-être les embeddings de mots pré-entraînés sont plus appropriés pour la tâche que les embeddings de caractères ou de sous-mots lorsqu'ils sont utilisés directement. Bien que prometteurs, ces résultats suggèrent une ample opportunité d'amélioration par rapport à un expert juridique compétent qui peut éventuellement récupérer tous les articles pertinents pour toute question et ainsi obtenir des scores parfaits. Concluons en discutant de deux limitations de notre jeu de données. Premièrement, le corpus d'articles est limité à ceux collectés à partir des 32 codes belges considérés, ce qui ne couvre pas l'ensemble de la loi belge, car les articles des décrets, directives et ordonnances sont manquants. Lors de la construction du jeu de données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions se retrouvent avec seulement une fraction de leur nombre initial d'articles pertinents. Cette perte d'information implique que la réponse contenue dans les articles pertinents restants pourrait être incomplète, bien qu'elle soit encore complètement appropriée. Deuxièmement, nous devons noter que toutes les questions juridiques ne peuvent pas être répondues avec les statuts seuls. Par exemple, la question \"puis-je expulser mes locataires s'ils font trop de bruit ?\" pourrait ne pas avoir de réponse détaillée dans la loi statutaire qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée. Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des précédents similaires à la situation actuelle. Par exemple, le locataire fait deux fêtes par semaine jusqu'à 2 heures du matin. Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche de récupération d'articles législatifs, et le domaine des moins adaptées reste à déterminer. Nous espérons que notre travail suscitera l'intérêt pour le développement de modèles de récupération d'articles législatifs pratiques et fiables qui peuvent aider à améliorer l'accès à la justice pour tous. Vous pouvez consulter notre article, notre ensemble de données et notre code aux liens suivants. Merci."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE, un benchmark indépendant des tâches, conçu pour tester les modèles de vision et de langage avec des phénomènes linguistiques spécifiques. Pourquoi nous sommes-nous donné la peine de mettre en place ce benchmark ? Eh bien, au cours des dernières années, nous avons assisté à une explosion de modèles de vision et de langage basés sur des transformateurs, pré-entraînés sur de grandes quantités de paires d'images et de textes. Chacun de ces modèles pousse les limites de l'état de l'art dans les tâches de vision et de langage, telles que la réponse à des questions visuelles, le raisonnement de bon sens visuel, la récupération d'images, l'ancrage de phrases. Nous avons donc reçu un message : les précisions sur ces benchmarks spécifiques aux tâches augmentent régulièrement. Mais savons-nous ce que les modèles ont réellement appris ? Que comprend un transformateur de vision et de langage lorsqu'il attribue un score élevé à cette image et à cette phrase pour correspondre, et un score faible à celle-ci ? Les modèles de vision et de langage se concentrent-ils sur la bonne chose, ou se concentrent-ils sur les biais, comme le montrent les travaux précédents ? Pour faire la lumière sur cet aspect, nous proposons une direction plus agnostique des tâches et introduisons VALSE, qui teste la sensibilité des modèles de vision et de langage à des phénomènes linguistiques spécifiques qui affectent à la fois les modalités linguistiques et visuelles. Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence des entités. Mais comment testons-nous si les modèles de vision et de langage ont capturé ces phénomènes ? Par le biais du \"foiling\", une méthode précédemment appliquée aux modèles de vision et de langage uniquement pour les non-phrases par Ravi Shekhar et ses collaborateurs, et sur le comptage par nous dans des travaux précédents. Le \"foiling\" signifie essentiellement que nous prenons la légende d'une image et produisons un \"foil\" en modifiant la légende de manière à ce qu'elle ne décrive plus l'image. Et nous faisons ces altérations de phrases en nous concentrant sur six éléments spécifiques, tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence des entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous trouverions plus d'une manière intéressante de créer des instances de \"foil\". Par exemple, dans le cas de l'élément des actions, nous avons deux instruments, l'un dans lequel le verbe d'action est changé par une action différente, et l'autre dans lequel les actants sont échangés. Le comptage et la coréférence sont également des éléments qui ont plus d'un instrument. Et nous créons ces \"foils\" en nous assurant qu'ils échouent à décrire l'image, qu'ils sont grammaticaux et autrement des phrases valides. Ce n'est pas facile à faire car une légende \"foiled\" peut être moins probable que la légende originale. Par exemple, bien qu'il ne soit pas impossible, il est statistiquement moins probable que des plantes coupent un homme qu'un homme coupe des plantes, et de grands modèles de vision et de langage pourraient s'en rendre compte. Par conséquent, pour obtenir des \"foils\" valides, nous devons prendre des mesures. Premièrement, nous utilisons des modèles linguistiques puissants pour proposer des \"foils\". Deuxièmement, nous utilisons l'inférence en langage naturel, ou NLI, pour filtrer les \"foils\" qui pourraient encore décrire l'image, car lors de la construction des \"foils\", nous devons nous assurer qu'ils échouent à décrire l'image. Pour tester cela automatiquement, nous appliquons l'inférence en langage naturel avec le raisonnement suivant. Nous considérons une image comme la prémisse et sa légende comme son hypothèse impliquée. En outre, nous considérons la légende comme la prémisse et le \"foil\" comme son hypothèse. Si un modèle NLI prédit que le \"foil\" contredit ou est neutre par rapport à la légende, nous prenons cela comme un indicateur d'un \"foil\" valide. Si un NLI prédit que le \"foil\" est impliqué par la légende, il ne peut pas être un bon \"foil\", car par transitivité, il donnera une description véridique de l'image, et nous filtrons ces \"foils\". Mais cette procédure n'est pas parfaite. C'est juste un indicateur de \"foils\" valides, donc en tant que troisième mesure pour générer des \"foils\" valides, nous employons des annotateurs humains pour valider les données utilisées dans VALSE. Donc, après le filtrage et l'évaluation humaine, nous avons autant d'instances de test que décrites dans ce tableau. Notez que VALSE ne fournit aucune donnée d'entraînement, mais seulement des données de test, car il s'agit d'un benchmark de test à zéro tir seulement. Il est conçu pour tirer parti des capacités existantes des modèles de vision et de langage après le pré-entraînement. Le réglage fin ne ferait que permettre aux modèles d'exploiter les artefacts ou les biais statistiques dans les données. Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis. Et comme nous l'avons dit, nous sommes intéressés à évaluer les capacités des modèles de vision et de langage après le pré-entraînement. Nous expérimentons avec cinq modèles de vision et de langage sur VALSE, à savoir avec CLIP, AlexMert, Wilbert, Wilbert 12 en 1 et VisualBert. Deux de nos métriques d'évaluation les plus importantes sont la précision des modèles à classer les paires d'images et de phrases en légendes et en \"foils\". Peut-être plus pertinent pour cette vidéo, nous allons montrer notre métrique plus permissive, la précision par paire, qui mesure si le score d'alignement image-phrase est plus élevé pour la paire image-texte correcte que pour sa paire \"foiled\". Pour plus de métriques et de résultats sur celles-ci, consultez notre article. Les résultats avec la précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres métriques. C'est que la meilleure performance à zéro tir est obtenue par Wilbert 12 en 1, suivie de Wilbert, AlexMert, Clip et enfin VisualBert. Il est notable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par Wilbert 12 en 1, soulignant que les modèles sont capables d'identifier les objets nommés et leur présence dans les images. Cependant, aucun des autres éléments ne peut être résolu de manière fiable dans nos paramètres de \"foiling\" adverses. Nous voyons à partir des instruments de pluralité et de comptage que les modèles de vision et de langage ont du mal à distinguer les références à un seul objet par rapport à plusieurs objets ou à les compter dans une image. L'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image. Ils ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité, comme nous le voyons dans l'élément des actions. À partir de l'élément de coréférence, nous découvrons que le suivi de plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de vision et de langage. En tant que vérification de la santé et parce que c'est une expérience intéressante, nous avons également évalué deux modèles de texte uniquement, GPT-1 et GPT-2, pour évaluer si VALSE est soluble par ces modèles unimodaux en calculant la perplexité de la légende correcte et de la légende \"foiled\", pas d'image ici, et en prédisant l'entrée avec la perplexité la plus faible. Si la perplexité est plus élevée pour le \"foil\", nous prenons cela comme un indicateur que la légende \"foiled\" peut souffrir de biais de plausibilité ou d'autres biais linguistiques. Et il est intéressant de voir que dans certains cas, les modèles GPT de texte uniquement ont capturé la plausibilité du monde mieux que les modèles de vision et de langage. Donc, pour résumer, VALSE est un benchmark qui utilise la lentille des constructions linguistiques pour aider la communauté à améliorer les modèles de vision et de langage en testant rigoureusement leurs capacités d'ancrage visuel. Nos expériences montrent que les modèles de vision et de langage identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais ont du mal à ancrer leur interdépendance et leurs relations dans les scènes visuelles lorsqu'ils sont forcés de respecter les indicateurs linguistiques. Nous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers l'ancrage du langage avec les modèles de vision et de langage. Et encore plus, VALSE pourrait être utilisé comme une évaluation indirecte des ensembles de données, car les modèles pourraient être évalués avant et après l'entraînement ou le réglage fin pour voir si un ensemble de données aide les modèles à s'améliorer sur l'un des aspects testés par VALSE. Si vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Kamizawa de l'Université de Tokyo. Je vais présenter un article intitulé « R&Sum, un grand ensemble de données pour la génération automatique de notes de version par résumé de journal de validation ». Je vais expliquer dans cet ordre. Tout d'abord, je vais introduire la génération automatique de notes de version sur laquelle nous travaillons dans cette recherche. Une note de version est un document technique qui résume les modifications distribuées avec chaque version d'un produit logiciel. L'image montre les notes de version de la version 2.6.4 de la bibliothèque BuildJS. Les notes de version jouent un rôle important dans le développement open source, mais elles sont chronophages à préparer manuellement. Il serait donc très utile de pouvoir générer automatiquement des notes de version de haute qualité. Je vais me référer à deux recherches antérieures sur la génération automatique de notes de version. La première est un système appelé Arena, publié en 2014. Il adopte une approche basée sur des règles, par exemple, en utilisant l'extracteur de modifications pour extraire les différences de base, les modifications de bibliothèque et les modifications de documentation à partir des différences entre les versions, et enfin en les combinant. La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit, qui doit être lié à Jira, le système de suivi des problèmes, et ne peut être appliqué qu'aux projets utilisant Jira. En d'autres termes, il ne peut pas être utilisé pour de nombreux projets sur GitHub. La deuxième est Grief, récemment annoncée en 2020. Elle est disponible sur Internet et peut être installée via pip. Ce système dispose d'un modèle de classification de texte simple basé sur l'apprentissage et produit l'une des cinq étiquettes, telles que les fonctionnalités ou les correctifs, pour chaque message de validation d'entrée. L'image est un exemple d'utilisation qui renvoie une étiquette de correctif. Les données d'entraînement de Grief sont assez petites, environ 5 000, et seront révisées dans l'expérience décrite ci-dessous. Les performances du modèle de classification de texte ne sont pas élevées. Je présente deux recherches connexes, mais il y a des problèmes de portée limitée et de ressources de données rares. Notre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité. Pour le problème de portée limitée, nous proposons une méthode de résumé de classification de haute qualité utilisant uniquement le message de validation comme entrée. Cette méthode proposée peut être utilisée pour tous les dépôts en anglais. Pour le deuxième problème de ressources de données rares, nous avons construit notre ensemble de données R&Sum, composé d'environ 82 000 éléments de données, en collectant des données à partir de dépôts GitHub publics en utilisant l'API GitHub. Ensuite, je décris notre ensemble de données. Voici un exemple de données. Le côté gauche est le message de validation et le côté droit est la note de version. Les notes de version sont étiquetées comme des améliorations, des correctifs, etc. Nous avons défini une tâche qui prend les messages de validation comme entrée et produit les notes de version étiquetées. Cela peut être considéré comme une tâche de résumé. Nous avons prédéfini toutes les étiquettes, fonctionnalités, améliorations, correctifs, duplications, suppressions et modifications importantes. Ceux-ci ont été définis sur la base de recherches antérieures et d'autres faits. Les notes de version en bas à droite sont extraites de la note de version montrée en bas à gauche. À ce moment-là, il est nécessaire de détecter les quatre étiquettes qui ont été définies à l'avance. Mais les étiquettes ne sont pas toujours cohérentes avec chaque dépôt. Par exemple, l'étiquette d'amélioration inclut des améliorations, des améliorations, des optimisations, etc. Nous avons préparé une liste de vocabulaire ou d'étiquettes d'étude pour chacune de ces variations de notation, l'avons utilisée pour détecter leur classe de notes de version et avons corrigé le texte de la liste qui suit comme phrase de note de version pour la classe. Ensuite, il y a le message de validation. Les messages de validation ne sont pas liés à chaque version, comme le montre l'image ci-dessous. Si la version actuelle est la version 2.5.19, nous devons identifier la version de la version précédente, 2.5.18, et obtenir sa différence. C'est un peu délicat et il n'est pas suffisant de simplement obtenir une liste de versions et de regarder avant et après. Nous avons créé une règle de correspondance heuristique pour obtenir les versions précédente et suivante. Analyse de l'ensemble de données. À la fin, 7 200 dépôts et 82 000 éléments de données ont été collectés. De plus, le nombre moyen de jetons de notes de version est de 63, ce qui est assez élevé pour une tâche de résumé. De plus, le nombre de jetons uniques est assez grand, à 8 830 000. Cela est dû au grand nombre de noms de classes et de méthodes uniques trouvés dans la bibliothèque. Ensuite, je vais expliquer la méthode proposée. Le modèle de résumé extractif et abstrait par classe se compose de deux modules neuronaux, un classificateur utilisant BART ou CodeBART et un générateur utilisant BART. Tout d'abord, CAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, que nous choisissons, améliorations, correctifs, duplications plus et autre. Les messages de validation classés comme autre sont rejetés. Ensuite, CAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe. Dans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues. Par conséquent, pour entraîner le classificateur, nous attribuons deux étiquettes à chaque message de validation d'entrée en utilisant les 10 premiers caractères de chaque message de validation. Nous modélisons le résumé abstrait par classe par deux méthodes différentes. Le premier modèle, que nous appelons CAS single, se compose d'un seul réseau de séquence à séquence et génère un seul texte de note de version long en donnant une concaténation de messages de validation d'entrée. Le texte de sortie peut être divisé en segments classés en fonction de symboles de fin de classe spécifiques. La deuxième méthode, que nous appelons CAS match, se compose de quatre réseaux de séquence à séquence différents, chacun correspondant à l'une des classes de notes de version. D'accord, laissez-moi expliquer l'expérience. Cinq méthodes ont été comparées, CAS, CAS single, CAS match, Plustering et l'étude précédente Grief. En ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases. Comme il est difficile de calculer le nombre de phrases, elles sont combinées avec des espaces et traitées comme une longue phrase. Le bleu est pénalisé lorsque le système produit une phrase courte. Cette pénalité entraîne une valeur bleue plus faible dans les résultats de l'expérience décrits ci-dessous. Enfin, nous calculons également la spécificité car Rouge et Bleu ne peuvent pas être calculés si les notes de version sont vides. Une spécificité élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées vides. Voici les résultats. Comme l'ensemble de données contient des adresses e-mail, des valeurs de hachage, etc., nous avons également évalué l'ensemble de données nettoyé, qui les exclut. CAS et CAS ont obtenu des scores Rouge L plus de 10 points supérieurs aux lignes de base. En particulier, sur l'ensemble de test nettoyé, l'écart de score entre la méthode proposée et la ligne de base a bondi à plus de 20 points. Ces résultats indiquent que CAS et CAS sont significativement efficaces. CAS a obtenu un meilleur score Rouge L que CAS, suggérant que la combinaison d'un classificateur et d'un générateur est efficace et que l'entraînement du classificateur en utilisant deux étiquettes. Une couverture élevée de CAS peut être un défi probablement parce que le classificateur peut se concentrer sur la sélection de messages de validation pertinents pour chaque classe. CAS match tendait à avoir un Rouge L plus élevé que CAS single, suggérant qu'il est également efficace de développer indépendamment différents modèles de résumé abstrait pour chaque classe de notes de version. Voici une analyse des erreurs. Les méthodes CAS tendent à produire des phrases plus courtes que les phrases de référence humaines. Dans la figure à droite, la phrase de référence a trois ou quatre phrases, tandis que CAS n'en a qu'une seule. La raison de cette réticence du modèle est que dans les données d'entraînement, seules 33 % des phrases sont présentes dans l'étiquette des fonctionnalités et 40 % dans l'étiquette des améliorations. De plus, les méthodes CAS ne peuvent pas générer de notes de version précises sans informations supplémentaires. L'exemple en haut à droite est un exemple d'un message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la demande ou au problème correspondant. L'exemple ci-dessous montre que les deux messages de validation dans l'entrée sont liés et devraient être combinés en une seule phrase, mais il hésite à le faire. Enfin, une conclusion. Nous avons construit un nouvel ensemble de données pour la génération automatique de notes de version. Nous avons également formulé la tâche de saisir des messages de validation et de les résumer de manière à ce qu'elle soit applicable à tous les projets écrits en anglais. Notre expérience montre que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les lignes de base. Veuillez consulter notre ensemble de données sur GitHub. Merci."}
