{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "pt", "output": "Meu nome é Asaf Farhi e apresentarei nosso artigo, enriquecimento rápido de dados tabulares usando arquiteturas de transformadores. Cientistas analisam dados e se concentram principalmente na manipulação das características de dados existentes. Mas às vezes essas características são limitadas. A geração de características usando outra fonte de dados pode adicionar informações substanciais. Nosso objetivo de pesquisa é enriquecimento automático de dados tabulares usando fontes de texto externas. Suponha que temos um conjunto de dados tabulares e uma base de conhecimento. Precisamos de um processo automático que envolva intilink e análise de texto para extrair novas características da base de conhecimento. O nosso framework, FIST, é exatamente esse processo automático. Vamos ver um exemplo. Em um conjunto de dados inserido em FIST, o objetivo do conjunto de dados é classificar universidades em universidades de baixa e alta classificação. Como base de conhecimento, usamos Wikipedia. A primeira fase de FIST é intilink, quando cada entidade. No exemplo, o nome da universidade é vinculado a uma entidade dentro da base de conhecimento. E o texto da entidade é extraído e adicionado ao conjunto de dados. Agora, precisamos gerar ou extrair características do texto do retractor. Precisamos de uma fase de extração de características, que inclui análise de texto. Esta é a principal vantagem deste artigo, e eu mergulhará em ela nas próximas slides. Após a fase de extração de características, há uma fase de geração de características, onde usamos as características extraídas para gerar um pequeno número de novas características. FIST gera características no número de classes do conjunto de dados original. No exemplo, o conjunto de dados original tem duas classes, então FIST gera duas novas características. Mas se o conjunto de dados tiver cinco classes, FIST gera cinco novas características. Cada característica representa a probabilidade de cada classe. Para analisar o texto, usamos a abordagem atual de análise de linguagem, que são modelos de linguagem baseados em transformadores, como BERT, GPT, XL, etc. Mas é improvável que possamos treinar um modelo de linguagem usando os conjuntos de dados de entrada. Então, um enfoque ingenuo será uma tarefa de fine-tuning. Na fase de extração de características, podemos baixar modelos de linguagem pré-treinados e fine-tune o modelo de linguagem sobre o conjunto de dados de alvo. No exemplo, para fine-tune o modelo de linguagem para classificar texto em duas classes, abstrato em classes baixas ou altas, recebemos a saída do modelo de linguagem, que é a probabilidade de cada classe, e usamos como novas características. O problema com esse enfoque é que o conjunto de dados pode ter poucas características de texto distintas. Em nossos experimentos, quase metade dos conjuntos de dados contêm menos de 400 amostras, e o conjunto de dados mais pequeno contém 35 amostras. Então, fine-tune um modelo de linguagem sobre este conjunto de dados será ineficaz. Mas podemos usar conhecimento pré-analisado sobre conjuntos de dados pré-analisados. Como FIST é aplicado sobre vários conjuntos de dados, podemos usar os dados pré-analisados para obter informações sobre os dados pré-analisados. E usamos essa informação quando analisamos o conjunto de dados alvo. O que sugerimos é adicionar outra fase de fine-tuning preliminar, onde fine-tune o modelo de linguagem sobre N-1 conjuntos de dados. E depois, executamos outra fase de fine-tuning, que é a fine-tuning da tarefa alvo, quando fine-tune o modelo de linguagem sobre o conjunto de dados alvo. O estado do arte em multi-tarefa, multi-tarefa, fine-tuning é M-T-D-N. Em M-T-D-N, M-T-D-N mantém o número de tarefas nas cabeças de treinamento. Então, no exemplo, há quatro tarefas no conjunto de treinamento, então M-T-D-N mantém quatro cabeças. E seleciona um lote aleatório do conjunto de treinamento. E se o lote aleatório pertence, por exemplo, a tarefa de classificação de frases, executa o caminho para frente e para trás através da primeira cabeça. E se o lote aleatório pertence, por exemplo, a tarefa de classificação de pares, executa o caminho para frente e para trás através da última cabeça. Em nosso cenário, conjuntos de dados tabulares variam no número de classes. Então, existem muitas tarefas. M-T-D-N mantém o número de classes, cabeças, camadas de saída e, adicionalmente, M-T-D-N precisa inicializar uma nova cabeça para um novo conjunto de dados com uma nova tarefa. Nosso abordagem, chamada fine-tuning de reformulação de tarefas, é que, em nosso método, reformulamos cada conjunto de dados em uma tarefa de classificação de sentença, que é uma tarefa de duas classes. Então, treinamos o modelo de linguagem para classificar a frase, a classe e a classe em verdadeiro ou falso. Ou, em outras palavras, treinamos o modelo de linguagem para classificar a frase, a classe e a classe em verdadeiro ou falso."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "pt", "output": "Olá a todos. Hoje vou apresentar nosso trabalho de pesquisa, \"Aprender a raciocinar: resolução de problemas matemáticos como extração de complexas razões\". Sou Alan do AI Lab, e este é um trabalho conjunto com Cheryll da Universidade de Texas em Austin e Wei Luo do SUTD. Primeiro, gostaria de falar sobre nossa motivação para raciocinar. Aqui mostramos exemplos em que raciocínio múltiplo é útil. Este gráfico é tomado do artigo de Palmer, onde realizamos prompt para resolver o problema matemático em um cenário de aprendizado de curto prazo. Aqui, no lado neto, podemos ver se damos alguns exemplos com apenas perguntas e respostas, não podemos obter as respostas corretas. Mas se damos mais raciocínio, a modelo é capaz de prever a descrição de raciocínio e também fazer uma previsão correta aqui. Então, é bom ter raciocínio interpretável como saída. E também pensamos que o problema matemático é uma aplicação direta para avaliar essas capacidades de raciocínio. Então, neste nosso problema de configuração, dado as perguntas, precisamos resolver essas perguntas e obter as respostas numéricas. Na nossa configuração de dados, também nos é dado a expressão matemática, que leva a esta resposta particular. Então, certas suposições também se aplicam, como no trabalho anterior. Nós assumimos que a precisão das quantidades é conhecida, e só consideramos operadores básicos, como adição, subtração, multiplicação, divisão e exponencial. Além disso, operadores complicados podem ser realmente decompostos em esses operadores básicos. Então, o trabalho anterior em resolução de problemas matemáticos pode ser categorizado em modelo sequencial e modelo sequencial. Então, o modelo tradicional sequencial sequencial converte a expressão para uma sequência específica para geração. E é bastante fácil de implementar, e pode generalizar para muitos problemas complicados. Mas a desvantagem é que o desempenho geralmente não é melhor do que o modelo de estrutura. E é falta de interpretabilidade para a previsão. Mas essa direção ainda é bastante popular devido ao modelo de transformador. Então, em modelos baseados em árvores, estruturamos essas expressões em uma forma de árvore e seguimos uma ordem de geração de árvores. Então, aqui, continuamos gerando os operadores até chegarmos aos folhas, que são os quantios. Então, a boa coisa é que, na verdade, dá-nos essa estrutura de árvore binária. E é bastante contencioso, porque geramos primeiro o operador, e depois, no final, geramos os quantios. E a segunda coisa é que também contém algumas computações repetitivas. Então, aqui, se olharmos a esta expressão, 8 vezes 3 mais 3, é gerado duas vezes. Mas, na verdade, devemos reutilizar os resultados. Então, em nosso método proposto, queremos resolver esses problemas em um passo a passo e interpretável. Então, por exemplo, aqui, no segundo passo, podemos obter essa divisão, que é 27. E também podemos encontrar o conteúdo relevante das perguntas originais. E no terceiro passo, realmente obtemos a razão. E depois desses três passos, podemos realmente obter os resultados do quarto passo. E finalmente, podemos obter os dividos. Então, aqui, geramos a expressão inteira, em vez de gerar operadores ou quantios individuais. Então, isso torna o processo mais preciso. Então, em nosso sistema de raciocínio dedutivo, primeiro começamos com uma série de quantios apresentados nas perguntas, e também incluímos alguns constantes como nosso estado inicial. Então, a expressão é representada por Eijop, onde realizamos o operador de QI para QJ. E essa expressão é realmente direcionada. Então, também temos subtração com verso para representar a direção oposta. Então, em um sistema dedutivo formal, em um passo T, aplicamos o operador entre o par QI e QJ. E então, obtemos essa nova expressão. Adicionamos a essa expressão ao próximo estado para se tornar um novo quantio. Então, essas ilhas realmente adicionam expressões às expressões atuais. Então, em nossas implementações, primeiro usamos um modelo de linguagem pré-treinado, que pode ser birds ou roberta. E depois codificamos a frase, e depois obtemos essas representações de quantios. Então, uma vez que obtemos as representações de quantios, podemos começar a fazer inferências. Aqui, mostramos um exemplo de Q1, para obter a representação para Q1 dividido por Q2 e vezes Q3. Primeiro, obtemos a representação de par, que é basicamente a concatenação entre Q1 e Q2. E então, aplicamos uma rede de pré-fooder, que é parametrizada pelo operador. E então, obtemos a representação de expressão, Q1 dividido por Q2. Mas, na prática, no estágio de inferência, podemos obter uma expressão incorreta também. Então, aqui, todos os possíveis expressões são iguais a três vezes o número de operadores. Então, a coisa boa aqui é que podemos adicionar restrições para controlar essa busca. Por exemplo, se essa expressão não for permitida, simplesmente removemos essa expressão na nossa busca. Então, no segundo passo, fazemos o mesmo, mas a única diferença é que o quantio vem do expressões calculadas anteriormente. Então, finalmente, obtemos essa expressão final, Q3 vezes Q4. E também podemos ver o número de todas as expressões possíveis diferentes do passo anterior. Então, essas diferenças tornam difícil aplicar a estratégia de busca de vácuo, porque a distribuição de probabilidade entre esses dois passos é desequilibrada. Então, a abordagem de treinamento é semelhante à de treinamento de modelo sequencial sequencial, onde otimizamos a perda em cada passo. E aqui, também usamos tau para representar quando devemos terminar esse processo de geração. E aqui, a espaço é diferente, porque o espaço é diferente em cada passo, enquanto no modelo sequencial sequencial, é o número de vocabulário. E também permite que possamos impor certas restrições de conhecimento anterior. Então, realizamos experimentos em conjuntos de dados de problemas de matemática comuns, MATH, WPS, MATH23K, MATHQA e SWAMP. E aqui, mostramos brevemente os resultados comparados com os melhores métodos anteriores. Então, nosso melhor desempenho é Roberta Deductive Reasoner. E, de fato, não usamos beam search em contraste. Os melhores métodos são frequentemente modelos baseados em árvores. Então, nosso raciocinador é capaz de superar significativamente esses modelos baseados em árvores. Mas, podemos ver que o número absoluto no MATHQA ou SWAMP não é realmente alto. Então, investigamos ainda mais os resultados em SWAMP. E este conjunto de dados é desafiador porque o autor tenta adicionar informações manuais, como adicionar informações relevantes e quantios adicionais. Então, em nossas previsões, encontramos alguns valores intermediários negativos. Por exemplo, em essas perguntas, estamos perguntando quantos maçãs Jake tem, mas temos algumas informações adicionais, como 17 uvas e Steven tem 8 uvas, que é totalmente ineficaz. Então, nosso modelo faz previsões como essas, que é produzir valores negativos. E observamos que essas duas expressões na verdade têm pontuações semelhantes. Então, podemos realmente limitar essa busca reduzindo esses resultados negativos. Então, essas restrições realmente melhoram bastante para alguns modelos. Por exemplo, para birds, melhoramos 7 pontos. E então, para o modelo baseado em roberta, realmente melhoramos 2 pontos. Então, o modelo de linguagem melhor tem uma capacidade de compreensão de linguagem melhor, então o número aqui é mais alto para roberta e mais baixo para birds. E também tentamos analisar a dificuldade por trás de todos esses dados. Então, assumimos que o número de quantios não usados pode ser considerado como informação ineficaz. Então, aqui, temos a porcentagem de amostras com quantios não usados, e o SWAMP tem a maior proporção. E aqui, também mostramos o desempenho geral. Para aquelas amostras sem quantios não usados, o desempenho é realmente mais alto do que o desempenho geral. Mas, com aquelas amostras com quantios não usados, o desempenho é muito pior do que o desempenho geral. Para MAWPS, não temos muito caso de desuso, então simplesmente ignoro essa parte. Então, para concluir nosso trabalho, queremos mostrar a interpretabilidade através de uma pergunta e resposta. Então, nosso modelo na verdade faz uma previsão errada no primeiro passo. Então, podemos correlacionar essa expressão com a frase aqui. Então, pensamos que essa frase pode ser enganosa e previsões incorretas. Então, misturamos e 35 faz o modelo pensar que deveria ser operadores de adição. Então, tentamos tornar a frase mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, fazemos uma representação mais precisa, como a frase, o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de peido são 35 a menos do que as árvores de maçã. Então, a frase pode ser mais precisa, como o número de árvores de"}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "pt", "output": "Meu nome é Antoine e eu sou da Universidade de Maastricht. Vou apresentar o meu trabalho em conjunto com o de Gerry, que é sobre um novo conjunto de dados para a recuperação de artigos legais. As questões legais são uma parte integral da vida de muitas pessoas, mas a maioria das pessoas não tem conhecimento sobre seus direitos e processos legais fundamentais. Como resultado, muitas pessoas vulneráveis que não podem pagar a assistência de um especialista jurídico são deixadas sem proteção ou, pior, exploradas. Nosso trabalho visa fechar a lacuna entre as pessoas e a lei, desenvolvendo um sistema de recuperação eficaz para artigos legais. Um tal sistema poderia fornecer um serviço jurídico profissional gratuito para humanos não qualificados. Antes de mergulharmos no principal contribuição do trabalho, vamos descrever o problema da recuperação de artigos legais. Dada uma simples pergunta sobre uma questão legal, como \"o que eu corrompo se violo a confidencialidade profissional?\", um modelo é necessário para recuperar todos os artigos legais relevantes de um corpo legal. Esta tarefa de recuperação vem com seu próprio conjunto de desafios. Primeiro, trata-se de dois tipos de linguagem: linguagem comum e natural para as perguntas e linguagem legal complexa para os artigos. Esta diferença na distribuição de linguagem torna mais difícil para um sistema recuperar candidatos, pois requer inerentemente um sistema de interpretação que possa traduzir uma pergunta natural para uma pergunta legal que corresponda à terminologia dos artigos. Além disso, a lei estatutária não é uma pilha de artigos legais independentes que podem ser tratados como uma fonte de informação completa, como notícias ou receitas, por exemplo. Em vez disso, é uma coleção estruturada de disposições legais que têm um significado só quando consideradas no contexto geral, que é junto com as informações suplementares dos artigos vizinhos, os campos e subcampos a que pertencem e seu lugar na estrutura da lei. Por fim, os artigos legais são longos, que podem ser até 6.000 palavras. As recentes avanços em NLP despertaram grande interesse em muitas tarefas legais, como previsão de julgamentos legais ou revisão de contratos automatizada. Mas a recuperação de artigos legais permanece principalmente inexplorada devido à falta de conjuntos de dados grandes e de alta qualidade. E este trabalho, apresentamos um novo conjunto de dados, de nativo francês, centrado nos cidadãos, para estudar se um modelo pode aproximar a eficiência e a confiabilidade de um especialista jurídico para a tarefa de recuperação de artigos legais. Nosso conjunto de dados de recuperação de artigos legais belgas consiste em mais de 1.100 perguntas feitas por cidadãos belgas. Essas perguntas cobrem uma ampla gama de tópicos, desde família, moradia, dinheiro, trabalho e segurança social. Cada uma delas foi rotulada por juristas experientes com referências a artigos legais relevantes de um corpo de mais de 22.633 artigos de códigos legais. Vamos falar sobre como coletamos este conjunto de dados. Primeiro, começamos compilando um grande corpus de artigos legais. Consideramos 32 códigos legais públicos disponíveis e extraiu todos os seus artigos e as seções correspondentes. Então, coletamos perguntas com referências a estatutos relevantes. Para fazer isso, pararmos com uma firma de direito que recebe cada ano cerca de 4.000 e-mails de cidadãos belgas que pedem conselhos sobre questões legais pessoais. Foi sorte de ter acesso aos seus sites, onde seu time de juristas experientes aborda as questões legais mais comuns dos cidadãos belgas. Coletamos milhares de perguntas, anotadas com categorias, subcategorias e referências legais aos estatutos relevantes. Finalmente, passamos as referências legais e filtramos as perguntas cujo referência não é a um dos códigos legais considerados. As referências restantes foram correspondidas e convertidas para os IDs de artigos do nosso grande corpus. Finalmente, terminamos com 1.108 perguntas, cada uma cuidadosamente rotuladas com os IDs dos artigos relevantes do nosso grande corpo de 22.633 artigos legais. Além disso, cada pergunta vem com uma categoria principal e uma concatenação de subcategorias, e cada artigo vem com uma concatenação de suas seções de cabeçais no código. Esta informação adicional não é usada no trabalho atual, mas pode ser de interesse para pesquisas futuras sobre recuperação de informações legais ou classificação de textos legais. Vamos olhar algumas características do nosso conjunto de dados. As perguntas são entre 5 e 44 palavras longas, com uma média de 40 palavras. Os artigos são muito mais longos, com uma média de 77 palavras, com 142 deles excedendo 1.000 palavras, o mais longo sendo até 5.790 palavras. Como mencionado anteriormente, as perguntas cobrem uma ampla gama de tópicos, com cerca de 85% delas sendo sobre família, moradia, dinheiro ou justiça, enquanto o restante 15% se concentra em segurança social, estrangeiros ou trabalho. Os artigos também são muito diversos, pois vêm de 32 diferentes códigos belgas que cobrem uma grande variedade de tópicos legais. Aqui está o número total de artigos coletados de cada um desses códigos belgas. Do 22.633 artigos, apenas 1.612 são referidos como relevantes para pelo menos uma pergunta no conjunto de dados. E cerca de 80% desses artigos citados vêm do código civil, código judicial, código de investigação criminal ou código penal. Enquanto isso, 18 dos 32 códigos têm menos de cinco artigos mencionados como relevantes para pelo menos uma pergunta. O número médio de citações para esses artigos citados é 2, e menos de 25% deles são citados mais de cinco vezes. Usando nossos conjuntos de dados, avaliamos vários métodos de recuperação, incluindo modelos lexicais e arquiteturas densas. Dada uma pergunta e um artigo, um modelo lexical atribui uma pontuação ao par de pergunta e artigo calculando a soma dos termos da pergunta em um artigo. Experimentamos com os métodos de classificação padrão tfidf e bm25. O principal problema com esses métodos é que podem recuperar apenas artigos que contêm palavras-chave presentes na consulta. Para superar essa limitação, experimentamos com uma arquitetura baseada em neurais que pode capturar a relação semântica entre as perguntas e os artigos. Usamos um modelo de b-encoder que mapeia perguntas e artigos em representações densas, e calculamos uma pontuação de relevância entre um par de pergunta e artigo calculando a semelhança de suas representações densas. Esses embeddings geralmente resultam de uma operação de pooling no output de um modelo de embebedimento de palavras. Primeiro, estudamos a eficácia de b-encoders sijs em um conjunto de avaliação zero-shot, o que significa que modelos de embebedimento de palavras pré-treinados são aplicados sem qualquer ajuste adicional. Experimentamos com b-encoders word2vec e fasttext, e b-encoders dependentes do contexto, nomeadamente roberta e, mais especificamente, camembert, que é um modelo de roberta francês. Além disso, treinamos nossos próprios b-encoders baseados em camembert no nosso conjunto de dados. Observe que, para o treinamento, experimentamos com os dois sabores do modelo de b-encoder. Simi, que usa um modelo de embebedimento de palavras único que mapeia a pergunta e o artigo juntos em um espaço de representação densa compartilhado. E dois-tower, que usa dois modelos de embebedimento de palavras independentes que codificam a pergunta e o artigo separadamente em espaços de representação diferentes. Experimentamos com mean, max e cls para calcular semelhanças, e dot product e cosin para calcular semelhanças. Aqui estão os resultados de nosso conjunto de teste, com os métodos de recuperação acima, b-encoders sijs avaliados em um conjunto de avaliação zero-shot no meio. O b-encoders refinados superam todos os outros métodos de baseline, o modelo dois-tower melhora em recall em 100, mas se as métricas de outros indicadores. Embora o b-encoders treinados superem significativamente os métodos de baseline, sua performance indica que ainda é uma forte baseline para recuperação de domínio específica. Além disso, observamos que o b-encoder word2vec baseado em camembert supera significativamente os modelos fasttext e b-encoders baseados em roberta, sugerindo que talvez embeddings de nível de palavra sejam mais apropriados para a tarefa de recuperação de informações. Embora promissores, esses resultados sugerem amplas oportunidades para melhoria, em comparação com um especialista jurídico que eventualmente pode recuperar todos os artigos relevantes para qualquer pergunta e, portanto, obter pontuações perfeitas. Vamos concluir discutindo duas limitações de nossos conjuntos de dados. Primeiro, o corpus de artigos é limitado a aqueles coletados dos 32 códigos considerados, que não cobrem toda a lei belga, como artigos de decretos, diretrizes e ordenanças estão ausentes. Durante a construção do conjunto de dados, todas as referências a esses artigos não coletados são ignoradas, o que faz com que algumas perguntas terminem com apenas uma fração do número inicial de artigos relevantes. Isso implica que a resposta contida nos artigos restantes pode ser incompleta, embora ainda seja completamente apropriada. Segundo, devemos notar que não todas as perguntas legais podem ser respondidas apenas com estatutos. Por exemplo, a pergunta \"Posso evicir meu inquilino se ele faz muito barulho?\" pode não ter uma resposta detalhada dentro da lei estatutária que quantifique um limite específico de ruído que permite a expulsão. Em vez disso, o contrato de locação provavelmente deve se basear mais em jurisprudência e encontrar precedentes semelhantes à situação atual. Portanto, algumas perguntas são mais adequadas do que outras para a tarefa de recuperação de artigos legais, e o domínio das menos adequadas ainda deve ser determinado. Esperamos que nosso trabalho inspire interesse em desenvolver modelos de recuperação de artigos legais práticos e confiáveis que possam melhorar o acesso à justiça para todos. Você pode verificar o nosso artigo e o código do conjunto de dados no seguinte link. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "pt", "output": "Olá. Estamos felizes em apresentar nosso trabalho sobre Valsa, um benchmark independente de tarefa destinado a testar modelos de visão e linguagem com fenômenos linguísticos específicos. Por que nos esforçamos para montar este benchmark? Durante os últimos anos, vimos uma explosão de modelos de visão e linguagem baseados em transformadores, treinados em grandes quantidades de pares de imagens e textos. Cada um desses modelos empurra o estado do arte em tarefas de visão e linguagem, como resposta visual a perguntas visuais, raciocínio visual, recuperação de imagens, fundamentação de frases, etc. Então, recebemos a mensagem: as precisões nesses benchmarks específicos estão aumentando constantemente. Mas sabemos o que os modelos realmente aprenderam? O que um modelo de visão e linguagem entende quando atribui uma pontuação alta a uma imagem e uma frase para corresponder, e uma pontuação baixa a outra? Focam no assunto certo, ou se concentram em vieses, como mostrado pelo trabalho anterior? Para iluminar essa dimensão, proponemos uma direção mais agnóstica e introduzimos Valsa, que testa a sensibilidade de modelos de visão e linguagem a fenômenos linguísticos que afetam tanto as modalidades linguísticas quanto visuais. Nosso objetivo é capturar esses fenômenos. Testamos se os modelos de visão e linguagem capturam esses fenômenos. Por meio de foiling, um método anteriormente aplicado apenas para modelos de visão e linguagem, mas também para modelos de linguagem, e sobre contagem por nós em trabalhos anteriores. Foiling significa que tomamos a legenda de uma imagem e producemos uma legenda de desvio alterando a legenda de tal forma que não descreve mais a imagem. E fazemos essas alterações de frase focando em seis peças específicas, como existência, pluralidade, contagem, relações espaciais, ações e referência de entidades. Cada peça pode consistir em uma ou mais instrumentos, caso encontremos mais maneiras interessantes de criar instâncias de desvio. Por exemplo, no caso das ações, temos dois instrumentos, um em que o verbo de ação é alterado com uma ação diferente, e outro em que os agentes são trocados. Contagem e referência também são peças que têm mais de um instrumento. E criamos essas legendas de desvio garantindo que falham em descrever a imagem, que são frases gramaticais e válidas. Isso não é fácil de fazer, porque uma legenda de desvio pode ser menos provável do que a legenda original. E se os modelos de visão e linguagem capturam isso, eles podem pegar. Para obter legendas de desvio válidas, fazemos o uso de modelos de linguagem fortes para propor legendas de desvio. Em segundo lugar, usamos inferência de linguagem natural, ou NLP, para filtrar as legendas de desvio que ainda poderiam descrever a imagem, pois, ao construir as legendas de desvio, precisamos garantir que falham em descrever a imagem. Se um modelo de NLP prever a legenda de desvio contradiz ou é neutra em relação à legenda, tomamos isso como um indicador de uma legenda de desvio válida. Se um modelo de NLP prever a legenda de desvio como derivada da legenda, não pode ser uma boa legenda de desvio, pois, por transitividade, dará uma descrição verdadeira da imagem. E este procedimento não é perfeito, é apenas um indicador de legendas de desvio válidas. Portanto, como uma terceira medida para gerar legendas de desvio válidas, empregamos anotadores humanos para validar os dados usados em Valsa. Após filtrar e avaliar humanos, temos tantas instâncias de teste descritas nesta tabela. Observe que Valsa não fornece nenhum treinamento, mas apenas dados de teste, pois é um benchmark de teste apenas. Ele aproveita as capacidades dos modelos de visão e linguagem após a pré-treinar. A fine-tuning só permitiria aos modelos explorar artefatos ou vieses estatísticos nos dados. E sabemos que esses modelos gostam de enganar e tomar atalhos. E como dizemos, estamos interessados em avaliar quais capacidades os modelos de visão e linguagem têm após a pré-treinar. Experimentamos com cinco modelos de visão e linguagem em Valsa, nomeadamente, com clip, elxmert, wilbert, wilbert12in1 e visualbert. Dois de nossos métricas de avaliação importantes são: a precisão dos modelos em classificar pares de imagem e frase em legendas e legendas de desvio. Talvez mais relevante para este vídeo, mostraremos nossos casos ou métricas mais permissivas, que medece se a pontuação de alinhamento de imagem e frase é maior para o par de texto correto do que para o seu par de legenda de desvio. Os resultados com métricas de pairwise accuracy são mostrados aqui, e são consistentes com os resultados que obtivemos com as outras métricas. É que o melhor desempenho zero shot é alcançado por Wilbert12in1, seguido por Wilbert, elxmert, clip e, finalmente, visualbert. É notável como as peças centrais em torno de objetos individuais, como existência e frases de substantivos, são quase resolvidas por Wilbert12in1, destacando que os modelos são capazes de identificar objetos nomeados e sua presença em imagens. No entanto, nenhuma das peças restantes pode ser resolvida em nossos ambientes de teste adversais. Vemos de as peças de referência que os modelos de visão e linguagem têm dificuldades em distinguir referências a objetos singulares versus múltiplos, ou contá-los em uma imagem. A peça de relação mostra que eles têm dificuldades em classificar corretamente uma relação espacial entre objetos nomeados em uma imagem. Eles também têm dificuldades em distinguir ações e identificar os participantes, mesmo que apoiados por vieses de plausibilidade, como vemos no caso das ações. Do a peça de referência, descobrimos que rastrear múltiplas referências ao mesmo objeto em uma imagem usando pronoms é também difícil para modelos de visão e linguagem. Como um saneamento, e porque é um experimento interessante, também avaliamos dois modelos de linguagem apenas para avaliar se Valsa é resolvido por esses modelos unimodais, computando a perplexidade da legenda correta e a legenda de desvio, e preditando a entrada com a perplexidade mais baixa. Se a perplexidade for maior para a legenda de desvio, tomamos isso como um indicador de que a legenda de desvio pode sofrer de vieses de plausibilidade ou outros vieses linguísticos. E é interessante ver que, em alguns casos, os modelos de linguagem apenas capturam a plausibilidade do mundo melhor do que os modelos de visão e linguagem. Em resumo, Valsa é um benchmark que usa a lente de construções linguísticas para ajudar a comunidade a melhorar modelos de visão e linguagem ao testar suas capacidades de fundamentação visual. Nossos experimentos mostram que modelos de visão e linguagem identificam bem objetos nomeados e sua presença em imagens, como mostrado pela peça de existência, mas têm dificuldades em fundamentar sua interdependência e relações em cenas visuais quando forçados a respeitar indicadores linguísticos. Nós realmente gostaríamos de encorajar a comunidade a usar Valsa para medir o progresso em fundamentação de linguagem com modelos de visão e linguagem. E Valsa também pode ser usado como uma avaliação indireta de conjuntos de dados, pois os modelos podem ser avaliados antes e após a treinamento ou fine-tuning para ver se um conjunto de dados ajuda os modelos a melhorar em qualquer uma das áreas testadas por Valsa. Se você estiver interessado, verifique os dados de Valsa no github e, se tiver alguma dúvida, não hesite em entrar em contato com nós."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Kamisaka, da Universidade de Tóquio. Vou apresentar um artigo intitulado \"RNSUM, um conjunto de dados de grande escala para a geração automática de release notes via commit log summarization\". Eu vou explicar em este orden. Primeiro, vou apresentar a geração automática de release note que estamos trabalhando em este trabalho. Uma release note é um documento técnico que resume as mudanças distribuídas com cada lançamento de um software. A imagem mostra a release note para a versão 2.6.4 do Javadoc Library. Release notes desempenham um papel importante no desenvolvimento aberto, mas são demorados para preparar manualmente. Portanto, seria muito útil ser capaz de gerar release notes de alta qualidade automaticamente. Eu vou referenciar dois estudos anteriores sobre geração automática de release notes. O primeiro é um sistema chamado Alena, lançado em 2014. Ele toma um enfoque baseado em regras, por exemplo, usando o extractor de mudanças para extrair diferenças de código, mudanças de bibliotecas e mudanças de documentos entre releases, e finalmente combiná-las. A característica mais notável do sistema é o extractor de problemas no canto superior direito, que deve ser vinculado ao sistema de problemas do Jira e pode ser aplicado apenas a projetos que usam Jira. Em outras palavras, não pode ser usado para muitos projetos em Github. O segundo é o Grriff, recentemente anunciado em 2020. Ele está disponível na internet e pode ser instalado via pip. Este sistema tem um modelo de classificação de texto simples e armazena uma das quatro probabilidades, como features ou patches de bug, para cada mensagem de commit. A imagem é um exemplo de uso que retorna uma release note de correção ou patch de bug. O treinamento do Grriff é bastante pequeno, cerca de 5.000, e será mostrado nos experimentos descritos abaixo. O desempenho do modelo de classificação de texto não é alto. Eu apresentarei dois estudos relacionados, mas têm problemas de aplicabilidade e escassez de recursos de dados. Nosso artigo soluciona esses dois problemas e gera release notes de alta qualidade automaticamente. Para o problema de aplicabilidade limitada, propomos um método de resumo de classifica de alta qualidade usando apenas mensagens de commit como entrada. Este método pode ser usado para todos os repositórios em inglês. Para o segundo problema de escassez de recursos de dados, construímos um conjunto de dados RNSUM composto por cerca de 82.000 peças de dados, coletando dados de repositórios públicos usando a API do Github. Em seguida, descrevo nosso conjunto de dados. Aqui está um exemplo de dados, o lado esquerdo é a mensagem de commit e o lado direito é a release note. As release notes são rotuladas como features, patches de bug, etc. Nós configuramos uma tarefa que leva as mensagens de commit como entrada e retorna as release notes rotuladas. Esta pode ser considerada como uma tarefa de resumo. Nós estabelecemos quatro rotuladores, features, patches de bug, etc. Baseados em pesquisas anteriores e outros fatores. As release notes no canto inferior direito e as release notes extraídas no canto inferior esquerdo. Agora, é necessário detectar os quatro rotuladores que foram definidos de antemão. Mas os rotuladores não são sempre consistentes com cada repositório. Por exemplo, o rotulador de melhorias inclui melhorias, melhorias, otimizações, etc. Preparamos uma lista de vocabulário de rotuladores para cada uma dessas variações notacional. Use-a para detectar a release note class e corrigir o texto da release note. A frase que segue é a release note. As mensagens de commit não estão vinculadas a cada release. Como mostrado na imagem abaixo, se a versão atual for 2.5.19, precisamos identificar a versão anterior 2.5.18 e obter a diferença. Isso é um pouco tedioso e não é suficiente apenas para obter uma lista de releases e olhar antes e depois. Criamos uma regra heurística para obter as versões anteriores e seguintes. A mensagem de commit. As mensagens de commit não estão vinculadas a cada release. Se a versão atual for 2.5.19, precisamos identificar a versão anterior 2.5.18 e obter a diferença. Isso é um pouco tedioso e não é suficiente apenas para obter uma lista de releases e olhar antes e depois."}
