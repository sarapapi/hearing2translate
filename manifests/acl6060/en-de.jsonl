{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 0, "src_audio": "/acl6060/audio/en/0.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo zusammen. Heute werde ich unsere Forschungsarbeit Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction vorstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 1, "src_audio": "/acl6060/audio/en/1.wav", "src_ref": "I'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich bin Allan vom ByteDance AI Lab, und dies ist eine gemeinsame Arbeit mit Jierui Li von der University of Texas in Austin und Wei Lu von SUTD.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 2, "src_audio": "/acl6060/audio/en/2.wav", "src_ref": "First, I'd like to talk about our motivation for reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst möchte ich über unsere Motivation für das Argumentieren sprechen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 3, "src_audio": "/acl6060/audio/en/3.wav", "src_ref": "So here we show an examples where multi-step reasoning is helpful.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deshalb zeigen wir hier ein Beispiel, bei dem mehrstufiges Argumentieren hilfreich ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 4, "src_audio": "/acl6060/audio/en/4.wav", "src_ref": "So this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Abbildung stammt aus dem PaLM-Paper, in dem sie Prompting durchführen, um das Netzwerkproblem in einem Few-Shot-Lernszenario zu lösen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 5, "src_audio": "/acl6060/audio/en/5.wav", "src_ref": "So on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auf der linken Seite können wir also sehen, dass wenn einige Beispiele nur aus Frage und Antworten bestehen, dann bekommen wir nicht die richtigen Antworten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 6, "src_audio": "/acl6060/audio/en/6.wav", "src_ref": "But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir jedoch eine weitere Argumentationsbeschreibung hinzufügen, ist das Modell in der Lage, die Argumentationsbeschreibung vorherzusagen und hier auch eine korrekte Vorhersage zu treffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 7, "src_audio": "/acl6060/audio/en/7.wav", "src_ref": "So it is good to have interpretable multi-step reasoning as output.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist also gut, eine interpretierbare mehrstufige Argumentation als Ergebnis zu haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 8, "src_audio": "/acl6060/audio/en/8.wav", "src_ref": "And we also think math word problem is a straightforward application to evaluate such reasoning abilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir denken auch, dass mathematische Wortprobleme eine einfache Anwendung sind, um solche Argumentationsfähigkeiten zu bewerten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 9, "src_audio": "/acl6060/audio/en/9.wav", "src_ref": "So, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem Problem-Setup müssen wir also angesichts der Fragen diese Frage lösen und die numerischen Antworten erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 10, "src_audio": "/acl6060/audio/en/10.wav", "src_ref": "So in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unseren Datensätzen finden wir auch den mathematischen Ausdruck, der uns zu dieser bestimmten Antwort führt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 11, "src_audio": "/acl6060/audio/en/11.wav", "src_ref": "So, certain assumptions ah also apply as in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es gelten also auch hier bestimmte Annahmen wie in früheren Arbeiten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 12, "src_audio": "/acl6060/audio/en/12.wav", "src_ref": "We assume the precision of quantities are known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir gehen davon aus, dass die Genauigkeit der Mengen bekannt ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 13, "src_audio": "/acl6060/audio/en/13.wav", "src_ref": "And we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und wir betrachten nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponentialrechnung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 14, "src_audio": "/acl6060/audio/en/14.wav", "src_ref": "Furthermore, complicated operators can be actually decomposed into these basic operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus können komplizierte Operatoren in diese Grundoperatoren zerlegt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 15, "src_audio": "/acl6060/audio/en/15.wav", "src_ref": "So, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die früheren Arbeiten zum Lösen von mathematischen Wortproblemen lassen sich also in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modell einteilen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 16, "src_audio": "/acl6060/audio/en/16.wav", "src_ref": "So, traditional sequence to sequence model convert the expression to a specific sequence for generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das traditionelle Sequenz-zu-Sequenz-Modell wandelt also den Ausdruck in eine spezifische Sequenz für die Generierung um.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 17, "src_audio": "/acl6060/audio/en/17.wav", "src_ref": "And it is pretty easy to implement and it can generalize to many different complicated problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist ziemlich einfach zu implementieren und kann für viele verschiedene komplizierte Probleme verallgemeinert werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 18, "src_audio": "/acl6060/audio/en/18.wav", "src_ref": "But the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Nachteile sind jedoch, dass die Leistung im Allgemeinen nicht besser ist als die des strukturierten Modells und dass es an Interpretierbarkeit für Vorhersagen mangelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 19, "src_audio": "/acl6060/audio/en/19.wav", "src_ref": "But actually this direction is still quite popular because of um the transformer model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber eigentlich ist diese Richtung immer noch recht beliebt wegen des Transformermodells.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 20, "src_audio": "/acl6060/audio/en/20.wav", "src_ref": "So, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In baumbasierten Modellen strukturieren wir diese Ausdrücke in Baumform und folgen einem geordneten Traversal in Baumgenerationen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 21, "src_audio": "/acl6060/audio/en/21.wav", "src_ref": "So here we keep generating the operators until we reach the leaves, which are the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier fahren wir also fort, die Operatoren zu generieren, bis wir die Blätter erreichen, die die Mengen darstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 22, "src_audio": "/acl6060/audio/en/22.wav", "src_ref": "So here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Gute daran ist, dass wir dadurch diese binäre Baumstruktur erhalten. Sie ist eigentlich ziemlich kontraintuitiv, weil wir zuerst den Operator und dann am Ende die Mengen generieren müssen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 23, "src_audio": "/acl6060/audio/en/23.wav", "src_ref": "And the second thing is that it also contains some repetitive computations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zudem enthält sie auch einige sich wiederholende Berechnungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 24, "src_audio": "/acl6060/audio/en/24.wav", "src_ref": "So here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir uns also diesen Ausdruck ansehen, wird 8 x 3 + 3 eigentlich zweimal generiert, aber eigentlich sollten wir die Ergebnisse wiederverwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 25, "src_audio": "/acl6060/audio/en/25.wav", "src_ref": "So, in our proposed approach we want to solve those problems in a step by step and interpretable manners.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem vorgeschlagenen Ansatz wollen wir diese Probleme schrittweise und auf interpretierbare Art und Weise lösen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 26, "src_audio": "/acl6060/audio/en/26.wav", "src_ref": "So for example, here in the second step, ah we can obtain these divisors which is twenty seven.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier im zweiten Schritt zum Beispiel können wir diese Teiler erhalten, was 27 ergibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 27, "src_audio": "/acl6060/audio/en/27.wav", "src_ref": "And we can also refer back to the original questions to find the relevant contents.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können auch auf die ursprünglichen Fragen zurückgreifen, um die relevanten Inhalte zu finden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 28, "src_audio": "/acl6060/audio/en/28.wav", "src_ref": "And in these steps we obtain the divisors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesen Schritten erhalten wir die Teiler.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 29, "src_audio": "/acl6060/audio/en/29.wav", "src_ref": "So, ah and then at this third step we actually get the quotient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und in diesem dritten Schritt erhalten wir dann den Quotienten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 30, "src_audio": "/acl6060/audio/en/30.wav", "src_ref": "Alright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Okay. Nach diesen drei Schritten können wir die Ergebnisse des zweiten Schritts wiederverwenden, dann die Ergebnisse des vierten Schritts erhalten und schließlich die Dividenden erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 31, "src_audio": "/acl6060/audio/en/31.wav", "src_ref": "So, here we actually generate the whole expression directly rather than generating a single operators or quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier wird also der gesamte Ausdruck direkt generiert und nicht nur ein einzelner Operator oder eine Menge.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 32, "src_audio": "/acl6060/audio/en/32.wav", "src_ref": "So this makes the process more accurate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dadurch wird der Prozess genauer.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 33, "src_audio": "/acl6060/audio/en/33.wav", "src_ref": "So, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem deduktiven System beginnen wir also zunächst mit einer Reihe von Mengen, die in den Fragen vorgestellt werden, und schließen auch mit einer Konstante als Anfangszustand.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 34, "src_audio": "/acl6060/audio/en/34.wav", "src_ref": "So, the expression is represented by e i j o p.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Ausdruck wird also durch e i j o p dargestellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 35, "src_audio": "/acl6060/audio/en/35.wav", "src_ref": "Where we perform operator from q_i to q_j, and such expression is actually directed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir führen den Operator von q_i nach q_j aus, und dieser Ausdruck ist tatsächlich geregelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 36, "src_audio": "/acl6060/audio/en/36.wav", "src_ref": "So, we also have subtraction with words here to represent the opposite direction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben hier also auch eine Subtraktion mit Wörtern, um die umgekehrte Richtung darzustellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 37, "src_audio": "/acl6060/audio/en/37.wav", "src_ref": "This is quite similar to relation extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies ist der Beziehungsextraktion recht ähnlich.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 38, "src_audio": "/acl6060/audio/en/38.wav", "src_ref": "So in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In einem formalen deduktiven System wenden wir also in einem Zeitschritt t den Operator zwischen dem Paar q_i und q_j an und erhalten dann diesen neuen Ausdruck.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 39, "src_audio": "/acl6060/audio/en/39.wav", "src_ref": "We add it to the next state to become a new quantity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir fügen ihn dem nächsten Zustand hinzu, um eine neue Menge zu erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 40, "src_audio": "/acl6060/audio/en/40.wav", "src_ref": "So, these slides actually visualize the evolution of the state where we keep adding expression to the current state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Folien veranschaulichen also die Entwicklung des Zustands, wobei wir dem aktuellen Zustand immer neue Ausdrücke hinzufügen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 41, "src_audio": "/acl6060/audio/en/41.wav", "src_ref": "So in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unseren Modellimplementierungen verwenden wir also zunächst ein vortrainiertes Sprachmodell, bei dem es sich um BERTs oder Robertas handeln kann, und dann kodieren wir den Satz und erhalten diese Mengendarstellungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 42, "src_audio": "/acl6060/audio/en/42.wav", "src_ref": "So, once we get the quantity representations, we can start to do inference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sobald wir also die Mengendarstellungen haben, können wir mit der Inferenz beginnen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 43, "src_audio": "/acl6060/audio/en/43.wav", "src_ref": "Here we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier zeigen wir ein Beispiel für q_1, um die Darstellung für q_2 geteilt durch q_2 und dann mal q_3 zu erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 44, "src_audio": "/acl6060/audio/en/44.wav", "src_ref": "First we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst erhalten wir die Paardarstellung, die im Grunde nur die Verkettung zwischen q_1 und q_2 ist, und dann wenden wir ein Feedforward-Netzwerk an, das durch den Operator parametrisiert ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 45, "src_audio": "/acl6060/audio/en/45.wav", "src_ref": "And then finally we obtain the expression representation q_1 divided by q_2.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und dann erhalten wir schließlich die Ausdrucksdarstellung q_1 geteilt durch q_2.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 46, "src_audio": "/acl6060/audio/en/46.wav", "src_ref": "But in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber in der Praxis könnten wir in der Inferenzphase auch den falschen Ausdruck erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 47, "src_audio": "/acl6060/audio/en/47.wav", "src_ref": "So, here all the possible expression is equals to three times the number of operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind also alle möglichen Ausdrücke gleich dem Dreifachen der Anzahl der Operatoren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 48, "src_audio": "/acl6060/audio/en/48.wav", "src_ref": "So the nice thing here is that we can easily add constraints to control this search this search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Schöne daran ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu kontrollieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 49, "src_audio": "/acl6060/audio/en/49.wav", "src_ref": "For example, if this expression is not allowed, we can simply remove this expression in our search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn dieser Ausdruck zum Beispiel nicht erlaubt ist, können wir ihn einfach aus unserem Suchraum entfernen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 50, "src_audio": "/acl6060/audio/en/50.wav", "src_ref": "So in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Im zweiten Schritt machen wir das Gleiche, aber der einzige Unterschied ist, dass wir nur eine Menge mehr haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 51, "src_audio": "/acl6060/audio/en/51.wav", "src_ref": "So this quantity come from the previous calculated expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Menge stammt also aus dem vorherigen berechneten Ausdruck.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 52, "src_audio": "/acl6060/audio/en/52.wav", "src_ref": "So finally we can obtain this final expression q_3 times q_4.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "So können wir schließlich diesen endgültigen Ausdruck q_3 mal q_4 erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 53, "src_audio": "/acl6060/audio/en/53.wav", "src_ref": "And we can also see the number of all the possible ah expression is different from the previous step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können auch sehen, dass die Anzahl aller möglichen Ausdrücke anders ist als beim vorherigen Schritt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 54, "src_audio": "/acl6060/audio/en/54.wav", "src_ref": "So, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein solcher Unterschied erschwert die Anwendung von einer Beam Search, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgewogen ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 55, "src_audio": "/acl6060/audio/en/55.wav", "src_ref": "So the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Trainingsverfahren ist also ähnlich dem Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust bei jedem Zeitschritt optimieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 56, "src_audio": "/acl6060/audio/en/56.wav", "src_ref": "And here we also use this tau to represent when we should terminate this generation process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auch hier verwenden wir dieses Tau, um darzustellen, wann wir diesen Generierungsprozess beenden sollten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 57, "src_audio": "/acl6060/audio/en/57.wav", "src_ref": "And here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier unterscheidet sich der Raum von Sequenz zu Sequenz, weil der Raum bei jedem Zeitschritt anders ist, während im traditionellen Sequenz-zu-Sequenz-Modell dies die Anzahl des Vokabulars ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 58, "src_audio": "/acl6060/audio/en/58.wav", "src_ref": "And it also allows us to impose certain constraints from prior from prior knowledge.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es erlaubt uns auch, bestimmte Beschränkungen aus dem Vorwissen aufzuerlegen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 59, "src_audio": "/acl6060/audio/en/59.wav", "src_ref": "So we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher führen wir Experimente mit den häufig verwendeten Datensätzen der mathematischen Wortprobleme durch, MAWPS, Math23K, MathQA und SVAMP.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 60, "src_audio": "/acl6060/audio/en/60.wav", "src_ref": "And here we briefly show the results compared with the previous best approaches.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier zeigen wir kurz die Ergebnisse im Vergleich zu den bisherigen besten Ansätzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 61, "src_audio": "/acl6060/audio/en/61.wav", "src_ref": "So our best performing variant is Roberta-DeductiveReasoner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere leistungsstärkste Variante ist also Roberta-DeductiveReasoner.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 62, "src_audio": "/acl6060/audio/en/62.wav", "src_ref": "And in fact we do not use beam search, in contrast all previous approaches are using beam search.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tatsächlich verwenden wir keine Beam Search, im Gegensatz zu allen früheren Ansätzen, die Beam Search verwendet haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 63, "src_audio": "/acl6060/audio/en/63.wav", "src_ref": "All right. So, the best approaches are often tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Okay. Daher sind die besten Ansätze oft baumbasierte Modelle.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 64, "src_audio": "/acl6060/audio/en/64.wav", "src_ref": "So, overall our reasoner is able to significantl significantly outperform this tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insgesamt ist unser Reasoner also in der Lage, dieses baumbasierte Modell signifikant zu übertreffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 65, "src_audio": "/acl6060/audio/en/65.wav", "src_ref": "But we can see the absolute numbers on MathQA or SVAMP are not really high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wir können sehen, dass die absoluten Zahlen bei MathQA oder SVAMP nicht wirklich hoch sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 66, "src_audio": "/acl6060/audio/en/66.wav", "src_ref": "So we further investigate the results on SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deshalb untersuchen wir die Ergebnisse bei SVAMP weiter.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 67, "src_audio": "/acl6060/audio/en/67.wav", "src_ref": "And this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieser Datensatz ist eine Herausforderung, weil der Autor versucht hat, manuell etwas hinzuzufügen, um das NLP-Modell zu verwirren, wie z. B. das Hinzufügen irrelevanter Informationen und zusätzlicher Mengen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 68, "src_audio": "/acl6060/audio/en/68.wav", "src_ref": "So, in our prediction we find some of the intermediate values are actually negatives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserer Vorhersage stellen wir also fest, dass einige der Zwischenwerte eigentlich negativ sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 69, "src_audio": "/acl6060/audio/en/69.wav", "src_ref": "For example, um, in these questions we are asking how many apples does Jake have?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel geht es in diesen Fragen darum, wie viele Äpfel Jake hat.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 70, "src_audio": "/acl6060/audio/en/70.wav", "src_ref": "But we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wir haben einige zusätzliche Informationen wie 17 Bilder weniger, und Steven hat acht Bilder, was völlig irrelevant ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 71, "src_audio": "/acl6060/audio/en/71.wav", "src_ref": "So, our model makes some prediction like this which is producing negative values.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Modell macht also eine Vorhersage wie diese, die negative Werte ergibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 72, "src_audio": "/acl6060/audio/en/72.wav", "src_ref": "And we observe these two expressions actually have similar scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir stellen fest, dass diese beiden Ausdrücke tatsächlich ähnliche Werte haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 73, "src_audio": "/acl6060/audio/en/73.wav", "src_ref": "So, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können also diesen Suchraum eingrenzen, indem wir die negativen Ergebnisse entfernen, damit die Antwort richtig ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 74, "src_audio": "/acl6060/audio/en/74.wav", "src_ref": "So um we further find such constraint actually improves quite a lot for some models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir stellen also fest, dass eine solche Bedingung für einige Modelle tatsächlich eine erhebliche Verbesserung darstellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 75, "src_audio": "/acl6060/audio/en/75.wav", "src_ref": "For example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei BERT haben wir uns zum Beispiel um sieben Punkte verbessert und beim Roberta-Basismodell haben wir uns sogar um zwei Punkte verbessert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 76, "src_audio": "/acl6060/audio/en/76.wav", "src_ref": "So better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein besseres Sprachmodell hat also bessere Sprachverständnisfähigkeiten, sodass die Zahl hier höher bei Roberta und niedriger bei BERT ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 77, "src_audio": "/acl6060/audio/en/77.wav", "src_ref": "And we also try to analyze the difficulty behind these behind all these datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir versuchen auch, die Schwierigkeiten hinter all diesen Datensätzen zu analysieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 78, "src_audio": "/acl6060/audio/en/78.wav", "src_ref": "We assume the number of unused quantities can be regarded as irrelevant information here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir gehen davon aus, dass die Anzahl der ungenutzten Mengen hier als irrelevante Informationen betrachtet werden kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 79, "src_audio": "/acl6060/audio/en/79.wav", "src_ref": "So ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier können wir also sehen, dass wir den Prozentsatz der Proben mit ungenutzten Mengen haben, und der SVAMP-Datensatz hat den größten Anteil.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 80, "src_audio": "/acl6060/audio/en/80.wav", "src_ref": "And here we also show the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier zeigen wir auch die Gesamtleistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 81, "src_audio": "/acl6060/audio/en/81.wav", "src_ref": "For those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei den Proben ohne ungenutzte Mengen ist die Leistung sogar höher als die Gesamtleistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 82, "src_audio": "/acl6060/audio/en/82.wav", "src_ref": "But with those samples that with unused quantity is actually way worse than the, worse than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber bei diesen Proben mit ungenutzten Mengen ist sie eigentlich viel schlechter als die Gesamtleistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 83, "src_audio": "/acl6060/audio/en/83.wav", "src_ref": "For MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei MAWPS haben wir nicht wirklich viele Testfälle, also ignoriere ich diesen Teil einfach.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 84, "src_audio": "/acl6060/audio/en/84.wav", "src_ref": "So, finally we want to show the interpretability through a question perturbation example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich wollen wir die Interpretierbarkeit anhand eines Frage-Störungsbeispiels zeigen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 85, "src_audio": "/acl6060/audio/en/85.wav", "src_ref": "So here our model actually makes a wrong prediction at the first step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier macht unser Modell also tatsächlich eine falsche Vorhersage im ersten Schritt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 86, "src_audio": "/acl6060/audio/en/86.wav", "src_ref": "So, we can actually correlate this expression with the sentence here. Alright.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können diesen Ausdruck also tatsächlich mit dem Satz hier korrelieren. Okay.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 87, "src_audio": "/acl6060/audio/en/87.wav", "src_ref": "So, we think this sentence might be misleading the model to an incorrect predictions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir denken also, dass dieser Satz das Modell zu falschen Vorhersagen verleiten könnte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 88, "src_audio": "/acl6060/audio/en/88.wav", "src_ref": "So here planting another thirty five makes the model makes the model think it should be an addition operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier führt eine weitere 35 dazu, dass das Modell denkt, dass es ein Additionsoperator sein sollte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 89, "src_audio": "/acl6060/audio/en/89.wav", "src_ref": "So we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir versuchen also den Satz so umzuformulieren, dass die Anzahl der Birnbäume um 35 weniger als die der Apfelbäume ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 90, "src_audio": "/acl6060/audio/en/90.wav", "src_ref": "So, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sorgen also dafür, dass eine genauere Semantik vermittelt wird, sodass das Modell in der Lage ist, die Vorhersage korrekt zu treffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 91, "src_audio": "/acl6060/audio/en/91.wav", "src_ref": "So, this study shows how the interpretable predictions help us understand the model behavior.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Studie zeigt also, wie uns die interpretierbaren Vorhersagen helfen, das Modellverhalten zu verstehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 92, "src_audio": "/acl6060/audio/en/92.wav", "src_ref": "So to conclude our work, so first our model is actually pretty efficient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um unsere Arbeit abzuschließen: Zunächst ist unser Modell tatsächlich ziemlich effizient.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 93, "src_audio": "/acl6060/audio/en/93.wav", "src_ref": "And we are able to provide interpretable solving procedure.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sind in der Lage, ein interpretierbares Lösungsverfahren anzubieten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 94, "src_audio": "/acl6060/audio/en/94.wav", "src_ref": "And we can easily incorporate some prior knowledge as constraint which can help improve the performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können einfach einiges Vorwissen als Bedingung einbeziehen, was zur Verbesserung der Leistung beitragen kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 95, "src_audio": "/acl6060/audio/en/95.wav", "src_ref": "And the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich gilt der zugrundeliegende Mechanismus nicht nur für das Lösen von Netzwerkproblemen, sondern auch für andere Aufgaben, die mehrstufiges Argumentieren erfordern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 96, "src_audio": "/acl6060/audio/en/96.wav", "src_ref": "We also have certain limitations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auch sind uns gewisse Grenzen gesetzt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 97, "src_audio": "/acl6060/audio/en/97.wav", "src_ref": "Ah, if we have a large number of operators or constants, the memory consumption could be pretty high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir eine große Anzahl von Operatoren oder Konstanten haben, könnte der Speicherverbrauch ziemlich hoch sein.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 98, "src_audio": "/acl6060/audio/en/98.wav", "src_ref": "And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wie bereits erwähnt ist es zudem aufgrund der unausgewogenen Wahrscheinlichkeitsverteilung zwischen den verschiedenen Zeitschritten eine ziemliche Herausforderung, die Beam Search-Strategie anzuwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 99, "src_audio": "/acl6060/audio/en/99.wav", "src_ref": "So this is the end of the talk, and questions are welcomed. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sind am Ende des Vortrags angekommen und freuen uns auf etwaige Fragen. Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 100, "src_audio": "/acl6060/audio/en/100.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo, mein Name ist Antoine und ich bin von der Universität Maastricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 101, "src_audio": "/acl6060/audio/en/101.wav", "src_ref": "I will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde meine gemeinsame Arbeit mit Jerry vorstellen, bei der es um einen neuen Datensatz für das Retrieval von Gesetzesartikeln geht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 102, "src_audio": "/acl6060/audio/en/102.wav", "src_ref": "Legal issues are an integral part of many people's lives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Rechtsfragen sind ein wesentlicher Bestandteil des Lebens vieler Menschen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 103, "src_audio": "/acl6060/audio/en/103.wav", "src_ref": "But the majority of citizens have little to know knowledge about their rights and fundamental legal processes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber die Mehrheit der Bürger verfügt nur über wenig Wissen über ihre Rechte und grundlegende rechtliche Verfahren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 104, "src_audio": "/acl6060/audio/en/104.wav", "src_ref": "As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies hat zur Folge, dass viele schutzbedürftige Bürger, die sich die kostspielige Hilfe eines Rechtsexperten nicht leisten können, schutzlos bleiben oder schlimmstenfalls ausgebeutet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 105, "src_audio": "/acl6060/audio/en/105.wav", "src_ref": "All work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere Arbeit zielt darauf ab, die Kluft zwischen den Menschen und dem Gesetz zu überbrücken, indem ein effektives Retrieval-System für Gesetzesartikel entwickelt wird.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 106, "src_audio": "/acl6060/audio/en/106.wav", "src_ref": "Such a system could provide a free professional legal help service for unskilled humans.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein solches System könnte einen kostenlosen professionellen Rechtshilfedienst für unerfahrene Menschen anbieten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 107, "src_audio": "/acl6060/audio/en/107.wav", "src_ref": "Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bevor wir uns dem Hauptbeitrag dieser Arbeit widmen, wollen wir zunächst das Problem des Retrievals von Gesetzesartikeln beschreiben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 108, "src_audio": "/acl6060/audio/en/108.wav", "src_ref": "Given a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eine einfache Frage zu einer Rechtsangelegenheit wäre: „Was riskiere ich, wenn ich das Berufsgeheimnis verletze?“", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 109, "src_audio": "/acl6060/audio/en/109.wav", "src_ref": "A model is required to retrieve all relevant statutory articles from a large body of legislation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier wird ein Modell benötigt, um alle relevanten Gesetzesartikel aus einem großen Bestand an Rechtsvorschriften abzurufen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 110, "src_audio": "/acl6060/audio/en/110.wav", "src_ref": "This information retrieval task comes with its own set of challenges.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Informationsbeschaffungsaufgabe bringt eine Reihe von Herausforderungen mit sich.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 111, "src_audio": "/acl6060/audio/en/111.wav", "src_ref": "First, it deals with two types of language.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Erstens: Es geht um zwei Arten von Sprache.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 112, "src_audio": "/acl6060/audio/en/112.wav", "src_ref": "Common natural language for the questions and complex legal language for the statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die gemeinsame natürliche Sprache für die Fragen und die komplexe juristische Sprache für die Gesetze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 113, "src_audio": "/acl6060/audio/en/113.wav", "src_ref": "This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieser Unterschied bei den Sprachverteilungen macht es für ein System schwieriger, relevante Kandidaten zu finden, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine rechtliche Frage übersetzen kann, die der Terminologie der Gesetze entspricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 114, "src_audio": "/acl6060/audio/en/114.wav", "src_ref": "Besides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Außerdem ist das Recht keine Ansammlung unabhängiger Artikel, die für sich allein als vollständige Informationsquelle betrachtet werden können, anders als beispielsweise Nachrichten oder Rezepte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 115, "src_audio": "/acl6060/audio/en/115.wav", "src_ref": "Instead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielmehr handelt es sich um eine strukturierte Sammlung von Rechtsvorschriften, die nur im Gesamtkontext, d. h. zusammen mit den ergänzenden Informationen aus den Nachbarartikeln, den Bereichen und Teilbereichen, denen sie angehören, und ihrer Stellung im Aufbau des Gesetzes, eine vollständige Bedeutung haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 116, "src_audio": "/acl6060/audio/en/116.wav", "src_ref": "Lastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich handelt es sich bei den Gesetzesartikeln nicht um kleine Absätze, die in den meisten Retrieval-Arbeiten die typische Retrieval-Einheit darstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 117, "src_audio": "/acl6060/audio/en/117.wav", "src_ref": "Here, there are long documents that may be up to six thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier gibt es lange Dokumente, die bis zu 6.000 Wörter umfassen können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 118, "src_audio": "/acl6060/audio/en/118.wav", "src_ref": "The recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die jüngsten Fortschritte im Bereich NLP haben großes Interesse an vielen juristischen Aufgaben geweckt, z. B. an der Vorhersage juristischer Urteile oder der automatisierten Prüfung von Verträgen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 119, "src_audio": "/acl6060/audio/en/119.wav", "src_ref": "But statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Doch das Retrieval von Gesetzesartikeln ist aufgrund des Mangels an großen und qualitativ hochwertig markierten Datensätzen weitgehend unberührt geblieben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 120, "src_audio": "/acl6060/audio/en/120.wav", "src_ref": "In this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In dieser Arbeit stellen wir einen neuen Datensatz in der französischen Muttersprache vor, mit dem wir Retrievalmodelle untersuchen und analysieren, ob sie bei der Aufgabe der Suche nach Gesetzesartikeln an die Effizienz und Zuverlässigkeit eines Rechtsexperten rankommen können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 121, "src_audio": "/acl6060/audio/en/121.wav", "src_ref": "Our Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser belgischer Datensatz für das Retrieval von Gesetzesartikeln, BSARD, besteht aus mehr als 1.100 juristischen Fragen, die von belgischen Bürgern gestellt wurden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 122, "src_audio": "/acl6060/audio/en/122.wav", "src_ref": "These questions cover a wide range of topics from family, housing, money, to work and social security.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Fragen decken ein breites Spektrum an Themen ab, von Familie, Wohnen, Geld bis hin zu Arbeit und sozialer Sicherheit.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 123, "src_audio": "/acl6060/audio/en/123.wav", "src_ref": "Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jede von ihnen wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als 22.600 Rechtsartikeln aus belgischen Gesetzbüchern markiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 124, "src_audio": "/acl6060/audio/en/124.wav", "src_ref": "Let's now talk about how we collected this dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lassen Sie uns nun darüber sprechen, wie wir diesen Datensatz gesammelt haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 125, "src_audio": "/acl6060/audio/en/125.wav", "src_ref": "First, we started by compiling a large corpus of legal articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst haben wir einen großen Korpus von Rechtsartikeln zusammengestellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 126, "src_audio": "/acl6060/audio/en/126.wav", "src_ref": "We considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben 32 öffentlich zugängliche belgische Gesetzbücher berücksichtigt und alle Artikel sowie die entsprechenden Abschnittsüberschriften extrahiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 127, "src_audio": "/acl6060/audio/en/127.wav", "src_ref": "Then we gathered legal questions with references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann haben wir rechtliche Fragen mit Verweisen auf einschlägige Gesetze zusammengestellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 128, "src_audio": "/acl6060/audio/en/128.wav", "src_ref": "To do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zu diesem Zweck arbeiten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr etwa 4.000 E-Mails von belgischen Bürgern erhält, die um Rat in einer persönlichen Rechtsangelegenheit bitten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 129, "src_audio": "/acl6060/audio/en/129.wav", "src_ref": "We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir hatten das Glück, Zugang zu ihren Websites zu erhalten, auf denen ihr Team aus erfahrenen Juristen die häufigsten Rechtsangelegenheiten der Belgier behandelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 130, "src_audio": "/acl6060/audio/en/130.wav", "src_ref": "We collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben Tausende von Fragen gesammelt und mit Kategorien, Unterkategorien und Verweisen auf einschlägige Gesetze annotiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 131, "src_audio": "/acl6060/audio/en/131.wav", "src_ref": "Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich haben wir die rechtlichen Verweise überprüft und die Fragen herausgefiltert, deren Verweise nicht in einem der von uns berücksichtigten Gesetzbücher enthalten waren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 132, "src_audio": "/acl6060/audio/en/132.wav", "src_ref": "The remaining references were matched and converted to the corresponding article ids from our corpus.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die übrigen Referenzen wurden zusammengeführt und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 133, "src_audio": "/acl6060/audio/en/133.wav", "src_ref": "We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Am Ende hatten wir 1.108 Fragen, jede sorgfältig markiert mit den IDs der relevanten Artikel aus unserem großen Korpus von 22.630 Gesetzesartikeln.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 134, "src_audio": "/acl6060/audio/en/134.wav", "src_ref": "In addition, each question comes with the main category and a concatenation of subcategories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus enthält jede Frage eine Hauptkategorie und eine Verkettung von Unterkategorien.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 135, "src_audio": "/acl6060/audio/en/135.wav", "src_ref": "And each articles comes with a concatenation of the subsequence heading in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jeder Artikel hat eine Verkettung mit der anschließenden Überschrift im Aufbau des Gesetzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 136, "src_audio": "/acl6060/audio/en/136.wav", "src_ref": "This extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für künftige Forschungen zum juristischen Informationen-Retrieval oder zur juristischen Textklassifikation von Interesse sein.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 137, "src_audio": "/acl6060/audio/en/137.wav", "src_ref": "Let's look at some characteristic of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schauen wir uns einige Merkmale unseres Datensatzes an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 138, "src_audio": "/acl6060/audio/en/138.wav", "src_ref": "The questions are between five and forty four words long with a median of fourteen words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Fragen sind zwischen fünf und 24 Wörter lang, mit einem Mittelwert von 14 Wörtern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 139, "src_audio": "/acl6060/audio/en/139.wav", "src_ref": "The articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Artikel sind mit einer durchschnittlichen Länge von 77 Wörtern sehr viel länger, wobei 142 von ihnen mehr als 1.000 Wörter umfassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 140, "src_audio": "/acl6060/audio/en/140.wav", "src_ref": "The lengthiest one being up to five thousand seven hundred and ninety words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der längste von ihnen hat bis zu 5.790 Wörter.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 141, "src_audio": "/acl6060/audio/en/141.wav", "src_ref": "As previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wie bereits erwähnt decken die Fragen ein breites Spektrum an Themen ab, wobei etwa 85 Prozent der Fragen entweder die Familie, das Wohnen, Geld oder die Gerechtigkeit betreffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 142, "src_audio": "/acl6060/audio/en/142.wav", "src_ref": "While the remaining fifteen percent concern either social security, foreigners or work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die restlichen 15 Prozent betreffen entweder soziale Sicherheit, Ausländer oder Arbeit.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 143, "src_audio": "/acl6060/audio/en/143.wav", "src_ref": "The article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Artikel sind auch sehr vielfältig, da sie aus 32 verschiedenen belgischen Gesetzbüchern stammen, die eine große Reihe von Rechtsthemen abdecken.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 144, "src_audio": "/acl6060/audio/en/144.wav", "src_ref": "Here's the total number of articles collected from each of these Belgian codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist die Gesamtzahl der Artikel, die aus diesen belgischen Gesetzbüchern gesammelt wurden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 145, "src_audio": "/acl6060/audio/en/145.wav", "src_ref": "Out of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Von den 22.633 Artikeln werden nur 1.612 als relevant für mindestens eine Frage im Datensatz genannt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 146, "src_audio": "/acl6060/audio/en/146.wav", "src_ref": "And around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Etwa 80 Prozent dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, dem Gerichtsgesetzbuch, dem Gesetzbuch für strafrechtliche Ermittlungen oder dem Strafgesetzbuch.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 147, "src_audio": "/acl6060/audio/en/147.wav", "src_ref": "Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei 18 von 32 Gesetzbüchern werden weniger als fünf Artikel als relevant für mindestens eine Frage bestimmt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 148, "src_audio": "/acl6060/audio/en/148.wav", "src_ref": "Which can be explained by the fact that those codes focused less on individuals and their concerns.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies lässt sich dadurch erklären, dass diese Gesetzbücher weniger auf den Einzelnen und seine Anliegen ausgerichtet waren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 149, "src_audio": "/acl6060/audio/en/149.wav", "src_ref": "Overall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insgesamt liegt der Durchschnitt der Anzahl der Zitate bei zwei, und weniger als 25 Prozent der zitierten Artikel werden mehr als fünfmal zitiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 150, "src_audio": "/acl6060/audio/en/150.wav", "src_ref": "Using all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unter Verwendung aller Datensätze haben wir verschiedene Retrieval-Ansätze, einschließlich lexikalischer und dichter Architektur, einem Benchmarking unterzogen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 151, "src_audio": "/acl6060/audio/en/151.wav", "src_ref": "Given a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei einer Abfrage und einem Artikel weist ein lexikalisches Modell dem Abfrage-Artikel-Paar eine Punktzahl zu, indem es die Summe der Gewichtungen von jedem der Begriffe in diesem Artikel über die Abfrage-Termini berechnet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 152, "src_audio": "/acl6060/audio/en/152.wav", "src_ref": "We experiment with the standard TF-IDF and BM25 ranking functions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit den Standard-Ranking-Funktionen TF-IDF und BM25.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 153, "src_audio": "/acl6060/audio/en/153.wav", "src_ref": "The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Hauptproblem bei diesen Ansätzen ist, dass sie nur Artikel finden können, die Schlüsselwörter enthalten, die in der Anfrage vorkommen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 154, "src_audio": "/acl6060/audio/en/154.wav", "src_ref": "To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um diese Einschränkung zu überwinden, experimentieren wir mit einer neuronal-basierten Architektur, die semantische Beziehungen zwischen Abfragen und Artikeln erfassen kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 155, "src_audio": "/acl6060/audio/en/155.wav", "src_ref": "We use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir verwenden ein Bi-Encoder-Modell, das Abfragen und Artikel in dichten Vektor-Darstellungen abbildet und einen Relevanzwert zwischen einem Abfrage-Artikel-Paar anhand der Ähnlichkeit ihrer Einbettungen berechnet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 156, "src_audio": "/acl6060/audio/en/156.wav", "src_ref": "These embeddings typically result from a pooling operation on the output of a word embedding model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Einbettungen resultieren in der Regel aus einer Pooling-Operation auf der Ausgabe eines Worteinbettungsmodells.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 157, "src_audio": "/acl6060/audio/en/157.wav", "src_ref": "First, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst untersuchen wir die Effektivität von Siamesischen Bi-Encodern in einem Zero-Shot-Evaluation-Setup, d. h. dass vortrainierte Worteinbettungsmodelle ohne zusätzliche Feinabstimmung sofort angewendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 158, "src_audio": "/acl6060/audio/en/158.wav", "src_ref": "We experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit kontextunabhängigen Textencodern, wie word2vec und fastText, und kontextabhängigen Einbettungsmodellen, wie Roberta und insbesondere CamemBERT, einem französischen Roberta-Modell.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 159, "src_audio": "/acl6060/audio/en/159.wav", "src_ref": "Additionally, we train our own CamemBERT based model ah bi-encoders on our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus trainieren wir unser eigenes, auf CamemBERT basierendes Modell, Bi-Encoder, auf unserem Datensatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 160, "src_audio": "/acl6060/audio/en/160.wav", "src_ref": "Note that for training, we experiment with the two flavors of the bi-encoder architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Beachten Sie, dass wir beim Training mit den beiden Varianten der Bi-Encoder-Architektur experimentieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 161, "src_audio": "/acl6060/audio/en/161.wav", "src_ref": "Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Siamese, das ein einzigartiges Worteinbettungsmodell verwendet, das die Abfrage und den Artikel zusammen in einem geteilten dichten Vektorraum abbildet, und Two-Tower, das zwei unabhängige Worteinbettungsmodelle verwendet, die die Abfrage und den Artikel getrennt in verschiedenen Einbettungsräumen kodieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 162, "src_audio": "/acl6060/audio/en/162.wav", "src_ref": "We experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit Mittelwert, Maximalwert und CLS-Pooling sowie mit Produkt und Kosinus für die Berechnung von Ähnlichkeiten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 163, "src_audio": "/acl6060/audio/en/163.wav", "src_ref": "Here are the result of our baseline on the test sets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind die Ergebnisse unserer Baseline auf den Testsätzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 164, "src_audio": "/acl6060/audio/en/164.wav", "src_ref": "With the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die lexikalischen Methoden werden oben, die Siamesischen Bi-Encoder in einem Zero-Shot-Setup in der Mitte und die fein abgestimmten Bi-Encoder werden unten ausgewertet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 165, "src_audio": "/acl6060/audio/en/165.wav", "src_ref": "Overall, the finetuned bi-encoder significantly outperforms all the other baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insgesamt übertrifft der fein abgestimmte Bi-Encoder alle anderen Baselines deutlich.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 166, "src_audio": "/acl6060/audio/en/166.wav", "src_ref": "The two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Two-Tower-Modell verbessert sich gegenüber seinen Siamesischen Varianten beim Recall um 100, schneidet aber bei den anderen Metriken ähnlich gut ab.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 167, "src_audio": "/acl6060/audio/en/167.wav", "src_ref": "Although BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Obwohl BM25 deutlich schlechter abschnitt als der trainierte Bi-Encoder, zeigte seine Leistung, dass er immer noch eine gute Basis für ein domänenspezifisches Retrieval ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 168, "src_audio": "/acl6060/audio/en/168.wav", "src_ref": "Regarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bezüglich der Zero-Shot-Evaluation des Siamesischen Bi-Encoders stellen wir fest, dass die direkte Verwendung der Einbettungen eines vortrainierten CamemBERT-Modells ohne Optimierung für die Informationen-Retrieval-Aufgabe schlechte Ergebnisse liefert, was mit früheren Erkenntnissen übereinstimmt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 169, "src_audio": "/acl6060/audio/en/169.wav", "src_ref": "Furthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus stellen wir fest, dass der word2vec-basierte Bi-Encoder die fastText- und BERT-basierten Modelle signifikant übertrifft, was darauf hindeutet, dass vortrainierte Wort-Ebenen-Einbettungen vielleicht besser für die Aufgabe geeignet sind als Zeichen- oder Unterwort-Ebenen-Einbettungen, wenn sie direkt verwendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 170, "src_audio": "/acl6060/audio/en/170.wav", "src_ref": "Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Obwohl diese Ergebnisse vielversprechend sind, lassen sie im Vergleich zu einem erfahrenen Rechtsexperten auf ein großes Verbesserungspotenzial schließen, der früher oder später alle relevanten Artikel zu jeder Frage abrufen kann und somit perfekte Ergebnisse erzielt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 171, "src_audio": "/acl6060/audio/en/171.wav", "src_ref": "Let's conclude by discussing two limitations of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Abschließend möchte ich auf zwei Einschränkungen unseres Datensatzes eingehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 172, "src_audio": "/acl6060/audio/en/172.wav", "src_ref": "First, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Erstens ist der Artikelkorpus auf die Artikel der 32 herangezogenen belgischen Gesetzbücher beschränkt, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Dekreten, Richtlinien und Verordnungen fehlen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 173, "src_audio": "/acl6060/audio/en/173.wav", "src_ref": "During the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei der Erstellung des Datensatzes werden alle Verweise auf diese nicht erfassten Artikel ignoriert, was dazu führt, dass sich einige Fragen am Ende nur auf einen Bruchteil der ursprünglichen Anzahl der relevanten Artikel beziehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 174, "src_audio": "/acl6060/audio/en/174.wav", "src_ref": "This information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Information impliziert also, dass die Antwort, die in den übrigen relevanten Artikeln enthalten ist, unvollständig sein könnte, obwohl sie immer noch völlig angemessen ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 175, "src_audio": "/acl6060/audio/en/175.wav", "src_ref": "Second, we should note that not all legal questions can be answered with statutes alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zweitens sollten wir beachten, dass nicht alle rechtlichen Fragen allein mit Gesetzen beantwortet werden können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 176, "src_audio": "/acl6060/audio/en/176.wav", "src_ref": "For instance, the question, can I evict my tenants if they make too much noise?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel die Frage: „Kann ich meinen Mietern kündigen, wenn sie zu viel Lärm machen?“", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 177, "src_audio": "/acl6060/audio/en/177.wav", "src_ref": "Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Möglicherweise gibt es im Gesetz keine detaillierte Antwort, die eine bestimmte Lärmschwelle festlegt, ab der eine Räumung zulässig ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 178, "src_audio": "/acl6060/audio/en/178.wav", "src_ref": "Instead, the landlord should probably rely more on case law and find precedents similar to their current situation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Stattdessen sollte sich der Vermieter wahrscheinlich eher auf das Fallrecht stützen und Präzedenzfälle finden, die seiner aktuellen Situation ähnlich sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 179, "src_audio": "/acl6060/audio/en/179.wav", "src_ref": "For example, the tenants makes two parties a week until two AM.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel veranstalten die Mieter zwei Partys pro Woche bis zwei Uhr morgens.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 180, "src_audio": "/acl6060/audio/en/180.wav", "src_ref": "Hence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Einige Fragen eignen sich daher besser als andere für die Aufgabe des Retrievals von Gesetzesartikeln. Die Domäne der weniger geeigneten Fragen muss noch bestimmt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 181, "src_audio": "/acl6060/audio/en/181.wav", "src_ref": "We hope that our work sparks interest in developing practical and reliable statutory article retrieval models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir hoffen, dass unsere Arbeit das Interesse an der Entwicklung praktischer und zuverlässiger Modelle für das Retrieval von Gesetzesartikeln weckt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 182, "src_audio": "/acl6060/audio/en/182.wav", "src_ref": "That can help improve access to justice for all.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie kann dazu beitragen, den Zugang zur Justiz für alle zu verbessern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 183, "src_audio": "/acl6060/audio/en/183.wav", "src_ref": "You can check out our paper, dataset and code at the following links. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie können unser Paper, den Datensatz und die Gesetzbücher unter den folgenden Links einsehen. Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 184, "src_audio": "/acl6060/audio/en/184.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo, wir freuen uns, Ihnen unsere Arbeit über VALSE vorstellen zu können. Das ist ein Aufgaben-unabhängiger Benchmark, der für das Testen von Seh- und Sprachmodellen mit spezifischen sprachlichen Phänomenen konzipiert ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 185, "src_audio": "/acl6060/audio/en/185.wav", "src_ref": "Why did we do the trouble in setting up this benchmark?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Warum haben wir uns die Mühe gemacht, diesen Benchmark einzurichten?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 186, "src_audio": "/acl6060/audio/en/186.wav", "src_ref": "Well, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In den letzten Jahren haben wir eine explosionsartige Zunahme von Transformer-basierten Seh- und Sprachmodellen erlebt, die mit großen Mengen an Bild-Text-Paaren vortrainiert wurden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 187, "src_audio": "/acl6060/audio/en/187.wav", "src_ref": "Each one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jedes dieser Modelle treibt den Fortschritt bei Seh- und Sprachaufgaben voran, wie z. B. die visuelle Fragenbeantwortung, das Argumentieren mit visuellem Menschenverstand, Bild-Retrieval, Phrasen-Erdung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 188, "src_audio": "/acl6060/audio/en/188.wav", "src_ref": "So we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben also die Nachricht erhalten, dass die Genauigkeiten bei diesen Aufgaben und spezifischen Benchmarks stetig steigen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 189, "src_audio": "/acl6060/audio/en/189.wav", "src_ref": "But do we know what the models have actually learned?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wissen wir, was die Modelle tatsächlich gelernt haben?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 190, "src_audio": "/acl6060/audio/en/190.wav", "src_ref": "What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Was hat ein Seh- und Sprach-Transformer verstanden, wenn er für dieses Bild und diesen Satz eine hohe Punktzahl zuweist?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 191, "src_audio": "/acl6060/audio/en/191.wav", "src_ref": "And the low score for this one?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und hier eine niedrige Punktzahl?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 192, "src_audio": "/acl6060/audio/en/192.wav", "src_ref": "Do vision and language models focus on the right thing?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Konzentrieren sich Seh- und Sprachmodelle auf die wesentlichen Punkte?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 193, "src_audio": "/acl6060/audio/en/193.wav", "src_ref": "Or do they focus on biases as shown by previous work?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Oder konzentrieren sie sich auf Verzerrungen, wie in früheren Arbeiten aufgezeigt wurde?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 194, "src_audio": "/acl6060/audio/en/194.wav", "src_ref": "To shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um diesen Aspekt näher zu beleuchten, schlagen wir eine eher aufgabenbezogene agnostische Richtung vor und führen VALSE ein, das die Empfindlichkeit von Seh- und Sprachmodellen für spezifische sprachliche Phänomene testet, die sowohl die sprachlichen als auch die visuellen Modalitäten betreffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 195, "src_audio": "/acl6060/audio/en/195.wav", "src_ref": "We target existence, plurality, counting, spatial relations, actions and entity coreference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir zielen auf Existenz, Pluralität, Zählung, räumliche Beziehungen, Handlungen und Entity-Koreferenz ab.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 196, "src_audio": "/acl6060/audio/en/196.wav", "src_ref": "But how do we test whether the vision and language models have captured this phenomena?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Doch wie lässt sich überprüfen, ob die Seh- und Sprachmodelle dieses Phänomen erfasst haben?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 197, "src_audio": "/acl6060/audio/en/197.wav", "src_ref": "By foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Verfälschen einer Methode wurde zuvor von Ravi Shekhar und Mitarbeitern nur für Substantivphrasen in den Seh- und Sprachmodellen und beim Zählen von uns in früheren Arbeiten angewandt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 198, "src_audio": "/acl6060/audio/en/198.wav", "src_ref": "Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Verfälschen bedeutet im Grunde, dass wir die Beschriftung eines Bildes nehmen und eine Verfälschung erzeugen, indem wir die Beschriftung so verändern, dass sie das Bild nicht mehr beschreibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 199, "src_audio": "/acl6060/audio/en/199.wav", "src_ref": "And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir führen diese Veränderungen der Phrase durch, indem wir uns auf sechs spezifische Teile konzentrieren, wie Existenz, Pluralität, Zählung, räumliche Relationen, Handlungen und Entität-Koreferenz, wobei jeder Teil aus einem oder mehreren Instrumenten bestehen kann, falls wir mehr als einen interessanten Weg gefunden haben, um Verfälschungsinstanzen zu erzeugen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 200, "src_audio": "/acl6060/audio/en/200.wav", "src_ref": "For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel haben wir im Fall des Handlungsteils zwei Instrumente, eines, bei dem das Handlungsverb durch eine andere Handlung ersetzt wird, und eines, bei dem die Aktanten ausgetauscht werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 201, "src_audio": "/acl6060/audio/en/201.wav", "src_ref": "Counting and coreference also are pieces that have more than one instrument.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zählung und Koreferenz sind auch Teile, die mehr als ein Instrument haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 202, "src_audio": "/acl6060/audio/en/202.wav", "src_ref": "And we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir erstellen diese Verfälschungen, indem wir sicherstellen, dass sie das Bild nicht beschreiben, dass sie grammatikalisch richtig sind und auch korrekte Sätze bilden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 203, "src_audio": "/acl6060/audio/en/203.wav", "src_ref": "This is not easy to do because a foiled caption may be less likely than the original caption.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das ist nicht einfach zu bewerkstelligen, denn eine verfälschte Beschriftung ist weniger wahrscheinlich als die ursprüngliche Beschriftung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 204, "src_audio": "/acl6060/audio/en/204.wav", "src_ref": "For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist zwar zum Beispiel nicht unmöglich, aber statistisch gesehen ist es unwahrscheinlicher, dass Pflanzen einen Menschen schneiden, als dass ein Mensch Pflanzen schneidet. Große Seh- und Sprachmodelle könnten dies erkennen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 205, "src_audio": "/acl6060/audio/en/205.wav", "src_ref": "Therefore, to obtain valid foils, we must take action.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher müssen wir handeln, wenn wir gültige Verfälschungen erhalten wollen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 206, "src_audio": "/acl6060/audio/en/206.wav", "src_ref": "First, we make use of strong language models to propose foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Erstens nutzen wir starke Sprachmodelle, um Verfälschungen vorzuschlagen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 207, "src_audio": "/acl6060/audio/en/207.wav", "src_ref": "Second, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zweitens verwenden wir die Inferenz der natürlichen Sprache oder kurz NLI, um Verfälschungen herauszufiltern, die das Bild noch beschreiben könnten. Bei der Konstruktion von Verfälschungen müssen wir sicherstellen, dass sie das Bild nicht beschreiben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 208, "src_audio": "/acl6060/audio/en/208.wav", "src_ref": "To test this automatically, we apply natural language inference with the following rationale.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um dies automatisch zu testen, wenden wir die Inferenz der natürlichen Sprache mit der folgenden Logik an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 209, "src_audio": "/acl6060/audio/en/209.wav", "src_ref": "We consider an image to be the premise and its caption its entailed hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir betrachten ein Bild als die Prämisse und seine Beschriftung als die daraus abgeleitete Hypothese.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 210, "src_audio": "/acl6060/audio/en/210.wav", "src_ref": "In addition, we consider the caption to be the premise, and the foil is its hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus betrachten wir die Beschriftung als Prämisse und die Verfälschung als ihre Hypothese.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 211, "src_audio": "/acl6060/audio/en/211.wav", "src_ref": "If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn ein NLI-Modell vorhersagt, dass die Verfälschung der Beschriftung widerspricht oder neutral ist, ist das für uns ein Indikator für eine gültige Verfälschung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 212, "src_audio": "/acl6060/audio/en/212.wav", "src_ref": "If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn ein NLI vorhersagt, dass die Verfälschung die Beschriftung beinhaltet, kann es keine gute Verfälschung sein, da sie durch Transitivität eine wahrheitsgemäße Beschreibung des Bildes liefert. Wir filtern diese Verfälschungen heraus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 213, "src_audio": "/acl6060/audio/en/213.wav", "src_ref": "But this procedure is not perfect, it is just an indicator for valid foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieses Verfahren ist aber nicht perfekt, sondern nur ein Indikator für gültige Verfälschungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 214, "src_audio": "/acl6060/audio/en/214.wav", "src_ref": "Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher setzen wir als dritte Maßnahme zur Generierung gültiger Verfälschungen menschliche Annotatoren ein, um die in VALSE verwendeten Daten zu validieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 215, "src_audio": "/acl6060/audio/en/215.wav", "src_ref": "So, after filtering and human evaluation, we have as many test instances as described in this table.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nach dem Filtern und der menschlichen Evaluation haben wir also so viele Testinstanzen, wie in dieser Tabelle beschrieben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 216, "src_audio": "/acl6060/audio/en/216.wav", "src_ref": "Note that VALSE does not deliver any training data but only test data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Beachten Sie hier, dass VALSE keine Trainingsdaten, sondern nur Testdaten liefert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 217, "src_audio": "/acl6060/audio/en/217.wav", "src_ref": "Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da es sich um einen reinen Zero-Shot-Test-Benchmark handelt, ist er so konzipiert, dass er die bestehenden Fähigkeiten der Seh- und Sprachmodelle nach dem Vortraining nutzt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 218, "src_audio": "/acl6060/audio/en/218.wav", "src_ref": "Finetuning would only enable models to exploit artifacts or statistical biases in the data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Feinabstimmung würde es den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 219, "src_audio": "/acl6060/audio/en/219.wav", "src_ref": "And we all know that these models like to cheat and take shortcuts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir alle wissen, dass diese Modells gerne schummeln und Abkürzungen nehmen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 220, "src_audio": "/acl6060/audio/en/220.wav", "src_ref": "And as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wie wir schon erwähnten, sind wir an der Bewertung interessiert, welche Fähigkeiten die Seh- und Sprachmodelle nach dem Vortraining haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 221, "src_audio": "/acl6060/audio/en/221.wav", "src_ref": "We experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit fünf Seh- und Sprachmodellen auf VALSE, nämlich mit CLIP, LXMert, ViLBERT, ViLBERT zwölf in einem und VisualBERT.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 222, "src_audio": "/acl6060/audio/en/222.wav", "src_ref": "Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zwei unserer wichtigsten Evaluationsmetriken sind die Genauigkeit der Modelle bei der Klassifikation von Bild-Satz-Paaren in Bildbeschreibungen und Verfälschungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 223, "src_audio": "/acl6060/audio/en/223.wav", "src_ref": "Perhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielleicht ist es für dieses Video relevanter, unsere tolerantere Metrik vorzustellen, die paarweise Genauigkeit. Diese misst, ob die Bild-Satz-Ausrichtung für das korrekte Bild-Text-Paar größer ist als für sein Verfälschungspaar.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 224, "src_audio": "/acl6060/audio/en/224.wav", "src_ref": "For more metrics and results on them, do check out our paper.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Weitere Metriken und Ergebnisse dazu finden Sie in unserem Paper.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 225, "src_audio": "/acl6060/audio/en/225.wav", "src_ref": "The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Ergebnisse mit paarweiser Genauigkeit werden hier gezeigt und stimmen mit den Ergebnissen überein, die wir von den anderen Metriken erhalten haben, nämlich dass die beste Zero-Shot-Leistung von ViLBERT zwölf zu eins erreicht wird, gefolgt von ViLBERT, LXMert, CLIP und schließlich VisualBERT.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 226, "src_audio": "/acl6060/audio/en/226.wav", "src_ref": "It's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist bemerkenswert, wie Instrumente, die sich auf die einzelnen Objekte konzentrieren, wie Existenz und Substantivphrasen, von ViLBERT zwölf zu eins fast gelöst werden. Das unterstreicht, dass Modelle in der Lage sind, benannte Objekte und ihre Existenz in Bildern zu identifizieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 227, "src_audio": "/acl6060/audio/en/227.wav", "src_ref": "However, none of the remaining pieces can be reliably solved in our adversarial foiling settings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Keines der verbleibenden Teile kann jedoch in unseren gegnerischen Verfälschungseinstellungen zuverlässig gelöst werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 228, "src_audio": "/acl6060/audio/en/228.wav", "src_ref": "We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sehen an den Instrumenten der Pluralität und der Zählung, dass Seh- und Sprachmodelle Schwierigkeiten haben, Verweise auf einzelne und mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 229, "src_audio": "/acl6060/audio/en/229.wav", "src_ref": "The relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Relationen-Teil zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Relation zwischen Objekten in einem Bild richtig zu klassifizieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 230, "src_audio": "/acl6060/audio/en/230.wav", "src_ref": "They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie durch Plausibilitätsbias unterstützt werden, wie wir bei dem Handlung-Teil sehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 231, "src_audio": "/acl6060/audio/en/231.wav", "src_ref": "From the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Beim Koreferenz-Teil geht hervor, dass die Verfolgung mehrerer Verweise auf dasselbe Objekt in einem Bild unter Verwendung von Pronomen auch für die Seh- und Sprachmodelle schwierig ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 232, "src_audio": "/acl6060/audio/en/232.wav", "src_ref": "As a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zur Überprüfung der Korrektheit und weil es ein interessantes Experiment ist, vergleichen wir auch zwei reine Textmodelle, GPT 1 und GPT 2. So wollen wir feststellen, ob VALSE durch diese unimodalen Modelle lösbar ist. Wir berechnen die Perplexität der korrekten und der verfälschten Beschriftung, ohne Bild, um den Eintrag mit der niedrigsten Perplexität vorherzusagen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 233, "src_audio": "/acl6060/audio/en/233.wav", "src_ref": "If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die Perplexität bei der Verfälschung höher ist, ist das für uns ein Hinweis, dass die verfälschte Beschriftung möglicherweise unter Plausibilitätsverzerrungen oder anderen sprachlichen Verzerrungen leidet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 234, "src_audio": "/acl6060/audio/en/234.wav", "src_ref": "And it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist interessant zu sehen, dass in einigen Fällen die GPT-Modelle mit reinem Text die Plausibilität der Welt besser erfasst haben als die Seh- und Sprachmodelle.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 235, "src_audio": "/acl6060/audio/en/235.wav", "src_ref": "So to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zusammenfassend lässt sich sagen, dass VALSE ein Benchmark ist, der durch das Objektiv von linguistischen Konstrukten schaut, um die Gemeinschaft bei der Verbesserung von Seh- und Sprachmodellen zu unterstützen, indem er ihre visuelle Erdung hart testet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 236, "src_audio": "/acl6060/audio/en/236.wav", "src_ref": "Our experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere Experimente zeigen, dass Seh- und Sprachmodelle benannte Objekte und ihre Existenz in Bildern gut erkennen, wie der Existenzteil zeigt. Sie haben aber Schwierigkeiten damit, ihre gegenseitige Abhängigkeit und Beziehungen in visuellen Szenen zu begründen, wenn sie gezwungen sind, sprachliche Indikatoren zu berücksichtigen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 237, "src_audio": "/acl6060/audio/en/237.wav", "src_ref": "We would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir möchten die Community wirklich dazu ermutigen, VALSE für die Messung des Fortschritts von Sprach- Erdung mit Seh- und Sprachmodellen zu nutzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 238, "src_audio": "/acl6060/audio/en/238.wav", "src_ref": "And even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zudem könnte VALSE als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten. So kann kontrolliert werden, ob ein Datensatz dazu beiträgt, dass sich Modelle in einem der von VALSE getesteten Aspekte verbessern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 239, "src_audio": "/acl6060/audio/en/239.wav", "src_ref": "If you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn Sie Interesse haben, schauen Sie sich die VALSE-Daten auf GitHub an. Wenn Sie irgendwelche Fragen haben, zögern Sie nicht, uns zu kontaktieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 240, "src_audio": "/acl6060/audio/en/240.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo, mein Name ist Kamezawa von der Universität Tokio.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 241, "src_audio": "/acl6060/audio/en/241.wav", "src_ref": "I'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde folgendes Paper vorstellen: RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 242, "src_audio": "/acl6060/audio/en/242.wav", "src_ref": "I'll be explaining in this order.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde es in dieser Reihenfolge erklären.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 243, "src_audio": "/acl6060/audio/en/243.wav", "src_ref": "First, I will introduce automatic release note generation that we are working on in this research.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst werde ich die automatische Generierung von Versionshinweisen vorstellen, an der wir in dieser Forschung arbeiten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 244, "src_audio": "/acl6060/audio/en/244.wav", "src_ref": "A release note is a technical document that summarizes the changes distributed with each release of a software product.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein Versionshinweis ist ein technisches Dokument, das die Änderungen zusammenfasst, die mit jeder Version eines Softwareprodukts vertrieben werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 245, "src_audio": "/acl6060/audio/en/245.wav", "src_ref": "The image shows a release note for version two point six point four of the vuejs library.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Bild zeigt einen Versionshinweis für Version 2.6.4 der Vuejs-Bibliothek.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 246, "src_audio": "/acl6060/audio/en/246.wav", "src_ref": "Release notes play an important role in open source development but they're time consuming to prepare manually.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Versionshinweise spielen bei der Open-Source-Entwicklung eine wichtige Rolle, aber ihre Erstellung ist manuell zeitaufwändig.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 247, "src_audio": "/acl6060/audio/en/247.wav", "src_ref": "Therefore, it would be very useful to be able to automatically generate high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher wäre es sehr nützlich, wenn man automatisch qualitativ hochwertige Versionshinweise generieren könnte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 248, "src_audio": "/acl6060/audio/en/248.wav", "src_ref": "I will defer to two previous researches on automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde zwei frühere Untersuchungen zur automatischen Generierung von Versionshinweisen erwähnen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 249, "src_audio": "/acl6060/audio/en/249.wav", "src_ref": "The first is a system called ARENA released in twenty fourteen.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zuerst gibt es ein System mit dem Namen ARENA, das im Jahr 2014 veröffentlicht wurde.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 250, "src_audio": "/acl6060/audio/en/250.wav", "src_ref": "It takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es verfolgt einen regelbasierten Ansatz, indem es zum Beispiel den Change-Extractor verwendet, um alle Unterschiede, Bibliotheksänderungen und Dokumentänderungen aus den verschiedenen Versionen zu extrahieren und sie schließlich zu kombinieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 251, "src_audio": "/acl6060/audio/en/251.wav", "src_ref": "The most notable feature of this system is the issue extractor in the upper right corner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die auffälligste Funktion dieses Systems ist der Ausgabe-Extraktor in der oberen rechten Ecke.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 252, "src_audio": "/acl6060/audio/en/252.wav", "src_ref": "Which must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies muss Jira, dem Fehlerverwaltungssystem, überlassen werden und kann nur auf Projekte angewendet werden, die Jira verwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 253, "src_audio": "/acl6060/audio/en/253.wav", "src_ref": "In other words, it cannot be used for many projects on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mit anderen Worten: Es kann nicht für viele Projekte auf GitHub verwendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 254, "src_audio": "/acl6060/audio/en/254.wav", "src_ref": "The second is Glyph, recently announced in twenty twenty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das zweite ist Glyph, das kürzlich in 2020 angekündigt wurde.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 255, "src_audio": "/acl6060/audio/en/255.wav", "src_ref": "It is available on the internet and can be installed via pip.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist im Internet verfügbar und kann über pip installiert werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 256, "src_audio": "/acl6060/audio/en/256.wav", "src_ref": "This system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieses System verfügt über ein einfaches, lernbasiertes Textklassifikationsmodell und gibt eine von fünf Markierung wie Funktionen oder Fehlerbehebungen für jede eingegebene Commit-Nachricht aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 257, "src_audio": "/acl6060/audio/en/257.wav", "src_ref": "This image is a sample usage that returns a corrective or bug fixes label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieses Bild ist eine Stichprobe, das eine Markierung für eine Korrektur oder Fehlerbehebung zurückgibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 258, "src_audio": "/acl6060/audio/en/258.wav", "src_ref": "Glyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Trainingsdaten von Glyph sind ziemlich klein, etwa 5.000, und werden in den unten beschriebenen Experimenten gezeigt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 259, "src_audio": "/acl6060/audio/en/259.wav", "src_ref": "The performance of the text classification model is not high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Leistung des Textklassifikationsmodells ist nicht hoch.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 260, "src_audio": "/acl6060/audio/en/260.wav", "src_ref": "I present two related researches, but their problems are limited applicability and scarce data resources.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich stelle zwei verwandte Forschungsarbeiten vor, deren Probleme jedoch die begrenzte Anwendbarkeit und die knappen Datenressourcen sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 261, "src_audio": "/acl6060/audio/en/261.wav", "src_ref": "Our paper solves these two problems and automatically generates high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Paper löst diese zwei Probleme und erzeugt automatisch qualitativ hochwertige Versionshinweise.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 262, "src_audio": "/acl6060/audio/en/262.wav", "src_ref": "With a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aufgrund des Problems der begrenzten Anwendbarkeit schlagen wir eine qualitativ hochwertige, klassenweise Zusammenfassungsmethode vor, die nur Commit-Nachrichten als Eingabe verwendet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 263, "src_audio": "/acl6060/audio/en/263.wav", "src_ref": "This proposed method can be used for all English repositories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 264, "src_audio": "/acl6060/audio/en/264.wav", "src_ref": "For the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Für das zweite Problem der knappen Datenressourcen haben wir unseren RNSum-Datensatz erstellt, der aus etwa 82.000 Daten besteht. Diese wurden aus den Daten der öffentlichen GitHub-Repositories mithilfe der GitHub-API gesammelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 265, "src_audio": "/acl6060/audio/en/265.wav", "src_ref": "Next, I'll describe our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Nächstes werde ich unseren Datensatz beschreiben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 266, "src_audio": "/acl6060/audio/en/266.wav", "src_ref": "Here is an example of data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist ein Beispiel für die Daten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 267, "src_audio": "/acl6060/audio/en/267.wav", "src_ref": "The left side is a commit message and the right side is the release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auf der linken Seite ist eine Commit-Nachricht und auf der rechten Seite sind die Versionshinweise.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 268, "src_audio": "/acl6060/audio/en/268.wav", "src_ref": "Release notes are labeled as improvements or fixes, etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Versionshinweise sind als Verbesserungen, Korrekturen usw. markiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 269, "src_audio": "/acl6060/audio/en/269.wav", "src_ref": "We have set up a task that takes the commit messages as input and outputs a labeled release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben eine Aufgabe eingerichtet, die die Commit-Nachricht als Eingabe aufnimmt und die Ausgabe ist ein markierter Versionshinweis.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 270, "src_audio": "/acl6060/audio/en/270.wav", "src_ref": "This can be regarded as a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies kann als eine Zusammenfassungsaufgabe betrachtet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 271, "src_audio": "/acl6060/audio/en/271.wav", "src_ref": "We have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben vier Markierungen vordefiniert: Funktionen, Verbesserungen, Fehlerbehebungen, Beseitigung von Verwerfungen und bahnbrechende Änderungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 272, "src_audio": "/acl6060/audio/en/272.wav", "src_ref": "These were set based on previous research and other factors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese wurden auf der Grundlage von früheren Forschungen und anderen Faktoren festgelegt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 273, "src_audio": "/acl6060/audio/en/273.wav", "src_ref": "The release note on the bottom right is extracted from the release note on the bottom left.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Versionshinweise unten rechts wurde aus den Versionshinweisen unten links extrahiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 274, "src_audio": "/acl6060/audio/en/274.wav", "src_ref": "At this time, it is necessary to detect the four labels that have been set up in advance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zu diesem Zeitpunkt müssen die vier Markierungen, die im Voraus erstellt wurden, erkannt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 275, "src_audio": "/acl6060/audio/en/275.wav", "src_ref": "But the labels are not always consistent with each repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Markierungen sind jedoch nicht immer mit jedem Repository konsistent.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 276, "src_audio": "/acl6060/audio/en/276.wav", "src_ref": "For example, the improvements label includes improvements, enhancements, optimizations, and so on.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Markierung „Verbesserungen“ umfasst beispielsweise Verbesserungen, Erweiterungen, Optimierungen und so weiter.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 277, "src_audio": "/acl6060/audio/en/277.wav", "src_ref": "We prepared a vocabulary list of about thirty labels for each of these notational variations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben eine Wortschatzliste mit etwa 30 Markierungen für jede dieser Notationsvarianten erstellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 278, "src_audio": "/acl6060/audio/en/278.wav", "src_ref": "This is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese dient dazu, die Klasse des Versionshinweises zu erkennen. Sie sammelt den Text des Versionshinweises, der als Versionshinweissatz für die Klasse folgt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 279, "src_audio": "/acl6060/audio/en/279.wav", "src_ref": "Next is a commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Nächstes folgt eine Commit-Nachricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 280, "src_audio": "/acl6060/audio/en/280.wav", "src_ref": "Commit messages are not tied to each release.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Commit-Nachrichten sind nicht an die einzelnen Versionen gebunden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 281, "src_audio": "/acl6060/audio/en/281.wav", "src_ref": "As shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die aktuelle Version 2.5 bis 19 ist, müssen wir, wie im Bild unten gezeigt, die frühere Version 2.5 bis 18 identifizieren und eine Diff erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 282, "src_audio": "/acl6060/audio/en/282.wav", "src_ref": "This is a bit tedious and it is not enough to just get a list of releases and look at the before and after.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies ist etwas mühsam und es reicht nicht aus, nur eine Liste von Versionen zu erstellen und die Vorher-Nachher-Bilanz zu betrachten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 283, "src_audio": "/acl6060/audio/en/283.wav", "src_ref": "We created a heuristic matching rule to get the previous and next versions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben eine heuristische Abgleichsregel erstellt, um die vorherige und die nächste Version zu bekommen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 284, "src_audio": "/acl6060/audio/en/284.wav", "src_ref": "Dataset analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Analyse des Datensatzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 285, "src_audio": "/acl6060/audio/en/285.wav", "src_ref": "In the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Am Ende wurden 7.200 Repositories und 82 Daten gesammelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 286, "src_audio": "/acl6060/audio/en/286.wav", "src_ref": "Also, the average number of release notes tokens is sixty three, which is quite high for a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Außerdem liegt die durchschnittliche Anzahl der Versionshinweis-Token bei 63, was für eine Zusammenfassungsaufgabe recht hoch ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 287, "src_audio": "/acl6060/audio/en/287.wav", "src_ref": "Also, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auch die Anzahl der einzigartigen Token ist mit 830.000 recht groß.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 288, "src_audio": "/acl6060/audio/en/288.wav", "src_ref": "This is due to the large number of unique class or method names found in the repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies ist auf die große Anzahl von eindeutigen Klassen- oder Methodennamen im Repository zurückzuführen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 289, "src_audio": "/acl6060/audio/en/289.wav", "src_ref": "Next, I will explain the proposed method.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Nächstes werde ich die vorgeschlagene Methode erläutern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 290, "src_audio": "/acl6060/audio/en/290.wav", "src_ref": "The classwise extractive then abstractive summarization model consists of two neural modules.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das klassenweise extraktive und dann abstrahierende Zusammenfassungsmodell besteht aus zwei neuronalen Modulen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 291, "src_audio": "/acl6060/audio/en/291.wav", "src_ref": "A classifier using BERT or CodeBERT and a generator using BART.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein Klassifikator verwendet BERT oder CodeBERT und ein Generator verwendet BART.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 292, "src_audio": "/acl6060/audio/en/292.wav", "src_ref": "First, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zuerst verwendet CEAS einen Klassifikator, um jede Commit-Nachricht in fünf Klassen von Versionshinweisen zu klassifizieren, die Verbesserungen, Fehlerbehebungen, Verwerfungen und eine weitere Klasse enthalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 293, "src_audio": "/acl6060/audio/en/293.wav", "src_ref": "The commit messages classified as other are discarded.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die als „Sonstiges“ eingestuften Commit-Nachrichten werden verworfen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 294, "src_audio": "/acl6060/audio/en/294.wav", "src_ref": "Then CEAS applies the generator to the four labeled documents independently and generates release notes for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann wendet CEAS den Generator auf die vier markierten Dokumente unabhängig voneinander an und erstellt Versionshinweise für jede Klasse.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 295, "src_audio": "/acl6060/audio/en/295.wav", "src_ref": "In this task, the direct correspondences between commit messages and release notes are not known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In dieser Aufgabe sind die direkten Korrespondenzen zwischen Commit-Nachrichten und Versionshinweisen nicht bekannt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 296, "src_audio": "/acl6060/audio/en/296.wav", "src_ref": "Therefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um den Klassifikator zu trainieren, haben wir daher jeder eingegebenen Commit-Nachricht eine Umfrage zugewiesen und dabei die ersten zehn Zeichen jeder Commit-Nachricht verwendet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 297, "src_audio": "/acl6060/audio/en/297.wav", "src_ref": "We modeled the classwise abstractive summarization approach by two different methods.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben den klassenweisen abstrakten Zusammenfassungsansatz durch zwei verschiedene Methoden modelliert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 298, "src_audio": "/acl6060/audio/en/298.wav", "src_ref": "The first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das erste Modell, das wir CAS-Single nennen, besteht aus einem einzigen 6-zu-6-Netzwerk und generiert einen einzigen Versionshinweistext, der eine Verkettung von eingegebenen Commit-Nachrichten ergibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 299, "src_audio": "/acl6060/audio/en/299.wav", "src_ref": "The output texts can be divided into classwise segments based on special class-specific endpoint symbols.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die ausgegebenen Texte können auf der Grundlage spezieller klassenspezifischer Endpunktsymbole in klassenweise Segmente unterteilt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 300, "src_audio": "/acl6060/audio/en/300.wav", "src_ref": "The second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die zweite Methode, die wir CAS-Multi nennen, besteht aus vier verschiedenen seq2seq-Netzwerken, von denen jedes einer der festen Versionshinweisklassen entspricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 301, "src_audio": "/acl6060/audio/en/301.wav", "src_ref": "Okay, let me explain the experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde nun die Experimente erklären.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 302, "src_audio": "/acl6060/audio/en/302.wav", "src_ref": "Five methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Fünf Methoden wurden verglichen: CEAS, CAS-Single, CAS-Multi, Clustering und Glyph aus einer früheren Studie.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 303, "src_audio": "/acl6060/audio/en/303.wav", "src_ref": "Regarding evaluation, in some cases, release notes are output in multiple sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Was die Evaluation betrifft, so werden in einigen Fällen die Versionshinweise in mehreren Sätzen ausgegeben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 304, "src_audio": "/acl6060/audio/en/304.wav", "src_ref": "Since it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da es schwierig ist, die Anzahl von Sätzen zu berechnen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 305, "src_audio": "/acl6060/audio/en/305.wav", "src_ref": "The BLEU is penalized when the system outputs a short sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der BLEU wird benachteiligt, wenn das System einen kurzen Satz ausgibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 306, "src_audio": "/acl6060/audio/en/306.wav", "src_ref": "This penalty results in a lower BLEU value in the experiment results described next.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieser Abzug führt zu einem niedrigeren BLEU-Wert in den nachfolgend beschriebenen Versuchsergebnissen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 307, "src_audio": "/acl6060/audio/en/307.wav", "src_ref": "Finally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich berechnen wir auch die Spezifität, da ROUGE und BLEU nicht berechnet werden können, wenn die Versionshinweise leer sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 308, "src_audio": "/acl6060/audio/en/308.wav", "src_ref": "A higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eine höhere Spezifität bedeutet, dass das Modell in Fällen, in denen die Versionshinweise leer sind, einen leeren Text korrekt ausgibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 309, "src_audio": "/acl6060/audio/en/309.wav", "src_ref": "Here are the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind die Resultate.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 310, "src_audio": "/acl6060/audio/en/310.wav", "src_ref": "Since the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da der Datensatz E-Mail-Adressen, Hash-Werte usw. enthält, haben wir auch den bereinigten Datensatz ausgewertet, der diese ausschließt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 311, "src_audio": "/acl6060/audio/en/311.wav", "src_ref": "CEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CEAS und CAS erreichten ROUGE-L-Werte, die mehr als zehn Punkte über den Baselines lagen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 312, "src_audio": "/acl6060/audio/en/312.wav", "src_ref": "In particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insbesondere beim bereinigten Testsatz ist der Abstand zwischen der vorgeschlagenen Methode und den Baselines auf mehr als zwanzig Punkte angestiegen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 313, "src_audio": "/acl6060/audio/en/313.wav", "src_ref": "These results indicate that CEAS and CAS are significantly affected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Ergebnisse zeigen, dass CEAS und CAS signifikant betroffen sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 314, "src_audio": "/acl6060/audio/en/314.wav", "src_ref": "CEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CEAS erzielte bessere ROUGE-L-Werte als CAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators beim Training des Klassifikators mit Pseudo-Markierungen effektiv ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 315, "src_audio": "/acl6060/audio/en/315.wav", "src_ref": "High coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eine hohe Abdeckung von CEAS kann wahrscheinlich erreicht werden, weil sich der Klassifikator auf die Auswahl relevanter Commit-Nachrichten für jede Klasse konzentrieren kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 316, "src_audio": "/acl6060/audio/en/316.wav", "src_ref": "CAS-Multi tended to yield higher ROUGE-L than CAS-Single.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CAS-Multi ergab tendenziell mehr ROUGE-L-Werte als CAS-Single.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 317, "src_audio": "/acl6060/audio/en/317.wav", "src_ref": "Suggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir schlagen vor, dass es auch effektiv ist, unterschiedliche abstrakte Zusammenfassungsmodelle, die unabhängig voneinander sind, für jede Klasse von den Versionshinweisen zu entwickeln.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 318, "src_audio": "/acl6060/audio/en/318.wav", "src_ref": "Here are an error analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist eine Fehleranalyse.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 319, "src_audio": "/acl6060/audio/en/319.wav", "src_ref": "CAS methods tend to output shorter sentences than human reference sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CAS-Methoden geben tendenziell kürzere Sätze aus als menschliche Referenzsätze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 320, "src_audio": "/acl6060/audio/en/320.wav", "src_ref": "In the figure on the right, the reference sentence has three or four sentences, while CAS has only one.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In der Abbildung rechts besteht der Referenzsatz aus drei oder vier Sätzen, während CAS aus nur einem besteht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 321, "src_audio": "/acl6060/audio/en/321.wav", "src_ref": "The reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Grund für die Reluktanz dieses Modells ist, dass in den Trainingsdaten nur 33 Prozent der Sätze in der Markierung Funktionen und 40 Prozent in der Markierung Verbesserung zu finden sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 322, "src_audio": "/acl6060/audio/en/322.wav", "src_ref": "Furthermore, CAS methods cannot generate accurate release notes without additional information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus können CAS-Methoden ohne zusätzliche Informationen keine genauen Versionshinweise erstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 323, "src_audio": "/acl6060/audio/en/323.wav", "src_ref": "The top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Oben auf der rechten Seite ist ein Beispiel für eine sehr unübersichtliche Commit-Nachricht. Der vollständige Satz kann nicht ohne Bezug auf den entsprechenden Fortschritt oder das Problem generiert werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 324, "src_audio": "/acl6060/audio/en/324.wav", "src_ref": "The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das folgende Beispiel zeigt, dass die beiden Commit-Nachrichten in der Eingabe zusammenhängen und zu einem Satz zusammengefasst werden sollten, was jedoch nicht gelingt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 325, "src_audio": "/acl6060/audio/en/325.wav", "src_ref": "Finally, a conclusion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nun kommen wir zum Fazit.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 326, "src_audio": "/acl6060/audio/en/326.wav", "src_ref": "We have built a new dataset for automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben einen neuen Datensatz für die automatische Generierung von Versionshinweisen erstellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 327, "src_audio": "/acl6060/audio/en/327.wav", "src_ref": "We have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben auch eine Aufgabe für die Eingabe von Commit-Nachrichten und deren Zusammenfassung formuliert, sodass sie für alle auf Englisch geschriebene Projekte anwendbar ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 328, "src_audio": "/acl6060/audio/en/328.wav", "src_ref": "Our experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere Experimente zeigen, dass die vorgeschlagene Methode weniger verrauschte Versionshinweise mit höherem Abdeckungsgrad als die Baselines erzeugt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 329, "src_audio": "/acl6060/audio/en/329.wav", "src_ref": "Please check out our dataset on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bitte sehen Sie sich unseren Datensatz auf GitHub an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 330, "src_audio": "/acl6060/audio/en/330.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 331, "src_audio": "/acl6060/audio/en/331.wav", "src_ref": "Hello. My name is Asaf Harari.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo! Mein Name ist Asaf Harari.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 332, "src_audio": "/acl6060/audio/en/332.wav", "src_ref": "And I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde unser Paper vorstellen: Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 333, "src_audio": "/acl6060/audio/en/333.wav", "src_ref": "Data scientists analyze data and mainly focus on the manipulating the data's existing features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Datenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der bestehenden Funktionen von Daten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 334, "src_audio": "/acl6060/audio/en/334.wav", "src_ref": "But sometimes, these features are limited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber manchmal sind diese Funktionen begrenzt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 335, "src_audio": "/acl6060/audio/en/335.wav", "src_ref": "Feature generation using another data source may add substantial information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Generierung von Funktionen unter Verwendung einer anderen Datenquelle kann wesentliche Informationen hinzufügen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 336, "src_audio": "/acl6060/audio/en/336.wav", "src_ref": "Our research goal is automatic tabular data enrichment using external sources' free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Forschungsziel ist die automatische Anreicherung tabellarischer Daten mit freiem Text aus externen Quellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 337, "src_audio": "/acl6060/audio/en/337.wav", "src_ref": "Assume we have a tabular dataset and a knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Angenommen, wir haben einen tabellarischen Datensatz und eine Wissensbasis.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 338, "src_audio": "/acl6060/audio/en/338.wav", "src_ref": "We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir brauchen einen automatischen Prozess, der Entity Linking und eine Textanalyse umfasst, um neue Funktionen aus dem freien Text der Wissensbasis zu extrahieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 339, "src_audio": "/acl6060/audio/en/339.wav", "src_ref": "Our framework FeSTE is exactly this automatic process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Rahmen FeSTE ist genau dieser automatische Prozess.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 340, "src_audio": "/acl6060/audio/en/340.wav", "src_ref": "So let's see an example in a dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sehen wir uns also ein Beispiel in einem Datensatz an, der in FeSTE eingespeist wird.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 341, "src_audio": "/acl6060/audio/en/341.wav", "src_ref": "In this example, the dataset is university dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel ist der Datensatz der Universitätsdatensatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 342, "src_audio": "/acl6060/audio/en/342.wav", "src_ref": "When its goal is to classify universities into low ranking universities and high-ranking universities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Ziel besteht darin, die Universitäten nach niedrigem und hohem Rang einzuteilen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 343, "src_audio": "/acl6060/audio/en/343.wav", "src_ref": "As knowledge base, we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Wissensbasis verwenden wir Wikipedia.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 344, "src_audio": "/acl6060/audio/en/344.wav", "src_ref": "The first phase of FeSTE is entity linking.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die erste Phase von FeSTE ist Entity Linking.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 345, "src_audio": "/acl6060/audio/en/345.wav", "src_ref": "When each entity, in this example the university name, is linked to an entity within the knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jede Entität, in diesem Beispiel der Name der Universität, ist mit einer Entität innerhalb der Wissensbasis verknüpft.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 346, "src_audio": "/acl6060/audio/en/346.wav", "src_ref": "And and the text of the entities of the knowledge base is extracted and added to the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Text der Entitäten aus der Wissensbasis wird extrahiert und dem Datensatz hinzugefügt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 347, "src_audio": "/acl6060/audio/en/347.wav", "src_ref": "In this example, the text is the Wikipedia page's abstract.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel ist der Text das Abstract der Wikipedia-Seite.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 348, "src_audio": "/acl6060/audio/en/348.wav", "src_ref": "Now, we need to generate or extract features from the retrieved text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nun müssen wir Funktionen aus dem abgerufenen Text generieren oder extrahieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 349, "src_audio": "/acl6060/audio/en/349.wav", "src_ref": "So, we need to ah feature extraction phase ah which includes text analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir müssen also eine Phase der Funktionsextraktion einleiten, zu der die Textanalyse gehört.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 350, "src_audio": "/acl6060/audio/en/350.wav", "src_ref": "And this is the main novelty of this paper and I will deep dive into it in the next slides.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das ist die wichtigste Neuerung dieses Papers, auf die ich auf den nächsten Folien näher eingehen werde.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 351, "src_audio": "/acl6060/audio/en/351.wav", "src_ref": "After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nach der Phase der Funktionsextraktion folgt eine Phase der Funktionsgenerierung, in der wir die extrahierten Funktionen verwenden, um eine kleine Anzahl neuer Funktionen zu erzeugen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 352, "src_audio": "/acl6060/audio/en/352.wav", "src_ref": "First generate ah features in the number of classes of the original dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst werden Funktionen in der Anzahl der Klassen des ursprünglichen Datensatzes generiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 353, "src_audio": "/acl6060/audio/en/353.wav", "src_ref": "In this example, the original dataset has two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 354, "src_audio": "/acl6060/audio/en/354.wav", "src_ref": "So, FeSTE generates two new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "FeSTE generiert also zwei neue Funktionen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 355, "src_audio": "/acl6060/audio/en/355.wav", "src_ref": "But if the dataset has five classes, FeSTE generates five new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn der Datensatz jedoch fünf Klassen hat, generiert FeSTE fünf neue Funktionen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 356, "src_audio": "/acl6060/audio/en/356.wav", "src_ref": "Each feature represents the likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jede Funktion stellt die Wahrscheinlichkeit für jede Klasse dar.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 357, "src_audio": "/acl6060/audio/en/357.wav", "src_ref": "To analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um den Text zu analysieren, verwenden wir die aktuellste Textanalyse, die auf Transformer-basierten Sprachmodellen wie BERT, GPT, XLNet und anderen beruht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 358, "src_audio": "/acl6060/audio/en/358.wav", "src_ref": "It is but it is not likely that we can train language models using the input datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist aber unwahrscheinlich, dass wir Sprachmodelle mit den eingegebenen Datensätzen trainieren können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 359, "src_audio": "/acl6060/audio/en/359.wav", "src_ref": "So a naive approach will be ah target task finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein naiver Ansatz ist also eine Feinabstimmung des Zieldatensatzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 360, "src_audio": "/acl6060/audio/en/360.wav", "src_ref": "So, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In der Phase der Funktionsextraktion können wir also vortrainierte Sprachmodelle herunterladen und das Sprachmodell über den Zieldatensatz feinabstimmen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 361, "src_audio": "/acl6060/audio/en/361.wav", "src_ref": "In this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel zur Feinabstimmung des Sprachmodells klassifizieren wir den Text in Klassen, d. h. wir klassifizieren das Abstract in die Klassen niedrig oder hoch.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 362, "src_audio": "/acl6060/audio/en/362.wav", "src_ref": "Receive the language model output, which is the likelihood for each class and use as new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir erhalten die Ausgabe des Sprachmodells, die die Wahrscheinlichkeit für jede Klasse darstellt, und verwenden sie als neue Funktionen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 363, "src_audio": "/acl6060/audio/en/363.wav", "src_ref": "The problem with this approach is datasets may have few distinct entities / texts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Problem bei diesem Ansatz ist, dass Datensätze möglicherweise nur wenige unterschiedliche Entitäten/Texte beinhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 364, "src_audio": "/acl6060/audio/en/364.wav", "src_ref": "In our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem Experiment enthält fast die Hälfte der Datensätze weniger als 400 Stichproben und der kleinste Datensatz enthält 35 Stichproben in einem Trainingssatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 365, "src_audio": "/acl6060/audio/en/365.wav", "src_ref": "So to finetune a language model over ah this dataset will be ineffective.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Feinabstimmung eines Sprachmodells über diesen Datensatz ist also unwirksam.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 366, "src_audio": "/acl6060/audio/en/366.wav", "src_ref": "But we can use prior knowledge about pre-analyzed datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wir können vorherige Kenntnisse über bereits analysierte Datensätze nutzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 367, "src_audio": "/acl6060/audio/en/367.wav", "src_ref": "Because FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da wir FeSTE auf mehrere Datensätze anwenden, können wir die „n minus 1“-Datensätze verwenden, um Informationen über die „n minus 1“-Datensätze zu sammeln, und diese Informationen verwenden, wenn wir den n-ten Datensatz analysieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 368, "src_audio": "/acl6060/audio/en/368.wav", "src_ref": "What we, what we suggest is to add, to add another finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir schlagen vor, eine weitere Feinabstimmungsphase hinzuzufügen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 369, "src_audio": "/acl6060/audio/en/369.wav", "src_ref": "A preliminary multitask finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das war eine vorläufige Multitask-Feinabstimmungsphase.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 370, "src_audio": "/acl6060/audio/en/370.wav", "src_ref": "When you finetune the language model over the n minus one datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Feinabstimmung des Sprachmodells geht über die „n minus 1“-Datensätze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 371, "src_audio": "/acl6060/audio/en/371.wav", "src_ref": "And, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann führen wir eine weitere Feinabstimmungsphase durch, die eine Feinabstimmung des Zieldatensatzes ist, wenn wir das Sprachmodell über den n-ten Zieldatensatz feinabstimmen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 372, "src_audio": "/acl6060/audio/en/372.wav", "src_ref": "The state-of-the-art in multitask ah multitask finetuning called MTDNN.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die modernste Multitask-Feinabstimmung wird MTDNN genannt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 373, "src_audio": "/acl6060/audio/en/373.wav", "src_ref": "In MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei der MTDNN bleiben die Überschriften in der Reihe der Aufgaben im Trainingssatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 374, "src_audio": "/acl6060/audio/en/374.wav", "src_ref": "So, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel gibt es also vier Aufgaben im Trainingssatz. Das MTDNN enthält also vier Überschriften, wie Sie auf dem Bild sehen können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 375, "src_audio": "/acl6060/audio/en/375.wav", "src_ref": "And it samples a random batch from ah from the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es nimmt sich eine Zufallsstichprobe aus dem Trainingssatz heraus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 376, "src_audio": "/acl6060/audio/en/376.wav", "src_ref": "And if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die Zufallsstichprobe zum Beispiel zu einer einzelnen Satzklassifikationsaufgabe gehört, führt sie Vorwärts- und Rückwärtspfade durch die erste Überschrift aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 377, "src_audio": "/acl6060/audio/en/377.wav", "src_ref": "And if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die Zufallsstichprobe zur paarweisen Ranking-Aufgabe gehört, führt sie den Vorwärts- und Rückwärtspfad über die letzte Überschrift aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 378, "src_audio": "/acl6060/audio/en/378.wav", "src_ref": "In our scenario, ah tabular datasets vary in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem Szenario variieren tabellarische Datensätze in der Anzahl der Klassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 379, "src_audio": "/acl6060/audio/en/379.wav", "src_ref": "So there are many tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es gibt also viele Aufgaben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 380, "src_audio": "/acl6060/audio/en/380.wav", "src_ref": "MTDNN maintained number of classes, heads, output layers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "MTDNN enthält eine Anzahl von Klassen, Überschriften und ausgegebenen Ebenen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 381, "src_audio": "/acl6060/audio/en/381.wav", "src_ref": "And the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und zusätzlich muss MTDNN neue Überschriften für einen neuen Datensatz mit einer neuen Aufgabe initialisieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 382, "src_audio": "/acl6060/audio/en/382.wav", "src_ref": "Our approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Ansatz, genannt Feinabstimmungsreformulierungsaufgabe, besteht darin, dass wir, anstatt mehrere Überschriften beizubehalten, jeden Datensatz in einen Satz pro Klassifikationssproblem umformulieren, was zwei Klassenaufgaben entspricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 383, "src_audio": "/acl6060/audio/en/383.wav", "src_ref": "So let's see an example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schauen wir uns ein Beispiel an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 384, "src_audio": "/acl6060/audio/en/384.wav", "src_ref": "Here is the our input dataset which consists of entities, features, text and classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist unser eingegebener Datensatz, der aus Entitäten, Funktionen, Text und Klassen besteht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 385, "src_audio": "/acl6060/audio/en/385.wav", "src_ref": "And, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir formulieren die Aufgabe von einer Klassifizierung des Textes in niedrig oder hoch zu einer Klassifizierung des Textes, des Abstracts und der Klasse in wahr oder falsch um.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 386, "src_audio": "/acl6060/audio/en/386.wav", "src_ref": "Or in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mit anderen Worten: Wir haben das Sprachmodell so trainiert, dass es ein Abstract und die Klasse als Abstract und die Klasse klassifiziert, wenn das Abstract zur Klasse gehört oder nicht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 387, "src_audio": "/acl6060/audio/en/387.wav", "src_ref": "So the label vector in this case stays always ah which consists always with two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "So bleibt die Vektor-Markierung in diesem Fall bestehen, die immer aus zwei Klassen besteht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 388, "src_audio": "/acl6060/audio/en/388.wav", "src_ref": "And this is the ah algorithm for our fine, reformulated finetuning approach.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das ist der Algorithmus für unseren neu formulierten Finabstimmungsansatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 389, "src_audio": "/acl6060/audio/en/389.wav", "src_ref": "So let's see the full framework.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schauen wir uns also den gesamten Rahmen an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 390, "src_audio": "/acl6060/audio/en/390.wav", "src_ref": "Dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Datensatz wird in FeSTE eingespeist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 391, "src_audio": "/acl6060/audio/en/391.wav", "src_ref": "And then ah FeSTE executes entity linking phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann führt FeSTE die Entity-Linking-Phase aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 392, "src_audio": "/acl6060/audio/en/392.wav", "src_ref": "It ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es extrahiert den Text aus der Wissensbasis, die in diesem Beispiel das Abstract der Wikipedia-Seite ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 393, "src_audio": "/acl6060/audio/en/393.wav", "src_ref": "Then it reformulated the task into a pairwise sentence classification task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann formulierte es die Aufgabe in eine paarweise Satzklassifikationsaufgabe um.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 394, "src_audio": "/acl6060/audio/en/394.wav", "src_ref": "Applied the language model to the new task and the output likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Sprachmodell wird auf die neue Aufgabe und die Ausgabewahrscheinlichkeit für jede Klasse angewandt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 395, "src_audio": "/acl6060/audio/en/395.wav", "src_ref": "And now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nun ist das Sprachmodell bereits über den „n minus 1“-Datensatz mithilfe einer vorläufigen Multitask-Feinabstimmung feinabgestimmt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 396, "src_audio": "/acl6060/audio/en/396.wav", "src_ref": "Then we use the output vector of the language model as a newly generated feature in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann verwenden wir den Ausgabe-Vektor des Sprachmodells als eine neu generierte Funktion in der Anzahl der Klassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 397, "src_audio": "/acl6060/audio/en/397.wav", "src_ref": "To evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zur Evaluation unseres Rahmens verwenden wir 17 tabellarische Klassifikationsdatensätze, die sich in Größe, Funktionen, Ausgewogenheit, Domäne und anfänglicher Leistung unterscheiden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 398, "src_audio": "/acl6060/audio/en/398.wav", "src_ref": "And as knowledge base we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Wissensbasis verwenden wir Wikipedia.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 399, "src_audio": "/acl6060/audio/en/399.wav", "src_ref": "We design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir konzipieren unser Experiment als Leave-One-Out-Evaluation, bei dem wir FeSTe mit 16 Datensätzen trainieren und auf den 17. Datensatz anwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 400, "src_audio": "/acl6060/audio/en/400.wav", "src_ref": "We also, we also split each dataset into four folds and apply four folds cross validation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Außerdem teilen wir jeden Datensatz in vier Brüche auf und wenden eine vierfache Kreuzvalidierung an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 401, "src_audio": "/acl6060/audio/en/401.wav", "src_ref": "Then, we generate the new features and evaluate them using five evaluation classifiers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann generieren wir die neuen Funktionen und bewerten sie mit fünf Evaluationssklassifikatoren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 402, "src_audio": "/acl6060/audio/en/402.wav", "src_ref": "We use in our experiments base BERT base architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir verwenden in unseren Experimenten die Basisarchitektur BERT.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 403, "src_audio": "/acl6060/audio/en/403.wav", "src_ref": "Here are the results for our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind die Ergebnisse unserer Experimente.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 404, "src_audio": "/acl6060/audio/en/404.wav", "src_ref": "You can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie können sehen, dass wir unseren Rahmen mit der Feinabstimmung des Zieldatensatzes, mit der Feinabstimmung der Zielaufgabe und einer vorläufigen MTDNN-Feinabstimmung vergleichen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 405, "src_audio": "/acl6060/audio/en/405.wav", "src_ref": "And our reformulated finetuning achieves the best result, the best performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere neu formulierte Feinabstimmung erreicht das beste Ergebnis und die beste Leistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 406, "src_audio": "/acl6060/audio/en/406.wav", "src_ref": "While MTDNN achieved two percent improvement over the target dataset finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "MTDNN erreichte zwei Prozent Verbesserung gegenüber der Feinabstimmung des Zieldatensatzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 407, "src_audio": "/acl6060/audio/en/407.wav", "src_ref": "Our approach achieved six percent improvement.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mit unserem Ansatz haben wir eine Verbesserung von sechs Prozent erreicht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 408, "src_audio": "/acl6060/audio/en/408.wav", "src_ref": "When we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir uns den kleinen Datensatz anschauen, sehen wir, dass die Leistung von MTDNN abnimmt und die Verbesserung der vorläufigen Multitask-Feinabstimmungsphase auf 1,5 Prozent sinkt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 409, "src_audio": "/acl6060/audio/en/409.wav", "src_ref": "But our performance increased to eleven percent compared to the target task finetuning alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber unsere Leistung stieg auf elf Prozent im Vergleich zur Feinabstimmung des Zieldatensatzes allein.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 410, "src_audio": "/acl6060/audio/en/410.wav", "src_ref": "For summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Für die Summierung ermöglicht FeSTE in unseren Experimenten eine Few-Shot-Anreicherung aus 35 Proben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 411, "src_audio": "/acl6060/audio/en/411.wav", "src_ref": "It uses one architecture for all tasks and datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es verwendet eine Architektur für alle Aufgaben und Datensätze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 412, "src_audio": "/acl6060/audio/en/412.wav", "src_ref": "And it keeps the head of ah of the model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und es behält den Hauptteil des Modells.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 413, "src_audio": "/acl6060/audio/en/413.wav", "src_ref": "But it adds reformulation phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber es fügt eine Reformulierungsphase hinzu.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 414, "src_audio": "/acl6060/audio/en/414.wav", "src_ref": "It augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es erweitert den Trainingssatz und benötigt einen Zielwert mit semantischer Bedeutung, damit wir ihn in das Sprachmodell einspeisen und für das Satzpaar-Klassifikationsproblem verwenden können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 415, "src_audio": "/acl6060/audio/en/415.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 0, "src_audio": "/acl6060/audio/en/0.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo zusammen. Heute werde ich unsere Forschungsarbeit Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction vorstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 1, "src_audio": "/acl6060/audio/en/1.wav", "src_ref": "I'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich bin Allan vom ByteDance AI Lab, und dies ist eine gemeinsame Arbeit mit Jierui Li von der University of Texas in Austin und Wei Lu von SUTD.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 2, "src_audio": "/acl6060/audio/en/2.wav", "src_ref": "First, I'd like to talk about our motivation for reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst möchte ich über unsere Motivation für das Argumentieren sprechen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 3, "src_audio": "/acl6060/audio/en/3.wav", "src_ref": "So here we show an examples where multi-step reasoning is helpful.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deshalb zeigen wir hier ein Beispiel, bei dem mehrstufiges Argumentieren hilfreich ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 4, "src_audio": "/acl6060/audio/en/4.wav", "src_ref": "So this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Abbildung stammt aus dem PaLM-Paper, in dem sie Prompting durchführen, um das Netzwerkproblem in einem Few-Shot-Lernszenario zu lösen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 5, "src_audio": "/acl6060/audio/en/5.wav", "src_ref": "So on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auf der linken Seite können wir also sehen, dass wenn einige Beispiele nur aus Frage und Antworten bestehen, dann bekommen wir nicht die richtigen Antworten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 6, "src_audio": "/acl6060/audio/en/6.wav", "src_ref": "But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir jedoch eine weitere Argumentationsbeschreibung hinzufügen, ist das Modell in der Lage, die Argumentationsbeschreibung vorherzusagen und hier auch eine korrekte Vorhersage zu treffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 7, "src_audio": "/acl6060/audio/en/7.wav", "src_ref": "So it is good to have interpretable multi-step reasoning as output.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist also gut, eine interpretierbare mehrstufige Argumentation als Ergebnis zu haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 8, "src_audio": "/acl6060/audio/en/8.wav", "src_ref": "And we also think math word problem is a straightforward application to evaluate such reasoning abilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir denken auch, dass mathematische Wortprobleme eine einfache Anwendung sind, um solche Argumentationsfähigkeiten zu bewerten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 9, "src_audio": "/acl6060/audio/en/9.wav", "src_ref": "So, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem Problem-Setup müssen wir also angesichts der Fragen diese Frage lösen und die numerischen Antworten erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 10, "src_audio": "/acl6060/audio/en/10.wav", "src_ref": "So in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unseren Datensätzen finden wir auch den mathematischen Ausdruck, der uns zu dieser bestimmten Antwort führt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 11, "src_audio": "/acl6060/audio/en/11.wav", "src_ref": "So, certain assumptions ah also apply as in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es gelten also auch hier bestimmte Annahmen wie in früheren Arbeiten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 12, "src_audio": "/acl6060/audio/en/12.wav", "src_ref": "We assume the precision of quantities are known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir gehen davon aus, dass die Genauigkeit der Mengen bekannt ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 13, "src_audio": "/acl6060/audio/en/13.wav", "src_ref": "And we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und wir betrachten nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponentialrechnung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 14, "src_audio": "/acl6060/audio/en/14.wav", "src_ref": "Furthermore, complicated operators can be actually decomposed into these basic operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus können komplizierte Operatoren in diese Grundoperatoren zerlegt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 15, "src_audio": "/acl6060/audio/en/15.wav", "src_ref": "So, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die früheren Arbeiten zum Lösen von mathematischen Wortproblemen lassen sich also in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modell einteilen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 16, "src_audio": "/acl6060/audio/en/16.wav", "src_ref": "So, traditional sequence to sequence model convert the expression to a specific sequence for generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das traditionelle Sequenz-zu-Sequenz-Modell wandelt also den Ausdruck in eine spezifische Sequenz für die Generierung um.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 17, "src_audio": "/acl6060/audio/en/17.wav", "src_ref": "And it is pretty easy to implement and it can generalize to many different complicated problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist ziemlich einfach zu implementieren und kann für viele verschiedene komplizierte Probleme verallgemeinert werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 18, "src_audio": "/acl6060/audio/en/18.wav", "src_ref": "But the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Nachteile sind jedoch, dass die Leistung im Allgemeinen nicht besser ist als die des strukturierten Modells und dass es an Interpretierbarkeit für Vorhersagen mangelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 19, "src_audio": "/acl6060/audio/en/19.wav", "src_ref": "But actually this direction is still quite popular because of um the transformer model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber eigentlich ist diese Richtung immer noch recht beliebt wegen des Transformermodells.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 20, "src_audio": "/acl6060/audio/en/20.wav", "src_ref": "So, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In baumbasierten Modellen strukturieren wir diese Ausdrücke in Baumform und folgen einem geordneten Traversal in Baumgenerationen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 21, "src_audio": "/acl6060/audio/en/21.wav", "src_ref": "So here we keep generating the operators until we reach the leaves, which are the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier fahren wir also fort, die Operatoren zu generieren, bis wir die Blätter erreichen, die die Mengen darstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 22, "src_audio": "/acl6060/audio/en/22.wav", "src_ref": "So here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Gute daran ist, dass wir dadurch diese binäre Baumstruktur erhalten. Sie ist eigentlich ziemlich kontraintuitiv, weil wir zuerst den Operator und dann am Ende die Mengen generieren müssen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 23, "src_audio": "/acl6060/audio/en/23.wav", "src_ref": "And the second thing is that it also contains some repetitive computations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zudem enthält sie auch einige sich wiederholende Berechnungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 24, "src_audio": "/acl6060/audio/en/24.wav", "src_ref": "So here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir uns also diesen Ausdruck ansehen, wird 8 x 3 + 3 eigentlich zweimal generiert, aber eigentlich sollten wir die Ergebnisse wiederverwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 25, "src_audio": "/acl6060/audio/en/25.wav", "src_ref": "So, in our proposed approach we want to solve those problems in a step by step and interpretable manners.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem vorgeschlagenen Ansatz wollen wir diese Probleme schrittweise und auf interpretierbare Art und Weise lösen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 26, "src_audio": "/acl6060/audio/en/26.wav", "src_ref": "So for example, here in the second step, ah we can obtain these divisors which is twenty seven.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier im zweiten Schritt zum Beispiel können wir diese Teiler erhalten, was 27 ergibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 27, "src_audio": "/acl6060/audio/en/27.wav", "src_ref": "And we can also refer back to the original questions to find the relevant contents.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können auch auf die ursprünglichen Fragen zurückgreifen, um die relevanten Inhalte zu finden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 28, "src_audio": "/acl6060/audio/en/28.wav", "src_ref": "And in these steps we obtain the divisors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesen Schritten erhalten wir die Teiler.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 29, "src_audio": "/acl6060/audio/en/29.wav", "src_ref": "So, ah and then at this third step we actually get the quotient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und in diesem dritten Schritt erhalten wir dann den Quotienten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 30, "src_audio": "/acl6060/audio/en/30.wav", "src_ref": "Alright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Okay. Nach diesen drei Schritten können wir die Ergebnisse des zweiten Schritts wiederverwenden, dann die Ergebnisse des vierten Schritts erhalten und schließlich die Dividenden erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 31, "src_audio": "/acl6060/audio/en/31.wav", "src_ref": "So, here we actually generate the whole expression directly rather than generating a single operators or quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier wird also der gesamte Ausdruck direkt generiert und nicht nur ein einzelner Operator oder eine Menge.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 32, "src_audio": "/acl6060/audio/en/32.wav", "src_ref": "So this makes the process more accurate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dadurch wird der Prozess genauer.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 33, "src_audio": "/acl6060/audio/en/33.wav", "src_ref": "So, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem deduktiven System beginnen wir also zunächst mit einer Reihe von Mengen, die in den Fragen vorgestellt werden, und schließen auch mit einer Konstante als Anfangszustand.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 34, "src_audio": "/acl6060/audio/en/34.wav", "src_ref": "So, the expression is represented by e i j o p.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Ausdruck wird also durch e i j o p dargestellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 35, "src_audio": "/acl6060/audio/en/35.wav", "src_ref": "Where we perform operator from q_i to q_j, and such expression is actually directed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir führen den Operator von q_i nach q_j aus, und dieser Ausdruck ist tatsächlich geregelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 36, "src_audio": "/acl6060/audio/en/36.wav", "src_ref": "So, we also have subtraction with words here to represent the opposite direction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben hier also auch eine Subtraktion mit Wörtern, um die umgekehrte Richtung darzustellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 37, "src_audio": "/acl6060/audio/en/37.wav", "src_ref": "This is quite similar to relation extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies ist der Beziehungsextraktion recht ähnlich.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 38, "src_audio": "/acl6060/audio/en/38.wav", "src_ref": "So in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In einem formalen deduktiven System wenden wir also in einem Zeitschritt t den Operator zwischen dem Paar q_i und q_j an und erhalten dann diesen neuen Ausdruck.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 39, "src_audio": "/acl6060/audio/en/39.wav", "src_ref": "We add it to the next state to become a new quantity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir fügen ihn dem nächsten Zustand hinzu, um eine neue Menge zu erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 40, "src_audio": "/acl6060/audio/en/40.wav", "src_ref": "So, these slides actually visualize the evolution of the state where we keep adding expression to the current state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Folien veranschaulichen also die Entwicklung des Zustands, wobei wir dem aktuellen Zustand immer neue Ausdrücke hinzufügen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 41, "src_audio": "/acl6060/audio/en/41.wav", "src_ref": "So in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unseren Modellimplementierungen verwenden wir also zunächst ein vortrainiertes Sprachmodell, bei dem es sich um BERTs oder Robertas handeln kann, und dann kodieren wir den Satz und erhalten diese Mengendarstellungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 42, "src_audio": "/acl6060/audio/en/42.wav", "src_ref": "So, once we get the quantity representations, we can start to do inference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sobald wir also die Mengendarstellungen haben, können wir mit der Inferenz beginnen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 43, "src_audio": "/acl6060/audio/en/43.wav", "src_ref": "Here we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier zeigen wir ein Beispiel für q_1, um die Darstellung für q_2 geteilt durch q_2 und dann mal q_3 zu erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 44, "src_audio": "/acl6060/audio/en/44.wav", "src_ref": "First we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst erhalten wir die Paardarstellung, die im Grunde nur die Verkettung zwischen q_1 und q_2 ist, und dann wenden wir ein Feedforward-Netzwerk an, das durch den Operator parametrisiert ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 45, "src_audio": "/acl6060/audio/en/45.wav", "src_ref": "And then finally we obtain the expression representation q_1 divided by q_2.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und dann erhalten wir schließlich die Ausdrucksdarstellung q_1 geteilt durch q_2.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 46, "src_audio": "/acl6060/audio/en/46.wav", "src_ref": "But in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber in der Praxis könnten wir in der Inferenzphase auch den falschen Ausdruck erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 47, "src_audio": "/acl6060/audio/en/47.wav", "src_ref": "So, here all the possible expression is equals to three times the number of operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind also alle möglichen Ausdrücke gleich dem Dreifachen der Anzahl der Operatoren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 48, "src_audio": "/acl6060/audio/en/48.wav", "src_ref": "So the nice thing here is that we can easily add constraints to control this search this search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Schöne daran ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu kontrollieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 49, "src_audio": "/acl6060/audio/en/49.wav", "src_ref": "For example, if this expression is not allowed, we can simply remove this expression in our search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn dieser Ausdruck zum Beispiel nicht erlaubt ist, können wir ihn einfach aus unserem Suchraum entfernen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 50, "src_audio": "/acl6060/audio/en/50.wav", "src_ref": "So in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Im zweiten Schritt machen wir das Gleiche, aber der einzige Unterschied ist, dass wir nur eine Menge mehr haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 51, "src_audio": "/acl6060/audio/en/51.wav", "src_ref": "So this quantity come from the previous calculated expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Menge stammt also aus dem vorherigen berechneten Ausdruck.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 52, "src_audio": "/acl6060/audio/en/52.wav", "src_ref": "So finally we can obtain this final expression q_3 times q_4.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "So können wir schließlich diesen endgültigen Ausdruck q_3 mal q_4 erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 53, "src_audio": "/acl6060/audio/en/53.wav", "src_ref": "And we can also see the number of all the possible ah expression is different from the previous step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können auch sehen, dass die Anzahl aller möglichen Ausdrücke anders ist als beim vorherigen Schritt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 54, "src_audio": "/acl6060/audio/en/54.wav", "src_ref": "So, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein solcher Unterschied erschwert die Anwendung von einer Beam Search, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgewogen ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 55, "src_audio": "/acl6060/audio/en/55.wav", "src_ref": "So the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Trainingsverfahren ist also ähnlich dem Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust bei jedem Zeitschritt optimieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 56, "src_audio": "/acl6060/audio/en/56.wav", "src_ref": "And here we also use this tau to represent when we should terminate this generation process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auch hier verwenden wir dieses Tau, um darzustellen, wann wir diesen Generierungsprozess beenden sollten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 57, "src_audio": "/acl6060/audio/en/57.wav", "src_ref": "And here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier unterscheidet sich der Raum von Sequenz zu Sequenz, weil der Raum bei jedem Zeitschritt anders ist, während im traditionellen Sequenz-zu-Sequenz-Modell dies die Anzahl des Vokabulars ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 58, "src_audio": "/acl6060/audio/en/58.wav", "src_ref": "And it also allows us to impose certain constraints from prior from prior knowledge.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es erlaubt uns auch, bestimmte Beschränkungen aus dem Vorwissen aufzuerlegen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 59, "src_audio": "/acl6060/audio/en/59.wav", "src_ref": "So we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher führen wir Experimente mit den häufig verwendeten Datensätzen der mathematischen Wortprobleme durch, MAWPS, Math23K, MathQA und SVAMP.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 60, "src_audio": "/acl6060/audio/en/60.wav", "src_ref": "And here we briefly show the results compared with the previous best approaches.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier zeigen wir kurz die Ergebnisse im Vergleich zu den bisherigen besten Ansätzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 61, "src_audio": "/acl6060/audio/en/61.wav", "src_ref": "So our best performing variant is Roberta-DeductiveReasoner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere leistungsstärkste Variante ist also Roberta-DeductiveReasoner.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 62, "src_audio": "/acl6060/audio/en/62.wav", "src_ref": "And in fact we do not use beam search, in contrast all previous approaches are using beam search.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tatsächlich verwenden wir keine Beam Search, im Gegensatz zu allen früheren Ansätzen, die Beam Search verwendet haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 63, "src_audio": "/acl6060/audio/en/63.wav", "src_ref": "All right. So, the best approaches are often tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Okay. Daher sind die besten Ansätze oft baumbasierte Modelle.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 64, "src_audio": "/acl6060/audio/en/64.wav", "src_ref": "So, overall our reasoner is able to significantl significantly outperform this tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insgesamt ist unser Reasoner also in der Lage, dieses baumbasierte Modell signifikant zu übertreffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 65, "src_audio": "/acl6060/audio/en/65.wav", "src_ref": "But we can see the absolute numbers on MathQA or SVAMP are not really high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wir können sehen, dass die absoluten Zahlen bei MathQA oder SVAMP nicht wirklich hoch sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 66, "src_audio": "/acl6060/audio/en/66.wav", "src_ref": "So we further investigate the results on SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deshalb untersuchen wir die Ergebnisse bei SVAMP weiter.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 67, "src_audio": "/acl6060/audio/en/67.wav", "src_ref": "And this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieser Datensatz ist eine Herausforderung, weil der Autor versucht hat, manuell etwas hinzuzufügen, um das NLP-Modell zu verwirren, wie z. B. das Hinzufügen irrelevanter Informationen und zusätzlicher Mengen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 68, "src_audio": "/acl6060/audio/en/68.wav", "src_ref": "So, in our prediction we find some of the intermediate values are actually negatives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserer Vorhersage stellen wir also fest, dass einige der Zwischenwerte eigentlich negativ sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 69, "src_audio": "/acl6060/audio/en/69.wav", "src_ref": "For example, um, in these questions we are asking how many apples does Jake have?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel geht es in diesen Fragen darum, wie viele Äpfel Jake hat.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 70, "src_audio": "/acl6060/audio/en/70.wav", "src_ref": "But we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wir haben einige zusätzliche Informationen wie 17 Bilder weniger, und Steven hat acht Bilder, was völlig irrelevant ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 71, "src_audio": "/acl6060/audio/en/71.wav", "src_ref": "So, our model makes some prediction like this which is producing negative values.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Modell macht also eine Vorhersage wie diese, die negative Werte ergibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 72, "src_audio": "/acl6060/audio/en/72.wav", "src_ref": "And we observe these two expressions actually have similar scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir stellen fest, dass diese beiden Ausdrücke tatsächlich ähnliche Werte haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 73, "src_audio": "/acl6060/audio/en/73.wav", "src_ref": "So, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können also diesen Suchraum eingrenzen, indem wir die negativen Ergebnisse entfernen, damit die Antwort richtig ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 74, "src_audio": "/acl6060/audio/en/74.wav", "src_ref": "So um we further find such constraint actually improves quite a lot for some models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir stellen also fest, dass eine solche Bedingung für einige Modelle tatsächlich eine erhebliche Verbesserung darstellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 75, "src_audio": "/acl6060/audio/en/75.wav", "src_ref": "For example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei BERT haben wir uns zum Beispiel um sieben Punkte verbessert und beim Roberta-Basismodell haben wir uns sogar um zwei Punkte verbessert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 76, "src_audio": "/acl6060/audio/en/76.wav", "src_ref": "So better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein besseres Sprachmodell hat also bessere Sprachverständnisfähigkeiten, sodass die Zahl hier höher bei Roberta und niedriger bei BERT ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 77, "src_audio": "/acl6060/audio/en/77.wav", "src_ref": "And we also try to analyze the difficulty behind these behind all these datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir versuchen auch, die Schwierigkeiten hinter all diesen Datensätzen zu analysieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 78, "src_audio": "/acl6060/audio/en/78.wav", "src_ref": "We assume the number of unused quantities can be regarded as irrelevant information here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir gehen davon aus, dass die Anzahl der ungenutzten Mengen hier als irrelevante Informationen betrachtet werden kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 79, "src_audio": "/acl6060/audio/en/79.wav", "src_ref": "So ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier können wir also sehen, dass wir den Prozentsatz der Proben mit ungenutzten Mengen haben, und der SVAMP-Datensatz hat den größten Anteil.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 80, "src_audio": "/acl6060/audio/en/80.wav", "src_ref": "And here we also show the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier zeigen wir auch die Gesamtleistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 81, "src_audio": "/acl6060/audio/en/81.wav", "src_ref": "For those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei den Proben ohne ungenutzte Mengen ist die Leistung sogar höher als die Gesamtleistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 82, "src_audio": "/acl6060/audio/en/82.wav", "src_ref": "But with those samples that with unused quantity is actually way worse than the, worse than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber bei diesen Proben mit ungenutzten Mengen ist sie eigentlich viel schlechter als die Gesamtleistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 83, "src_audio": "/acl6060/audio/en/83.wav", "src_ref": "For MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei MAWPS haben wir nicht wirklich viele Testfälle, also ignoriere ich diesen Teil einfach.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 84, "src_audio": "/acl6060/audio/en/84.wav", "src_ref": "So, finally we want to show the interpretability through a question perturbation example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich wollen wir die Interpretierbarkeit anhand eines Frage-Störungsbeispiels zeigen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 85, "src_audio": "/acl6060/audio/en/85.wav", "src_ref": "So here our model actually makes a wrong prediction at the first step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier macht unser Modell also tatsächlich eine falsche Vorhersage im ersten Schritt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 86, "src_audio": "/acl6060/audio/en/86.wav", "src_ref": "So, we can actually correlate this expression with the sentence here. Alright.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können diesen Ausdruck also tatsächlich mit dem Satz hier korrelieren. Okay.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 87, "src_audio": "/acl6060/audio/en/87.wav", "src_ref": "So, we think this sentence might be misleading the model to an incorrect predictions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir denken also, dass dieser Satz das Modell zu falschen Vorhersagen verleiten könnte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 88, "src_audio": "/acl6060/audio/en/88.wav", "src_ref": "So here planting another thirty five makes the model makes the model think it should be an addition operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier führt eine weitere 35 dazu, dass das Modell denkt, dass es ein Additionsoperator sein sollte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 89, "src_audio": "/acl6060/audio/en/89.wav", "src_ref": "So we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir versuchen also den Satz so umzuformulieren, dass die Anzahl der Birnbäume um 35 weniger als die der Apfelbäume ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 90, "src_audio": "/acl6060/audio/en/90.wav", "src_ref": "So, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sorgen also dafür, dass eine genauere Semantik vermittelt wird, sodass das Modell in der Lage ist, die Vorhersage korrekt zu treffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 91, "src_audio": "/acl6060/audio/en/91.wav", "src_ref": "So, this study shows how the interpretable predictions help us understand the model behavior.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Studie zeigt also, wie uns die interpretierbaren Vorhersagen helfen, das Modellverhalten zu verstehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 92, "src_audio": "/acl6060/audio/en/92.wav", "src_ref": "So to conclude our work, so first our model is actually pretty efficient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um unsere Arbeit abzuschließen: Zunächst ist unser Modell tatsächlich ziemlich effizient.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 93, "src_audio": "/acl6060/audio/en/93.wav", "src_ref": "And we are able to provide interpretable solving procedure.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sind in der Lage, ein interpretierbares Lösungsverfahren anzubieten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 94, "src_audio": "/acl6060/audio/en/94.wav", "src_ref": "And we can easily incorporate some prior knowledge as constraint which can help improve the performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können einfach einiges Vorwissen als Bedingung einbeziehen, was zur Verbesserung der Leistung beitragen kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 95, "src_audio": "/acl6060/audio/en/95.wav", "src_ref": "And the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich gilt der zugrundeliegende Mechanismus nicht nur für das Lösen von Netzwerkproblemen, sondern auch für andere Aufgaben, die mehrstufiges Argumentieren erfordern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 96, "src_audio": "/acl6060/audio/en/96.wav", "src_ref": "We also have certain limitations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auch sind uns gewisse Grenzen gesetzt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 97, "src_audio": "/acl6060/audio/en/97.wav", "src_ref": "Ah, if we have a large number of operators or constants, the memory consumption could be pretty high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir eine große Anzahl von Operatoren oder Konstanten haben, könnte der Speicherverbrauch ziemlich hoch sein.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 98, "src_audio": "/acl6060/audio/en/98.wav", "src_ref": "And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wie bereits erwähnt ist es zudem aufgrund der unausgewogenen Wahrscheinlichkeitsverteilung zwischen den verschiedenen Zeitschritten eine ziemliche Herausforderung, die Beam Search-Strategie anzuwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 99, "src_audio": "/acl6060/audio/en/99.wav", "src_ref": "So this is the end of the talk, and questions are welcomed. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sind am Ende des Vortrags angekommen und freuen uns auf etwaige Fragen. Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 100, "src_audio": "/acl6060/audio/en/100.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo, mein Name ist Antoine und ich bin von der Universität Maastricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 101, "src_audio": "/acl6060/audio/en/101.wav", "src_ref": "I will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde meine gemeinsame Arbeit mit Jerry vorstellen, bei der es um einen neuen Datensatz für das Retrieval von Gesetzesartikeln geht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 102, "src_audio": "/acl6060/audio/en/102.wav", "src_ref": "Legal issues are an integral part of many people's lives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Rechtsfragen sind ein wesentlicher Bestandteil des Lebens vieler Menschen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 103, "src_audio": "/acl6060/audio/en/103.wav", "src_ref": "But the majority of citizens have little to know knowledge about their rights and fundamental legal processes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber die Mehrheit der Bürger verfügt nur über wenig Wissen über ihre Rechte und grundlegende rechtliche Verfahren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 104, "src_audio": "/acl6060/audio/en/104.wav", "src_ref": "As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies hat zur Folge, dass viele schutzbedürftige Bürger, die sich die kostspielige Hilfe eines Rechtsexperten nicht leisten können, schutzlos bleiben oder schlimmstenfalls ausgebeutet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 105, "src_audio": "/acl6060/audio/en/105.wav", "src_ref": "All work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere Arbeit zielt darauf ab, die Kluft zwischen den Menschen und dem Gesetz zu überbrücken, indem ein effektives Retrieval-System für Gesetzesartikel entwickelt wird.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 106, "src_audio": "/acl6060/audio/en/106.wav", "src_ref": "Such a system could provide a free professional legal help service for unskilled humans.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein solches System könnte einen kostenlosen professionellen Rechtshilfedienst für unerfahrene Menschen anbieten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 107, "src_audio": "/acl6060/audio/en/107.wav", "src_ref": "Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bevor wir uns dem Hauptbeitrag dieser Arbeit widmen, wollen wir zunächst das Problem des Retrievals von Gesetzesartikeln beschreiben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 108, "src_audio": "/acl6060/audio/en/108.wav", "src_ref": "Given a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eine einfache Frage zu einer Rechtsangelegenheit wäre: „Was riskiere ich, wenn ich das Berufsgeheimnis verletze?“", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 109, "src_audio": "/acl6060/audio/en/109.wav", "src_ref": "A model is required to retrieve all relevant statutory articles from a large body of legislation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier wird ein Modell benötigt, um alle relevanten Gesetzesartikel aus einem großen Bestand an Rechtsvorschriften abzurufen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 110, "src_audio": "/acl6060/audio/en/110.wav", "src_ref": "This information retrieval task comes with its own set of challenges.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Informationsbeschaffungsaufgabe bringt eine Reihe von Herausforderungen mit sich.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 111, "src_audio": "/acl6060/audio/en/111.wav", "src_ref": "First, it deals with two types of language.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Erstens: Es geht um zwei Arten von Sprache.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 112, "src_audio": "/acl6060/audio/en/112.wav", "src_ref": "Common natural language for the questions and complex legal language for the statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die gemeinsame natürliche Sprache für die Fragen und die komplexe juristische Sprache für die Gesetze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 113, "src_audio": "/acl6060/audio/en/113.wav", "src_ref": "This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieser Unterschied bei den Sprachverteilungen macht es für ein System schwieriger, relevante Kandidaten zu finden, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine rechtliche Frage übersetzen kann, die der Terminologie der Gesetze entspricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 114, "src_audio": "/acl6060/audio/en/114.wav", "src_ref": "Besides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Außerdem ist das Recht keine Ansammlung unabhängiger Artikel, die für sich allein als vollständige Informationsquelle betrachtet werden können, anders als beispielsweise Nachrichten oder Rezepte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 115, "src_audio": "/acl6060/audio/en/115.wav", "src_ref": "Instead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielmehr handelt es sich um eine strukturierte Sammlung von Rechtsvorschriften, die nur im Gesamtkontext, d. h. zusammen mit den ergänzenden Informationen aus den Nachbarartikeln, den Bereichen und Teilbereichen, denen sie angehören, und ihrer Stellung im Aufbau des Gesetzes, eine vollständige Bedeutung haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 116, "src_audio": "/acl6060/audio/en/116.wav", "src_ref": "Lastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich handelt es sich bei den Gesetzesartikeln nicht um kleine Absätze, die in den meisten Retrieval-Arbeiten die typische Retrieval-Einheit darstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 117, "src_audio": "/acl6060/audio/en/117.wav", "src_ref": "Here, there are long documents that may be up to six thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier gibt es lange Dokumente, die bis zu 6.000 Wörter umfassen können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 118, "src_audio": "/acl6060/audio/en/118.wav", "src_ref": "The recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die jüngsten Fortschritte im Bereich NLP haben großes Interesse an vielen juristischen Aufgaben geweckt, z. B. an der Vorhersage juristischer Urteile oder der automatisierten Prüfung von Verträgen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 119, "src_audio": "/acl6060/audio/en/119.wav", "src_ref": "But statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Doch das Retrieval von Gesetzesartikeln ist aufgrund des Mangels an großen und qualitativ hochwertig markierten Datensätzen weitgehend unberührt geblieben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 120, "src_audio": "/acl6060/audio/en/120.wav", "src_ref": "In this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In dieser Arbeit stellen wir einen neuen Datensatz in der französischen Muttersprache vor, mit dem wir Retrievalmodelle untersuchen und analysieren, ob sie bei der Aufgabe der Suche nach Gesetzesartikeln an die Effizienz und Zuverlässigkeit eines Rechtsexperten rankommen können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 121, "src_audio": "/acl6060/audio/en/121.wav", "src_ref": "Our Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser belgischer Datensatz für das Retrieval von Gesetzesartikeln, BSARD, besteht aus mehr als 1.100 juristischen Fragen, die von belgischen Bürgern gestellt wurden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 122, "src_audio": "/acl6060/audio/en/122.wav", "src_ref": "These questions cover a wide range of topics from family, housing, money, to work and social security.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Fragen decken ein breites Spektrum an Themen ab, von Familie, Wohnen, Geld bis hin zu Arbeit und sozialer Sicherheit.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 123, "src_audio": "/acl6060/audio/en/123.wav", "src_ref": "Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jede von ihnen wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als 22.600 Rechtsartikeln aus belgischen Gesetzbüchern markiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 124, "src_audio": "/acl6060/audio/en/124.wav", "src_ref": "Let's now talk about how we collected this dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lassen Sie uns nun darüber sprechen, wie wir diesen Datensatz gesammelt haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 125, "src_audio": "/acl6060/audio/en/125.wav", "src_ref": "First, we started by compiling a large corpus of legal articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst haben wir einen großen Korpus von Rechtsartikeln zusammengestellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 126, "src_audio": "/acl6060/audio/en/126.wav", "src_ref": "We considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben 32 öffentlich zugängliche belgische Gesetzbücher berücksichtigt und alle Artikel sowie die entsprechenden Abschnittsüberschriften extrahiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 127, "src_audio": "/acl6060/audio/en/127.wav", "src_ref": "Then we gathered legal questions with references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann haben wir rechtliche Fragen mit Verweisen auf einschlägige Gesetze zusammengestellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 128, "src_audio": "/acl6060/audio/en/128.wav", "src_ref": "To do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zu diesem Zweck arbeiten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr etwa 4.000 E-Mails von belgischen Bürgern erhält, die um Rat in einer persönlichen Rechtsangelegenheit bitten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 129, "src_audio": "/acl6060/audio/en/129.wav", "src_ref": "We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir hatten das Glück, Zugang zu ihren Websites zu erhalten, auf denen ihr Team aus erfahrenen Juristen die häufigsten Rechtsangelegenheiten der Belgier behandelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 130, "src_audio": "/acl6060/audio/en/130.wav", "src_ref": "We collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben Tausende von Fragen gesammelt und mit Kategorien, Unterkategorien und Verweisen auf einschlägige Gesetze annotiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 131, "src_audio": "/acl6060/audio/en/131.wav", "src_ref": "Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich haben wir die rechtlichen Verweise überprüft und die Fragen herausgefiltert, deren Verweise nicht in einem der von uns berücksichtigten Gesetzbücher enthalten waren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 132, "src_audio": "/acl6060/audio/en/132.wav", "src_ref": "The remaining references were matched and converted to the corresponding article ids from our corpus.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die übrigen Referenzen wurden zusammengeführt und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 133, "src_audio": "/acl6060/audio/en/133.wav", "src_ref": "We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Am Ende hatten wir 1.108 Fragen, jede sorgfältig markiert mit den IDs der relevanten Artikel aus unserem großen Korpus von 22.630 Gesetzesartikeln.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 134, "src_audio": "/acl6060/audio/en/134.wav", "src_ref": "In addition, each question comes with the main category and a concatenation of subcategories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus enthält jede Frage eine Hauptkategorie und eine Verkettung von Unterkategorien.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 135, "src_audio": "/acl6060/audio/en/135.wav", "src_ref": "And each articles comes with a concatenation of the subsequence heading in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jeder Artikel hat eine Verkettung mit der anschließenden Überschrift im Aufbau des Gesetzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 136, "src_audio": "/acl6060/audio/en/136.wav", "src_ref": "This extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für künftige Forschungen zum juristischen Informationen-Retrieval oder zur juristischen Textklassifikation von Interesse sein.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 137, "src_audio": "/acl6060/audio/en/137.wav", "src_ref": "Let's look at some characteristic of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schauen wir uns einige Merkmale unseres Datensatzes an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 138, "src_audio": "/acl6060/audio/en/138.wav", "src_ref": "The questions are between five and forty four words long with a median of fourteen words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Fragen sind zwischen fünf und 24 Wörter lang, mit einem Mittelwert von 14 Wörtern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 139, "src_audio": "/acl6060/audio/en/139.wav", "src_ref": "The articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Artikel sind mit einer durchschnittlichen Länge von 77 Wörtern sehr viel länger, wobei 142 von ihnen mehr als 1.000 Wörter umfassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 140, "src_audio": "/acl6060/audio/en/140.wav", "src_ref": "The lengthiest one being up to five thousand seven hundred and ninety words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der längste von ihnen hat bis zu 5.790 Wörter.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 141, "src_audio": "/acl6060/audio/en/141.wav", "src_ref": "As previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wie bereits erwähnt decken die Fragen ein breites Spektrum an Themen ab, wobei etwa 85 Prozent der Fragen entweder die Familie, das Wohnen, Geld oder die Gerechtigkeit betreffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 142, "src_audio": "/acl6060/audio/en/142.wav", "src_ref": "While the remaining fifteen percent concern either social security, foreigners or work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die restlichen 15 Prozent betreffen entweder soziale Sicherheit, Ausländer oder Arbeit.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 143, "src_audio": "/acl6060/audio/en/143.wav", "src_ref": "The article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Artikel sind auch sehr vielfältig, da sie aus 32 verschiedenen belgischen Gesetzbüchern stammen, die eine große Reihe von Rechtsthemen abdecken.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 144, "src_audio": "/acl6060/audio/en/144.wav", "src_ref": "Here's the total number of articles collected from each of these Belgian codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist die Gesamtzahl der Artikel, die aus diesen belgischen Gesetzbüchern gesammelt wurden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 145, "src_audio": "/acl6060/audio/en/145.wav", "src_ref": "Out of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Von den 22.633 Artikeln werden nur 1.612 als relevant für mindestens eine Frage im Datensatz genannt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 146, "src_audio": "/acl6060/audio/en/146.wav", "src_ref": "And around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Etwa 80 Prozent dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, dem Gerichtsgesetzbuch, dem Gesetzbuch für strafrechtliche Ermittlungen oder dem Strafgesetzbuch.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 147, "src_audio": "/acl6060/audio/en/147.wav", "src_ref": "Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei 18 von 32 Gesetzbüchern werden weniger als fünf Artikel als relevant für mindestens eine Frage bestimmt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 148, "src_audio": "/acl6060/audio/en/148.wav", "src_ref": "Which can be explained by the fact that those codes focused less on individuals and their concerns.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies lässt sich dadurch erklären, dass diese Gesetzbücher weniger auf den Einzelnen und seine Anliegen ausgerichtet waren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 149, "src_audio": "/acl6060/audio/en/149.wav", "src_ref": "Overall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insgesamt liegt der Durchschnitt der Anzahl der Zitate bei zwei, und weniger als 25 Prozent der zitierten Artikel werden mehr als fünfmal zitiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 150, "src_audio": "/acl6060/audio/en/150.wav", "src_ref": "Using all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unter Verwendung aller Datensätze haben wir verschiedene Retrieval-Ansätze, einschließlich lexikalischer und dichter Architektur, einem Benchmarking unterzogen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 151, "src_audio": "/acl6060/audio/en/151.wav", "src_ref": "Given a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei einer Abfrage und einem Artikel weist ein lexikalisches Modell dem Abfrage-Artikel-Paar eine Punktzahl zu, indem es die Summe der Gewichtungen von jedem der Begriffe in diesem Artikel über die Abfrage-Termini berechnet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 152, "src_audio": "/acl6060/audio/en/152.wav", "src_ref": "We experiment with the standard TF-IDF and BM25 ranking functions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit den Standard-Ranking-Funktionen TF-IDF und BM25.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 153, "src_audio": "/acl6060/audio/en/153.wav", "src_ref": "The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Hauptproblem bei diesen Ansätzen ist, dass sie nur Artikel finden können, die Schlüsselwörter enthalten, die in der Anfrage vorkommen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 154, "src_audio": "/acl6060/audio/en/154.wav", "src_ref": "To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um diese Einschränkung zu überwinden, experimentieren wir mit einer neuronal-basierten Architektur, die semantische Beziehungen zwischen Abfragen und Artikeln erfassen kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 155, "src_audio": "/acl6060/audio/en/155.wav", "src_ref": "We use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir verwenden ein Bi-Encoder-Modell, das Abfragen und Artikel in dichten Vektor-Darstellungen abbildet und einen Relevanzwert zwischen einem Abfrage-Artikel-Paar anhand der Ähnlichkeit ihrer Einbettungen berechnet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 156, "src_audio": "/acl6060/audio/en/156.wav", "src_ref": "These embeddings typically result from a pooling operation on the output of a word embedding model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Einbettungen resultieren in der Regel aus einer Pooling-Operation auf der Ausgabe eines Worteinbettungsmodells.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 157, "src_audio": "/acl6060/audio/en/157.wav", "src_ref": "First, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst untersuchen wir die Effektivität von Siamesischen Bi-Encodern in einem Zero-Shot-Evaluation-Setup, d. h. dass vortrainierte Worteinbettungsmodelle ohne zusätzliche Feinabstimmung sofort angewendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 158, "src_audio": "/acl6060/audio/en/158.wav", "src_ref": "We experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit kontextunabhängigen Textencodern, wie word2vec und fastText, und kontextabhängigen Einbettungsmodellen, wie Roberta und insbesondere CamemBERT, einem französischen Roberta-Modell.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 159, "src_audio": "/acl6060/audio/en/159.wav", "src_ref": "Additionally, we train our own CamemBERT based model ah bi-encoders on our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus trainieren wir unser eigenes, auf CamemBERT basierendes Modell, Bi-Encoder, auf unserem Datensatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 160, "src_audio": "/acl6060/audio/en/160.wav", "src_ref": "Note that for training, we experiment with the two flavors of the bi-encoder architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Beachten Sie, dass wir beim Training mit den beiden Varianten der Bi-Encoder-Architektur experimentieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 161, "src_audio": "/acl6060/audio/en/161.wav", "src_ref": "Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Siamese, das ein einzigartiges Worteinbettungsmodell verwendet, das die Abfrage und den Artikel zusammen in einem geteilten dichten Vektorraum abbildet, und Two-Tower, das zwei unabhängige Worteinbettungsmodelle verwendet, die die Abfrage und den Artikel getrennt in verschiedenen Einbettungsräumen kodieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 162, "src_audio": "/acl6060/audio/en/162.wav", "src_ref": "We experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit Mittelwert, Maximalwert und CLS-Pooling sowie mit Produkt und Kosinus für die Berechnung von Ähnlichkeiten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 163, "src_audio": "/acl6060/audio/en/163.wav", "src_ref": "Here are the result of our baseline on the test sets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind die Ergebnisse unserer Baseline auf den Testsätzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 164, "src_audio": "/acl6060/audio/en/164.wav", "src_ref": "With the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die lexikalischen Methoden werden oben, die Siamesischen Bi-Encoder in einem Zero-Shot-Setup in der Mitte und die fein abgestimmten Bi-Encoder werden unten ausgewertet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 165, "src_audio": "/acl6060/audio/en/165.wav", "src_ref": "Overall, the finetuned bi-encoder significantly outperforms all the other baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insgesamt übertrifft der fein abgestimmte Bi-Encoder alle anderen Baselines deutlich.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 166, "src_audio": "/acl6060/audio/en/166.wav", "src_ref": "The two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Two-Tower-Modell verbessert sich gegenüber seinen Siamesischen Varianten beim Recall um 100, schneidet aber bei den anderen Metriken ähnlich gut ab.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 167, "src_audio": "/acl6060/audio/en/167.wav", "src_ref": "Although BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Obwohl BM25 deutlich schlechter abschnitt als der trainierte Bi-Encoder, zeigte seine Leistung, dass er immer noch eine gute Basis für ein domänenspezifisches Retrieval ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 168, "src_audio": "/acl6060/audio/en/168.wav", "src_ref": "Regarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bezüglich der Zero-Shot-Evaluation des Siamesischen Bi-Encoders stellen wir fest, dass die direkte Verwendung der Einbettungen eines vortrainierten CamemBERT-Modells ohne Optimierung für die Informationen-Retrieval-Aufgabe schlechte Ergebnisse liefert, was mit früheren Erkenntnissen übereinstimmt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 169, "src_audio": "/acl6060/audio/en/169.wav", "src_ref": "Furthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus stellen wir fest, dass der word2vec-basierte Bi-Encoder die fastText- und BERT-basierten Modelle signifikant übertrifft, was darauf hindeutet, dass vortrainierte Wort-Ebenen-Einbettungen vielleicht besser für die Aufgabe geeignet sind als Zeichen- oder Unterwort-Ebenen-Einbettungen, wenn sie direkt verwendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 170, "src_audio": "/acl6060/audio/en/170.wav", "src_ref": "Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Obwohl diese Ergebnisse vielversprechend sind, lassen sie im Vergleich zu einem erfahrenen Rechtsexperten auf ein großes Verbesserungspotenzial schließen, der früher oder später alle relevanten Artikel zu jeder Frage abrufen kann und somit perfekte Ergebnisse erzielt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 171, "src_audio": "/acl6060/audio/en/171.wav", "src_ref": "Let's conclude by discussing two limitations of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Abschließend möchte ich auf zwei Einschränkungen unseres Datensatzes eingehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 172, "src_audio": "/acl6060/audio/en/172.wav", "src_ref": "First, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Erstens ist der Artikelkorpus auf die Artikel der 32 herangezogenen belgischen Gesetzbücher beschränkt, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Dekreten, Richtlinien und Verordnungen fehlen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 173, "src_audio": "/acl6060/audio/en/173.wav", "src_ref": "During the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei der Erstellung des Datensatzes werden alle Verweise auf diese nicht erfassten Artikel ignoriert, was dazu führt, dass sich einige Fragen am Ende nur auf einen Bruchteil der ursprünglichen Anzahl der relevanten Artikel beziehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 174, "src_audio": "/acl6060/audio/en/174.wav", "src_ref": "This information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Information impliziert also, dass die Antwort, die in den übrigen relevanten Artikeln enthalten ist, unvollständig sein könnte, obwohl sie immer noch völlig angemessen ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 175, "src_audio": "/acl6060/audio/en/175.wav", "src_ref": "Second, we should note that not all legal questions can be answered with statutes alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zweitens sollten wir beachten, dass nicht alle rechtlichen Fragen allein mit Gesetzen beantwortet werden können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 176, "src_audio": "/acl6060/audio/en/176.wav", "src_ref": "For instance, the question, can I evict my tenants if they make too much noise?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel die Frage: „Kann ich meinen Mietern kündigen, wenn sie zu viel Lärm machen?“", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 177, "src_audio": "/acl6060/audio/en/177.wav", "src_ref": "Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Möglicherweise gibt es im Gesetz keine detaillierte Antwort, die eine bestimmte Lärmschwelle festlegt, ab der eine Räumung zulässig ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 178, "src_audio": "/acl6060/audio/en/178.wav", "src_ref": "Instead, the landlord should probably rely more on case law and find precedents similar to their current situation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Stattdessen sollte sich der Vermieter wahrscheinlich eher auf das Fallrecht stützen und Präzedenzfälle finden, die seiner aktuellen Situation ähnlich sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 179, "src_audio": "/acl6060/audio/en/179.wav", "src_ref": "For example, the tenants makes two parties a week until two AM.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel veranstalten die Mieter zwei Partys pro Woche bis zwei Uhr morgens.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 180, "src_audio": "/acl6060/audio/en/180.wav", "src_ref": "Hence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Einige Fragen eignen sich daher besser als andere für die Aufgabe des Retrievals von Gesetzesartikeln. Die Domäne der weniger geeigneten Fragen muss noch bestimmt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 181, "src_audio": "/acl6060/audio/en/181.wav", "src_ref": "We hope that our work sparks interest in developing practical and reliable statutory article retrieval models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir hoffen, dass unsere Arbeit das Interesse an der Entwicklung praktischer und zuverlässiger Modelle für das Retrieval von Gesetzesartikeln weckt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 182, "src_audio": "/acl6060/audio/en/182.wav", "src_ref": "That can help improve access to justice for all.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie kann dazu beitragen, den Zugang zur Justiz für alle zu verbessern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 183, "src_audio": "/acl6060/audio/en/183.wav", "src_ref": "You can check out our paper, dataset and code at the following links. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie können unser Paper, den Datensatz und die Gesetzbücher unter den folgenden Links einsehen. Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 184, "src_audio": "/acl6060/audio/en/184.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo, wir freuen uns, Ihnen unsere Arbeit über VALSE vorstellen zu können. Das ist ein Aufgaben-unabhängiger Benchmark, der für das Testen von Seh- und Sprachmodellen mit spezifischen sprachlichen Phänomenen konzipiert ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 185, "src_audio": "/acl6060/audio/en/185.wav", "src_ref": "Why did we do the trouble in setting up this benchmark?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Warum haben wir uns die Mühe gemacht, diesen Benchmark einzurichten?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 186, "src_audio": "/acl6060/audio/en/186.wav", "src_ref": "Well, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In den letzten Jahren haben wir eine explosionsartige Zunahme von Transformer-basierten Seh- und Sprachmodellen erlebt, die mit großen Mengen an Bild-Text-Paaren vortrainiert wurden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 187, "src_audio": "/acl6060/audio/en/187.wav", "src_ref": "Each one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jedes dieser Modelle treibt den Fortschritt bei Seh- und Sprachaufgaben voran, wie z. B. die visuelle Fragenbeantwortung, das Argumentieren mit visuellem Menschenverstand, Bild-Retrieval, Phrasen-Erdung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 188, "src_audio": "/acl6060/audio/en/188.wav", "src_ref": "So we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben also die Nachricht erhalten, dass die Genauigkeiten bei diesen Aufgaben und spezifischen Benchmarks stetig steigen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 189, "src_audio": "/acl6060/audio/en/189.wav", "src_ref": "But do we know what the models have actually learned?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wissen wir, was die Modelle tatsächlich gelernt haben?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 190, "src_audio": "/acl6060/audio/en/190.wav", "src_ref": "What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Was hat ein Seh- und Sprach-Transformer verstanden, wenn er für dieses Bild und diesen Satz eine hohe Punktzahl zuweist?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 191, "src_audio": "/acl6060/audio/en/191.wav", "src_ref": "And the low score for this one?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und hier eine niedrige Punktzahl?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 192, "src_audio": "/acl6060/audio/en/192.wav", "src_ref": "Do vision and language models focus on the right thing?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Konzentrieren sich Seh- und Sprachmodelle auf die wesentlichen Punkte?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 193, "src_audio": "/acl6060/audio/en/193.wav", "src_ref": "Or do they focus on biases as shown by previous work?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Oder konzentrieren sie sich auf Verzerrungen, wie in früheren Arbeiten aufgezeigt wurde?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 194, "src_audio": "/acl6060/audio/en/194.wav", "src_ref": "To shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um diesen Aspekt näher zu beleuchten, schlagen wir eine eher aufgabenbezogene agnostische Richtung vor und führen VALSE ein, das die Empfindlichkeit von Seh- und Sprachmodellen für spezifische sprachliche Phänomene testet, die sowohl die sprachlichen als auch die visuellen Modalitäten betreffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 195, "src_audio": "/acl6060/audio/en/195.wav", "src_ref": "We target existence, plurality, counting, spatial relations, actions and entity coreference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir zielen auf Existenz, Pluralität, Zählung, räumliche Beziehungen, Handlungen und Entity-Koreferenz ab.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 196, "src_audio": "/acl6060/audio/en/196.wav", "src_ref": "But how do we test whether the vision and language models have captured this phenomena?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Doch wie lässt sich überprüfen, ob die Seh- und Sprachmodelle dieses Phänomen erfasst haben?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 197, "src_audio": "/acl6060/audio/en/197.wav", "src_ref": "By foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Verfälschen einer Methode wurde zuvor von Ravi Shekhar und Mitarbeitern nur für Substantivphrasen in den Seh- und Sprachmodellen und beim Zählen von uns in früheren Arbeiten angewandt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 198, "src_audio": "/acl6060/audio/en/198.wav", "src_ref": "Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Verfälschen bedeutet im Grunde, dass wir die Beschriftung eines Bildes nehmen und eine Verfälschung erzeugen, indem wir die Beschriftung so verändern, dass sie das Bild nicht mehr beschreibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 199, "src_audio": "/acl6060/audio/en/199.wav", "src_ref": "And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir führen diese Veränderungen der Phrase durch, indem wir uns auf sechs spezifische Teile konzentrieren, wie Existenz, Pluralität, Zählung, räumliche Relationen, Handlungen und Entität-Koreferenz, wobei jeder Teil aus einem oder mehreren Instrumenten bestehen kann, falls wir mehr als einen interessanten Weg gefunden haben, um Verfälschungsinstanzen zu erzeugen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 200, "src_audio": "/acl6060/audio/en/200.wav", "src_ref": "For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel haben wir im Fall des Handlungsteils zwei Instrumente, eines, bei dem das Handlungsverb durch eine andere Handlung ersetzt wird, und eines, bei dem die Aktanten ausgetauscht werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 201, "src_audio": "/acl6060/audio/en/201.wav", "src_ref": "Counting and coreference also are pieces that have more than one instrument.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zählung und Koreferenz sind auch Teile, die mehr als ein Instrument haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 202, "src_audio": "/acl6060/audio/en/202.wav", "src_ref": "And we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir erstellen diese Verfälschungen, indem wir sicherstellen, dass sie das Bild nicht beschreiben, dass sie grammatikalisch richtig sind und auch korrekte Sätze bilden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 203, "src_audio": "/acl6060/audio/en/203.wav", "src_ref": "This is not easy to do because a foiled caption may be less likely than the original caption.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das ist nicht einfach zu bewerkstelligen, denn eine verfälschte Beschriftung ist weniger wahrscheinlich als die ursprüngliche Beschriftung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 204, "src_audio": "/acl6060/audio/en/204.wav", "src_ref": "For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist zwar zum Beispiel nicht unmöglich, aber statistisch gesehen ist es unwahrscheinlicher, dass Pflanzen einen Menschen schneiden, als dass ein Mensch Pflanzen schneidet. Große Seh- und Sprachmodelle könnten dies erkennen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 205, "src_audio": "/acl6060/audio/en/205.wav", "src_ref": "Therefore, to obtain valid foils, we must take action.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher müssen wir handeln, wenn wir gültige Verfälschungen erhalten wollen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 206, "src_audio": "/acl6060/audio/en/206.wav", "src_ref": "First, we make use of strong language models to propose foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Erstens nutzen wir starke Sprachmodelle, um Verfälschungen vorzuschlagen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 207, "src_audio": "/acl6060/audio/en/207.wav", "src_ref": "Second, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zweitens verwenden wir die Inferenz der natürlichen Sprache oder kurz NLI, um Verfälschungen herauszufiltern, die das Bild noch beschreiben könnten. Bei der Konstruktion von Verfälschungen müssen wir sicherstellen, dass sie das Bild nicht beschreiben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 208, "src_audio": "/acl6060/audio/en/208.wav", "src_ref": "To test this automatically, we apply natural language inference with the following rationale.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um dies automatisch zu testen, wenden wir die Inferenz der natürlichen Sprache mit der folgenden Logik an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 209, "src_audio": "/acl6060/audio/en/209.wav", "src_ref": "We consider an image to be the premise and its caption its entailed hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir betrachten ein Bild als die Prämisse und seine Beschriftung als die daraus abgeleitete Hypothese.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 210, "src_audio": "/acl6060/audio/en/210.wav", "src_ref": "In addition, we consider the caption to be the premise, and the foil is its hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus betrachten wir die Beschriftung als Prämisse und die Verfälschung als ihre Hypothese.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 211, "src_audio": "/acl6060/audio/en/211.wav", "src_ref": "If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn ein NLI-Modell vorhersagt, dass die Verfälschung der Beschriftung widerspricht oder neutral ist, ist das für uns ein Indikator für eine gültige Verfälschung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 212, "src_audio": "/acl6060/audio/en/212.wav", "src_ref": "If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn ein NLI vorhersagt, dass die Verfälschung die Beschriftung beinhaltet, kann es keine gute Verfälschung sein, da sie durch Transitivität eine wahrheitsgemäße Beschreibung des Bildes liefert. Wir filtern diese Verfälschungen heraus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 213, "src_audio": "/acl6060/audio/en/213.wav", "src_ref": "But this procedure is not perfect, it is just an indicator for valid foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieses Verfahren ist aber nicht perfekt, sondern nur ein Indikator für gültige Verfälschungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 214, "src_audio": "/acl6060/audio/en/214.wav", "src_ref": "Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher setzen wir als dritte Maßnahme zur Generierung gültiger Verfälschungen menschliche Annotatoren ein, um die in VALSE verwendeten Daten zu validieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 215, "src_audio": "/acl6060/audio/en/215.wav", "src_ref": "So, after filtering and human evaluation, we have as many test instances as described in this table.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nach dem Filtern und der menschlichen Evaluation haben wir also so viele Testinstanzen, wie in dieser Tabelle beschrieben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 216, "src_audio": "/acl6060/audio/en/216.wav", "src_ref": "Note that VALSE does not deliver any training data but only test data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Beachten Sie hier, dass VALSE keine Trainingsdaten, sondern nur Testdaten liefert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 217, "src_audio": "/acl6060/audio/en/217.wav", "src_ref": "Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da es sich um einen reinen Zero-Shot-Test-Benchmark handelt, ist er so konzipiert, dass er die bestehenden Fähigkeiten der Seh- und Sprachmodelle nach dem Vortraining nutzt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 218, "src_audio": "/acl6060/audio/en/218.wav", "src_ref": "Finetuning would only enable models to exploit artifacts or statistical biases in the data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Feinabstimmung würde es den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 219, "src_audio": "/acl6060/audio/en/219.wav", "src_ref": "And we all know that these models like to cheat and take shortcuts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir alle wissen, dass diese Modells gerne schummeln und Abkürzungen nehmen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 220, "src_audio": "/acl6060/audio/en/220.wav", "src_ref": "And as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wie wir schon erwähnten, sind wir an der Bewertung interessiert, welche Fähigkeiten die Seh- und Sprachmodelle nach dem Vortraining haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 221, "src_audio": "/acl6060/audio/en/221.wav", "src_ref": "We experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit fünf Seh- und Sprachmodellen auf VALSE, nämlich mit CLIP, LXMert, ViLBERT, ViLBERT zwölf in einem und VisualBERT.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 222, "src_audio": "/acl6060/audio/en/222.wav", "src_ref": "Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zwei unserer wichtigsten Evaluationsmetriken sind die Genauigkeit der Modelle bei der Klassifikation von Bild-Satz-Paaren in Bildbeschreibungen und Verfälschungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 223, "src_audio": "/acl6060/audio/en/223.wav", "src_ref": "Perhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielleicht ist es für dieses Video relevanter, unsere tolerantere Metrik vorzustellen, die paarweise Genauigkeit. Diese misst, ob die Bild-Satz-Ausrichtung für das korrekte Bild-Text-Paar größer ist als für sein Verfälschungspaar.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 224, "src_audio": "/acl6060/audio/en/224.wav", "src_ref": "For more metrics and results on them, do check out our paper.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Weitere Metriken und Ergebnisse dazu finden Sie in unserem Paper.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 225, "src_audio": "/acl6060/audio/en/225.wav", "src_ref": "The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Ergebnisse mit paarweiser Genauigkeit werden hier gezeigt und stimmen mit den Ergebnissen überein, die wir von den anderen Metriken erhalten haben, nämlich dass die beste Zero-Shot-Leistung von ViLBERT zwölf zu eins erreicht wird, gefolgt von ViLBERT, LXMert, CLIP und schließlich VisualBERT.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 226, "src_audio": "/acl6060/audio/en/226.wav", "src_ref": "It's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist bemerkenswert, wie Instrumente, die sich auf die einzelnen Objekte konzentrieren, wie Existenz und Substantivphrasen, von ViLBERT zwölf zu eins fast gelöst werden. Das unterstreicht, dass Modelle in der Lage sind, benannte Objekte und ihre Existenz in Bildern zu identifizieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 227, "src_audio": "/acl6060/audio/en/227.wav", "src_ref": "However, none of the remaining pieces can be reliably solved in our adversarial foiling settings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Keines der verbleibenden Teile kann jedoch in unseren gegnerischen Verfälschungseinstellungen zuverlässig gelöst werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 228, "src_audio": "/acl6060/audio/en/228.wav", "src_ref": "We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sehen an den Instrumenten der Pluralität und der Zählung, dass Seh- und Sprachmodelle Schwierigkeiten haben, Verweise auf einzelne und mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 229, "src_audio": "/acl6060/audio/en/229.wav", "src_ref": "The relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Relationen-Teil zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Relation zwischen Objekten in einem Bild richtig zu klassifizieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 230, "src_audio": "/acl6060/audio/en/230.wav", "src_ref": "They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie durch Plausibilitätsbias unterstützt werden, wie wir bei dem Handlung-Teil sehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 231, "src_audio": "/acl6060/audio/en/231.wav", "src_ref": "From the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Beim Koreferenz-Teil geht hervor, dass die Verfolgung mehrerer Verweise auf dasselbe Objekt in einem Bild unter Verwendung von Pronomen auch für die Seh- und Sprachmodelle schwierig ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 232, "src_audio": "/acl6060/audio/en/232.wav", "src_ref": "As a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zur Überprüfung der Korrektheit und weil es ein interessantes Experiment ist, vergleichen wir auch zwei reine Textmodelle, GPT 1 und GPT 2. So wollen wir feststellen, ob VALSE durch diese unimodalen Modelle lösbar ist. Wir berechnen die Perplexität der korrekten und der verfälschten Beschriftung, ohne Bild, um den Eintrag mit der niedrigsten Perplexität vorherzusagen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 233, "src_audio": "/acl6060/audio/en/233.wav", "src_ref": "If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die Perplexität bei der Verfälschung höher ist, ist das für uns ein Hinweis, dass die verfälschte Beschriftung möglicherweise unter Plausibilitätsverzerrungen oder anderen sprachlichen Verzerrungen leidet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 234, "src_audio": "/acl6060/audio/en/234.wav", "src_ref": "And it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist interessant zu sehen, dass in einigen Fällen die GPT-Modelle mit reinem Text die Plausibilität der Welt besser erfasst haben als die Seh- und Sprachmodelle.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 235, "src_audio": "/acl6060/audio/en/235.wav", "src_ref": "So to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zusammenfassend lässt sich sagen, dass VALSE ein Benchmark ist, der durch das Objektiv von linguistischen Konstrukten schaut, um die Gemeinschaft bei der Verbesserung von Seh- und Sprachmodellen zu unterstützen, indem er ihre visuelle Erdung hart testet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 236, "src_audio": "/acl6060/audio/en/236.wav", "src_ref": "Our experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere Experimente zeigen, dass Seh- und Sprachmodelle benannte Objekte und ihre Existenz in Bildern gut erkennen, wie der Existenzteil zeigt. Sie haben aber Schwierigkeiten damit, ihre gegenseitige Abhängigkeit und Beziehungen in visuellen Szenen zu begründen, wenn sie gezwungen sind, sprachliche Indikatoren zu berücksichtigen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 237, "src_audio": "/acl6060/audio/en/237.wav", "src_ref": "We would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir möchten die Community wirklich dazu ermutigen, VALSE für die Messung des Fortschritts von Sprach- Erdung mit Seh- und Sprachmodellen zu nutzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 238, "src_audio": "/acl6060/audio/en/238.wav", "src_ref": "And even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zudem könnte VALSE als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten. So kann kontrolliert werden, ob ein Datensatz dazu beiträgt, dass sich Modelle in einem der von VALSE getesteten Aspekte verbessern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 239, "src_audio": "/acl6060/audio/en/239.wav", "src_ref": "If you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn Sie Interesse haben, schauen Sie sich die VALSE-Daten auf GitHub an. Wenn Sie irgendwelche Fragen haben, zögern Sie nicht, uns zu kontaktieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 240, "src_audio": "/acl6060/audio/en/240.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo, mein Name ist Kamezawa von der Universität Tokio.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 241, "src_audio": "/acl6060/audio/en/241.wav", "src_ref": "I'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde folgendes Paper vorstellen: RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 242, "src_audio": "/acl6060/audio/en/242.wav", "src_ref": "I'll be explaining in this order.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde es in dieser Reihenfolge erklären.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 243, "src_audio": "/acl6060/audio/en/243.wav", "src_ref": "First, I will introduce automatic release note generation that we are working on in this research.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst werde ich die automatische Generierung von Versionshinweisen vorstellen, an der wir in dieser Forschung arbeiten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 244, "src_audio": "/acl6060/audio/en/244.wav", "src_ref": "A release note is a technical document that summarizes the changes distributed with each release of a software product.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein Versionshinweis ist ein technisches Dokument, das die Änderungen zusammenfasst, die mit jeder Version eines Softwareprodukts vertrieben werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 245, "src_audio": "/acl6060/audio/en/245.wav", "src_ref": "The image shows a release note for version two point six point four of the vuejs library.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Bild zeigt einen Versionshinweis für Version 2.6.4 der Vuejs-Bibliothek.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 246, "src_audio": "/acl6060/audio/en/246.wav", "src_ref": "Release notes play an important role in open source development but they're time consuming to prepare manually.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Versionshinweise spielen bei der Open-Source-Entwicklung eine wichtige Rolle, aber ihre Erstellung ist manuell zeitaufwändig.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 247, "src_audio": "/acl6060/audio/en/247.wav", "src_ref": "Therefore, it would be very useful to be able to automatically generate high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher wäre es sehr nützlich, wenn man automatisch qualitativ hochwertige Versionshinweise generieren könnte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 248, "src_audio": "/acl6060/audio/en/248.wav", "src_ref": "I will defer to two previous researches on automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde zwei frühere Untersuchungen zur automatischen Generierung von Versionshinweisen erwähnen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 249, "src_audio": "/acl6060/audio/en/249.wav", "src_ref": "The first is a system called ARENA released in twenty fourteen.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zuerst gibt es ein System mit dem Namen ARENA, das im Jahr 2014 veröffentlicht wurde.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 250, "src_audio": "/acl6060/audio/en/250.wav", "src_ref": "It takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es verfolgt einen regelbasierten Ansatz, indem es zum Beispiel den Change-Extractor verwendet, um alle Unterschiede, Bibliotheksänderungen und Dokumentänderungen aus den verschiedenen Versionen zu extrahieren und sie schließlich zu kombinieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 251, "src_audio": "/acl6060/audio/en/251.wav", "src_ref": "The most notable feature of this system is the issue extractor in the upper right corner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die auffälligste Funktion dieses Systems ist der Ausgabe-Extraktor in der oberen rechten Ecke.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 252, "src_audio": "/acl6060/audio/en/252.wav", "src_ref": "Which must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies muss Jira, dem Fehlerverwaltungssystem, überlassen werden und kann nur auf Projekte angewendet werden, die Jira verwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 253, "src_audio": "/acl6060/audio/en/253.wav", "src_ref": "In other words, it cannot be used for many projects on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mit anderen Worten: Es kann nicht für viele Projekte auf GitHub verwendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 254, "src_audio": "/acl6060/audio/en/254.wav", "src_ref": "The second is Glyph, recently announced in twenty twenty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das zweite ist Glyph, das kürzlich in 2020 angekündigt wurde.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 255, "src_audio": "/acl6060/audio/en/255.wav", "src_ref": "It is available on the internet and can be installed via pip.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist im Internet verfügbar und kann über pip installiert werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 256, "src_audio": "/acl6060/audio/en/256.wav", "src_ref": "This system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieses System verfügt über ein einfaches, lernbasiertes Textklassifikationsmodell und gibt eine von fünf Markierung wie Funktionen oder Fehlerbehebungen für jede eingegebene Commit-Nachricht aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 257, "src_audio": "/acl6060/audio/en/257.wav", "src_ref": "This image is a sample usage that returns a corrective or bug fixes label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieses Bild ist eine Stichprobe, das eine Markierung für eine Korrektur oder Fehlerbehebung zurückgibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 258, "src_audio": "/acl6060/audio/en/258.wav", "src_ref": "Glyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Trainingsdaten von Glyph sind ziemlich klein, etwa 5.000, und werden in den unten beschriebenen Experimenten gezeigt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 259, "src_audio": "/acl6060/audio/en/259.wav", "src_ref": "The performance of the text classification model is not high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Leistung des Textklassifikationsmodells ist nicht hoch.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 260, "src_audio": "/acl6060/audio/en/260.wav", "src_ref": "I present two related researches, but their problems are limited applicability and scarce data resources.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich stelle zwei verwandte Forschungsarbeiten vor, deren Probleme jedoch die begrenzte Anwendbarkeit und die knappen Datenressourcen sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 261, "src_audio": "/acl6060/audio/en/261.wav", "src_ref": "Our paper solves these two problems and automatically generates high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Paper löst diese zwei Probleme und erzeugt automatisch qualitativ hochwertige Versionshinweise.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 262, "src_audio": "/acl6060/audio/en/262.wav", "src_ref": "With a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aufgrund des Problems der begrenzten Anwendbarkeit schlagen wir eine qualitativ hochwertige, klassenweise Zusammenfassungsmethode vor, die nur Commit-Nachrichten als Eingabe verwendet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 263, "src_audio": "/acl6060/audio/en/263.wav", "src_ref": "This proposed method can be used for all English repositories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 264, "src_audio": "/acl6060/audio/en/264.wav", "src_ref": "For the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Für das zweite Problem der knappen Datenressourcen haben wir unseren RNSum-Datensatz erstellt, der aus etwa 82.000 Daten besteht. Diese wurden aus den Daten der öffentlichen GitHub-Repositories mithilfe der GitHub-API gesammelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 265, "src_audio": "/acl6060/audio/en/265.wav", "src_ref": "Next, I'll describe our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Nächstes werde ich unseren Datensatz beschreiben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 266, "src_audio": "/acl6060/audio/en/266.wav", "src_ref": "Here is an example of data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist ein Beispiel für die Daten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 267, "src_audio": "/acl6060/audio/en/267.wav", "src_ref": "The left side is a commit message and the right side is the release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auf der linken Seite ist eine Commit-Nachricht und auf der rechten Seite sind die Versionshinweise.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 268, "src_audio": "/acl6060/audio/en/268.wav", "src_ref": "Release notes are labeled as improvements or fixes, etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Versionshinweise sind als Verbesserungen, Korrekturen usw. markiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 269, "src_audio": "/acl6060/audio/en/269.wav", "src_ref": "We have set up a task that takes the commit messages as input and outputs a labeled release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben eine Aufgabe eingerichtet, die die Commit-Nachricht als Eingabe aufnimmt und die Ausgabe ist ein markierter Versionshinweis.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 270, "src_audio": "/acl6060/audio/en/270.wav", "src_ref": "This can be regarded as a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies kann als eine Zusammenfassungsaufgabe betrachtet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 271, "src_audio": "/acl6060/audio/en/271.wav", "src_ref": "We have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben vier Markierungen vordefiniert: Funktionen, Verbesserungen, Fehlerbehebungen, Beseitigung von Verwerfungen und bahnbrechende Änderungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 272, "src_audio": "/acl6060/audio/en/272.wav", "src_ref": "These were set based on previous research and other factors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese wurden auf der Grundlage von früheren Forschungen und anderen Faktoren festgelegt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 273, "src_audio": "/acl6060/audio/en/273.wav", "src_ref": "The release note on the bottom right is extracted from the release note on the bottom left.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Versionshinweise unten rechts wurde aus den Versionshinweisen unten links extrahiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 274, "src_audio": "/acl6060/audio/en/274.wav", "src_ref": "At this time, it is necessary to detect the four labels that have been set up in advance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zu diesem Zeitpunkt müssen die vier Markierungen, die im Voraus erstellt wurden, erkannt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 275, "src_audio": "/acl6060/audio/en/275.wav", "src_ref": "But the labels are not always consistent with each repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Markierungen sind jedoch nicht immer mit jedem Repository konsistent.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 276, "src_audio": "/acl6060/audio/en/276.wav", "src_ref": "For example, the improvements label includes improvements, enhancements, optimizations, and so on.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Markierung „Verbesserungen“ umfasst beispielsweise Verbesserungen, Erweiterungen, Optimierungen und so weiter.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 277, "src_audio": "/acl6060/audio/en/277.wav", "src_ref": "We prepared a vocabulary list of about thirty labels for each of these notational variations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben eine Wortschatzliste mit etwa 30 Markierungen für jede dieser Notationsvarianten erstellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 278, "src_audio": "/acl6060/audio/en/278.wav", "src_ref": "This is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese dient dazu, die Klasse des Versionshinweises zu erkennen. Sie sammelt den Text des Versionshinweises, der als Versionshinweissatz für die Klasse folgt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 279, "src_audio": "/acl6060/audio/en/279.wav", "src_ref": "Next is a commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Nächstes folgt eine Commit-Nachricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 280, "src_audio": "/acl6060/audio/en/280.wav", "src_ref": "Commit messages are not tied to each release.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Commit-Nachrichten sind nicht an die einzelnen Versionen gebunden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 281, "src_audio": "/acl6060/audio/en/281.wav", "src_ref": "As shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die aktuelle Version 2.5 bis 19 ist, müssen wir, wie im Bild unten gezeigt, die frühere Version 2.5 bis 18 identifizieren und eine Diff erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 282, "src_audio": "/acl6060/audio/en/282.wav", "src_ref": "This is a bit tedious and it is not enough to just get a list of releases and look at the before and after.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies ist etwas mühsam und es reicht nicht aus, nur eine Liste von Versionen zu erstellen und die Vorher-Nachher-Bilanz zu betrachten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 283, "src_audio": "/acl6060/audio/en/283.wav", "src_ref": "We created a heuristic matching rule to get the previous and next versions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben eine heuristische Abgleichsregel erstellt, um die vorherige und die nächste Version zu bekommen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 284, "src_audio": "/acl6060/audio/en/284.wav", "src_ref": "Dataset analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Analyse des Datensatzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 285, "src_audio": "/acl6060/audio/en/285.wav", "src_ref": "In the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Am Ende wurden 7.200 Repositories und 82 Daten gesammelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 286, "src_audio": "/acl6060/audio/en/286.wav", "src_ref": "Also, the average number of release notes tokens is sixty three, which is quite high for a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Außerdem liegt die durchschnittliche Anzahl der Versionshinweis-Token bei 63, was für eine Zusammenfassungsaufgabe recht hoch ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 287, "src_audio": "/acl6060/audio/en/287.wav", "src_ref": "Also, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auch die Anzahl der einzigartigen Token ist mit 830.000 recht groß.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 288, "src_audio": "/acl6060/audio/en/288.wav", "src_ref": "This is due to the large number of unique class or method names found in the repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies ist auf die große Anzahl von eindeutigen Klassen- oder Methodennamen im Repository zurückzuführen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 289, "src_audio": "/acl6060/audio/en/289.wav", "src_ref": "Next, I will explain the proposed method.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Nächstes werde ich die vorgeschlagene Methode erläutern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 290, "src_audio": "/acl6060/audio/en/290.wav", "src_ref": "The classwise extractive then abstractive summarization model consists of two neural modules.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das klassenweise extraktive und dann abstrahierende Zusammenfassungsmodell besteht aus zwei neuronalen Modulen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 291, "src_audio": "/acl6060/audio/en/291.wav", "src_ref": "A classifier using BERT or CodeBERT and a generator using BART.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein Klassifikator verwendet BERT oder CodeBERT und ein Generator verwendet BART.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 292, "src_audio": "/acl6060/audio/en/292.wav", "src_ref": "First, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zuerst verwendet CEAS einen Klassifikator, um jede Commit-Nachricht in fünf Klassen von Versionshinweisen zu klassifizieren, die Verbesserungen, Fehlerbehebungen, Verwerfungen und eine weitere Klasse enthalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 293, "src_audio": "/acl6060/audio/en/293.wav", "src_ref": "The commit messages classified as other are discarded.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die als „Sonstiges“ eingestuften Commit-Nachrichten werden verworfen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 294, "src_audio": "/acl6060/audio/en/294.wav", "src_ref": "Then CEAS applies the generator to the four labeled documents independently and generates release notes for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann wendet CEAS den Generator auf die vier markierten Dokumente unabhängig voneinander an und erstellt Versionshinweise für jede Klasse.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 295, "src_audio": "/acl6060/audio/en/295.wav", "src_ref": "In this task, the direct correspondences between commit messages and release notes are not known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In dieser Aufgabe sind die direkten Korrespondenzen zwischen Commit-Nachrichten und Versionshinweisen nicht bekannt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 296, "src_audio": "/acl6060/audio/en/296.wav", "src_ref": "Therefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um den Klassifikator zu trainieren, haben wir daher jeder eingegebenen Commit-Nachricht eine Umfrage zugewiesen und dabei die ersten zehn Zeichen jeder Commit-Nachricht verwendet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 297, "src_audio": "/acl6060/audio/en/297.wav", "src_ref": "We modeled the classwise abstractive summarization approach by two different methods.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben den klassenweisen abstrakten Zusammenfassungsansatz durch zwei verschiedene Methoden modelliert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 298, "src_audio": "/acl6060/audio/en/298.wav", "src_ref": "The first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das erste Modell, das wir CAS-Single nennen, besteht aus einem einzigen 6-zu-6-Netzwerk und generiert einen einzigen Versionshinweistext, der eine Verkettung von eingegebenen Commit-Nachrichten ergibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 299, "src_audio": "/acl6060/audio/en/299.wav", "src_ref": "The output texts can be divided into classwise segments based on special class-specific endpoint symbols.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die ausgegebenen Texte können auf der Grundlage spezieller klassenspezifischer Endpunktsymbole in klassenweise Segmente unterteilt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 300, "src_audio": "/acl6060/audio/en/300.wav", "src_ref": "The second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die zweite Methode, die wir CAS-Multi nennen, besteht aus vier verschiedenen seq2seq-Netzwerken, von denen jedes einer der festen Versionshinweisklassen entspricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 301, "src_audio": "/acl6060/audio/en/301.wav", "src_ref": "Okay, let me explain the experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde nun die Experimente erklären.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 302, "src_audio": "/acl6060/audio/en/302.wav", "src_ref": "Five methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Fünf Methoden wurden verglichen: CEAS, CAS-Single, CAS-Multi, Clustering und Glyph aus einer früheren Studie.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 303, "src_audio": "/acl6060/audio/en/303.wav", "src_ref": "Regarding evaluation, in some cases, release notes are output in multiple sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Was die Evaluation betrifft, so werden in einigen Fällen die Versionshinweise in mehreren Sätzen ausgegeben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 304, "src_audio": "/acl6060/audio/en/304.wav", "src_ref": "Since it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da es schwierig ist, die Anzahl von Sätzen zu berechnen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 305, "src_audio": "/acl6060/audio/en/305.wav", "src_ref": "The BLEU is penalized when the system outputs a short sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der BLEU wird benachteiligt, wenn das System einen kurzen Satz ausgibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 306, "src_audio": "/acl6060/audio/en/306.wav", "src_ref": "This penalty results in a lower BLEU value in the experiment results described next.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieser Abzug führt zu einem niedrigeren BLEU-Wert in den nachfolgend beschriebenen Versuchsergebnissen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 307, "src_audio": "/acl6060/audio/en/307.wav", "src_ref": "Finally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich berechnen wir auch die Spezifität, da ROUGE und BLEU nicht berechnet werden können, wenn die Versionshinweise leer sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 308, "src_audio": "/acl6060/audio/en/308.wav", "src_ref": "A higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eine höhere Spezifität bedeutet, dass das Modell in Fällen, in denen die Versionshinweise leer sind, einen leeren Text korrekt ausgibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 309, "src_audio": "/acl6060/audio/en/309.wav", "src_ref": "Here are the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind die Resultate.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 310, "src_audio": "/acl6060/audio/en/310.wav", "src_ref": "Since the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da der Datensatz E-Mail-Adressen, Hash-Werte usw. enthält, haben wir auch den bereinigten Datensatz ausgewertet, der diese ausschließt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 311, "src_audio": "/acl6060/audio/en/311.wav", "src_ref": "CEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CEAS und CAS erreichten ROUGE-L-Werte, die mehr als zehn Punkte über den Baselines lagen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 312, "src_audio": "/acl6060/audio/en/312.wav", "src_ref": "In particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insbesondere beim bereinigten Testsatz ist der Abstand zwischen der vorgeschlagenen Methode und den Baselines auf mehr als zwanzig Punkte angestiegen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 313, "src_audio": "/acl6060/audio/en/313.wav", "src_ref": "These results indicate that CEAS and CAS are significantly affected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Ergebnisse zeigen, dass CEAS und CAS signifikant betroffen sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 314, "src_audio": "/acl6060/audio/en/314.wav", "src_ref": "CEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CEAS erzielte bessere ROUGE-L-Werte als CAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators beim Training des Klassifikators mit Pseudo-Markierungen effektiv ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 315, "src_audio": "/acl6060/audio/en/315.wav", "src_ref": "High coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eine hohe Abdeckung von CEAS kann wahrscheinlich erreicht werden, weil sich der Klassifikator auf die Auswahl relevanter Commit-Nachrichten für jede Klasse konzentrieren kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 316, "src_audio": "/acl6060/audio/en/316.wav", "src_ref": "CAS-Multi tended to yield higher ROUGE-L than CAS-Single.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CAS-Multi ergab tendenziell mehr ROUGE-L-Werte als CAS-Single.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 317, "src_audio": "/acl6060/audio/en/317.wav", "src_ref": "Suggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir schlagen vor, dass es auch effektiv ist, unterschiedliche abstrakte Zusammenfassungsmodelle, die unabhängig voneinander sind, für jede Klasse von den Versionshinweisen zu entwickeln.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 318, "src_audio": "/acl6060/audio/en/318.wav", "src_ref": "Here are an error analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist eine Fehleranalyse.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 319, "src_audio": "/acl6060/audio/en/319.wav", "src_ref": "CAS methods tend to output shorter sentences than human reference sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CAS-Methoden geben tendenziell kürzere Sätze aus als menschliche Referenzsätze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 320, "src_audio": "/acl6060/audio/en/320.wav", "src_ref": "In the figure on the right, the reference sentence has three or four sentences, while CAS has only one.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In der Abbildung rechts besteht der Referenzsatz aus drei oder vier Sätzen, während CAS aus nur einem besteht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 321, "src_audio": "/acl6060/audio/en/321.wav", "src_ref": "The reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Grund für die Reluktanz dieses Modells ist, dass in den Trainingsdaten nur 33 Prozent der Sätze in der Markierung Funktionen und 40 Prozent in der Markierung Verbesserung zu finden sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 322, "src_audio": "/acl6060/audio/en/322.wav", "src_ref": "Furthermore, CAS methods cannot generate accurate release notes without additional information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus können CAS-Methoden ohne zusätzliche Informationen keine genauen Versionshinweise erstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 323, "src_audio": "/acl6060/audio/en/323.wav", "src_ref": "The top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Oben auf der rechten Seite ist ein Beispiel für eine sehr unübersichtliche Commit-Nachricht. Der vollständige Satz kann nicht ohne Bezug auf den entsprechenden Fortschritt oder das Problem generiert werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 324, "src_audio": "/acl6060/audio/en/324.wav", "src_ref": "The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das folgende Beispiel zeigt, dass die beiden Commit-Nachrichten in der Eingabe zusammenhängen und zu einem Satz zusammengefasst werden sollten, was jedoch nicht gelingt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 325, "src_audio": "/acl6060/audio/en/325.wav", "src_ref": "Finally, a conclusion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nun kommen wir zum Fazit.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 326, "src_audio": "/acl6060/audio/en/326.wav", "src_ref": "We have built a new dataset for automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben einen neuen Datensatz für die automatische Generierung von Versionshinweisen erstellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 327, "src_audio": "/acl6060/audio/en/327.wav", "src_ref": "We have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben auch eine Aufgabe für die Eingabe von Commit-Nachrichten und deren Zusammenfassung formuliert, sodass sie für alle auf Englisch geschriebene Projekte anwendbar ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 328, "src_audio": "/acl6060/audio/en/328.wav", "src_ref": "Our experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere Experimente zeigen, dass die vorgeschlagene Methode weniger verrauschte Versionshinweise mit höherem Abdeckungsgrad als die Baselines erzeugt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 329, "src_audio": "/acl6060/audio/en/329.wav", "src_ref": "Please check out our dataset on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bitte sehen Sie sich unseren Datensatz auf GitHub an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 330, "src_audio": "/acl6060/audio/en/330.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 331, "src_audio": "/acl6060/audio/en/331.wav", "src_ref": "Hello. My name is Asaf Harari.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo! Mein Name ist Asaf Harari.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 332, "src_audio": "/acl6060/audio/en/332.wav", "src_ref": "And I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde unser Paper vorstellen: Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 333, "src_audio": "/acl6060/audio/en/333.wav", "src_ref": "Data scientists analyze data and mainly focus on the manipulating the data's existing features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Datenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der bestehenden Funktionen von Daten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 334, "src_audio": "/acl6060/audio/en/334.wav", "src_ref": "But sometimes, these features are limited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber manchmal sind diese Funktionen begrenzt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 335, "src_audio": "/acl6060/audio/en/335.wav", "src_ref": "Feature generation using another data source may add substantial information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Generierung von Funktionen unter Verwendung einer anderen Datenquelle kann wesentliche Informationen hinzufügen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 336, "src_audio": "/acl6060/audio/en/336.wav", "src_ref": "Our research goal is automatic tabular data enrichment using external sources' free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Forschungsziel ist die automatische Anreicherung tabellarischer Daten mit freiem Text aus externen Quellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 337, "src_audio": "/acl6060/audio/en/337.wav", "src_ref": "Assume we have a tabular dataset and a knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Angenommen, wir haben einen tabellarischen Datensatz und eine Wissensbasis.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 338, "src_audio": "/acl6060/audio/en/338.wav", "src_ref": "We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir brauchen einen automatischen Prozess, der Entity Linking und eine Textanalyse umfasst, um neue Funktionen aus dem freien Text der Wissensbasis zu extrahieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 339, "src_audio": "/acl6060/audio/en/339.wav", "src_ref": "Our framework FeSTE is exactly this automatic process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Rahmen FeSTE ist genau dieser automatische Prozess.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 340, "src_audio": "/acl6060/audio/en/340.wav", "src_ref": "So let's see an example in a dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sehen wir uns also ein Beispiel in einem Datensatz an, der in FeSTE eingespeist wird.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 341, "src_audio": "/acl6060/audio/en/341.wav", "src_ref": "In this example, the dataset is university dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel ist der Datensatz der Universitätsdatensatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 342, "src_audio": "/acl6060/audio/en/342.wav", "src_ref": "When its goal is to classify universities into low ranking universities and high-ranking universities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Ziel besteht darin, die Universitäten nach niedrigem und hohem Rang einzuteilen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 343, "src_audio": "/acl6060/audio/en/343.wav", "src_ref": "As knowledge base, we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Wissensbasis verwenden wir Wikipedia.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 344, "src_audio": "/acl6060/audio/en/344.wav", "src_ref": "The first phase of FeSTE is entity linking.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die erste Phase von FeSTE ist Entity Linking.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 345, "src_audio": "/acl6060/audio/en/345.wav", "src_ref": "When each entity, in this example the university name, is linked to an entity within the knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jede Entität, in diesem Beispiel der Name der Universität, ist mit einer Entität innerhalb der Wissensbasis verknüpft.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 346, "src_audio": "/acl6060/audio/en/346.wav", "src_ref": "And and the text of the entities of the knowledge base is extracted and added to the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Text der Entitäten aus der Wissensbasis wird extrahiert und dem Datensatz hinzugefügt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 347, "src_audio": "/acl6060/audio/en/347.wav", "src_ref": "In this example, the text is the Wikipedia page's abstract.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel ist der Text das Abstract der Wikipedia-Seite.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 348, "src_audio": "/acl6060/audio/en/348.wav", "src_ref": "Now, we need to generate or extract features from the retrieved text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nun müssen wir Funktionen aus dem abgerufenen Text generieren oder extrahieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 349, "src_audio": "/acl6060/audio/en/349.wav", "src_ref": "So, we need to ah feature extraction phase ah which includes text analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir müssen also eine Phase der Funktionsextraktion einleiten, zu der die Textanalyse gehört.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 350, "src_audio": "/acl6060/audio/en/350.wav", "src_ref": "And this is the main novelty of this paper and I will deep dive into it in the next slides.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das ist die wichtigste Neuerung dieses Papers, auf die ich auf den nächsten Folien näher eingehen werde.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 351, "src_audio": "/acl6060/audio/en/351.wav", "src_ref": "After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nach der Phase der Funktionsextraktion folgt eine Phase der Funktionsgenerierung, in der wir die extrahierten Funktionen verwenden, um eine kleine Anzahl neuer Funktionen zu erzeugen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 352, "src_audio": "/acl6060/audio/en/352.wav", "src_ref": "First generate ah features in the number of classes of the original dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst werden Funktionen in der Anzahl der Klassen des ursprünglichen Datensatzes generiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 353, "src_audio": "/acl6060/audio/en/353.wav", "src_ref": "In this example, the original dataset has two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 354, "src_audio": "/acl6060/audio/en/354.wav", "src_ref": "So, FeSTE generates two new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "FeSTE generiert also zwei neue Funktionen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 355, "src_audio": "/acl6060/audio/en/355.wav", "src_ref": "But if the dataset has five classes, FeSTE generates five new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn der Datensatz jedoch fünf Klassen hat, generiert FeSTE fünf neue Funktionen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 356, "src_audio": "/acl6060/audio/en/356.wav", "src_ref": "Each feature represents the likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jede Funktion stellt die Wahrscheinlichkeit für jede Klasse dar.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 357, "src_audio": "/acl6060/audio/en/357.wav", "src_ref": "To analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um den Text zu analysieren, verwenden wir die aktuellste Textanalyse, die auf Transformer-basierten Sprachmodellen wie BERT, GPT, XLNet und anderen beruht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 358, "src_audio": "/acl6060/audio/en/358.wav", "src_ref": "It is but it is not likely that we can train language models using the input datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist aber unwahrscheinlich, dass wir Sprachmodelle mit den eingegebenen Datensätzen trainieren können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 359, "src_audio": "/acl6060/audio/en/359.wav", "src_ref": "So a naive approach will be ah target task finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein naiver Ansatz ist also eine Feinabstimmung des Zieldatensatzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 360, "src_audio": "/acl6060/audio/en/360.wav", "src_ref": "So, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In der Phase der Funktionsextraktion können wir also vortrainierte Sprachmodelle herunterladen und das Sprachmodell über den Zieldatensatz feinabstimmen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 361, "src_audio": "/acl6060/audio/en/361.wav", "src_ref": "In this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel zur Feinabstimmung des Sprachmodells klassifizieren wir den Text in Klassen, d. h. wir klassifizieren das Abstract in die Klassen niedrig oder hoch.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 362, "src_audio": "/acl6060/audio/en/362.wav", "src_ref": "Receive the language model output, which is the likelihood for each class and use as new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir erhalten die Ausgabe des Sprachmodells, die die Wahrscheinlichkeit für jede Klasse darstellt, und verwenden sie als neue Funktionen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 363, "src_audio": "/acl6060/audio/en/363.wav", "src_ref": "The problem with this approach is datasets may have few distinct entities / texts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Problem bei diesem Ansatz ist, dass Datensätze möglicherweise nur wenige unterschiedliche Entitäten/Texte beinhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 364, "src_audio": "/acl6060/audio/en/364.wav", "src_ref": "In our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem Experiment enthält fast die Hälfte der Datensätze weniger als 400 Stichproben und der kleinste Datensatz enthält 35 Stichproben in einem Trainingssatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 365, "src_audio": "/acl6060/audio/en/365.wav", "src_ref": "So to finetune a language model over ah this dataset will be ineffective.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Feinabstimmung eines Sprachmodells über diesen Datensatz ist also unwirksam.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 366, "src_audio": "/acl6060/audio/en/366.wav", "src_ref": "But we can use prior knowledge about pre-analyzed datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wir können vorherige Kenntnisse über bereits analysierte Datensätze nutzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 367, "src_audio": "/acl6060/audio/en/367.wav", "src_ref": "Because FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da wir FeSTE auf mehrere Datensätze anwenden, können wir die „n minus 1“-Datensätze verwenden, um Informationen über die „n minus 1“-Datensätze zu sammeln, und diese Informationen verwenden, wenn wir den n-ten Datensatz analysieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 368, "src_audio": "/acl6060/audio/en/368.wav", "src_ref": "What we, what we suggest is to add, to add another finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir schlagen vor, eine weitere Feinabstimmungsphase hinzuzufügen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 369, "src_audio": "/acl6060/audio/en/369.wav", "src_ref": "A preliminary multitask finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das war eine vorläufige Multitask-Feinabstimmungsphase.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 370, "src_audio": "/acl6060/audio/en/370.wav", "src_ref": "When you finetune the language model over the n minus one datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Feinabstimmung des Sprachmodells geht über die „n minus 1“-Datensätze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 371, "src_audio": "/acl6060/audio/en/371.wav", "src_ref": "And, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann führen wir eine weitere Feinabstimmungsphase durch, die eine Feinabstimmung des Zieldatensatzes ist, wenn wir das Sprachmodell über den n-ten Zieldatensatz feinabstimmen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 372, "src_audio": "/acl6060/audio/en/372.wav", "src_ref": "The state-of-the-art in multitask ah multitask finetuning called MTDNN.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die modernste Multitask-Feinabstimmung wird MTDNN genannt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 373, "src_audio": "/acl6060/audio/en/373.wav", "src_ref": "In MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei der MTDNN bleiben die Überschriften in der Reihe der Aufgaben im Trainingssatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 374, "src_audio": "/acl6060/audio/en/374.wav", "src_ref": "So, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel gibt es also vier Aufgaben im Trainingssatz. Das MTDNN enthält also vier Überschriften, wie Sie auf dem Bild sehen können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 375, "src_audio": "/acl6060/audio/en/375.wav", "src_ref": "And it samples a random batch from ah from the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es nimmt sich eine Zufallsstichprobe aus dem Trainingssatz heraus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 376, "src_audio": "/acl6060/audio/en/376.wav", "src_ref": "And if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die Zufallsstichprobe zum Beispiel zu einer einzelnen Satzklassifikationsaufgabe gehört, führt sie Vorwärts- und Rückwärtspfade durch die erste Überschrift aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 377, "src_audio": "/acl6060/audio/en/377.wav", "src_ref": "And if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die Zufallsstichprobe zur paarweisen Ranking-Aufgabe gehört, führt sie den Vorwärts- und Rückwärtspfad über die letzte Überschrift aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 378, "src_audio": "/acl6060/audio/en/378.wav", "src_ref": "In our scenario, ah tabular datasets vary in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem Szenario variieren tabellarische Datensätze in der Anzahl der Klassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 379, "src_audio": "/acl6060/audio/en/379.wav", "src_ref": "So there are many tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es gibt also viele Aufgaben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 380, "src_audio": "/acl6060/audio/en/380.wav", "src_ref": "MTDNN maintained number of classes, heads, output layers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "MTDNN enthält eine Anzahl von Klassen, Überschriften und ausgegebenen Ebenen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 381, "src_audio": "/acl6060/audio/en/381.wav", "src_ref": "And the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und zusätzlich muss MTDNN neue Überschriften für einen neuen Datensatz mit einer neuen Aufgabe initialisieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 382, "src_audio": "/acl6060/audio/en/382.wav", "src_ref": "Our approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Ansatz, genannt Feinabstimmungsreformulierungsaufgabe, besteht darin, dass wir, anstatt mehrere Überschriften beizubehalten, jeden Datensatz in einen Satz pro Klassifikationssproblem umformulieren, was zwei Klassenaufgaben entspricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 383, "src_audio": "/acl6060/audio/en/383.wav", "src_ref": "So let's see an example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schauen wir uns ein Beispiel an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 384, "src_audio": "/acl6060/audio/en/384.wav", "src_ref": "Here is the our input dataset which consists of entities, features, text and classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist unser eingegebener Datensatz, der aus Entitäten, Funktionen, Text und Klassen besteht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 385, "src_audio": "/acl6060/audio/en/385.wav", "src_ref": "And, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir formulieren die Aufgabe von einer Klassifizierung des Textes in niedrig oder hoch zu einer Klassifizierung des Textes, des Abstracts und der Klasse in wahr oder falsch um.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 386, "src_audio": "/acl6060/audio/en/386.wav", "src_ref": "Or in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mit anderen Worten: Wir haben das Sprachmodell so trainiert, dass es ein Abstract und die Klasse als Abstract und die Klasse klassifiziert, wenn das Abstract zur Klasse gehört oder nicht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 387, "src_audio": "/acl6060/audio/en/387.wav", "src_ref": "So the label vector in this case stays always ah which consists always with two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "So bleibt die Vektor-Markierung in diesem Fall bestehen, die immer aus zwei Klassen besteht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 388, "src_audio": "/acl6060/audio/en/388.wav", "src_ref": "And this is the ah algorithm for our fine, reformulated finetuning approach.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das ist der Algorithmus für unseren neu formulierten Finabstimmungsansatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 389, "src_audio": "/acl6060/audio/en/389.wav", "src_ref": "So let's see the full framework.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schauen wir uns also den gesamten Rahmen an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 390, "src_audio": "/acl6060/audio/en/390.wav", "src_ref": "Dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Datensatz wird in FeSTE eingespeist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 391, "src_audio": "/acl6060/audio/en/391.wav", "src_ref": "And then ah FeSTE executes entity linking phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann führt FeSTE die Entity-Linking-Phase aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 392, "src_audio": "/acl6060/audio/en/392.wav", "src_ref": "It ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es extrahiert den Text aus der Wissensbasis, die in diesem Beispiel das Abstract der Wikipedia-Seite ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 393, "src_audio": "/acl6060/audio/en/393.wav", "src_ref": "Then it reformulated the task into a pairwise sentence classification task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann formulierte es die Aufgabe in eine paarweise Satzklassifikationsaufgabe um.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 394, "src_audio": "/acl6060/audio/en/394.wav", "src_ref": "Applied the language model to the new task and the output likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Sprachmodell wird auf die neue Aufgabe und die Ausgabewahrscheinlichkeit für jede Klasse angewandt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 395, "src_audio": "/acl6060/audio/en/395.wav", "src_ref": "And now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nun ist das Sprachmodell bereits über den „n minus 1“-Datensatz mithilfe einer vorläufigen Multitask-Feinabstimmung feinabgestimmt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 396, "src_audio": "/acl6060/audio/en/396.wav", "src_ref": "Then we use the output vector of the language model as a newly generated feature in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann verwenden wir den Ausgabe-Vektor des Sprachmodells als eine neu generierte Funktion in der Anzahl der Klassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 397, "src_audio": "/acl6060/audio/en/397.wav", "src_ref": "To evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zur Evaluation unseres Rahmens verwenden wir 17 tabellarische Klassifikationsdatensätze, die sich in Größe, Funktionen, Ausgewogenheit, Domäne und anfänglicher Leistung unterscheiden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 398, "src_audio": "/acl6060/audio/en/398.wav", "src_ref": "And as knowledge base we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Wissensbasis verwenden wir Wikipedia.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 399, "src_audio": "/acl6060/audio/en/399.wav", "src_ref": "We design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir konzipieren unser Experiment als Leave-One-Out-Evaluation, bei dem wir FeSTe mit 16 Datensätzen trainieren und auf den 17. Datensatz anwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 400, "src_audio": "/acl6060/audio/en/400.wav", "src_ref": "We also, we also split each dataset into four folds and apply four folds cross validation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Außerdem teilen wir jeden Datensatz in vier Brüche auf und wenden eine vierfache Kreuzvalidierung an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 401, "src_audio": "/acl6060/audio/en/401.wav", "src_ref": "Then, we generate the new features and evaluate them using five evaluation classifiers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann generieren wir die neuen Funktionen und bewerten sie mit fünf Evaluationssklassifikatoren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 402, "src_audio": "/acl6060/audio/en/402.wav", "src_ref": "We use in our experiments base BERT base architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir verwenden in unseren Experimenten die Basisarchitektur BERT.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 403, "src_audio": "/acl6060/audio/en/403.wav", "src_ref": "Here are the results for our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind die Ergebnisse unserer Experimente.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 404, "src_audio": "/acl6060/audio/en/404.wav", "src_ref": "You can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie können sehen, dass wir unseren Rahmen mit der Feinabstimmung des Zieldatensatzes, mit der Feinabstimmung der Zielaufgabe und einer vorläufigen MTDNN-Feinabstimmung vergleichen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 405, "src_audio": "/acl6060/audio/en/405.wav", "src_ref": "And our reformulated finetuning achieves the best result, the best performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere neu formulierte Feinabstimmung erreicht das beste Ergebnis und die beste Leistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 406, "src_audio": "/acl6060/audio/en/406.wav", "src_ref": "While MTDNN achieved two percent improvement over the target dataset finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "MTDNN erreichte zwei Prozent Verbesserung gegenüber der Feinabstimmung des Zieldatensatzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 407, "src_audio": "/acl6060/audio/en/407.wav", "src_ref": "Our approach achieved six percent improvement.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mit unserem Ansatz haben wir eine Verbesserung von sechs Prozent erreicht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 408, "src_audio": "/acl6060/audio/en/408.wav", "src_ref": "When we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir uns den kleinen Datensatz anschauen, sehen wir, dass die Leistung von MTDNN abnimmt und die Verbesserung der vorläufigen Multitask-Feinabstimmungsphase auf 1,5 Prozent sinkt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 409, "src_audio": "/acl6060/audio/en/409.wav", "src_ref": "But our performance increased to eleven percent compared to the target task finetuning alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber unsere Leistung stieg auf elf Prozent im Vergleich zur Feinabstimmung des Zieldatensatzes allein.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 410, "src_audio": "/acl6060/audio/en/410.wav", "src_ref": "For summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Für die Summierung ermöglicht FeSTE in unseren Experimenten eine Few-Shot-Anreicherung aus 35 Proben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 411, "src_audio": "/acl6060/audio/en/411.wav", "src_ref": "It uses one architecture for all tasks and datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es verwendet eine Architektur für alle Aufgaben und Datensätze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 412, "src_audio": "/acl6060/audio/en/412.wav", "src_ref": "And it keeps the head of ah of the model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und es behält den Hauptteil des Modells.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 413, "src_audio": "/acl6060/audio/en/413.wav", "src_ref": "But it adds reformulation phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber es fügt eine Reformulierungsphase hinzu.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 414, "src_audio": "/acl6060/audio/en/414.wav", "src_ref": "It augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es erweitert den Trainingssatz und benötigt einen Zielwert mit semantischer Bedeutung, damit wir ihn in das Sprachmodell einspeisen und für das Satzpaar-Klassifikationsproblem verwenden können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 415, "src_audio": "/acl6060/audio/en/415.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 0, "src_audio": "/acl6060/audio/en/0.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo zusammen. Heute werde ich unsere Forschungsarbeit Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction vorstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 1, "src_audio": "/acl6060/audio/en/1.wav", "src_ref": "I'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich bin Allan vom ByteDance AI Lab, und dies ist eine gemeinsame Arbeit mit Jierui Li von der University of Texas in Austin und Wei Lu von SUTD.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 2, "src_audio": "/acl6060/audio/en/2.wav", "src_ref": "First, I'd like to talk about our motivation for reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst möchte ich über unsere Motivation für das Argumentieren sprechen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 3, "src_audio": "/acl6060/audio/en/3.wav", "src_ref": "So here we show an examples where multi-step reasoning is helpful.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deshalb zeigen wir hier ein Beispiel, bei dem mehrstufiges Argumentieren hilfreich ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 4, "src_audio": "/acl6060/audio/en/4.wav", "src_ref": "So this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Abbildung stammt aus dem PaLM-Paper, in dem sie Prompting durchführen, um das Netzwerkproblem in einem Few-Shot-Lernszenario zu lösen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 5, "src_audio": "/acl6060/audio/en/5.wav", "src_ref": "So on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auf der linken Seite können wir also sehen, dass wenn einige Beispiele nur aus Frage und Antworten bestehen, dann bekommen wir nicht die richtigen Antworten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 6, "src_audio": "/acl6060/audio/en/6.wav", "src_ref": "But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir jedoch eine weitere Argumentationsbeschreibung hinzufügen, ist das Modell in der Lage, die Argumentationsbeschreibung vorherzusagen und hier auch eine korrekte Vorhersage zu treffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 7, "src_audio": "/acl6060/audio/en/7.wav", "src_ref": "So it is good to have interpretable multi-step reasoning as output.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist also gut, eine interpretierbare mehrstufige Argumentation als Ergebnis zu haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 8, "src_audio": "/acl6060/audio/en/8.wav", "src_ref": "And we also think math word problem is a straightforward application to evaluate such reasoning abilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir denken auch, dass mathematische Wortprobleme eine einfache Anwendung sind, um solche Argumentationsfähigkeiten zu bewerten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 9, "src_audio": "/acl6060/audio/en/9.wav", "src_ref": "So, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem Problem-Setup müssen wir also angesichts der Fragen diese Frage lösen und die numerischen Antworten erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 10, "src_audio": "/acl6060/audio/en/10.wav", "src_ref": "So in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unseren Datensätzen finden wir auch den mathematischen Ausdruck, der uns zu dieser bestimmten Antwort führt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 11, "src_audio": "/acl6060/audio/en/11.wav", "src_ref": "So, certain assumptions ah also apply as in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es gelten also auch hier bestimmte Annahmen wie in früheren Arbeiten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 12, "src_audio": "/acl6060/audio/en/12.wav", "src_ref": "We assume the precision of quantities are known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir gehen davon aus, dass die Genauigkeit der Mengen bekannt ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 13, "src_audio": "/acl6060/audio/en/13.wav", "src_ref": "And we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und wir betrachten nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponentialrechnung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 14, "src_audio": "/acl6060/audio/en/14.wav", "src_ref": "Furthermore, complicated operators can be actually decomposed into these basic operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus können komplizierte Operatoren in diese Grundoperatoren zerlegt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 15, "src_audio": "/acl6060/audio/en/15.wav", "src_ref": "So, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die früheren Arbeiten zum Lösen von mathematischen Wortproblemen lassen sich also in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modell einteilen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 16, "src_audio": "/acl6060/audio/en/16.wav", "src_ref": "So, traditional sequence to sequence model convert the expression to a specific sequence for generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das traditionelle Sequenz-zu-Sequenz-Modell wandelt also den Ausdruck in eine spezifische Sequenz für die Generierung um.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 17, "src_audio": "/acl6060/audio/en/17.wav", "src_ref": "And it is pretty easy to implement and it can generalize to many different complicated problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist ziemlich einfach zu implementieren und kann für viele verschiedene komplizierte Probleme verallgemeinert werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 18, "src_audio": "/acl6060/audio/en/18.wav", "src_ref": "But the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Nachteile sind jedoch, dass die Leistung im Allgemeinen nicht besser ist als die des strukturierten Modells und dass es an Interpretierbarkeit für Vorhersagen mangelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 19, "src_audio": "/acl6060/audio/en/19.wav", "src_ref": "But actually this direction is still quite popular because of um the transformer model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber eigentlich ist diese Richtung immer noch recht beliebt wegen des Transformermodells.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 20, "src_audio": "/acl6060/audio/en/20.wav", "src_ref": "So, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In baumbasierten Modellen strukturieren wir diese Ausdrücke in Baumform und folgen einem geordneten Traversal in Baumgenerationen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 21, "src_audio": "/acl6060/audio/en/21.wav", "src_ref": "So here we keep generating the operators until we reach the leaves, which are the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier fahren wir also fort, die Operatoren zu generieren, bis wir die Blätter erreichen, die die Mengen darstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 22, "src_audio": "/acl6060/audio/en/22.wav", "src_ref": "So here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Gute daran ist, dass wir dadurch diese binäre Baumstruktur erhalten. Sie ist eigentlich ziemlich kontraintuitiv, weil wir zuerst den Operator und dann am Ende die Mengen generieren müssen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 23, "src_audio": "/acl6060/audio/en/23.wav", "src_ref": "And the second thing is that it also contains some repetitive computations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zudem enthält sie auch einige sich wiederholende Berechnungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 24, "src_audio": "/acl6060/audio/en/24.wav", "src_ref": "So here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir uns also diesen Ausdruck ansehen, wird 8 x 3 + 3 eigentlich zweimal generiert, aber eigentlich sollten wir die Ergebnisse wiederverwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 25, "src_audio": "/acl6060/audio/en/25.wav", "src_ref": "So, in our proposed approach we want to solve those problems in a step by step and interpretable manners.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem vorgeschlagenen Ansatz wollen wir diese Probleme schrittweise und auf interpretierbare Art und Weise lösen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 26, "src_audio": "/acl6060/audio/en/26.wav", "src_ref": "So for example, here in the second step, ah we can obtain these divisors which is twenty seven.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier im zweiten Schritt zum Beispiel können wir diese Teiler erhalten, was 27 ergibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 27, "src_audio": "/acl6060/audio/en/27.wav", "src_ref": "And we can also refer back to the original questions to find the relevant contents.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können auch auf die ursprünglichen Fragen zurückgreifen, um die relevanten Inhalte zu finden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 28, "src_audio": "/acl6060/audio/en/28.wav", "src_ref": "And in these steps we obtain the divisors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesen Schritten erhalten wir die Teiler.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 29, "src_audio": "/acl6060/audio/en/29.wav", "src_ref": "So, ah and then at this third step we actually get the quotient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und in diesem dritten Schritt erhalten wir dann den Quotienten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 30, "src_audio": "/acl6060/audio/en/30.wav", "src_ref": "Alright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Okay. Nach diesen drei Schritten können wir die Ergebnisse des zweiten Schritts wiederverwenden, dann die Ergebnisse des vierten Schritts erhalten und schließlich die Dividenden erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 31, "src_audio": "/acl6060/audio/en/31.wav", "src_ref": "So, here we actually generate the whole expression directly rather than generating a single operators or quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier wird also der gesamte Ausdruck direkt generiert und nicht nur ein einzelner Operator oder eine Menge.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 32, "src_audio": "/acl6060/audio/en/32.wav", "src_ref": "So this makes the process more accurate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dadurch wird der Prozess genauer.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 33, "src_audio": "/acl6060/audio/en/33.wav", "src_ref": "So, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem deduktiven System beginnen wir also zunächst mit einer Reihe von Mengen, die in den Fragen vorgestellt werden, und schließen auch mit einer Konstante als Anfangszustand.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 34, "src_audio": "/acl6060/audio/en/34.wav", "src_ref": "So, the expression is represented by e i j o p.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Ausdruck wird also durch e i j o p dargestellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 35, "src_audio": "/acl6060/audio/en/35.wav", "src_ref": "Where we perform operator from q_i to q_j, and such expression is actually directed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir führen den Operator von q_i nach q_j aus, und dieser Ausdruck ist tatsächlich geregelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 36, "src_audio": "/acl6060/audio/en/36.wav", "src_ref": "So, we also have subtraction with words here to represent the opposite direction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben hier also auch eine Subtraktion mit Wörtern, um die umgekehrte Richtung darzustellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 37, "src_audio": "/acl6060/audio/en/37.wav", "src_ref": "This is quite similar to relation extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies ist der Beziehungsextraktion recht ähnlich.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 38, "src_audio": "/acl6060/audio/en/38.wav", "src_ref": "So in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In einem formalen deduktiven System wenden wir also in einem Zeitschritt t den Operator zwischen dem Paar q_i und q_j an und erhalten dann diesen neuen Ausdruck.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 39, "src_audio": "/acl6060/audio/en/39.wav", "src_ref": "We add it to the next state to become a new quantity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir fügen ihn dem nächsten Zustand hinzu, um eine neue Menge zu erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 40, "src_audio": "/acl6060/audio/en/40.wav", "src_ref": "So, these slides actually visualize the evolution of the state where we keep adding expression to the current state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Folien veranschaulichen also die Entwicklung des Zustands, wobei wir dem aktuellen Zustand immer neue Ausdrücke hinzufügen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 41, "src_audio": "/acl6060/audio/en/41.wav", "src_ref": "So in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unseren Modellimplementierungen verwenden wir also zunächst ein vortrainiertes Sprachmodell, bei dem es sich um BERTs oder Robertas handeln kann, und dann kodieren wir den Satz und erhalten diese Mengendarstellungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 42, "src_audio": "/acl6060/audio/en/42.wav", "src_ref": "So, once we get the quantity representations, we can start to do inference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sobald wir also die Mengendarstellungen haben, können wir mit der Inferenz beginnen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 43, "src_audio": "/acl6060/audio/en/43.wav", "src_ref": "Here we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier zeigen wir ein Beispiel für q_1, um die Darstellung für q_2 geteilt durch q_2 und dann mal q_3 zu erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 44, "src_audio": "/acl6060/audio/en/44.wav", "src_ref": "First we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst erhalten wir die Paardarstellung, die im Grunde nur die Verkettung zwischen q_1 und q_2 ist, und dann wenden wir ein Feedforward-Netzwerk an, das durch den Operator parametrisiert ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 45, "src_audio": "/acl6060/audio/en/45.wav", "src_ref": "And then finally we obtain the expression representation q_1 divided by q_2.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und dann erhalten wir schließlich die Ausdrucksdarstellung q_1 geteilt durch q_2.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 46, "src_audio": "/acl6060/audio/en/46.wav", "src_ref": "But in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber in der Praxis könnten wir in der Inferenzphase auch den falschen Ausdruck erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 47, "src_audio": "/acl6060/audio/en/47.wav", "src_ref": "So, here all the possible expression is equals to three times the number of operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind also alle möglichen Ausdrücke gleich dem Dreifachen der Anzahl der Operatoren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 48, "src_audio": "/acl6060/audio/en/48.wav", "src_ref": "So the nice thing here is that we can easily add constraints to control this search this search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Schöne daran ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu kontrollieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 49, "src_audio": "/acl6060/audio/en/49.wav", "src_ref": "For example, if this expression is not allowed, we can simply remove this expression in our search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn dieser Ausdruck zum Beispiel nicht erlaubt ist, können wir ihn einfach aus unserem Suchraum entfernen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 50, "src_audio": "/acl6060/audio/en/50.wav", "src_ref": "So in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Im zweiten Schritt machen wir das Gleiche, aber der einzige Unterschied ist, dass wir nur eine Menge mehr haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 51, "src_audio": "/acl6060/audio/en/51.wav", "src_ref": "So this quantity come from the previous calculated expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Menge stammt also aus dem vorherigen berechneten Ausdruck.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 52, "src_audio": "/acl6060/audio/en/52.wav", "src_ref": "So finally we can obtain this final expression q_3 times q_4.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "So können wir schließlich diesen endgültigen Ausdruck q_3 mal q_4 erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 53, "src_audio": "/acl6060/audio/en/53.wav", "src_ref": "And we can also see the number of all the possible ah expression is different from the previous step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können auch sehen, dass die Anzahl aller möglichen Ausdrücke anders ist als beim vorherigen Schritt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 54, "src_audio": "/acl6060/audio/en/54.wav", "src_ref": "So, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein solcher Unterschied erschwert die Anwendung von einer Beam Search, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgewogen ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 55, "src_audio": "/acl6060/audio/en/55.wav", "src_ref": "So the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Trainingsverfahren ist also ähnlich dem Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust bei jedem Zeitschritt optimieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 56, "src_audio": "/acl6060/audio/en/56.wav", "src_ref": "And here we also use this tau to represent when we should terminate this generation process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auch hier verwenden wir dieses Tau, um darzustellen, wann wir diesen Generierungsprozess beenden sollten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 57, "src_audio": "/acl6060/audio/en/57.wav", "src_ref": "And here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier unterscheidet sich der Raum von Sequenz zu Sequenz, weil der Raum bei jedem Zeitschritt anders ist, während im traditionellen Sequenz-zu-Sequenz-Modell dies die Anzahl des Vokabulars ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 58, "src_audio": "/acl6060/audio/en/58.wav", "src_ref": "And it also allows us to impose certain constraints from prior from prior knowledge.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es erlaubt uns auch, bestimmte Beschränkungen aus dem Vorwissen aufzuerlegen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 59, "src_audio": "/acl6060/audio/en/59.wav", "src_ref": "So we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher führen wir Experimente mit den häufig verwendeten Datensätzen der mathematischen Wortprobleme durch, MAWPS, Math23K, MathQA und SVAMP.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 60, "src_audio": "/acl6060/audio/en/60.wav", "src_ref": "And here we briefly show the results compared with the previous best approaches.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier zeigen wir kurz die Ergebnisse im Vergleich zu den bisherigen besten Ansätzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 61, "src_audio": "/acl6060/audio/en/61.wav", "src_ref": "So our best performing variant is Roberta-DeductiveReasoner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere leistungsstärkste Variante ist also Roberta-DeductiveReasoner.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 62, "src_audio": "/acl6060/audio/en/62.wav", "src_ref": "And in fact we do not use beam search, in contrast all previous approaches are using beam search.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tatsächlich verwenden wir keine Beam Search, im Gegensatz zu allen früheren Ansätzen, die Beam Search verwendet haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 63, "src_audio": "/acl6060/audio/en/63.wav", "src_ref": "All right. So, the best approaches are often tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Okay. Daher sind die besten Ansätze oft baumbasierte Modelle.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 64, "src_audio": "/acl6060/audio/en/64.wav", "src_ref": "So, overall our reasoner is able to significantl significantly outperform this tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insgesamt ist unser Reasoner also in der Lage, dieses baumbasierte Modell signifikant zu übertreffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 65, "src_audio": "/acl6060/audio/en/65.wav", "src_ref": "But we can see the absolute numbers on MathQA or SVAMP are not really high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wir können sehen, dass die absoluten Zahlen bei MathQA oder SVAMP nicht wirklich hoch sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 66, "src_audio": "/acl6060/audio/en/66.wav", "src_ref": "So we further investigate the results on SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deshalb untersuchen wir die Ergebnisse bei SVAMP weiter.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 67, "src_audio": "/acl6060/audio/en/67.wav", "src_ref": "And this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieser Datensatz ist eine Herausforderung, weil der Autor versucht hat, manuell etwas hinzuzufügen, um das NLP-Modell zu verwirren, wie z. B. das Hinzufügen irrelevanter Informationen und zusätzlicher Mengen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 68, "src_audio": "/acl6060/audio/en/68.wav", "src_ref": "So, in our prediction we find some of the intermediate values are actually negatives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserer Vorhersage stellen wir also fest, dass einige der Zwischenwerte eigentlich negativ sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 69, "src_audio": "/acl6060/audio/en/69.wav", "src_ref": "For example, um, in these questions we are asking how many apples does Jake have?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel geht es in diesen Fragen darum, wie viele Äpfel Jake hat.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 70, "src_audio": "/acl6060/audio/en/70.wav", "src_ref": "But we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wir haben einige zusätzliche Informationen wie 17 Bilder weniger, und Steven hat acht Bilder, was völlig irrelevant ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 71, "src_audio": "/acl6060/audio/en/71.wav", "src_ref": "So, our model makes some prediction like this which is producing negative values.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Modell macht also eine Vorhersage wie diese, die negative Werte ergibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 72, "src_audio": "/acl6060/audio/en/72.wav", "src_ref": "And we observe these two expressions actually have similar scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir stellen fest, dass diese beiden Ausdrücke tatsächlich ähnliche Werte haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 73, "src_audio": "/acl6060/audio/en/73.wav", "src_ref": "So, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können also diesen Suchraum eingrenzen, indem wir die negativen Ergebnisse entfernen, damit die Antwort richtig ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 74, "src_audio": "/acl6060/audio/en/74.wav", "src_ref": "So um we further find such constraint actually improves quite a lot for some models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir stellen also fest, dass eine solche Bedingung für einige Modelle tatsächlich eine erhebliche Verbesserung darstellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 75, "src_audio": "/acl6060/audio/en/75.wav", "src_ref": "For example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei BERT haben wir uns zum Beispiel um sieben Punkte verbessert und beim Roberta-Basismodell haben wir uns sogar um zwei Punkte verbessert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 76, "src_audio": "/acl6060/audio/en/76.wav", "src_ref": "So better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein besseres Sprachmodell hat also bessere Sprachverständnisfähigkeiten, sodass die Zahl hier höher bei Roberta und niedriger bei BERT ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 77, "src_audio": "/acl6060/audio/en/77.wav", "src_ref": "And we also try to analyze the difficulty behind these behind all these datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir versuchen auch, die Schwierigkeiten hinter all diesen Datensätzen zu analysieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 78, "src_audio": "/acl6060/audio/en/78.wav", "src_ref": "We assume the number of unused quantities can be regarded as irrelevant information here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir gehen davon aus, dass die Anzahl der ungenutzten Mengen hier als irrelevante Informationen betrachtet werden kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 79, "src_audio": "/acl6060/audio/en/79.wav", "src_ref": "So ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier können wir also sehen, dass wir den Prozentsatz der Proben mit ungenutzten Mengen haben, und der SVAMP-Datensatz hat den größten Anteil.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 80, "src_audio": "/acl6060/audio/en/80.wav", "src_ref": "And here we also show the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier zeigen wir auch die Gesamtleistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 81, "src_audio": "/acl6060/audio/en/81.wav", "src_ref": "For those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei den Proben ohne ungenutzte Mengen ist die Leistung sogar höher als die Gesamtleistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 82, "src_audio": "/acl6060/audio/en/82.wav", "src_ref": "But with those samples that with unused quantity is actually way worse than the, worse than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber bei diesen Proben mit ungenutzten Mengen ist sie eigentlich viel schlechter als die Gesamtleistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 83, "src_audio": "/acl6060/audio/en/83.wav", "src_ref": "For MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei MAWPS haben wir nicht wirklich viele Testfälle, also ignoriere ich diesen Teil einfach.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 84, "src_audio": "/acl6060/audio/en/84.wav", "src_ref": "So, finally we want to show the interpretability through a question perturbation example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich wollen wir die Interpretierbarkeit anhand eines Frage-Störungsbeispiels zeigen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 85, "src_audio": "/acl6060/audio/en/85.wav", "src_ref": "So here our model actually makes a wrong prediction at the first step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier macht unser Modell also tatsächlich eine falsche Vorhersage im ersten Schritt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 86, "src_audio": "/acl6060/audio/en/86.wav", "src_ref": "So, we can actually correlate this expression with the sentence here. Alright.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können diesen Ausdruck also tatsächlich mit dem Satz hier korrelieren. Okay.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 87, "src_audio": "/acl6060/audio/en/87.wav", "src_ref": "So, we think this sentence might be misleading the model to an incorrect predictions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir denken also, dass dieser Satz das Modell zu falschen Vorhersagen verleiten könnte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 88, "src_audio": "/acl6060/audio/en/88.wav", "src_ref": "So here planting another thirty five makes the model makes the model think it should be an addition operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier führt eine weitere 35 dazu, dass das Modell denkt, dass es ein Additionsoperator sein sollte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 89, "src_audio": "/acl6060/audio/en/89.wav", "src_ref": "So we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir versuchen also den Satz so umzuformulieren, dass die Anzahl der Birnbäume um 35 weniger als die der Apfelbäume ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 90, "src_audio": "/acl6060/audio/en/90.wav", "src_ref": "So, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sorgen also dafür, dass eine genauere Semantik vermittelt wird, sodass das Modell in der Lage ist, die Vorhersage korrekt zu treffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 91, "src_audio": "/acl6060/audio/en/91.wav", "src_ref": "So, this study shows how the interpretable predictions help us understand the model behavior.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Studie zeigt also, wie uns die interpretierbaren Vorhersagen helfen, das Modellverhalten zu verstehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 92, "src_audio": "/acl6060/audio/en/92.wav", "src_ref": "So to conclude our work, so first our model is actually pretty efficient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um unsere Arbeit abzuschließen: Zunächst ist unser Modell tatsächlich ziemlich effizient.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 93, "src_audio": "/acl6060/audio/en/93.wav", "src_ref": "And we are able to provide interpretable solving procedure.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sind in der Lage, ein interpretierbares Lösungsverfahren anzubieten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 94, "src_audio": "/acl6060/audio/en/94.wav", "src_ref": "And we can easily incorporate some prior knowledge as constraint which can help improve the performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können einfach einiges Vorwissen als Bedingung einbeziehen, was zur Verbesserung der Leistung beitragen kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 95, "src_audio": "/acl6060/audio/en/95.wav", "src_ref": "And the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich gilt der zugrundeliegende Mechanismus nicht nur für das Lösen von Netzwerkproblemen, sondern auch für andere Aufgaben, die mehrstufiges Argumentieren erfordern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 96, "src_audio": "/acl6060/audio/en/96.wav", "src_ref": "We also have certain limitations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auch sind uns gewisse Grenzen gesetzt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 97, "src_audio": "/acl6060/audio/en/97.wav", "src_ref": "Ah, if we have a large number of operators or constants, the memory consumption could be pretty high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir eine große Anzahl von Operatoren oder Konstanten haben, könnte der Speicherverbrauch ziemlich hoch sein.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 98, "src_audio": "/acl6060/audio/en/98.wav", "src_ref": "And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wie bereits erwähnt ist es zudem aufgrund der unausgewogenen Wahrscheinlichkeitsverteilung zwischen den verschiedenen Zeitschritten eine ziemliche Herausforderung, die Beam Search-Strategie anzuwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 99, "src_audio": "/acl6060/audio/en/99.wav", "src_ref": "So this is the end of the talk, and questions are welcomed. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sind am Ende des Vortrags angekommen und freuen uns auf etwaige Fragen. Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 100, "src_audio": "/acl6060/audio/en/100.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo, mein Name ist Antoine und ich bin von der Universität Maastricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 101, "src_audio": "/acl6060/audio/en/101.wav", "src_ref": "I will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde meine gemeinsame Arbeit mit Jerry vorstellen, bei der es um einen neuen Datensatz für das Retrieval von Gesetzesartikeln geht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 102, "src_audio": "/acl6060/audio/en/102.wav", "src_ref": "Legal issues are an integral part of many people's lives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Rechtsfragen sind ein wesentlicher Bestandteil des Lebens vieler Menschen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 103, "src_audio": "/acl6060/audio/en/103.wav", "src_ref": "But the majority of citizens have little to know knowledge about their rights and fundamental legal processes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber die Mehrheit der Bürger verfügt nur über wenig Wissen über ihre Rechte und grundlegende rechtliche Verfahren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 104, "src_audio": "/acl6060/audio/en/104.wav", "src_ref": "As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies hat zur Folge, dass viele schutzbedürftige Bürger, die sich die kostspielige Hilfe eines Rechtsexperten nicht leisten können, schutzlos bleiben oder schlimmstenfalls ausgebeutet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 105, "src_audio": "/acl6060/audio/en/105.wav", "src_ref": "All work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere Arbeit zielt darauf ab, die Kluft zwischen den Menschen und dem Gesetz zu überbrücken, indem ein effektives Retrieval-System für Gesetzesartikel entwickelt wird.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 106, "src_audio": "/acl6060/audio/en/106.wav", "src_ref": "Such a system could provide a free professional legal help service for unskilled humans.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein solches System könnte einen kostenlosen professionellen Rechtshilfedienst für unerfahrene Menschen anbieten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 107, "src_audio": "/acl6060/audio/en/107.wav", "src_ref": "Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bevor wir uns dem Hauptbeitrag dieser Arbeit widmen, wollen wir zunächst das Problem des Retrievals von Gesetzesartikeln beschreiben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 108, "src_audio": "/acl6060/audio/en/108.wav", "src_ref": "Given a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eine einfache Frage zu einer Rechtsangelegenheit wäre: „Was riskiere ich, wenn ich das Berufsgeheimnis verletze?“", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 109, "src_audio": "/acl6060/audio/en/109.wav", "src_ref": "A model is required to retrieve all relevant statutory articles from a large body of legislation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier wird ein Modell benötigt, um alle relevanten Gesetzesartikel aus einem großen Bestand an Rechtsvorschriften abzurufen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 110, "src_audio": "/acl6060/audio/en/110.wav", "src_ref": "This information retrieval task comes with its own set of challenges.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Informationsbeschaffungsaufgabe bringt eine Reihe von Herausforderungen mit sich.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 111, "src_audio": "/acl6060/audio/en/111.wav", "src_ref": "First, it deals with two types of language.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Erstens: Es geht um zwei Arten von Sprache.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 112, "src_audio": "/acl6060/audio/en/112.wav", "src_ref": "Common natural language for the questions and complex legal language for the statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die gemeinsame natürliche Sprache für die Fragen und die komplexe juristische Sprache für die Gesetze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 113, "src_audio": "/acl6060/audio/en/113.wav", "src_ref": "This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieser Unterschied bei den Sprachverteilungen macht es für ein System schwieriger, relevante Kandidaten zu finden, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine rechtliche Frage übersetzen kann, die der Terminologie der Gesetze entspricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 114, "src_audio": "/acl6060/audio/en/114.wav", "src_ref": "Besides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Außerdem ist das Recht keine Ansammlung unabhängiger Artikel, die für sich allein als vollständige Informationsquelle betrachtet werden können, anders als beispielsweise Nachrichten oder Rezepte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 115, "src_audio": "/acl6060/audio/en/115.wav", "src_ref": "Instead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielmehr handelt es sich um eine strukturierte Sammlung von Rechtsvorschriften, die nur im Gesamtkontext, d. h. zusammen mit den ergänzenden Informationen aus den Nachbarartikeln, den Bereichen und Teilbereichen, denen sie angehören, und ihrer Stellung im Aufbau des Gesetzes, eine vollständige Bedeutung haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 116, "src_audio": "/acl6060/audio/en/116.wav", "src_ref": "Lastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich handelt es sich bei den Gesetzesartikeln nicht um kleine Absätze, die in den meisten Retrieval-Arbeiten die typische Retrieval-Einheit darstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 117, "src_audio": "/acl6060/audio/en/117.wav", "src_ref": "Here, there are long documents that may be up to six thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier gibt es lange Dokumente, die bis zu 6.000 Wörter umfassen können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 118, "src_audio": "/acl6060/audio/en/118.wav", "src_ref": "The recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die jüngsten Fortschritte im Bereich NLP haben großes Interesse an vielen juristischen Aufgaben geweckt, z. B. an der Vorhersage juristischer Urteile oder der automatisierten Prüfung von Verträgen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 119, "src_audio": "/acl6060/audio/en/119.wav", "src_ref": "But statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Doch das Retrieval von Gesetzesartikeln ist aufgrund des Mangels an großen und qualitativ hochwertig markierten Datensätzen weitgehend unberührt geblieben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 120, "src_audio": "/acl6060/audio/en/120.wav", "src_ref": "In this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In dieser Arbeit stellen wir einen neuen Datensatz in der französischen Muttersprache vor, mit dem wir Retrievalmodelle untersuchen und analysieren, ob sie bei der Aufgabe der Suche nach Gesetzesartikeln an die Effizienz und Zuverlässigkeit eines Rechtsexperten rankommen können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 121, "src_audio": "/acl6060/audio/en/121.wav", "src_ref": "Our Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser belgischer Datensatz für das Retrieval von Gesetzesartikeln, BSARD, besteht aus mehr als 1.100 juristischen Fragen, die von belgischen Bürgern gestellt wurden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 122, "src_audio": "/acl6060/audio/en/122.wav", "src_ref": "These questions cover a wide range of topics from family, housing, money, to work and social security.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Fragen decken ein breites Spektrum an Themen ab, von Familie, Wohnen, Geld bis hin zu Arbeit und sozialer Sicherheit.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 123, "src_audio": "/acl6060/audio/en/123.wav", "src_ref": "Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jede von ihnen wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als 22.600 Rechtsartikeln aus belgischen Gesetzbüchern markiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 124, "src_audio": "/acl6060/audio/en/124.wav", "src_ref": "Let's now talk about how we collected this dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lassen Sie uns nun darüber sprechen, wie wir diesen Datensatz gesammelt haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 125, "src_audio": "/acl6060/audio/en/125.wav", "src_ref": "First, we started by compiling a large corpus of legal articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst haben wir einen großen Korpus von Rechtsartikeln zusammengestellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 126, "src_audio": "/acl6060/audio/en/126.wav", "src_ref": "We considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben 32 öffentlich zugängliche belgische Gesetzbücher berücksichtigt und alle Artikel sowie die entsprechenden Abschnittsüberschriften extrahiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 127, "src_audio": "/acl6060/audio/en/127.wav", "src_ref": "Then we gathered legal questions with references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann haben wir rechtliche Fragen mit Verweisen auf einschlägige Gesetze zusammengestellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 128, "src_audio": "/acl6060/audio/en/128.wav", "src_ref": "To do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zu diesem Zweck arbeiten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr etwa 4.000 E-Mails von belgischen Bürgern erhält, die um Rat in einer persönlichen Rechtsangelegenheit bitten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 129, "src_audio": "/acl6060/audio/en/129.wav", "src_ref": "We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir hatten das Glück, Zugang zu ihren Websites zu erhalten, auf denen ihr Team aus erfahrenen Juristen die häufigsten Rechtsangelegenheiten der Belgier behandelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 130, "src_audio": "/acl6060/audio/en/130.wav", "src_ref": "We collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben Tausende von Fragen gesammelt und mit Kategorien, Unterkategorien und Verweisen auf einschlägige Gesetze annotiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 131, "src_audio": "/acl6060/audio/en/131.wav", "src_ref": "Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich haben wir die rechtlichen Verweise überprüft und die Fragen herausgefiltert, deren Verweise nicht in einem der von uns berücksichtigten Gesetzbücher enthalten waren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 132, "src_audio": "/acl6060/audio/en/132.wav", "src_ref": "The remaining references were matched and converted to the corresponding article ids from our corpus.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die übrigen Referenzen wurden zusammengeführt und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 133, "src_audio": "/acl6060/audio/en/133.wav", "src_ref": "We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Am Ende hatten wir 1.108 Fragen, jede sorgfältig markiert mit den IDs der relevanten Artikel aus unserem großen Korpus von 22.630 Gesetzesartikeln.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 134, "src_audio": "/acl6060/audio/en/134.wav", "src_ref": "In addition, each question comes with the main category and a concatenation of subcategories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus enthält jede Frage eine Hauptkategorie und eine Verkettung von Unterkategorien.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 135, "src_audio": "/acl6060/audio/en/135.wav", "src_ref": "And each articles comes with a concatenation of the subsequence heading in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jeder Artikel hat eine Verkettung mit der anschließenden Überschrift im Aufbau des Gesetzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 136, "src_audio": "/acl6060/audio/en/136.wav", "src_ref": "This extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für künftige Forschungen zum juristischen Informationen-Retrieval oder zur juristischen Textklassifikation von Interesse sein.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 137, "src_audio": "/acl6060/audio/en/137.wav", "src_ref": "Let's look at some characteristic of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schauen wir uns einige Merkmale unseres Datensatzes an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 138, "src_audio": "/acl6060/audio/en/138.wav", "src_ref": "The questions are between five and forty four words long with a median of fourteen words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Fragen sind zwischen fünf und 24 Wörter lang, mit einem Mittelwert von 14 Wörtern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 139, "src_audio": "/acl6060/audio/en/139.wav", "src_ref": "The articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Artikel sind mit einer durchschnittlichen Länge von 77 Wörtern sehr viel länger, wobei 142 von ihnen mehr als 1.000 Wörter umfassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 140, "src_audio": "/acl6060/audio/en/140.wav", "src_ref": "The lengthiest one being up to five thousand seven hundred and ninety words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der längste von ihnen hat bis zu 5.790 Wörter.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 141, "src_audio": "/acl6060/audio/en/141.wav", "src_ref": "As previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wie bereits erwähnt decken die Fragen ein breites Spektrum an Themen ab, wobei etwa 85 Prozent der Fragen entweder die Familie, das Wohnen, Geld oder die Gerechtigkeit betreffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 142, "src_audio": "/acl6060/audio/en/142.wav", "src_ref": "While the remaining fifteen percent concern either social security, foreigners or work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die restlichen 15 Prozent betreffen entweder soziale Sicherheit, Ausländer oder Arbeit.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 143, "src_audio": "/acl6060/audio/en/143.wav", "src_ref": "The article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Artikel sind auch sehr vielfältig, da sie aus 32 verschiedenen belgischen Gesetzbüchern stammen, die eine große Reihe von Rechtsthemen abdecken.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 144, "src_audio": "/acl6060/audio/en/144.wav", "src_ref": "Here's the total number of articles collected from each of these Belgian codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist die Gesamtzahl der Artikel, die aus diesen belgischen Gesetzbüchern gesammelt wurden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 145, "src_audio": "/acl6060/audio/en/145.wav", "src_ref": "Out of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Von den 22.633 Artikeln werden nur 1.612 als relevant für mindestens eine Frage im Datensatz genannt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 146, "src_audio": "/acl6060/audio/en/146.wav", "src_ref": "And around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Etwa 80 Prozent dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, dem Gerichtsgesetzbuch, dem Gesetzbuch für strafrechtliche Ermittlungen oder dem Strafgesetzbuch.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 147, "src_audio": "/acl6060/audio/en/147.wav", "src_ref": "Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei 18 von 32 Gesetzbüchern werden weniger als fünf Artikel als relevant für mindestens eine Frage bestimmt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 148, "src_audio": "/acl6060/audio/en/148.wav", "src_ref": "Which can be explained by the fact that those codes focused less on individuals and their concerns.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies lässt sich dadurch erklären, dass diese Gesetzbücher weniger auf den Einzelnen und seine Anliegen ausgerichtet waren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 149, "src_audio": "/acl6060/audio/en/149.wav", "src_ref": "Overall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insgesamt liegt der Durchschnitt der Anzahl der Zitate bei zwei, und weniger als 25 Prozent der zitierten Artikel werden mehr als fünfmal zitiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 150, "src_audio": "/acl6060/audio/en/150.wav", "src_ref": "Using all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unter Verwendung aller Datensätze haben wir verschiedene Retrieval-Ansätze, einschließlich lexikalischer und dichter Architektur, einem Benchmarking unterzogen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 151, "src_audio": "/acl6060/audio/en/151.wav", "src_ref": "Given a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei einer Abfrage und einem Artikel weist ein lexikalisches Modell dem Abfrage-Artikel-Paar eine Punktzahl zu, indem es die Summe der Gewichtungen von jedem der Begriffe in diesem Artikel über die Abfrage-Termini berechnet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 152, "src_audio": "/acl6060/audio/en/152.wav", "src_ref": "We experiment with the standard TF-IDF and BM25 ranking functions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit den Standard-Ranking-Funktionen TF-IDF und BM25.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 153, "src_audio": "/acl6060/audio/en/153.wav", "src_ref": "The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Hauptproblem bei diesen Ansätzen ist, dass sie nur Artikel finden können, die Schlüsselwörter enthalten, die in der Anfrage vorkommen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 154, "src_audio": "/acl6060/audio/en/154.wav", "src_ref": "To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um diese Einschränkung zu überwinden, experimentieren wir mit einer neuronal-basierten Architektur, die semantische Beziehungen zwischen Abfragen und Artikeln erfassen kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 155, "src_audio": "/acl6060/audio/en/155.wav", "src_ref": "We use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir verwenden ein Bi-Encoder-Modell, das Abfragen und Artikel in dichten Vektor-Darstellungen abbildet und einen Relevanzwert zwischen einem Abfrage-Artikel-Paar anhand der Ähnlichkeit ihrer Einbettungen berechnet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 156, "src_audio": "/acl6060/audio/en/156.wav", "src_ref": "These embeddings typically result from a pooling operation on the output of a word embedding model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Einbettungen resultieren in der Regel aus einer Pooling-Operation auf der Ausgabe eines Worteinbettungsmodells.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 157, "src_audio": "/acl6060/audio/en/157.wav", "src_ref": "First, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst untersuchen wir die Effektivität von Siamesischen Bi-Encodern in einem Zero-Shot-Evaluation-Setup, d. h. dass vortrainierte Worteinbettungsmodelle ohne zusätzliche Feinabstimmung sofort angewendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 158, "src_audio": "/acl6060/audio/en/158.wav", "src_ref": "We experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit kontextunabhängigen Textencodern, wie word2vec und fastText, und kontextabhängigen Einbettungsmodellen, wie Roberta und insbesondere CamemBERT, einem französischen Roberta-Modell.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 159, "src_audio": "/acl6060/audio/en/159.wav", "src_ref": "Additionally, we train our own CamemBERT based model ah bi-encoders on our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus trainieren wir unser eigenes, auf CamemBERT basierendes Modell, Bi-Encoder, auf unserem Datensatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 160, "src_audio": "/acl6060/audio/en/160.wav", "src_ref": "Note that for training, we experiment with the two flavors of the bi-encoder architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Beachten Sie, dass wir beim Training mit den beiden Varianten der Bi-Encoder-Architektur experimentieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 161, "src_audio": "/acl6060/audio/en/161.wav", "src_ref": "Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Siamese, das ein einzigartiges Worteinbettungsmodell verwendet, das die Abfrage und den Artikel zusammen in einem geteilten dichten Vektorraum abbildet, und Two-Tower, das zwei unabhängige Worteinbettungsmodelle verwendet, die die Abfrage und den Artikel getrennt in verschiedenen Einbettungsräumen kodieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 162, "src_audio": "/acl6060/audio/en/162.wav", "src_ref": "We experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit Mittelwert, Maximalwert und CLS-Pooling sowie mit Produkt und Kosinus für die Berechnung von Ähnlichkeiten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 163, "src_audio": "/acl6060/audio/en/163.wav", "src_ref": "Here are the result of our baseline on the test sets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind die Ergebnisse unserer Baseline auf den Testsätzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 164, "src_audio": "/acl6060/audio/en/164.wav", "src_ref": "With the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die lexikalischen Methoden werden oben, die Siamesischen Bi-Encoder in einem Zero-Shot-Setup in der Mitte und die fein abgestimmten Bi-Encoder werden unten ausgewertet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 165, "src_audio": "/acl6060/audio/en/165.wav", "src_ref": "Overall, the finetuned bi-encoder significantly outperforms all the other baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insgesamt übertrifft der fein abgestimmte Bi-Encoder alle anderen Baselines deutlich.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 166, "src_audio": "/acl6060/audio/en/166.wav", "src_ref": "The two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Two-Tower-Modell verbessert sich gegenüber seinen Siamesischen Varianten beim Recall um 100, schneidet aber bei den anderen Metriken ähnlich gut ab.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 167, "src_audio": "/acl6060/audio/en/167.wav", "src_ref": "Although BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Obwohl BM25 deutlich schlechter abschnitt als der trainierte Bi-Encoder, zeigte seine Leistung, dass er immer noch eine gute Basis für ein domänenspezifisches Retrieval ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 168, "src_audio": "/acl6060/audio/en/168.wav", "src_ref": "Regarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bezüglich der Zero-Shot-Evaluation des Siamesischen Bi-Encoders stellen wir fest, dass die direkte Verwendung der Einbettungen eines vortrainierten CamemBERT-Modells ohne Optimierung für die Informationen-Retrieval-Aufgabe schlechte Ergebnisse liefert, was mit früheren Erkenntnissen übereinstimmt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 169, "src_audio": "/acl6060/audio/en/169.wav", "src_ref": "Furthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus stellen wir fest, dass der word2vec-basierte Bi-Encoder die fastText- und BERT-basierten Modelle signifikant übertrifft, was darauf hindeutet, dass vortrainierte Wort-Ebenen-Einbettungen vielleicht besser für die Aufgabe geeignet sind als Zeichen- oder Unterwort-Ebenen-Einbettungen, wenn sie direkt verwendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 170, "src_audio": "/acl6060/audio/en/170.wav", "src_ref": "Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Obwohl diese Ergebnisse vielversprechend sind, lassen sie im Vergleich zu einem erfahrenen Rechtsexperten auf ein großes Verbesserungspotenzial schließen, der früher oder später alle relevanten Artikel zu jeder Frage abrufen kann und somit perfekte Ergebnisse erzielt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 171, "src_audio": "/acl6060/audio/en/171.wav", "src_ref": "Let's conclude by discussing two limitations of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Abschließend möchte ich auf zwei Einschränkungen unseres Datensatzes eingehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 172, "src_audio": "/acl6060/audio/en/172.wav", "src_ref": "First, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Erstens ist der Artikelkorpus auf die Artikel der 32 herangezogenen belgischen Gesetzbücher beschränkt, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Dekreten, Richtlinien und Verordnungen fehlen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 173, "src_audio": "/acl6060/audio/en/173.wav", "src_ref": "During the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei der Erstellung des Datensatzes werden alle Verweise auf diese nicht erfassten Artikel ignoriert, was dazu führt, dass sich einige Fragen am Ende nur auf einen Bruchteil der ursprünglichen Anzahl der relevanten Artikel beziehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 174, "src_audio": "/acl6060/audio/en/174.wav", "src_ref": "This information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Information impliziert also, dass die Antwort, die in den übrigen relevanten Artikeln enthalten ist, unvollständig sein könnte, obwohl sie immer noch völlig angemessen ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 175, "src_audio": "/acl6060/audio/en/175.wav", "src_ref": "Second, we should note that not all legal questions can be answered with statutes alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zweitens sollten wir beachten, dass nicht alle rechtlichen Fragen allein mit Gesetzen beantwortet werden können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 176, "src_audio": "/acl6060/audio/en/176.wav", "src_ref": "For instance, the question, can I evict my tenants if they make too much noise?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel die Frage: „Kann ich meinen Mietern kündigen, wenn sie zu viel Lärm machen?“", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 177, "src_audio": "/acl6060/audio/en/177.wav", "src_ref": "Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Möglicherweise gibt es im Gesetz keine detaillierte Antwort, die eine bestimmte Lärmschwelle festlegt, ab der eine Räumung zulässig ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 178, "src_audio": "/acl6060/audio/en/178.wav", "src_ref": "Instead, the landlord should probably rely more on case law and find precedents similar to their current situation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Stattdessen sollte sich der Vermieter wahrscheinlich eher auf das Fallrecht stützen und Präzedenzfälle finden, die seiner aktuellen Situation ähnlich sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 179, "src_audio": "/acl6060/audio/en/179.wav", "src_ref": "For example, the tenants makes two parties a week until two AM.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel veranstalten die Mieter zwei Partys pro Woche bis zwei Uhr morgens.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 180, "src_audio": "/acl6060/audio/en/180.wav", "src_ref": "Hence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Einige Fragen eignen sich daher besser als andere für die Aufgabe des Retrievals von Gesetzesartikeln. Die Domäne der weniger geeigneten Fragen muss noch bestimmt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 181, "src_audio": "/acl6060/audio/en/181.wav", "src_ref": "We hope that our work sparks interest in developing practical and reliable statutory article retrieval models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir hoffen, dass unsere Arbeit das Interesse an der Entwicklung praktischer und zuverlässiger Modelle für das Retrieval von Gesetzesartikeln weckt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 182, "src_audio": "/acl6060/audio/en/182.wav", "src_ref": "That can help improve access to justice for all.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie kann dazu beitragen, den Zugang zur Justiz für alle zu verbessern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 183, "src_audio": "/acl6060/audio/en/183.wav", "src_ref": "You can check out our paper, dataset and code at the following links. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie können unser Paper, den Datensatz und die Gesetzbücher unter den folgenden Links einsehen. Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 184, "src_audio": "/acl6060/audio/en/184.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo, wir freuen uns, Ihnen unsere Arbeit über VALSE vorstellen zu können. Das ist ein Aufgaben-unabhängiger Benchmark, der für das Testen von Seh- und Sprachmodellen mit spezifischen sprachlichen Phänomenen konzipiert ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 185, "src_audio": "/acl6060/audio/en/185.wav", "src_ref": "Why did we do the trouble in setting up this benchmark?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Warum haben wir uns die Mühe gemacht, diesen Benchmark einzurichten?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 186, "src_audio": "/acl6060/audio/en/186.wav", "src_ref": "Well, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In den letzten Jahren haben wir eine explosionsartige Zunahme von Transformer-basierten Seh- und Sprachmodellen erlebt, die mit großen Mengen an Bild-Text-Paaren vortrainiert wurden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 187, "src_audio": "/acl6060/audio/en/187.wav", "src_ref": "Each one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jedes dieser Modelle treibt den Fortschritt bei Seh- und Sprachaufgaben voran, wie z. B. die visuelle Fragenbeantwortung, das Argumentieren mit visuellem Menschenverstand, Bild-Retrieval, Phrasen-Erdung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 188, "src_audio": "/acl6060/audio/en/188.wav", "src_ref": "So we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben also die Nachricht erhalten, dass die Genauigkeiten bei diesen Aufgaben und spezifischen Benchmarks stetig steigen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 189, "src_audio": "/acl6060/audio/en/189.wav", "src_ref": "But do we know what the models have actually learned?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wissen wir, was die Modelle tatsächlich gelernt haben?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 190, "src_audio": "/acl6060/audio/en/190.wav", "src_ref": "What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Was hat ein Seh- und Sprach-Transformer verstanden, wenn er für dieses Bild und diesen Satz eine hohe Punktzahl zuweist?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 191, "src_audio": "/acl6060/audio/en/191.wav", "src_ref": "And the low score for this one?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und hier eine niedrige Punktzahl?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 192, "src_audio": "/acl6060/audio/en/192.wav", "src_ref": "Do vision and language models focus on the right thing?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Konzentrieren sich Seh- und Sprachmodelle auf die wesentlichen Punkte?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 193, "src_audio": "/acl6060/audio/en/193.wav", "src_ref": "Or do they focus on biases as shown by previous work?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Oder konzentrieren sie sich auf Verzerrungen, wie in früheren Arbeiten aufgezeigt wurde?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 194, "src_audio": "/acl6060/audio/en/194.wav", "src_ref": "To shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um diesen Aspekt näher zu beleuchten, schlagen wir eine eher aufgabenbezogene agnostische Richtung vor und führen VALSE ein, das die Empfindlichkeit von Seh- und Sprachmodellen für spezifische sprachliche Phänomene testet, die sowohl die sprachlichen als auch die visuellen Modalitäten betreffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 195, "src_audio": "/acl6060/audio/en/195.wav", "src_ref": "We target existence, plurality, counting, spatial relations, actions and entity coreference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir zielen auf Existenz, Pluralität, Zählung, räumliche Beziehungen, Handlungen und Entity-Koreferenz ab.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 196, "src_audio": "/acl6060/audio/en/196.wav", "src_ref": "But how do we test whether the vision and language models have captured this phenomena?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Doch wie lässt sich überprüfen, ob die Seh- und Sprachmodelle dieses Phänomen erfasst haben?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 197, "src_audio": "/acl6060/audio/en/197.wav", "src_ref": "By foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Verfälschen einer Methode wurde zuvor von Ravi Shekhar und Mitarbeitern nur für Substantivphrasen in den Seh- und Sprachmodellen und beim Zählen von uns in früheren Arbeiten angewandt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 198, "src_audio": "/acl6060/audio/en/198.wav", "src_ref": "Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Verfälschen bedeutet im Grunde, dass wir die Beschriftung eines Bildes nehmen und eine Verfälschung erzeugen, indem wir die Beschriftung so verändern, dass sie das Bild nicht mehr beschreibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 199, "src_audio": "/acl6060/audio/en/199.wav", "src_ref": "And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir führen diese Veränderungen der Phrase durch, indem wir uns auf sechs spezifische Teile konzentrieren, wie Existenz, Pluralität, Zählung, räumliche Relationen, Handlungen und Entität-Koreferenz, wobei jeder Teil aus einem oder mehreren Instrumenten bestehen kann, falls wir mehr als einen interessanten Weg gefunden haben, um Verfälschungsinstanzen zu erzeugen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 200, "src_audio": "/acl6060/audio/en/200.wav", "src_ref": "For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel haben wir im Fall des Handlungsteils zwei Instrumente, eines, bei dem das Handlungsverb durch eine andere Handlung ersetzt wird, und eines, bei dem die Aktanten ausgetauscht werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 201, "src_audio": "/acl6060/audio/en/201.wav", "src_ref": "Counting and coreference also are pieces that have more than one instrument.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zählung und Koreferenz sind auch Teile, die mehr als ein Instrument haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 202, "src_audio": "/acl6060/audio/en/202.wav", "src_ref": "And we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir erstellen diese Verfälschungen, indem wir sicherstellen, dass sie das Bild nicht beschreiben, dass sie grammatikalisch richtig sind und auch korrekte Sätze bilden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 203, "src_audio": "/acl6060/audio/en/203.wav", "src_ref": "This is not easy to do because a foiled caption may be less likely than the original caption.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das ist nicht einfach zu bewerkstelligen, denn eine verfälschte Beschriftung ist weniger wahrscheinlich als die ursprüngliche Beschriftung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 204, "src_audio": "/acl6060/audio/en/204.wav", "src_ref": "For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist zwar zum Beispiel nicht unmöglich, aber statistisch gesehen ist es unwahrscheinlicher, dass Pflanzen einen Menschen schneiden, als dass ein Mensch Pflanzen schneidet. Große Seh- und Sprachmodelle könnten dies erkennen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 205, "src_audio": "/acl6060/audio/en/205.wav", "src_ref": "Therefore, to obtain valid foils, we must take action.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher müssen wir handeln, wenn wir gültige Verfälschungen erhalten wollen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 206, "src_audio": "/acl6060/audio/en/206.wav", "src_ref": "First, we make use of strong language models to propose foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Erstens nutzen wir starke Sprachmodelle, um Verfälschungen vorzuschlagen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 207, "src_audio": "/acl6060/audio/en/207.wav", "src_ref": "Second, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zweitens verwenden wir die Inferenz der natürlichen Sprache oder kurz NLI, um Verfälschungen herauszufiltern, die das Bild noch beschreiben könnten. Bei der Konstruktion von Verfälschungen müssen wir sicherstellen, dass sie das Bild nicht beschreiben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 208, "src_audio": "/acl6060/audio/en/208.wav", "src_ref": "To test this automatically, we apply natural language inference with the following rationale.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um dies automatisch zu testen, wenden wir die Inferenz der natürlichen Sprache mit der folgenden Logik an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 209, "src_audio": "/acl6060/audio/en/209.wav", "src_ref": "We consider an image to be the premise and its caption its entailed hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir betrachten ein Bild als die Prämisse und seine Beschriftung als die daraus abgeleitete Hypothese.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 210, "src_audio": "/acl6060/audio/en/210.wav", "src_ref": "In addition, we consider the caption to be the premise, and the foil is its hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus betrachten wir die Beschriftung als Prämisse und die Verfälschung als ihre Hypothese.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 211, "src_audio": "/acl6060/audio/en/211.wav", "src_ref": "If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn ein NLI-Modell vorhersagt, dass die Verfälschung der Beschriftung widerspricht oder neutral ist, ist das für uns ein Indikator für eine gültige Verfälschung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 212, "src_audio": "/acl6060/audio/en/212.wav", "src_ref": "If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn ein NLI vorhersagt, dass die Verfälschung die Beschriftung beinhaltet, kann es keine gute Verfälschung sein, da sie durch Transitivität eine wahrheitsgemäße Beschreibung des Bildes liefert. Wir filtern diese Verfälschungen heraus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 213, "src_audio": "/acl6060/audio/en/213.wav", "src_ref": "But this procedure is not perfect, it is just an indicator for valid foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieses Verfahren ist aber nicht perfekt, sondern nur ein Indikator für gültige Verfälschungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 214, "src_audio": "/acl6060/audio/en/214.wav", "src_ref": "Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher setzen wir als dritte Maßnahme zur Generierung gültiger Verfälschungen menschliche Annotatoren ein, um die in VALSE verwendeten Daten zu validieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 215, "src_audio": "/acl6060/audio/en/215.wav", "src_ref": "So, after filtering and human evaluation, we have as many test instances as described in this table.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nach dem Filtern und der menschlichen Evaluation haben wir also so viele Testinstanzen, wie in dieser Tabelle beschrieben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 216, "src_audio": "/acl6060/audio/en/216.wav", "src_ref": "Note that VALSE does not deliver any training data but only test data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Beachten Sie hier, dass VALSE keine Trainingsdaten, sondern nur Testdaten liefert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 217, "src_audio": "/acl6060/audio/en/217.wav", "src_ref": "Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da es sich um einen reinen Zero-Shot-Test-Benchmark handelt, ist er so konzipiert, dass er die bestehenden Fähigkeiten der Seh- und Sprachmodelle nach dem Vortraining nutzt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 218, "src_audio": "/acl6060/audio/en/218.wav", "src_ref": "Finetuning would only enable models to exploit artifacts or statistical biases in the data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Feinabstimmung würde es den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 219, "src_audio": "/acl6060/audio/en/219.wav", "src_ref": "And we all know that these models like to cheat and take shortcuts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir alle wissen, dass diese Modells gerne schummeln und Abkürzungen nehmen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 220, "src_audio": "/acl6060/audio/en/220.wav", "src_ref": "And as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wie wir schon erwähnten, sind wir an der Bewertung interessiert, welche Fähigkeiten die Seh- und Sprachmodelle nach dem Vortraining haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 221, "src_audio": "/acl6060/audio/en/221.wav", "src_ref": "We experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit fünf Seh- und Sprachmodellen auf VALSE, nämlich mit CLIP, LXMert, ViLBERT, ViLBERT zwölf in einem und VisualBERT.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 222, "src_audio": "/acl6060/audio/en/222.wav", "src_ref": "Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zwei unserer wichtigsten Evaluationsmetriken sind die Genauigkeit der Modelle bei der Klassifikation von Bild-Satz-Paaren in Bildbeschreibungen und Verfälschungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 223, "src_audio": "/acl6060/audio/en/223.wav", "src_ref": "Perhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielleicht ist es für dieses Video relevanter, unsere tolerantere Metrik vorzustellen, die paarweise Genauigkeit. Diese misst, ob die Bild-Satz-Ausrichtung für das korrekte Bild-Text-Paar größer ist als für sein Verfälschungspaar.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 224, "src_audio": "/acl6060/audio/en/224.wav", "src_ref": "For more metrics and results on them, do check out our paper.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Weitere Metriken und Ergebnisse dazu finden Sie in unserem Paper.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 225, "src_audio": "/acl6060/audio/en/225.wav", "src_ref": "The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Ergebnisse mit paarweiser Genauigkeit werden hier gezeigt und stimmen mit den Ergebnissen überein, die wir von den anderen Metriken erhalten haben, nämlich dass die beste Zero-Shot-Leistung von ViLBERT zwölf zu eins erreicht wird, gefolgt von ViLBERT, LXMert, CLIP und schließlich VisualBERT.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 226, "src_audio": "/acl6060/audio/en/226.wav", "src_ref": "It's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist bemerkenswert, wie Instrumente, die sich auf die einzelnen Objekte konzentrieren, wie Existenz und Substantivphrasen, von ViLBERT zwölf zu eins fast gelöst werden. Das unterstreicht, dass Modelle in der Lage sind, benannte Objekte und ihre Existenz in Bildern zu identifizieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 227, "src_audio": "/acl6060/audio/en/227.wav", "src_ref": "However, none of the remaining pieces can be reliably solved in our adversarial foiling settings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Keines der verbleibenden Teile kann jedoch in unseren gegnerischen Verfälschungseinstellungen zuverlässig gelöst werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 228, "src_audio": "/acl6060/audio/en/228.wav", "src_ref": "We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sehen an den Instrumenten der Pluralität und der Zählung, dass Seh- und Sprachmodelle Schwierigkeiten haben, Verweise auf einzelne und mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 229, "src_audio": "/acl6060/audio/en/229.wav", "src_ref": "The relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Relationen-Teil zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Relation zwischen Objekten in einem Bild richtig zu klassifizieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 230, "src_audio": "/acl6060/audio/en/230.wav", "src_ref": "They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie durch Plausibilitätsbias unterstützt werden, wie wir bei dem Handlung-Teil sehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 231, "src_audio": "/acl6060/audio/en/231.wav", "src_ref": "From the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Beim Koreferenz-Teil geht hervor, dass die Verfolgung mehrerer Verweise auf dasselbe Objekt in einem Bild unter Verwendung von Pronomen auch für die Seh- und Sprachmodelle schwierig ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 232, "src_audio": "/acl6060/audio/en/232.wav", "src_ref": "As a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zur Überprüfung der Korrektheit und weil es ein interessantes Experiment ist, vergleichen wir auch zwei reine Textmodelle, GPT 1 und GPT 2. So wollen wir feststellen, ob VALSE durch diese unimodalen Modelle lösbar ist. Wir berechnen die Perplexität der korrekten und der verfälschten Beschriftung, ohne Bild, um den Eintrag mit der niedrigsten Perplexität vorherzusagen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 233, "src_audio": "/acl6060/audio/en/233.wav", "src_ref": "If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die Perplexität bei der Verfälschung höher ist, ist das für uns ein Hinweis, dass die verfälschte Beschriftung möglicherweise unter Plausibilitätsverzerrungen oder anderen sprachlichen Verzerrungen leidet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 234, "src_audio": "/acl6060/audio/en/234.wav", "src_ref": "And it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist interessant zu sehen, dass in einigen Fällen die GPT-Modelle mit reinem Text die Plausibilität der Welt besser erfasst haben als die Seh- und Sprachmodelle.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 235, "src_audio": "/acl6060/audio/en/235.wav", "src_ref": "So to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zusammenfassend lässt sich sagen, dass VALSE ein Benchmark ist, der durch das Objektiv von linguistischen Konstrukten schaut, um die Gemeinschaft bei der Verbesserung von Seh- und Sprachmodellen zu unterstützen, indem er ihre visuelle Erdung hart testet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 236, "src_audio": "/acl6060/audio/en/236.wav", "src_ref": "Our experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere Experimente zeigen, dass Seh- und Sprachmodelle benannte Objekte und ihre Existenz in Bildern gut erkennen, wie der Existenzteil zeigt. Sie haben aber Schwierigkeiten damit, ihre gegenseitige Abhängigkeit und Beziehungen in visuellen Szenen zu begründen, wenn sie gezwungen sind, sprachliche Indikatoren zu berücksichtigen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 237, "src_audio": "/acl6060/audio/en/237.wav", "src_ref": "We would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir möchten die Community wirklich dazu ermutigen, VALSE für die Messung des Fortschritts von Sprach- Erdung mit Seh- und Sprachmodellen zu nutzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 238, "src_audio": "/acl6060/audio/en/238.wav", "src_ref": "And even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zudem könnte VALSE als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten. So kann kontrolliert werden, ob ein Datensatz dazu beiträgt, dass sich Modelle in einem der von VALSE getesteten Aspekte verbessern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 239, "src_audio": "/acl6060/audio/en/239.wav", "src_ref": "If you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn Sie Interesse haben, schauen Sie sich die VALSE-Daten auf GitHub an. Wenn Sie irgendwelche Fragen haben, zögern Sie nicht, uns zu kontaktieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 240, "src_audio": "/acl6060/audio/en/240.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo, mein Name ist Kamezawa von der Universität Tokio.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 241, "src_audio": "/acl6060/audio/en/241.wav", "src_ref": "I'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde folgendes Paper vorstellen: RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 242, "src_audio": "/acl6060/audio/en/242.wav", "src_ref": "I'll be explaining in this order.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde es in dieser Reihenfolge erklären.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 243, "src_audio": "/acl6060/audio/en/243.wav", "src_ref": "First, I will introduce automatic release note generation that we are working on in this research.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst werde ich die automatische Generierung von Versionshinweisen vorstellen, an der wir in dieser Forschung arbeiten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 244, "src_audio": "/acl6060/audio/en/244.wav", "src_ref": "A release note is a technical document that summarizes the changes distributed with each release of a software product.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein Versionshinweis ist ein technisches Dokument, das die Änderungen zusammenfasst, die mit jeder Version eines Softwareprodukts vertrieben werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 245, "src_audio": "/acl6060/audio/en/245.wav", "src_ref": "The image shows a release note for version two point six point four of the vuejs library.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Bild zeigt einen Versionshinweis für Version 2.6.4 der Vuejs-Bibliothek.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 246, "src_audio": "/acl6060/audio/en/246.wav", "src_ref": "Release notes play an important role in open source development but they're time consuming to prepare manually.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Versionshinweise spielen bei der Open-Source-Entwicklung eine wichtige Rolle, aber ihre Erstellung ist manuell zeitaufwändig.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 247, "src_audio": "/acl6060/audio/en/247.wav", "src_ref": "Therefore, it would be very useful to be able to automatically generate high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher wäre es sehr nützlich, wenn man automatisch qualitativ hochwertige Versionshinweise generieren könnte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 248, "src_audio": "/acl6060/audio/en/248.wav", "src_ref": "I will defer to two previous researches on automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde zwei frühere Untersuchungen zur automatischen Generierung von Versionshinweisen erwähnen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 249, "src_audio": "/acl6060/audio/en/249.wav", "src_ref": "The first is a system called ARENA released in twenty fourteen.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zuerst gibt es ein System mit dem Namen ARENA, das im Jahr 2014 veröffentlicht wurde.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 250, "src_audio": "/acl6060/audio/en/250.wav", "src_ref": "It takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es verfolgt einen regelbasierten Ansatz, indem es zum Beispiel den Change-Extractor verwendet, um alle Unterschiede, Bibliotheksänderungen und Dokumentänderungen aus den verschiedenen Versionen zu extrahieren und sie schließlich zu kombinieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 251, "src_audio": "/acl6060/audio/en/251.wav", "src_ref": "The most notable feature of this system is the issue extractor in the upper right corner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die auffälligste Funktion dieses Systems ist der Ausgabe-Extraktor in der oberen rechten Ecke.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 252, "src_audio": "/acl6060/audio/en/252.wav", "src_ref": "Which must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies muss Jira, dem Fehlerverwaltungssystem, überlassen werden und kann nur auf Projekte angewendet werden, die Jira verwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 253, "src_audio": "/acl6060/audio/en/253.wav", "src_ref": "In other words, it cannot be used for many projects on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mit anderen Worten: Es kann nicht für viele Projekte auf GitHub verwendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 254, "src_audio": "/acl6060/audio/en/254.wav", "src_ref": "The second is Glyph, recently announced in twenty twenty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das zweite ist Glyph, das kürzlich in 2020 angekündigt wurde.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 255, "src_audio": "/acl6060/audio/en/255.wav", "src_ref": "It is available on the internet and can be installed via pip.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist im Internet verfügbar und kann über pip installiert werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 256, "src_audio": "/acl6060/audio/en/256.wav", "src_ref": "This system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieses System verfügt über ein einfaches, lernbasiertes Textklassifikationsmodell und gibt eine von fünf Markierung wie Funktionen oder Fehlerbehebungen für jede eingegebene Commit-Nachricht aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 257, "src_audio": "/acl6060/audio/en/257.wav", "src_ref": "This image is a sample usage that returns a corrective or bug fixes label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieses Bild ist eine Stichprobe, das eine Markierung für eine Korrektur oder Fehlerbehebung zurückgibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 258, "src_audio": "/acl6060/audio/en/258.wav", "src_ref": "Glyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Trainingsdaten von Glyph sind ziemlich klein, etwa 5.000, und werden in den unten beschriebenen Experimenten gezeigt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 259, "src_audio": "/acl6060/audio/en/259.wav", "src_ref": "The performance of the text classification model is not high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Leistung des Textklassifikationsmodells ist nicht hoch.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 260, "src_audio": "/acl6060/audio/en/260.wav", "src_ref": "I present two related researches, but their problems are limited applicability and scarce data resources.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich stelle zwei verwandte Forschungsarbeiten vor, deren Probleme jedoch die begrenzte Anwendbarkeit und die knappen Datenressourcen sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 261, "src_audio": "/acl6060/audio/en/261.wav", "src_ref": "Our paper solves these two problems and automatically generates high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Paper löst diese zwei Probleme und erzeugt automatisch qualitativ hochwertige Versionshinweise.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 262, "src_audio": "/acl6060/audio/en/262.wav", "src_ref": "With a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aufgrund des Problems der begrenzten Anwendbarkeit schlagen wir eine qualitativ hochwertige, klassenweise Zusammenfassungsmethode vor, die nur Commit-Nachrichten als Eingabe verwendet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 263, "src_audio": "/acl6060/audio/en/263.wav", "src_ref": "This proposed method can be used for all English repositories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 264, "src_audio": "/acl6060/audio/en/264.wav", "src_ref": "For the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Für das zweite Problem der knappen Datenressourcen haben wir unseren RNSum-Datensatz erstellt, der aus etwa 82.000 Daten besteht. Diese wurden aus den Daten der öffentlichen GitHub-Repositories mithilfe der GitHub-API gesammelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 265, "src_audio": "/acl6060/audio/en/265.wav", "src_ref": "Next, I'll describe our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Nächstes werde ich unseren Datensatz beschreiben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 266, "src_audio": "/acl6060/audio/en/266.wav", "src_ref": "Here is an example of data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist ein Beispiel für die Daten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 267, "src_audio": "/acl6060/audio/en/267.wav", "src_ref": "The left side is a commit message and the right side is the release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auf der linken Seite ist eine Commit-Nachricht und auf der rechten Seite sind die Versionshinweise.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 268, "src_audio": "/acl6060/audio/en/268.wav", "src_ref": "Release notes are labeled as improvements or fixes, etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Versionshinweise sind als Verbesserungen, Korrekturen usw. markiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 269, "src_audio": "/acl6060/audio/en/269.wav", "src_ref": "We have set up a task that takes the commit messages as input and outputs a labeled release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben eine Aufgabe eingerichtet, die die Commit-Nachricht als Eingabe aufnimmt und die Ausgabe ist ein markierter Versionshinweis.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 270, "src_audio": "/acl6060/audio/en/270.wav", "src_ref": "This can be regarded as a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies kann als eine Zusammenfassungsaufgabe betrachtet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 271, "src_audio": "/acl6060/audio/en/271.wav", "src_ref": "We have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben vier Markierungen vordefiniert: Funktionen, Verbesserungen, Fehlerbehebungen, Beseitigung von Verwerfungen und bahnbrechende Änderungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 272, "src_audio": "/acl6060/audio/en/272.wav", "src_ref": "These were set based on previous research and other factors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese wurden auf der Grundlage von früheren Forschungen und anderen Faktoren festgelegt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 273, "src_audio": "/acl6060/audio/en/273.wav", "src_ref": "The release note on the bottom right is extracted from the release note on the bottom left.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Versionshinweise unten rechts wurde aus den Versionshinweisen unten links extrahiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 274, "src_audio": "/acl6060/audio/en/274.wav", "src_ref": "At this time, it is necessary to detect the four labels that have been set up in advance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zu diesem Zeitpunkt müssen die vier Markierungen, die im Voraus erstellt wurden, erkannt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 275, "src_audio": "/acl6060/audio/en/275.wav", "src_ref": "But the labels are not always consistent with each repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Markierungen sind jedoch nicht immer mit jedem Repository konsistent.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 276, "src_audio": "/acl6060/audio/en/276.wav", "src_ref": "For example, the improvements label includes improvements, enhancements, optimizations, and so on.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Markierung „Verbesserungen“ umfasst beispielsweise Verbesserungen, Erweiterungen, Optimierungen und so weiter.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 277, "src_audio": "/acl6060/audio/en/277.wav", "src_ref": "We prepared a vocabulary list of about thirty labels for each of these notational variations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben eine Wortschatzliste mit etwa 30 Markierungen für jede dieser Notationsvarianten erstellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 278, "src_audio": "/acl6060/audio/en/278.wav", "src_ref": "This is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese dient dazu, die Klasse des Versionshinweises zu erkennen. Sie sammelt den Text des Versionshinweises, der als Versionshinweissatz für die Klasse folgt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 279, "src_audio": "/acl6060/audio/en/279.wav", "src_ref": "Next is a commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Nächstes folgt eine Commit-Nachricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 280, "src_audio": "/acl6060/audio/en/280.wav", "src_ref": "Commit messages are not tied to each release.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Commit-Nachrichten sind nicht an die einzelnen Versionen gebunden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 281, "src_audio": "/acl6060/audio/en/281.wav", "src_ref": "As shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die aktuelle Version 2.5 bis 19 ist, müssen wir, wie im Bild unten gezeigt, die frühere Version 2.5 bis 18 identifizieren und eine Diff erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 282, "src_audio": "/acl6060/audio/en/282.wav", "src_ref": "This is a bit tedious and it is not enough to just get a list of releases and look at the before and after.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies ist etwas mühsam und es reicht nicht aus, nur eine Liste von Versionen zu erstellen und die Vorher-Nachher-Bilanz zu betrachten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 283, "src_audio": "/acl6060/audio/en/283.wav", "src_ref": "We created a heuristic matching rule to get the previous and next versions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben eine heuristische Abgleichsregel erstellt, um die vorherige und die nächste Version zu bekommen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 284, "src_audio": "/acl6060/audio/en/284.wav", "src_ref": "Dataset analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Analyse des Datensatzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 285, "src_audio": "/acl6060/audio/en/285.wav", "src_ref": "In the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Am Ende wurden 7.200 Repositories und 82 Daten gesammelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 286, "src_audio": "/acl6060/audio/en/286.wav", "src_ref": "Also, the average number of release notes tokens is sixty three, which is quite high for a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Außerdem liegt die durchschnittliche Anzahl der Versionshinweis-Token bei 63, was für eine Zusammenfassungsaufgabe recht hoch ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 287, "src_audio": "/acl6060/audio/en/287.wav", "src_ref": "Also, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auch die Anzahl der einzigartigen Token ist mit 830.000 recht groß.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 288, "src_audio": "/acl6060/audio/en/288.wav", "src_ref": "This is due to the large number of unique class or method names found in the repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies ist auf die große Anzahl von eindeutigen Klassen- oder Methodennamen im Repository zurückzuführen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 289, "src_audio": "/acl6060/audio/en/289.wav", "src_ref": "Next, I will explain the proposed method.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Nächstes werde ich die vorgeschlagene Methode erläutern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 290, "src_audio": "/acl6060/audio/en/290.wav", "src_ref": "The classwise extractive then abstractive summarization model consists of two neural modules.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das klassenweise extraktive und dann abstrahierende Zusammenfassungsmodell besteht aus zwei neuronalen Modulen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 291, "src_audio": "/acl6060/audio/en/291.wav", "src_ref": "A classifier using BERT or CodeBERT and a generator using BART.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein Klassifikator verwendet BERT oder CodeBERT und ein Generator verwendet BART.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 292, "src_audio": "/acl6060/audio/en/292.wav", "src_ref": "First, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zuerst verwendet CEAS einen Klassifikator, um jede Commit-Nachricht in fünf Klassen von Versionshinweisen zu klassifizieren, die Verbesserungen, Fehlerbehebungen, Verwerfungen und eine weitere Klasse enthalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 293, "src_audio": "/acl6060/audio/en/293.wav", "src_ref": "The commit messages classified as other are discarded.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die als „Sonstiges“ eingestuften Commit-Nachrichten werden verworfen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 294, "src_audio": "/acl6060/audio/en/294.wav", "src_ref": "Then CEAS applies the generator to the four labeled documents independently and generates release notes for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann wendet CEAS den Generator auf die vier markierten Dokumente unabhängig voneinander an und erstellt Versionshinweise für jede Klasse.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 295, "src_audio": "/acl6060/audio/en/295.wav", "src_ref": "In this task, the direct correspondences between commit messages and release notes are not known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In dieser Aufgabe sind die direkten Korrespondenzen zwischen Commit-Nachrichten und Versionshinweisen nicht bekannt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 296, "src_audio": "/acl6060/audio/en/296.wav", "src_ref": "Therefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um den Klassifikator zu trainieren, haben wir daher jeder eingegebenen Commit-Nachricht eine Umfrage zugewiesen und dabei die ersten zehn Zeichen jeder Commit-Nachricht verwendet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 297, "src_audio": "/acl6060/audio/en/297.wav", "src_ref": "We modeled the classwise abstractive summarization approach by two different methods.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben den klassenweisen abstrakten Zusammenfassungsansatz durch zwei verschiedene Methoden modelliert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 298, "src_audio": "/acl6060/audio/en/298.wav", "src_ref": "The first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das erste Modell, das wir CAS-Single nennen, besteht aus einem einzigen 6-zu-6-Netzwerk und generiert einen einzigen Versionshinweistext, der eine Verkettung von eingegebenen Commit-Nachrichten ergibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 299, "src_audio": "/acl6060/audio/en/299.wav", "src_ref": "The output texts can be divided into classwise segments based on special class-specific endpoint symbols.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die ausgegebenen Texte können auf der Grundlage spezieller klassenspezifischer Endpunktsymbole in klassenweise Segmente unterteilt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 300, "src_audio": "/acl6060/audio/en/300.wav", "src_ref": "The second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die zweite Methode, die wir CAS-Multi nennen, besteht aus vier verschiedenen seq2seq-Netzwerken, von denen jedes einer der festen Versionshinweisklassen entspricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 301, "src_audio": "/acl6060/audio/en/301.wav", "src_ref": "Okay, let me explain the experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde nun die Experimente erklären.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 302, "src_audio": "/acl6060/audio/en/302.wav", "src_ref": "Five methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Fünf Methoden wurden verglichen: CEAS, CAS-Single, CAS-Multi, Clustering und Glyph aus einer früheren Studie.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 303, "src_audio": "/acl6060/audio/en/303.wav", "src_ref": "Regarding evaluation, in some cases, release notes are output in multiple sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Was die Evaluation betrifft, so werden in einigen Fällen die Versionshinweise in mehreren Sätzen ausgegeben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 304, "src_audio": "/acl6060/audio/en/304.wav", "src_ref": "Since it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da es schwierig ist, die Anzahl von Sätzen zu berechnen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 305, "src_audio": "/acl6060/audio/en/305.wav", "src_ref": "The BLEU is penalized when the system outputs a short sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der BLEU wird benachteiligt, wenn das System einen kurzen Satz ausgibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 306, "src_audio": "/acl6060/audio/en/306.wav", "src_ref": "This penalty results in a lower BLEU value in the experiment results described next.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieser Abzug führt zu einem niedrigeren BLEU-Wert in den nachfolgend beschriebenen Versuchsergebnissen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 307, "src_audio": "/acl6060/audio/en/307.wav", "src_ref": "Finally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich berechnen wir auch die Spezifität, da ROUGE und BLEU nicht berechnet werden können, wenn die Versionshinweise leer sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 308, "src_audio": "/acl6060/audio/en/308.wav", "src_ref": "A higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eine höhere Spezifität bedeutet, dass das Modell in Fällen, in denen die Versionshinweise leer sind, einen leeren Text korrekt ausgibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 309, "src_audio": "/acl6060/audio/en/309.wav", "src_ref": "Here are the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind die Resultate.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 310, "src_audio": "/acl6060/audio/en/310.wav", "src_ref": "Since the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da der Datensatz E-Mail-Adressen, Hash-Werte usw. enthält, haben wir auch den bereinigten Datensatz ausgewertet, der diese ausschließt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 311, "src_audio": "/acl6060/audio/en/311.wav", "src_ref": "CEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CEAS und CAS erreichten ROUGE-L-Werte, die mehr als zehn Punkte über den Baselines lagen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 312, "src_audio": "/acl6060/audio/en/312.wav", "src_ref": "In particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insbesondere beim bereinigten Testsatz ist der Abstand zwischen der vorgeschlagenen Methode und den Baselines auf mehr als zwanzig Punkte angestiegen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 313, "src_audio": "/acl6060/audio/en/313.wav", "src_ref": "These results indicate that CEAS and CAS are significantly affected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Ergebnisse zeigen, dass CEAS und CAS signifikant betroffen sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 314, "src_audio": "/acl6060/audio/en/314.wav", "src_ref": "CEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CEAS erzielte bessere ROUGE-L-Werte als CAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators beim Training des Klassifikators mit Pseudo-Markierungen effektiv ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 315, "src_audio": "/acl6060/audio/en/315.wav", "src_ref": "High coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eine hohe Abdeckung von CEAS kann wahrscheinlich erreicht werden, weil sich der Klassifikator auf die Auswahl relevanter Commit-Nachrichten für jede Klasse konzentrieren kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 316, "src_audio": "/acl6060/audio/en/316.wav", "src_ref": "CAS-Multi tended to yield higher ROUGE-L than CAS-Single.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CAS-Multi ergab tendenziell mehr ROUGE-L-Werte als CAS-Single.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 317, "src_audio": "/acl6060/audio/en/317.wav", "src_ref": "Suggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir schlagen vor, dass es auch effektiv ist, unterschiedliche abstrakte Zusammenfassungsmodelle, die unabhängig voneinander sind, für jede Klasse von den Versionshinweisen zu entwickeln.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 318, "src_audio": "/acl6060/audio/en/318.wav", "src_ref": "Here are an error analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist eine Fehleranalyse.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 319, "src_audio": "/acl6060/audio/en/319.wav", "src_ref": "CAS methods tend to output shorter sentences than human reference sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CAS-Methoden geben tendenziell kürzere Sätze aus als menschliche Referenzsätze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 320, "src_audio": "/acl6060/audio/en/320.wav", "src_ref": "In the figure on the right, the reference sentence has three or four sentences, while CAS has only one.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In der Abbildung rechts besteht der Referenzsatz aus drei oder vier Sätzen, während CAS aus nur einem besteht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 321, "src_audio": "/acl6060/audio/en/321.wav", "src_ref": "The reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Grund für die Reluktanz dieses Modells ist, dass in den Trainingsdaten nur 33 Prozent der Sätze in der Markierung Funktionen und 40 Prozent in der Markierung Verbesserung zu finden sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 322, "src_audio": "/acl6060/audio/en/322.wav", "src_ref": "Furthermore, CAS methods cannot generate accurate release notes without additional information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus können CAS-Methoden ohne zusätzliche Informationen keine genauen Versionshinweise erstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 323, "src_audio": "/acl6060/audio/en/323.wav", "src_ref": "The top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Oben auf der rechten Seite ist ein Beispiel für eine sehr unübersichtliche Commit-Nachricht. Der vollständige Satz kann nicht ohne Bezug auf den entsprechenden Fortschritt oder das Problem generiert werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 324, "src_audio": "/acl6060/audio/en/324.wav", "src_ref": "The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das folgende Beispiel zeigt, dass die beiden Commit-Nachrichten in der Eingabe zusammenhängen und zu einem Satz zusammengefasst werden sollten, was jedoch nicht gelingt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 325, "src_audio": "/acl6060/audio/en/325.wav", "src_ref": "Finally, a conclusion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nun kommen wir zum Fazit.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 326, "src_audio": "/acl6060/audio/en/326.wav", "src_ref": "We have built a new dataset for automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben einen neuen Datensatz für die automatische Generierung von Versionshinweisen erstellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 327, "src_audio": "/acl6060/audio/en/327.wav", "src_ref": "We have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben auch eine Aufgabe für die Eingabe von Commit-Nachrichten und deren Zusammenfassung formuliert, sodass sie für alle auf Englisch geschriebene Projekte anwendbar ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 328, "src_audio": "/acl6060/audio/en/328.wav", "src_ref": "Our experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere Experimente zeigen, dass die vorgeschlagene Methode weniger verrauschte Versionshinweise mit höherem Abdeckungsgrad als die Baselines erzeugt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 329, "src_audio": "/acl6060/audio/en/329.wav", "src_ref": "Please check out our dataset on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bitte sehen Sie sich unseren Datensatz auf GitHub an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 330, "src_audio": "/acl6060/audio/en/330.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 331, "src_audio": "/acl6060/audio/en/331.wav", "src_ref": "Hello. My name is Asaf Harari.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo! Mein Name ist Asaf Harari.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 332, "src_audio": "/acl6060/audio/en/332.wav", "src_ref": "And I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde unser Paper vorstellen: Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 333, "src_audio": "/acl6060/audio/en/333.wav", "src_ref": "Data scientists analyze data and mainly focus on the manipulating the data's existing features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Datenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der bestehenden Funktionen von Daten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 334, "src_audio": "/acl6060/audio/en/334.wav", "src_ref": "But sometimes, these features are limited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber manchmal sind diese Funktionen begrenzt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 335, "src_audio": "/acl6060/audio/en/335.wav", "src_ref": "Feature generation using another data source may add substantial information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Generierung von Funktionen unter Verwendung einer anderen Datenquelle kann wesentliche Informationen hinzufügen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 336, "src_audio": "/acl6060/audio/en/336.wav", "src_ref": "Our research goal is automatic tabular data enrichment using external sources' free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Forschungsziel ist die automatische Anreicherung tabellarischer Daten mit freiem Text aus externen Quellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 337, "src_audio": "/acl6060/audio/en/337.wav", "src_ref": "Assume we have a tabular dataset and a knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Angenommen, wir haben einen tabellarischen Datensatz und eine Wissensbasis.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 338, "src_audio": "/acl6060/audio/en/338.wav", "src_ref": "We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir brauchen einen automatischen Prozess, der Entity Linking und eine Textanalyse umfasst, um neue Funktionen aus dem freien Text der Wissensbasis zu extrahieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 339, "src_audio": "/acl6060/audio/en/339.wav", "src_ref": "Our framework FeSTE is exactly this automatic process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Rahmen FeSTE ist genau dieser automatische Prozess.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 340, "src_audio": "/acl6060/audio/en/340.wav", "src_ref": "So let's see an example in a dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sehen wir uns also ein Beispiel in einem Datensatz an, der in FeSTE eingespeist wird.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 341, "src_audio": "/acl6060/audio/en/341.wav", "src_ref": "In this example, the dataset is university dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel ist der Datensatz der Universitätsdatensatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 342, "src_audio": "/acl6060/audio/en/342.wav", "src_ref": "When its goal is to classify universities into low ranking universities and high-ranking universities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Ziel besteht darin, die Universitäten nach niedrigem und hohem Rang einzuteilen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 343, "src_audio": "/acl6060/audio/en/343.wav", "src_ref": "As knowledge base, we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Wissensbasis verwenden wir Wikipedia.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 344, "src_audio": "/acl6060/audio/en/344.wav", "src_ref": "The first phase of FeSTE is entity linking.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die erste Phase von FeSTE ist Entity Linking.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 345, "src_audio": "/acl6060/audio/en/345.wav", "src_ref": "When each entity, in this example the university name, is linked to an entity within the knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jede Entität, in diesem Beispiel der Name der Universität, ist mit einer Entität innerhalb der Wissensbasis verknüpft.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 346, "src_audio": "/acl6060/audio/en/346.wav", "src_ref": "And and the text of the entities of the knowledge base is extracted and added to the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Text der Entitäten aus der Wissensbasis wird extrahiert und dem Datensatz hinzugefügt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 347, "src_audio": "/acl6060/audio/en/347.wav", "src_ref": "In this example, the text is the Wikipedia page's abstract.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel ist der Text das Abstract der Wikipedia-Seite.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 348, "src_audio": "/acl6060/audio/en/348.wav", "src_ref": "Now, we need to generate or extract features from the retrieved text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nun müssen wir Funktionen aus dem abgerufenen Text generieren oder extrahieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 349, "src_audio": "/acl6060/audio/en/349.wav", "src_ref": "So, we need to ah feature extraction phase ah which includes text analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir müssen also eine Phase der Funktionsextraktion einleiten, zu der die Textanalyse gehört.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 350, "src_audio": "/acl6060/audio/en/350.wav", "src_ref": "And this is the main novelty of this paper and I will deep dive into it in the next slides.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das ist die wichtigste Neuerung dieses Papers, auf die ich auf den nächsten Folien näher eingehen werde.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 351, "src_audio": "/acl6060/audio/en/351.wav", "src_ref": "After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nach der Phase der Funktionsextraktion folgt eine Phase der Funktionsgenerierung, in der wir die extrahierten Funktionen verwenden, um eine kleine Anzahl neuer Funktionen zu erzeugen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 352, "src_audio": "/acl6060/audio/en/352.wav", "src_ref": "First generate ah features in the number of classes of the original dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst werden Funktionen in der Anzahl der Klassen des ursprünglichen Datensatzes generiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 353, "src_audio": "/acl6060/audio/en/353.wav", "src_ref": "In this example, the original dataset has two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 354, "src_audio": "/acl6060/audio/en/354.wav", "src_ref": "So, FeSTE generates two new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "FeSTE generiert also zwei neue Funktionen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 355, "src_audio": "/acl6060/audio/en/355.wav", "src_ref": "But if the dataset has five classes, FeSTE generates five new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn der Datensatz jedoch fünf Klassen hat, generiert FeSTE fünf neue Funktionen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 356, "src_audio": "/acl6060/audio/en/356.wav", "src_ref": "Each feature represents the likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jede Funktion stellt die Wahrscheinlichkeit für jede Klasse dar.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 357, "src_audio": "/acl6060/audio/en/357.wav", "src_ref": "To analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um den Text zu analysieren, verwenden wir die aktuellste Textanalyse, die auf Transformer-basierten Sprachmodellen wie BERT, GPT, XLNet und anderen beruht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 358, "src_audio": "/acl6060/audio/en/358.wav", "src_ref": "It is but it is not likely that we can train language models using the input datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist aber unwahrscheinlich, dass wir Sprachmodelle mit den eingegebenen Datensätzen trainieren können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 359, "src_audio": "/acl6060/audio/en/359.wav", "src_ref": "So a naive approach will be ah target task finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein naiver Ansatz ist also eine Feinabstimmung des Zieldatensatzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 360, "src_audio": "/acl6060/audio/en/360.wav", "src_ref": "So, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In der Phase der Funktionsextraktion können wir also vortrainierte Sprachmodelle herunterladen und das Sprachmodell über den Zieldatensatz feinabstimmen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 361, "src_audio": "/acl6060/audio/en/361.wav", "src_ref": "In this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel zur Feinabstimmung des Sprachmodells klassifizieren wir den Text in Klassen, d. h. wir klassifizieren das Abstract in die Klassen niedrig oder hoch.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 362, "src_audio": "/acl6060/audio/en/362.wav", "src_ref": "Receive the language model output, which is the likelihood for each class and use as new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir erhalten die Ausgabe des Sprachmodells, die die Wahrscheinlichkeit für jede Klasse darstellt, und verwenden sie als neue Funktionen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 363, "src_audio": "/acl6060/audio/en/363.wav", "src_ref": "The problem with this approach is datasets may have few distinct entities / texts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Problem bei diesem Ansatz ist, dass Datensätze möglicherweise nur wenige unterschiedliche Entitäten/Texte beinhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 364, "src_audio": "/acl6060/audio/en/364.wav", "src_ref": "In our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem Experiment enthält fast die Hälfte der Datensätze weniger als 400 Stichproben und der kleinste Datensatz enthält 35 Stichproben in einem Trainingssatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 365, "src_audio": "/acl6060/audio/en/365.wav", "src_ref": "So to finetune a language model over ah this dataset will be ineffective.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Feinabstimmung eines Sprachmodells über diesen Datensatz ist also unwirksam.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 366, "src_audio": "/acl6060/audio/en/366.wav", "src_ref": "But we can use prior knowledge about pre-analyzed datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wir können vorherige Kenntnisse über bereits analysierte Datensätze nutzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 367, "src_audio": "/acl6060/audio/en/367.wav", "src_ref": "Because FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da wir FeSTE auf mehrere Datensätze anwenden, können wir die „n minus 1“-Datensätze verwenden, um Informationen über die „n minus 1“-Datensätze zu sammeln, und diese Informationen verwenden, wenn wir den n-ten Datensatz analysieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 368, "src_audio": "/acl6060/audio/en/368.wav", "src_ref": "What we, what we suggest is to add, to add another finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir schlagen vor, eine weitere Feinabstimmungsphase hinzuzufügen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 369, "src_audio": "/acl6060/audio/en/369.wav", "src_ref": "A preliminary multitask finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das war eine vorläufige Multitask-Feinabstimmungsphase.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 370, "src_audio": "/acl6060/audio/en/370.wav", "src_ref": "When you finetune the language model over the n minus one datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Feinabstimmung des Sprachmodells geht über die „n minus 1“-Datensätze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 371, "src_audio": "/acl6060/audio/en/371.wav", "src_ref": "And, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann führen wir eine weitere Feinabstimmungsphase durch, die eine Feinabstimmung des Zieldatensatzes ist, wenn wir das Sprachmodell über den n-ten Zieldatensatz feinabstimmen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 372, "src_audio": "/acl6060/audio/en/372.wav", "src_ref": "The state-of-the-art in multitask ah multitask finetuning called MTDNN.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die modernste Multitask-Feinabstimmung wird MTDNN genannt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 373, "src_audio": "/acl6060/audio/en/373.wav", "src_ref": "In MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei der MTDNN bleiben die Überschriften in der Reihe der Aufgaben im Trainingssatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 374, "src_audio": "/acl6060/audio/en/374.wav", "src_ref": "So, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel gibt es also vier Aufgaben im Trainingssatz. Das MTDNN enthält also vier Überschriften, wie Sie auf dem Bild sehen können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 375, "src_audio": "/acl6060/audio/en/375.wav", "src_ref": "And it samples a random batch from ah from the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es nimmt sich eine Zufallsstichprobe aus dem Trainingssatz heraus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 376, "src_audio": "/acl6060/audio/en/376.wav", "src_ref": "And if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die Zufallsstichprobe zum Beispiel zu einer einzelnen Satzklassifikationsaufgabe gehört, führt sie Vorwärts- und Rückwärtspfade durch die erste Überschrift aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 377, "src_audio": "/acl6060/audio/en/377.wav", "src_ref": "And if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die Zufallsstichprobe zur paarweisen Ranking-Aufgabe gehört, führt sie den Vorwärts- und Rückwärtspfad über die letzte Überschrift aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 378, "src_audio": "/acl6060/audio/en/378.wav", "src_ref": "In our scenario, ah tabular datasets vary in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem Szenario variieren tabellarische Datensätze in der Anzahl der Klassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 379, "src_audio": "/acl6060/audio/en/379.wav", "src_ref": "So there are many tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es gibt also viele Aufgaben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 380, "src_audio": "/acl6060/audio/en/380.wav", "src_ref": "MTDNN maintained number of classes, heads, output layers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "MTDNN enthält eine Anzahl von Klassen, Überschriften und ausgegebenen Ebenen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 381, "src_audio": "/acl6060/audio/en/381.wav", "src_ref": "And the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und zusätzlich muss MTDNN neue Überschriften für einen neuen Datensatz mit einer neuen Aufgabe initialisieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 382, "src_audio": "/acl6060/audio/en/382.wav", "src_ref": "Our approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Ansatz, genannt Feinabstimmungsreformulierungsaufgabe, besteht darin, dass wir, anstatt mehrere Überschriften beizubehalten, jeden Datensatz in einen Satz pro Klassifikationssproblem umformulieren, was zwei Klassenaufgaben entspricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 383, "src_audio": "/acl6060/audio/en/383.wav", "src_ref": "So let's see an example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schauen wir uns ein Beispiel an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 384, "src_audio": "/acl6060/audio/en/384.wav", "src_ref": "Here is the our input dataset which consists of entities, features, text and classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist unser eingegebener Datensatz, der aus Entitäten, Funktionen, Text und Klassen besteht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 385, "src_audio": "/acl6060/audio/en/385.wav", "src_ref": "And, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir formulieren die Aufgabe von einer Klassifizierung des Textes in niedrig oder hoch zu einer Klassifizierung des Textes, des Abstracts und der Klasse in wahr oder falsch um.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 386, "src_audio": "/acl6060/audio/en/386.wav", "src_ref": "Or in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mit anderen Worten: Wir haben das Sprachmodell so trainiert, dass es ein Abstract und die Klasse als Abstract und die Klasse klassifiziert, wenn das Abstract zur Klasse gehört oder nicht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 387, "src_audio": "/acl6060/audio/en/387.wav", "src_ref": "So the label vector in this case stays always ah which consists always with two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "So bleibt die Vektor-Markierung in diesem Fall bestehen, die immer aus zwei Klassen besteht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 388, "src_audio": "/acl6060/audio/en/388.wav", "src_ref": "And this is the ah algorithm for our fine, reformulated finetuning approach.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das ist der Algorithmus für unseren neu formulierten Finabstimmungsansatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 389, "src_audio": "/acl6060/audio/en/389.wav", "src_ref": "So let's see the full framework.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schauen wir uns also den gesamten Rahmen an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 390, "src_audio": "/acl6060/audio/en/390.wav", "src_ref": "Dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Datensatz wird in FeSTE eingespeist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 391, "src_audio": "/acl6060/audio/en/391.wav", "src_ref": "And then ah FeSTE executes entity linking phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann führt FeSTE die Entity-Linking-Phase aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 392, "src_audio": "/acl6060/audio/en/392.wav", "src_ref": "It ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es extrahiert den Text aus der Wissensbasis, die in diesem Beispiel das Abstract der Wikipedia-Seite ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 393, "src_audio": "/acl6060/audio/en/393.wav", "src_ref": "Then it reformulated the task into a pairwise sentence classification task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann formulierte es die Aufgabe in eine paarweise Satzklassifikationsaufgabe um.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 394, "src_audio": "/acl6060/audio/en/394.wav", "src_ref": "Applied the language model to the new task and the output likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Sprachmodell wird auf die neue Aufgabe und die Ausgabewahrscheinlichkeit für jede Klasse angewandt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 395, "src_audio": "/acl6060/audio/en/395.wav", "src_ref": "And now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nun ist das Sprachmodell bereits über den „n minus 1“-Datensatz mithilfe einer vorläufigen Multitask-Feinabstimmung feinabgestimmt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 396, "src_audio": "/acl6060/audio/en/396.wav", "src_ref": "Then we use the output vector of the language model as a newly generated feature in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann verwenden wir den Ausgabe-Vektor des Sprachmodells als eine neu generierte Funktion in der Anzahl der Klassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 397, "src_audio": "/acl6060/audio/en/397.wav", "src_ref": "To evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zur Evaluation unseres Rahmens verwenden wir 17 tabellarische Klassifikationsdatensätze, die sich in Größe, Funktionen, Ausgewogenheit, Domäne und anfänglicher Leistung unterscheiden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 398, "src_audio": "/acl6060/audio/en/398.wav", "src_ref": "And as knowledge base we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Wissensbasis verwenden wir Wikipedia.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 399, "src_audio": "/acl6060/audio/en/399.wav", "src_ref": "We design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir konzipieren unser Experiment als Leave-One-Out-Evaluation, bei dem wir FeSTe mit 16 Datensätzen trainieren und auf den 17. Datensatz anwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 400, "src_audio": "/acl6060/audio/en/400.wav", "src_ref": "We also, we also split each dataset into four folds and apply four folds cross validation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Außerdem teilen wir jeden Datensatz in vier Brüche auf und wenden eine vierfache Kreuzvalidierung an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 401, "src_audio": "/acl6060/audio/en/401.wav", "src_ref": "Then, we generate the new features and evaluate them using five evaluation classifiers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann generieren wir die neuen Funktionen und bewerten sie mit fünf Evaluationssklassifikatoren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 402, "src_audio": "/acl6060/audio/en/402.wav", "src_ref": "We use in our experiments base BERT base architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir verwenden in unseren Experimenten die Basisarchitektur BERT.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 403, "src_audio": "/acl6060/audio/en/403.wav", "src_ref": "Here are the results for our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind die Ergebnisse unserer Experimente.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 404, "src_audio": "/acl6060/audio/en/404.wav", "src_ref": "You can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie können sehen, dass wir unseren Rahmen mit der Feinabstimmung des Zieldatensatzes, mit der Feinabstimmung der Zielaufgabe und einer vorläufigen MTDNN-Feinabstimmung vergleichen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 405, "src_audio": "/acl6060/audio/en/405.wav", "src_ref": "And our reformulated finetuning achieves the best result, the best performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere neu formulierte Feinabstimmung erreicht das beste Ergebnis und die beste Leistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 406, "src_audio": "/acl6060/audio/en/406.wav", "src_ref": "While MTDNN achieved two percent improvement over the target dataset finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "MTDNN erreichte zwei Prozent Verbesserung gegenüber der Feinabstimmung des Zieldatensatzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 407, "src_audio": "/acl6060/audio/en/407.wav", "src_ref": "Our approach achieved six percent improvement.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mit unserem Ansatz haben wir eine Verbesserung von sechs Prozent erreicht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 408, "src_audio": "/acl6060/audio/en/408.wav", "src_ref": "When we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir uns den kleinen Datensatz anschauen, sehen wir, dass die Leistung von MTDNN abnimmt und die Verbesserung der vorläufigen Multitask-Feinabstimmungsphase auf 1,5 Prozent sinkt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 409, "src_audio": "/acl6060/audio/en/409.wav", "src_ref": "But our performance increased to eleven percent compared to the target task finetuning alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber unsere Leistung stieg auf elf Prozent im Vergleich zur Feinabstimmung des Zieldatensatzes allein.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 410, "src_audio": "/acl6060/audio/en/410.wav", "src_ref": "For summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Für die Summierung ermöglicht FeSTE in unseren Experimenten eine Few-Shot-Anreicherung aus 35 Proben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 411, "src_audio": "/acl6060/audio/en/411.wav", "src_ref": "It uses one architecture for all tasks and datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es verwendet eine Architektur für alle Aufgaben und Datensätze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 412, "src_audio": "/acl6060/audio/en/412.wav", "src_ref": "And it keeps the head of ah of the model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und es behält den Hauptteil des Modells.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 413, "src_audio": "/acl6060/audio/en/413.wav", "src_ref": "But it adds reformulation phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber es fügt eine Reformulierungsphase hinzu.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 414, "src_audio": "/acl6060/audio/en/414.wav", "src_ref": "It augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es erweitert den Trainingssatz und benötigt einen Zielwert mit semantischer Bedeutung, damit wir ihn in das Sprachmodell einspeisen und für das Satzpaar-Klassifikationsproblem verwenden können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 415, "src_audio": "/acl6060/audio/en/415.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 416, "src_audio": "/acl6060/audio/en/416.wav", "src_ref": "Hello. My name is Asaf Harari.\nAnd I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.\nData scientists analyze data and mainly focus on the manipulating the data's existing features.\nBut sometimes, these features are limited.\nFeature generation using another data source may add substantial information.\nOur research goal is automatic tabular data enrichment using external sources' free text.\nAssume we have a tabular dataset and a knowledge base.\nWe need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.\nOur framework FeSTE is exactly this automatic process.\nSo let's see an example in a dataset fed into FeSTE.\nIn this example, the dataset is university dataset.\nWhen its goal is to classify universities into low ranking universities and high-ranking universities.\nAs knowledge base, we use Wikipedia.\nThe first phase of FeSTE is entity linking.\nWhen each entity, in this example the university name, is linked to an entity within the knowledge base.\nAnd and the text of the entities of the knowledge base is extracted and added to the dataset.\nIn this example, the text is the Wikipedia page's abstract.\nNow, we need to generate or extract features from the retrieved text.\nSo, we need to ah feature extraction phase ah which includes text analysis.\nAnd this is the main novelty of this paper and I will deep dive into it in the next slides.\nAfter the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.\nFirst generate ah features in the number of classes of the original dataset.\nIn this example, the original dataset has two classes.\nSo, FeSTE generates two new features.\nBut if the dataset has five classes, FeSTE generates five new features.\nEach feature represents the likelihood for each class.\nTo analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.\nIt is but it is not likely that we can train language models using the input datasets.\nSo a naive approach will be ah target task finetuning.\nSo, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.\nIn this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.\nReceive the language model output, which is the likelihood for each class and use as new features.\nThe problem with this approach is datasets may have few distinct entities / texts.\nIn our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.\nSo to finetune a language model over ah this dataset will be ineffective.\nBut we can use prior knowledge about pre-analyzed datasets.\nBecause FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.\nWhat we, what we suggest is to add, to add another finetuning phase.\nA preliminary multitask finetuning phase.\nWhen you finetune the language model over the n minus one datasets.\nAnd, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.\nThe state-of-the-art in multitask ah multitask finetuning called MTDNN.\nIn MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.\nSo, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.\nAnd it samples a random batch from ah from the training set.\nAnd if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.\nAnd if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.\nIn our scenario, ah tabular datasets vary in the number of classes.\nSo there are many tasks.\nMTDNN maintained number of classes, heads, output layers.\nAnd the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.\nOur approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.\nSo let's see an example.\nHere is the our input dataset which consists of entities, features, text and classes.\nAnd, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.\nOr in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.\nSo the label vector in this case stays always ah which consists always with two classes.\nAnd this is the ah algorithm for our fine, reformulated finetuning approach.\nSo let's see the full framework.\nDataset fed into FeSTE.\nAnd then ah FeSTE executes entity linking phase.\nIt ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.\nThen it reformulated the task into a pairwise sentence classification task.\nApplied the language model to the new task and the output likelihood for each class.\nAnd now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.\nThen we use the output vector of the language model as a newly generated feature in the number of classes.\nTo evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.\nAnd as knowledge base we use Wikipedia.\nWe design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.\nWe also, we also split each dataset into four folds and apply four folds cross validation.\nThen, we generate the new features and evaluate them using five evaluation classifiers.\nWe use in our experiments base BERT base architecture.\nHere are the results for our experiments.\nYou can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.\nAnd our reformulated finetuning achieves the best result, the best performance.\nWhile MTDNN achieved two percent improvement over the target dataset finetuning.\nOur approach achieved six percent improvement.\nWhen we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.\nBut our performance increased to eleven percent compared to the target task finetuning alone.\nFor summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.\nIt uses one architecture for all tasks and datasets.\nAnd it keeps the head of ah of the model.\nBut it adds reformulation phase.\nIt augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.\nThank you.", "tgt_ref": "Hallo! Mein Name ist Asaf Harari.\nIch werde unser Paper vorstellen: Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.\nDatenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der bestehenden Funktionen von Daten.\nAber manchmal sind diese Funktionen begrenzt.\nDie Generierung von Funktionen unter Verwendung einer anderen Datenquelle kann wesentliche Informationen hinzufügen.\nUnser Forschungsziel ist die automatische Anreicherung tabellarischer Daten mit freiem Text aus externen Quellen.\nAngenommen, wir haben einen tabellarischen Datensatz und eine Wissensbasis.\nWir brauchen einen automatischen Prozess, der Entity Linking und eine Textanalyse umfasst, um neue Funktionen aus dem freien Text der Wissensbasis zu extrahieren.\nUnser Rahmen FeSTE ist genau dieser automatische Prozess.\nSehen wir uns also ein Beispiel in einem Datensatz an, der in FeSTE eingespeist wird.\nIn diesem Beispiel ist der Datensatz der Universitätsdatensatz.\nDas Ziel besteht darin, die Universitäten nach niedrigem und hohem Rang einzuteilen.\nAls Wissensbasis verwenden wir Wikipedia.\nDie erste Phase von FeSTE ist Entity Linking.\nJede Entität, in diesem Beispiel der Name der Universität, ist mit einer Entität innerhalb der Wissensbasis verknüpft.\nDer Text der Entitäten aus der Wissensbasis wird extrahiert und dem Datensatz hinzugefügt.\nIn diesem Beispiel ist der Text das Abstract der Wikipedia-Seite.\nNun müssen wir Funktionen aus dem abgerufenen Text generieren oder extrahieren.\nWir müssen also eine Phase der Funktionsextraktion einleiten, zu der die Textanalyse gehört.\nDas ist die wichtigste Neuerung dieses Papers, auf die ich auf den nächsten Folien näher eingehen werde.\nNach der Phase der Funktionsextraktion folgt eine Phase der Funktionsgenerierung, in der wir die extrahierten Funktionen verwenden, um eine kleine Anzahl neuer Funktionen zu erzeugen.\nZunächst werden Funktionen in der Anzahl der Klassen des ursprünglichen Datensatzes generiert.\nIn diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen.\nFeSTE generiert also zwei neue Funktionen.\nWenn der Datensatz jedoch fünf Klassen hat, generiert FeSTE fünf neue Funktionen.\nJede Funktion stellt die Wahrscheinlichkeit für jede Klasse dar.\nUm den Text zu analysieren, verwenden wir die aktuellste Textanalyse, die auf Transformer-basierten Sprachmodellen wie BERT, GPT, XLNet und anderen beruht.\nEs ist aber unwahrscheinlich, dass wir Sprachmodelle mit den eingegebenen Datensätzen trainieren können.\nEin naiver Ansatz ist also eine Feinabstimmung des Zieldatensatzes.\nIn der Phase der Funktionsextraktion können wir also vortrainierte Sprachmodelle herunterladen und das Sprachmodell über den Zieldatensatz feinabstimmen.\nIn diesem Beispiel zur Feinabstimmung des Sprachmodells klassifizieren wir den Text in Klassen, d. h. wir klassifizieren das Abstract in die Klassen niedrig oder hoch.\nWir erhalten die Ausgabe des Sprachmodells, die die Wahrscheinlichkeit für jede Klasse darstellt, und verwenden sie als neue Funktionen.\nDas Problem bei diesem Ansatz ist, dass Datensätze möglicherweise nur wenige unterschiedliche Entitäten/Texte beinhalten.\nIn unserem Experiment enthält fast die Hälfte der Datensätze weniger als 400 Stichproben und der kleinste Datensatz enthält 35 Stichproben in einem Trainingssatz.\nDie Feinabstimmung eines Sprachmodells über diesen Datensatz ist also unwirksam.\nAber wir können vorherige Kenntnisse über bereits analysierte Datensätze nutzen.\nDa wir FeSTE auf mehrere Datensätze anwenden, können wir die „n minus 1“-Datensätze verwenden, um Informationen über die „n minus 1“-Datensätze zu sammeln, und diese Informationen verwenden, wenn wir den n-ten Datensatz analysieren.\nWir schlagen vor, eine weitere Feinabstimmungsphase hinzuzufügen.\nDas war eine vorläufige Multitask-Feinabstimmungsphase.\nDie Feinabstimmung des Sprachmodells geht über die „n minus 1“-Datensätze.\nDann führen wir eine weitere Feinabstimmungsphase durch, die eine Feinabstimmung des Zieldatensatzes ist, wenn wir das Sprachmodell über den n-ten Zieldatensatz feinabstimmen.\nDie modernste Multitask-Feinabstimmung wird MTDNN genannt.\nBei der MTDNN bleiben die Überschriften in der Reihe der Aufgaben im Trainingssatz.\nIn diesem Beispiel gibt es also vier Aufgaben im Trainingssatz. Das MTDNN enthält also vier Überschriften, wie Sie auf dem Bild sehen können.\nEs nimmt sich eine Zufallsstichprobe aus dem Trainingssatz heraus.\nWenn die Zufallsstichprobe zum Beispiel zu einer einzelnen Satzklassifikationsaufgabe gehört, führt sie Vorwärts- und Rückwärtspfade durch die erste Überschrift aus.\nWenn die Zufallsstichprobe zur paarweisen Ranking-Aufgabe gehört, führt sie den Vorwärts- und Rückwärtspfad über die letzte Überschrift aus.\nIn unserem Szenario variieren tabellarische Datensätze in der Anzahl der Klassen.\nEs gibt also viele Aufgaben.\nMTDNN enthält eine Anzahl von Klassen, Überschriften und ausgegebenen Ebenen.\nUnd zusätzlich muss MTDNN neue Überschriften für einen neuen Datensatz mit einer neuen Aufgabe initialisieren.\nUnser Ansatz, genannt Feinabstimmungsreformulierungsaufgabe, besteht darin, dass wir, anstatt mehrere Überschriften beizubehalten, jeden Datensatz in einen Satz pro Klassifikationssproblem umformulieren, was zwei Klassenaufgaben entspricht.\nSchauen wir uns ein Beispiel an.\nHier ist unser eingegebener Datensatz, der aus Entitäten, Funktionen, Text und Klassen besteht.\nWir formulieren die Aufgabe von einer Klassifizierung des Textes in niedrig oder hoch zu einer Klassifizierung des Textes, des Abstracts und der Klasse in wahr oder falsch um.\nMit anderen Worten: Wir haben das Sprachmodell so trainiert, dass es ein Abstract und die Klasse als Abstract und die Klasse klassifiziert, wenn das Abstract zur Klasse gehört oder nicht.\nSo bleibt die Vektor-Markierung in diesem Fall bestehen, die immer aus zwei Klassen besteht.\nDas ist der Algorithmus für unseren neu formulierten Finabstimmungsansatz.\nSchauen wir uns also den gesamten Rahmen an.\nDer Datensatz wird in FeSTE eingespeist.\nDann führt FeSTE die Entity-Linking-Phase aus.\nEs extrahiert den Text aus der Wissensbasis, die in diesem Beispiel das Abstract der Wikipedia-Seite ist.\nDann formulierte es die Aufgabe in eine paarweise Satzklassifikationsaufgabe um.\nDas Sprachmodell wird auf die neue Aufgabe und die Ausgabewahrscheinlichkeit für jede Klasse angewandt.\nNun ist das Sprachmodell bereits über den „n minus 1“-Datensatz mithilfe einer vorläufigen Multitask-Feinabstimmung feinabgestimmt.\nDann verwenden wir den Ausgabe-Vektor des Sprachmodells als eine neu generierte Funktion in der Anzahl der Klassen.\nZur Evaluation unseres Rahmens verwenden wir 17 tabellarische Klassifikationsdatensätze, die sich in Größe, Funktionen, Ausgewogenheit, Domäne und anfänglicher Leistung unterscheiden.\nAls Wissensbasis verwenden wir Wikipedia.\nWir konzipieren unser Experiment als Leave-One-Out-Evaluation, bei dem wir FeSTe mit 16 Datensätzen trainieren und auf den 17. Datensatz anwenden.\nAußerdem teilen wir jeden Datensatz in vier Brüche auf und wenden eine vierfache Kreuzvalidierung an.\nDann generieren wir die neuen Funktionen und bewerten sie mit fünf Evaluationssklassifikatoren.\nWir verwenden in unseren Experimenten die Basisarchitektur BERT.\nHier sind die Ergebnisse unserer Experimente.\nSie können sehen, dass wir unseren Rahmen mit der Feinabstimmung des Zieldatensatzes, mit der Feinabstimmung der Zielaufgabe und einer vorläufigen MTDNN-Feinabstimmung vergleichen.\nUnsere neu formulierte Feinabstimmung erreicht das beste Ergebnis und die beste Leistung.\nMTDNN erreichte zwei Prozent Verbesserung gegenüber der Feinabstimmung des Zieldatensatzes.\nMit unserem Ansatz haben wir eine Verbesserung von sechs Prozent erreicht.\nWenn wir uns den kleinen Datensatz anschauen, sehen wir, dass die Leistung von MTDNN abnimmt und die Verbesserung der vorläufigen Multitask-Feinabstimmungsphase auf 1,5 Prozent sinkt.\nAber unsere Leistung stieg auf elf Prozent im Vergleich zur Feinabstimmung des Zieldatensatzes allein.\nFür die Summierung ermöglicht FeSTE in unseren Experimenten eine Few-Shot-Anreicherung aus 35 Proben.\nEs verwendet eine Architektur für alle Aufgaben und Datensätze.\nUnd es behält den Hauptteil des Modells.\nAber es fügt eine Reformulierungsphase hinzu.\nEs erweitert den Trainingssatz und benötigt einen Zielwert mit semantischer Bedeutung, damit wir ihn in das Sprachmodell einspeisen und für das Satzpaar-Klassifikationsproblem verwenden können.\nVielen Dank!", "src_lang": "en", "tgt_lang": "de", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 417, "src_audio": "/acl6060/audio/en/417.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.\nI'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.\nFirst, I'd like to talk about our motivation for reasoning.\nSo here we show an examples where multi-step reasoning is helpful.\nSo this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.\nSo on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.\nBut if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.\nSo it is good to have interpretable multi-step reasoning as output.\nAnd we also think math word problem is a straightforward application to evaluate such reasoning abilities.\nSo, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.\nSo in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.\nSo, certain assumptions ah also apply as in previous work.\nWe assume the precision of quantities are known.\nAnd we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.\nFurthermore, complicated operators can be actually decomposed into these basic operators.\nSo, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.\nSo, traditional sequence to sequence model convert the expression to a specific sequence for generation.\nAnd it is pretty easy to implement and it can generalize to many different complicated problem.\nBut the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.\nBut actually this direction is still quite popular because of um the transformer model.\nSo, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.\nSo here we keep generating the operators until we reach the leaves, which are the quantities.\nSo here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.\nAnd the second thing is that it also contains some repetitive computations.\nSo here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.\nSo, in our proposed approach we want to solve those problems in a step by step and interpretable manners.\nSo for example, here in the second step, ah we can obtain these divisors which is twenty seven.\nAnd we can also refer back to the original questions to find the relevant contents.\nAnd in these steps we obtain the divisors.\nSo, ah and then at this third step we actually get the quotient.\nAlright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.\nSo, here we actually generate the whole expression directly rather than generating a single operators or quantities.\nSo this makes the process more accurate.\nSo, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.\nSo, the expression is represented by e i j o p.\nWhere we perform operator from q_i to q_j, and such expression is actually directed.\nSo, we also have subtraction with words here to represent the opposite direction.\nThis is quite similar to relation extraction.\nSo in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.\nWe add it to the next state to become a new quantity.\nSo, these slides actually visualize the evolution of the state where we keep adding expression to the current state.\nSo in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.\nSo, once we get the quantity representations, we can start to do inference.\nHere we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.\nFirst we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.\nAnd then finally we obtain the expression representation q_1 divided by q_2.\nBut in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.\nSo, here all the possible expression is equals to three times the number of operators.\nSo the nice thing here is that we can easily add constraints to control this search this search space.\nFor example, if this expression is not allowed, we can simply remove this expression in our search space.\nSo in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.\nSo this quantity come from the previous calculated expression.\nSo finally we can obtain this final expression q_3 times q_4.\nAnd we can also see the number of all the possible ah expression is different from the previous step.\nSo, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.\nSo the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.\nAnd here we also use this tau to represent when we should terminate this generation process.\nAnd here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.\nAnd it also allows us to impose certain constraints from prior from prior knowledge.\nSo we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.\nAnd here we briefly show the results compared with the previous best approaches.\nSo our best performing variant is Roberta-DeductiveReasoner.\nAnd in fact we do not use beam search, in contrast all previous approaches are using beam search.\nAll right. So, the best approaches are often tree based model.\nSo, overall our reasoner is able to significantl significantly outperform this tree based model.\nBut we can see the absolute numbers on MathQA or SVAMP are not really high.\nSo we further investigate the results on SVAMP.\nAnd this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.\nSo, in our prediction we find some of the intermediate values are actually negatives.\nFor example, um, in these questions we are asking how many apples does Jake have?\nBut we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.\nSo, our model makes some prediction like this which is producing negative values.\nAnd we observe these two expressions actually have similar scores.\nSo, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.\nSo um we further find such constraint actually improves quite a lot for some models.\nFor example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.\nSo better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.\nAnd we also try to analyze the difficulty behind these behind all these datasets.\nWe assume the number of unused quantities can be regarded as irrelevant information here.\nSo ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.\nAnd here we also show the overall performance.\nFor those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.\nBut with those samples that with unused quantity is actually way worse than the, worse than the overall performance.\nFor MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.\nSo, finally we want to show the interpretability through a question perturbation example.\nSo here our model actually makes a wrong prediction at the first step.\nSo, we can actually correlate this expression with the sentence here. Alright.\nSo, we think this sentence might be misleading the model to an incorrect predictions.\nSo here planting another thirty five makes the model makes the model think it should be an addition operator.\nSo we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.\nSo, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.\nSo, this study shows how the interpretable predictions help us understand the model behavior.\nSo to conclude our work, so first our model is actually pretty efficient.\nAnd we are able to provide interpretable solving procedure.\nAnd we can easily incorporate some prior knowledge as constraint which can help improve the performance.\nAnd the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.\nWe also have certain limitations.\nAh, if we have a large number of operators or constants, the memory consumption could be pretty high.\nAnd the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.\nSo this is the end of the talk, and questions are welcomed. Thank you.", "tgt_ref": "Hallo zusammen. Heute werde ich unsere Forschungsarbeit Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction vorstellen.\nIch bin Allan vom ByteDance AI Lab, und dies ist eine gemeinsame Arbeit mit Jierui Li von der University of Texas in Austin und Wei Lu von SUTD.\nZunächst möchte ich über unsere Motivation für das Argumentieren sprechen.\nDeshalb zeigen wir hier ein Beispiel, bei dem mehrstufiges Argumentieren hilfreich ist.\nDiese Abbildung stammt aus dem PaLM-Paper, in dem sie Prompting durchführen, um das Netzwerkproblem in einem Few-Shot-Lernszenario zu lösen.\nAuf der linken Seite können wir also sehen, dass wenn einige Beispiele nur aus Frage und Antworten bestehen, dann bekommen wir nicht die richtigen Antworten.\nWenn wir jedoch eine weitere Argumentationsbeschreibung hinzufügen, ist das Modell in der Lage, die Argumentationsbeschreibung vorherzusagen und hier auch eine korrekte Vorhersage zu treffen.\nEs ist also gut, eine interpretierbare mehrstufige Argumentation als Ergebnis zu haben.\nWir denken auch, dass mathematische Wortprobleme eine einfache Anwendung sind, um solche Argumentationsfähigkeiten zu bewerten.\nIn unserem Problem-Setup müssen wir also angesichts der Fragen diese Frage lösen und die numerischen Antworten erhalten.\nIn unseren Datensätzen finden wir auch den mathematischen Ausdruck, der uns zu dieser bestimmten Antwort führt.\nEs gelten also auch hier bestimmte Annahmen wie in früheren Arbeiten.\nWir gehen davon aus, dass die Genauigkeit der Mengen bekannt ist.\nUnd wir betrachten nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponentialrechnung.\nDarüber hinaus können komplizierte Operatoren in diese Grundoperatoren zerlegt werden.\nDie früheren Arbeiten zum Lösen von mathematischen Wortproblemen lassen sich also in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modell einteilen.\nDas traditionelle Sequenz-zu-Sequenz-Modell wandelt also den Ausdruck in eine spezifische Sequenz für die Generierung um.\nEs ist ziemlich einfach zu implementieren und kann für viele verschiedene komplizierte Probleme verallgemeinert werden.\nDie Nachteile sind jedoch, dass die Leistung im Allgemeinen nicht besser ist als die des strukturierten Modells und dass es an Interpretierbarkeit für Vorhersagen mangelt.\nAber eigentlich ist diese Richtung immer noch recht beliebt wegen des Transformermodells.\nIn baumbasierten Modellen strukturieren wir diese Ausdrücke in Baumform und folgen einem geordneten Traversal in Baumgenerationen.\nHier fahren wir also fort, die Operatoren zu generieren, bis wir die Blätter erreichen, die die Mengen darstellen.\nDas Gute daran ist, dass wir dadurch diese binäre Baumstruktur erhalten. Sie ist eigentlich ziemlich kontraintuitiv, weil wir zuerst den Operator und dann am Ende die Mengen generieren müssen.\nZudem enthält sie auch einige sich wiederholende Berechnungen.\nWenn wir uns also diesen Ausdruck ansehen, wird 8 x 3 + 3 eigentlich zweimal generiert, aber eigentlich sollten wir die Ergebnisse wiederverwenden.\nIn unserem vorgeschlagenen Ansatz wollen wir diese Probleme schrittweise und auf interpretierbare Art und Weise lösen.\nHier im zweiten Schritt zum Beispiel können wir diese Teiler erhalten, was 27 ergibt.\nWir können auch auf die ursprünglichen Fragen zurückgreifen, um die relevanten Inhalte zu finden.\nIn diesen Schritten erhalten wir die Teiler.\nUnd in diesem dritten Schritt erhalten wir dann den Quotienten.\nOkay. Nach diesen drei Schritten können wir die Ergebnisse des zweiten Schritts wiederverwenden, dann die Ergebnisse des vierten Schritts erhalten und schließlich die Dividenden erhalten.\nHier wird also der gesamte Ausdruck direkt generiert und nicht nur ein einzelner Operator oder eine Menge.\nDadurch wird der Prozess genauer.\nIn unserem deduktiven System beginnen wir also zunächst mit einer Reihe von Mengen, die in den Fragen vorgestellt werden, und schließen auch mit einer Konstante als Anfangszustand.\nDer Ausdruck wird also durch e i j o p dargestellt.\nWir führen den Operator von q_i nach q_j aus, und dieser Ausdruck ist tatsächlich geregelt.\nWir haben hier also auch eine Subtraktion mit Wörtern, um die umgekehrte Richtung darzustellen.\nDies ist der Beziehungsextraktion recht ähnlich.\nIn einem formalen deduktiven System wenden wir also in einem Zeitschritt t den Operator zwischen dem Paar q_i und q_j an und erhalten dann diesen neuen Ausdruck.\nWir fügen ihn dem nächsten Zustand hinzu, um eine neue Menge zu erhalten.\nDiese Folien veranschaulichen also die Entwicklung des Zustands, wobei wir dem aktuellen Zustand immer neue Ausdrücke hinzufügen.\nIn unseren Modellimplementierungen verwenden wir also zunächst ein vortrainiertes Sprachmodell, bei dem es sich um BERTs oder Robertas handeln kann, und dann kodieren wir den Satz und erhalten diese Mengendarstellungen.\nSobald wir also die Mengendarstellungen haben, können wir mit der Inferenz beginnen.\nHier zeigen wir ein Beispiel für q_1, um die Darstellung für q_2 geteilt durch q_2 und dann mal q_3 zu erhalten.\nZunächst erhalten wir die Paardarstellung, die im Grunde nur die Verkettung zwischen q_1 und q_2 ist, und dann wenden wir ein Feedforward-Netzwerk an, das durch den Operator parametrisiert ist.\nUnd dann erhalten wir schließlich die Ausdrucksdarstellung q_1 geteilt durch q_2.\nAber in der Praxis könnten wir in der Inferenzphase auch den falschen Ausdruck erhalten.\nHier sind also alle möglichen Ausdrücke gleich dem Dreifachen der Anzahl der Operatoren.\nDas Schöne daran ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu kontrollieren.\nWenn dieser Ausdruck zum Beispiel nicht erlaubt ist, können wir ihn einfach aus unserem Suchraum entfernen.\nIm zweiten Schritt machen wir das Gleiche, aber der einzige Unterschied ist, dass wir nur eine Menge mehr haben.\nDiese Menge stammt also aus dem vorherigen berechneten Ausdruck.\nSo können wir schließlich diesen endgültigen Ausdruck q_3 mal q_4 erhalten.\nWir können auch sehen, dass die Anzahl aller möglichen Ausdrücke anders ist als beim vorherigen Schritt.\nEin solcher Unterschied erschwert die Anwendung von einer Beam Search, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgewogen ist.\nDas Trainingsverfahren ist also ähnlich dem Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust bei jedem Zeitschritt optimieren.\nAuch hier verwenden wir dieses Tau, um darzustellen, wann wir diesen Generierungsprozess beenden sollten.\nHier unterscheidet sich der Raum von Sequenz zu Sequenz, weil der Raum bei jedem Zeitschritt anders ist, während im traditionellen Sequenz-zu-Sequenz-Modell dies die Anzahl des Vokabulars ist.\nEs erlaubt uns auch, bestimmte Beschränkungen aus dem Vorwissen aufzuerlegen.\nDaher führen wir Experimente mit den häufig verwendeten Datensätzen der mathematischen Wortprobleme durch, MAWPS, Math23K, MathQA und SVAMP.\nHier zeigen wir kurz die Ergebnisse im Vergleich zu den bisherigen besten Ansätzen.\nUnsere leistungsstärkste Variante ist also Roberta-DeductiveReasoner.\nTatsächlich verwenden wir keine Beam Search, im Gegensatz zu allen früheren Ansätzen, die Beam Search verwendet haben.\nOkay. Daher sind die besten Ansätze oft baumbasierte Modelle.\nInsgesamt ist unser Reasoner also in der Lage, dieses baumbasierte Modell signifikant zu übertreffen.\nAber wir können sehen, dass die absoluten Zahlen bei MathQA oder SVAMP nicht wirklich hoch sind.\nDeshalb untersuchen wir die Ergebnisse bei SVAMP weiter.\nDieser Datensatz ist eine Herausforderung, weil der Autor versucht hat, manuell etwas hinzuzufügen, um das NLP-Modell zu verwirren, wie z. B. das Hinzufügen irrelevanter Informationen und zusätzlicher Mengen.\nIn unserer Vorhersage stellen wir also fest, dass einige der Zwischenwerte eigentlich negativ sind.\nZum Beispiel geht es in diesen Fragen darum, wie viele Äpfel Jake hat.\nAber wir haben einige zusätzliche Informationen wie 17 Bilder weniger, und Steven hat acht Bilder, was völlig irrelevant ist.\nUnser Modell macht also eine Vorhersage wie diese, die negative Werte ergibt.\nWir stellen fest, dass diese beiden Ausdrücke tatsächlich ähnliche Werte haben.\nWir können also diesen Suchraum eingrenzen, indem wir die negativen Ergebnisse entfernen, damit die Antwort richtig ist.\nWir stellen also fest, dass eine solche Bedingung für einige Modelle tatsächlich eine erhebliche Verbesserung darstellt.\nBei BERT haben wir uns zum Beispiel um sieben Punkte verbessert und beim Roberta-Basismodell haben wir uns sogar um zwei Punkte verbessert.\nEin besseres Sprachmodell hat also bessere Sprachverständnisfähigkeiten, sodass die Zahl hier höher bei Roberta und niedriger bei BERT ist.\nWir versuchen auch, die Schwierigkeiten hinter all diesen Datensätzen zu analysieren.\nWir gehen davon aus, dass die Anzahl der ungenutzten Mengen hier als irrelevante Informationen betrachtet werden kann.\nHier können wir also sehen, dass wir den Prozentsatz der Proben mit ungenutzten Mengen haben, und der SVAMP-Datensatz hat den größten Anteil.\nHier zeigen wir auch die Gesamtleistung.\nBei den Proben ohne ungenutzte Mengen ist die Leistung sogar höher als die Gesamtleistung.\nAber bei diesen Proben mit ungenutzten Mengen ist sie eigentlich viel schlechter als die Gesamtleistung.\nBei MAWPS haben wir nicht wirklich viele Testfälle, also ignoriere ich diesen Teil einfach.\nSchließlich wollen wir die Interpretierbarkeit anhand eines Frage-Störungsbeispiels zeigen.\nHier macht unser Modell also tatsächlich eine falsche Vorhersage im ersten Schritt.\nWir können diesen Ausdruck also tatsächlich mit dem Satz hier korrelieren. Okay.\nWir denken also, dass dieser Satz das Modell zu falschen Vorhersagen verleiten könnte.\nHier führt eine weitere 35 dazu, dass das Modell denkt, dass es ein Additionsoperator sein sollte.\nWir versuchen also den Satz so umzuformulieren, dass die Anzahl der Birnbäume um 35 weniger als die der Apfelbäume ist.\nWir sorgen also dafür, dass eine genauere Semantik vermittelt wird, sodass das Modell in der Lage ist, die Vorhersage korrekt zu treffen.\nDiese Studie zeigt also, wie uns die interpretierbaren Vorhersagen helfen, das Modellverhalten zu verstehen.\nUm unsere Arbeit abzuschließen: Zunächst ist unser Modell tatsächlich ziemlich effizient.\nWir sind in der Lage, ein interpretierbares Lösungsverfahren anzubieten.\nWir können einfach einiges Vorwissen als Bedingung einbeziehen, was zur Verbesserung der Leistung beitragen kann.\nSchließlich gilt der zugrundeliegende Mechanismus nicht nur für das Lösen von Netzwerkproblemen, sondern auch für andere Aufgaben, die mehrstufiges Argumentieren erfordern.\nAuch sind uns gewisse Grenzen gesetzt.\nWenn wir eine große Anzahl von Operatoren oder Konstanten haben, könnte der Speicherverbrauch ziemlich hoch sein.\nWie bereits erwähnt ist es zudem aufgrund der unausgewogenen Wahrscheinlichkeitsverteilung zwischen den verschiedenen Zeitschritten eine ziemliche Herausforderung, die Beam Search-Strategie anzuwenden.\nWir sind am Ende des Vortrags angekommen und freuen uns auf etwaige Fragen. Vielen Dank!", "src_lang": "en", "tgt_lang": "de", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 418, "src_audio": "/acl6060/audio/en/418.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.\nI will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.\nLegal issues are an integral part of many people's lives.\nBut the majority of citizens have little to know knowledge about their rights and fundamental legal processes.\nAs a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.\nAll work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.\nSuch a system could provide a free professional legal help service for unskilled humans.\nBefore diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.\nGiven a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?\nA model is required to retrieve all relevant statutory articles from a large body of legislation.\nThis information retrieval task comes with its own set of challenges.\nFirst, it deals with two types of language.\nCommon natural language for the questions and complex legal language for the statutes.\nThis difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.\nBesides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.\nInstead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.\nLastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.\nHere, there are long documents that may be up to six thousand words.\nThe recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.\nBut statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.\nIn this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.\nOur Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.\nThese questions cover a wide range of topics from family, housing, money, to work and social security.\nEach of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.\nLet's now talk about how we collected this dataset.\nFirst, we started by compiling a large corpus of legal articles.\nWe considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.\nThen we gathered legal questions with references to relevant statutes.\nTo do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.\nWe were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.\nWe collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.\nLastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.\nThe remaining references were matched and converted to the corresponding article ids from our corpus.\nWe eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.\nIn addition, each question comes with the main category and a concatenation of subcategories.\nAnd each articles comes with a concatenation of the subsequence heading in the structure of the law.\nThis extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.\nLet's look at some characteristic of our dataset.\nThe questions are between five and forty four words long with a median of fourteen words.\nThe articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.\nThe lengthiest one being up to five thousand seven hundred and ninety words.\nAs previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.\nWhile the remaining fifteen percent concern either social security, foreigners or work.\nThe article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.\nHere's the total number of articles collected from each of these Belgian codes.\nOut of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.\nAnd around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.\nMeanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.\nWhich can be explained by the fact that those codes focused less on individuals and their concerns.\nOverall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.\nUsing all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.\nGiven a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.\nWe experiment with the standard TF-IDF and BM25 ranking functions.\nThe main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.\nTo overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.\nWe use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.\nThese embeddings typically result from a pooling operation on the output of a word embedding model.\nFirst, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.\nWe experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.\nAdditionally, we train our own CamemBERT based model ah bi-encoders on our dataset.\nNote that for training, we experiment with the two flavors of the bi-encoder architecture.\nSiamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.\nWe experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.\nHere are the result of our baseline on the test sets.\nWith the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.\nOverall, the finetuned bi-encoder significantly outperforms all the other baselines.\nThe two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.\nAlthough BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.\nRegarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.\nFurthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.\nAlthough promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.\nLet's conclude by discussing two limitations of our dataset.\nFirst, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.\nDuring the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.\nThis information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.\nSecond, we should note that not all legal questions can be answered with statutes alone.\nFor instance, the question, can I evict my tenants if they make too much noise?\nMight not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.\nInstead, the landlord should probably rely more on case law and find precedents similar to their current situation.\nFor example, the tenants makes two parties a week until two AM.\nHence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.\nWe hope that our work sparks interest in developing practical and reliable statutory article retrieval models.\nThat can help improve access to justice for all.\nYou can check out our paper, dataset and code at the following links. Thank you.", "tgt_ref": "Hallo, mein Name ist Antoine und ich bin von der Universität Maastricht.\nIch werde meine gemeinsame Arbeit mit Jerry vorstellen, bei der es um einen neuen Datensatz für das Retrieval von Gesetzesartikeln geht.\nRechtsfragen sind ein wesentlicher Bestandteil des Lebens vieler Menschen.\nAber die Mehrheit der Bürger verfügt nur über wenig Wissen über ihre Rechte und grundlegende rechtliche Verfahren.\nDies hat zur Folge, dass viele schutzbedürftige Bürger, die sich die kostspielige Hilfe eines Rechtsexperten nicht leisten können, schutzlos bleiben oder schlimmstenfalls ausgebeutet werden.\nUnsere Arbeit zielt darauf ab, die Kluft zwischen den Menschen und dem Gesetz zu überbrücken, indem ein effektives Retrieval-System für Gesetzesartikel entwickelt wird.\nEin solches System könnte einen kostenlosen professionellen Rechtshilfedienst für unerfahrene Menschen anbieten.\nBevor wir uns dem Hauptbeitrag dieser Arbeit widmen, wollen wir zunächst das Problem des Retrievals von Gesetzesartikeln beschreiben.\nEine einfache Frage zu einer Rechtsangelegenheit wäre: „Was riskiere ich, wenn ich das Berufsgeheimnis verletze?“\nHier wird ein Modell benötigt, um alle relevanten Gesetzesartikel aus einem großen Bestand an Rechtsvorschriften abzurufen.\nDiese Informationsbeschaffungsaufgabe bringt eine Reihe von Herausforderungen mit sich.\nErstens: Es geht um zwei Arten von Sprache.\nDie gemeinsame natürliche Sprache für die Fragen und die komplexe juristische Sprache für die Gesetze.\nDieser Unterschied bei den Sprachverteilungen macht es für ein System schwieriger, relevante Kandidaten zu finden, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine rechtliche Frage übersetzen kann, die der Terminologie der Gesetze entspricht.\nAußerdem ist das Recht keine Ansammlung unabhängiger Artikel, die für sich allein als vollständige Informationsquelle betrachtet werden können, anders als beispielsweise Nachrichten oder Rezepte.\nVielmehr handelt es sich um eine strukturierte Sammlung von Rechtsvorschriften, die nur im Gesamtkontext, d. h. zusammen mit den ergänzenden Informationen aus den Nachbarartikeln, den Bereichen und Teilbereichen, denen sie angehören, und ihrer Stellung im Aufbau des Gesetzes, eine vollständige Bedeutung haben.\nSchließlich handelt es sich bei den Gesetzesartikeln nicht um kleine Absätze, die in den meisten Retrieval-Arbeiten die typische Retrieval-Einheit darstellen.\nHier gibt es lange Dokumente, die bis zu 6.000 Wörter umfassen können.\nDie jüngsten Fortschritte im Bereich NLP haben großes Interesse an vielen juristischen Aufgaben geweckt, z. B. an der Vorhersage juristischer Urteile oder der automatisierten Prüfung von Verträgen.\nDoch das Retrieval von Gesetzesartikeln ist aufgrund des Mangels an großen und qualitativ hochwertig markierten Datensätzen weitgehend unberührt geblieben.\nIn dieser Arbeit stellen wir einen neuen Datensatz in der französischen Muttersprache vor, mit dem wir Retrievalmodelle untersuchen und analysieren, ob sie bei der Aufgabe der Suche nach Gesetzesartikeln an die Effizienz und Zuverlässigkeit eines Rechtsexperten rankommen können.\nUnser belgischer Datensatz für das Retrieval von Gesetzesartikeln, BSARD, besteht aus mehr als 1.100 juristischen Fragen, die von belgischen Bürgern gestellt wurden.\nDiese Fragen decken ein breites Spektrum an Themen ab, von Familie, Wohnen, Geld bis hin zu Arbeit und sozialer Sicherheit.\nJede von ihnen wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als 22.600 Rechtsartikeln aus belgischen Gesetzbüchern markiert.\nLassen Sie uns nun darüber sprechen, wie wir diesen Datensatz gesammelt haben.\nZunächst haben wir einen großen Korpus von Rechtsartikeln zusammengestellt.\nWir haben 32 öffentlich zugängliche belgische Gesetzbücher berücksichtigt und alle Artikel sowie die entsprechenden Abschnittsüberschriften extrahiert.\nDann haben wir rechtliche Fragen mit Verweisen auf einschlägige Gesetze zusammengestellt.\nZu diesem Zweck arbeiten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr etwa 4.000 E-Mails von belgischen Bürgern erhält, die um Rat in einer persönlichen Rechtsangelegenheit bitten.\nWir hatten das Glück, Zugang zu ihren Websites zu erhalten, auf denen ihr Team aus erfahrenen Juristen die häufigsten Rechtsangelegenheiten der Belgier behandelt.\nWir haben Tausende von Fragen gesammelt und mit Kategorien, Unterkategorien und Verweisen auf einschlägige Gesetze annotiert.\nSchließlich haben wir die rechtlichen Verweise überprüft und die Fragen herausgefiltert, deren Verweise nicht in einem der von uns berücksichtigten Gesetzbücher enthalten waren.\nDie übrigen Referenzen wurden zusammengeführt und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt.\nAm Ende hatten wir 1.108 Fragen, jede sorgfältig markiert mit den IDs der relevanten Artikel aus unserem großen Korpus von 22.630 Gesetzesartikeln.\nDarüber hinaus enthält jede Frage eine Hauptkategorie und eine Verkettung von Unterkategorien.\nJeder Artikel hat eine Verkettung mit der anschließenden Überschrift im Aufbau des Gesetzes.\nDiese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für künftige Forschungen zum juristischen Informationen-Retrieval oder zur juristischen Textklassifikation von Interesse sein.\nSchauen wir uns einige Merkmale unseres Datensatzes an.\nDie Fragen sind zwischen fünf und 24 Wörter lang, mit einem Mittelwert von 14 Wörtern.\nDie Artikel sind mit einer durchschnittlichen Länge von 77 Wörtern sehr viel länger, wobei 142 von ihnen mehr als 1.000 Wörter umfassen.\nDer längste von ihnen hat bis zu 5.790 Wörter.\nWie bereits erwähnt decken die Fragen ein breites Spektrum an Themen ab, wobei etwa 85 Prozent der Fragen entweder die Familie, das Wohnen, Geld oder die Gerechtigkeit betreffen.\nDie restlichen 15 Prozent betreffen entweder soziale Sicherheit, Ausländer oder Arbeit.\nDie Artikel sind auch sehr vielfältig, da sie aus 32 verschiedenen belgischen Gesetzbüchern stammen, die eine große Reihe von Rechtsthemen abdecken.\nHier ist die Gesamtzahl der Artikel, die aus diesen belgischen Gesetzbüchern gesammelt wurden.\nVon den 22.633 Artikeln werden nur 1.612 als relevant für mindestens eine Frage im Datensatz genannt.\nEtwa 80 Prozent dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, dem Gerichtsgesetzbuch, dem Gesetzbuch für strafrechtliche Ermittlungen oder dem Strafgesetzbuch.\nBei 18 von 32 Gesetzbüchern werden weniger als fünf Artikel als relevant für mindestens eine Frage bestimmt.\nDies lässt sich dadurch erklären, dass diese Gesetzbücher weniger auf den Einzelnen und seine Anliegen ausgerichtet waren.\nInsgesamt liegt der Durchschnitt der Anzahl der Zitate bei zwei, und weniger als 25 Prozent der zitierten Artikel werden mehr als fünfmal zitiert.\nUnter Verwendung aller Datensätze haben wir verschiedene Retrieval-Ansätze, einschließlich lexikalischer und dichter Architektur, einem Benchmarking unterzogen.\nBei einer Abfrage und einem Artikel weist ein lexikalisches Modell dem Abfrage-Artikel-Paar eine Punktzahl zu, indem es die Summe der Gewichtungen von jedem der Begriffe in diesem Artikel über die Abfrage-Termini berechnet.\nWir experimentieren mit den Standard-Ranking-Funktionen TF-IDF und BM25.\nDas Hauptproblem bei diesen Ansätzen ist, dass sie nur Artikel finden können, die Schlüsselwörter enthalten, die in der Anfrage vorkommen.\nUm diese Einschränkung zu überwinden, experimentieren wir mit einer neuronal-basierten Architektur, die semantische Beziehungen zwischen Abfragen und Artikeln erfassen kann.\nWir verwenden ein Bi-Encoder-Modell, das Abfragen und Artikel in dichten Vektor-Darstellungen abbildet und einen Relevanzwert zwischen einem Abfrage-Artikel-Paar anhand der Ähnlichkeit ihrer Einbettungen berechnet.\nDiese Einbettungen resultieren in der Regel aus einer Pooling-Operation auf der Ausgabe eines Worteinbettungsmodells.\nZunächst untersuchen wir die Effektivität von Siamesischen Bi-Encodern in einem Zero-Shot-Evaluation-Setup, d. h. dass vortrainierte Worteinbettungsmodelle ohne zusätzliche Feinabstimmung sofort angewendet werden.\nWir experimentieren mit kontextunabhängigen Textencodern, wie word2vec und fastText, und kontextabhängigen Einbettungsmodellen, wie Roberta und insbesondere CamemBERT, einem französischen Roberta-Modell.\nDarüber hinaus trainieren wir unser eigenes, auf CamemBERT basierendes Modell, Bi-Encoder, auf unserem Datensatz.\nBeachten Sie, dass wir beim Training mit den beiden Varianten der Bi-Encoder-Architektur experimentieren.\nSiamese, das ein einzigartiges Worteinbettungsmodell verwendet, das die Abfrage und den Artikel zusammen in einem geteilten dichten Vektorraum abbildet, und Two-Tower, das zwei unabhängige Worteinbettungsmodelle verwendet, die die Abfrage und den Artikel getrennt in verschiedenen Einbettungsräumen kodieren.\nWir experimentieren mit Mittelwert, Maximalwert und CLS-Pooling sowie mit Produkt und Kosinus für die Berechnung von Ähnlichkeiten.\nHier sind die Ergebnisse unserer Baseline auf den Testsätzen.\nDie lexikalischen Methoden werden oben, die Siamesischen Bi-Encoder in einem Zero-Shot-Setup in der Mitte und die fein abgestimmten Bi-Encoder werden unten ausgewertet.\nInsgesamt übertrifft der fein abgestimmte Bi-Encoder alle anderen Baselines deutlich.\nDas Two-Tower-Modell verbessert sich gegenüber seinen Siamesischen Varianten beim Recall um 100, schneidet aber bei den anderen Metriken ähnlich gut ab.\nObwohl BM25 deutlich schlechter abschnitt als der trainierte Bi-Encoder, zeigte seine Leistung, dass er immer noch eine gute Basis für ein domänenspezifisches Retrieval ist.\nBezüglich der Zero-Shot-Evaluation des Siamesischen Bi-Encoders stellen wir fest, dass die direkte Verwendung der Einbettungen eines vortrainierten CamemBERT-Modells ohne Optimierung für die Informationen-Retrieval-Aufgabe schlechte Ergebnisse liefert, was mit früheren Erkenntnissen übereinstimmt.\nDarüber hinaus stellen wir fest, dass der word2vec-basierte Bi-Encoder die fastText- und BERT-basierten Modelle signifikant übertrifft, was darauf hindeutet, dass vortrainierte Wort-Ebenen-Einbettungen vielleicht besser für die Aufgabe geeignet sind als Zeichen- oder Unterwort-Ebenen-Einbettungen, wenn sie direkt verwendet werden.\nObwohl diese Ergebnisse vielversprechend sind, lassen sie im Vergleich zu einem erfahrenen Rechtsexperten auf ein großes Verbesserungspotenzial schließen, der früher oder später alle relevanten Artikel zu jeder Frage abrufen kann und somit perfekte Ergebnisse erzielt.\nAbschließend möchte ich auf zwei Einschränkungen unseres Datensatzes eingehen.\nErstens ist der Artikelkorpus auf die Artikel der 32 herangezogenen belgischen Gesetzbücher beschränkt, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Dekreten, Richtlinien und Verordnungen fehlen.\nBei der Erstellung des Datensatzes werden alle Verweise auf diese nicht erfassten Artikel ignoriert, was dazu führt, dass sich einige Fragen am Ende nur auf einen Bruchteil der ursprünglichen Anzahl der relevanten Artikel beziehen.\nDiese Information impliziert also, dass die Antwort, die in den übrigen relevanten Artikeln enthalten ist, unvollständig sein könnte, obwohl sie immer noch völlig angemessen ist.\nZweitens sollten wir beachten, dass nicht alle rechtlichen Fragen allein mit Gesetzen beantwortet werden können.\nZum Beispiel die Frage: „Kann ich meinen Mietern kündigen, wenn sie zu viel Lärm machen?“\nMöglicherweise gibt es im Gesetz keine detaillierte Antwort, die eine bestimmte Lärmschwelle festlegt, ab der eine Räumung zulässig ist.\nStattdessen sollte sich der Vermieter wahrscheinlich eher auf das Fallrecht stützen und Präzedenzfälle finden, die seiner aktuellen Situation ähnlich sind.\nZum Beispiel veranstalten die Mieter zwei Partys pro Woche bis zwei Uhr morgens.\nEinige Fragen eignen sich daher besser als andere für die Aufgabe des Retrievals von Gesetzesartikeln. Die Domäne der weniger geeigneten Fragen muss noch bestimmt werden.\nWir hoffen, dass unsere Arbeit das Interesse an der Entwicklung praktischer und zuverlässiger Modelle für das Retrieval von Gesetzesartikeln weckt.\nSie kann dazu beitragen, den Zugang zur Justiz für alle zu verbessern.\nSie können unser Paper, den Datensatz und die Gesetzbücher unter den folgenden Links einsehen. Vielen Dank!", "src_lang": "en", "tgt_lang": "de", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 419, "src_audio": "/acl6060/audio/en/419.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.\nWhy did we do the trouble in setting up this benchmark?\nWell, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.\nEach one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.\nSo we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.\nBut do we know what the models have actually learned?\nWhat is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?\nAnd the low score for this one?\nDo vision and language models focus on the right thing?\nOr do they focus on biases as shown by previous work?\nTo shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.\nWe target existence, plurality, counting, spatial relations, actions and entity coreference.\nBut how do we test whether the vision and language models have captured this phenomena?\nBy foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.\nFoiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.\nAnd we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.\nFor example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.\nCounting and coreference also are pieces that have more than one instrument.\nAnd we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.\nThis is not easy to do because a foiled caption may be less likely than the original caption.\nFor example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.\nTherefore, to obtain valid foils, we must take action.\nFirst, we make use of strong language models to propose foils.\nSecond, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.\nTo test this automatically, we apply natural language inference with the following rationale.\nWe consider an image to be the premise and its caption its entailed hypothesis.\nIn addition, we consider the caption to be the premise, and the foil is its hypothesis.\nIf an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.\nIf an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.\nBut this procedure is not perfect, it is just an indicator for valid foils.\nTherefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.\nSo, after filtering and human evaluation, we have as many test instances as described in this table.\nNote that VALSE does not deliver any training data but only test data.\nSince it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.\nFinetuning would only enable models to exploit artifacts or statistical biases in the data.\nAnd we all know that these models like to cheat and take shortcuts.\nAnd as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.\nWe experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.\nTwo of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.\nPerhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.\nFor more metrics and results on them, do check out our paper.\nThe results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.\nIt's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.\nHowever, none of the remaining pieces can be reliably solved in our adversarial foiling settings.\nWe see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.\nThe relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.\nThey also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.\nFrom the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.\nAs a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.\nIf the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.\nAnd it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.\nSo to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.\nOur experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.\nWe would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.\nAnd even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.\nIf you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "tgt_ref": "Hallo, wir freuen uns, Ihnen unsere Arbeit über VALSE vorstellen zu können. Das ist ein Aufgaben-unabhängiger Benchmark, der für das Testen von Seh- und Sprachmodellen mit spezifischen sprachlichen Phänomenen konzipiert ist.\nWarum haben wir uns die Mühe gemacht, diesen Benchmark einzurichten?\nIn den letzten Jahren haben wir eine explosionsartige Zunahme von Transformer-basierten Seh- und Sprachmodellen erlebt, die mit großen Mengen an Bild-Text-Paaren vortrainiert wurden.\nJedes dieser Modelle treibt den Fortschritt bei Seh- und Sprachaufgaben voran, wie z. B. die visuelle Fragenbeantwortung, das Argumentieren mit visuellem Menschenverstand, Bild-Retrieval, Phrasen-Erdung.\nWir haben also die Nachricht erhalten, dass die Genauigkeiten bei diesen Aufgaben und spezifischen Benchmarks stetig steigen.\nAber wissen wir, was die Modelle tatsächlich gelernt haben?\nWas hat ein Seh- und Sprach-Transformer verstanden, wenn er für dieses Bild und diesen Satz eine hohe Punktzahl zuweist?\nUnd hier eine niedrige Punktzahl?\nKonzentrieren sich Seh- und Sprachmodelle auf die wesentlichen Punkte?\nOder konzentrieren sie sich auf Verzerrungen, wie in früheren Arbeiten aufgezeigt wurde?\nUm diesen Aspekt näher zu beleuchten, schlagen wir eine eher aufgabenbezogene agnostische Richtung vor und führen VALSE ein, das die Empfindlichkeit von Seh- und Sprachmodellen für spezifische sprachliche Phänomene testet, die sowohl die sprachlichen als auch die visuellen Modalitäten betreffen.\nWir zielen auf Existenz, Pluralität, Zählung, räumliche Beziehungen, Handlungen und Entity-Koreferenz ab.\nDoch wie lässt sich überprüfen, ob die Seh- und Sprachmodelle dieses Phänomen erfasst haben?\nDas Verfälschen einer Methode wurde zuvor von Ravi Shekhar und Mitarbeitern nur für Substantivphrasen in den Seh- und Sprachmodellen und beim Zählen von uns in früheren Arbeiten angewandt.\nVerfälschen bedeutet im Grunde, dass wir die Beschriftung eines Bildes nehmen und eine Verfälschung erzeugen, indem wir die Beschriftung so verändern, dass sie das Bild nicht mehr beschreibt.\nWir führen diese Veränderungen der Phrase durch, indem wir uns auf sechs spezifische Teile konzentrieren, wie Existenz, Pluralität, Zählung, räumliche Relationen, Handlungen und Entität-Koreferenz, wobei jeder Teil aus einem oder mehreren Instrumenten bestehen kann, falls wir mehr als einen interessanten Weg gefunden haben, um Verfälschungsinstanzen zu erzeugen.\nZum Beispiel haben wir im Fall des Handlungsteils zwei Instrumente, eines, bei dem das Handlungsverb durch eine andere Handlung ersetzt wird, und eines, bei dem die Aktanten ausgetauscht werden.\nZählung und Koreferenz sind auch Teile, die mehr als ein Instrument haben.\nWir erstellen diese Verfälschungen, indem wir sicherstellen, dass sie das Bild nicht beschreiben, dass sie grammatikalisch richtig sind und auch korrekte Sätze bilden.\nDas ist nicht einfach zu bewerkstelligen, denn eine verfälschte Beschriftung ist weniger wahrscheinlich als die ursprüngliche Beschriftung.\nEs ist zwar zum Beispiel nicht unmöglich, aber statistisch gesehen ist es unwahrscheinlicher, dass Pflanzen einen Menschen schneiden, als dass ein Mensch Pflanzen schneidet. Große Seh- und Sprachmodelle könnten dies erkennen.\nDaher müssen wir handeln, wenn wir gültige Verfälschungen erhalten wollen.\nErstens nutzen wir starke Sprachmodelle, um Verfälschungen vorzuschlagen.\nZweitens verwenden wir die Inferenz der natürlichen Sprache oder kurz NLI, um Verfälschungen herauszufiltern, die das Bild noch beschreiben könnten. Bei der Konstruktion von Verfälschungen müssen wir sicherstellen, dass sie das Bild nicht beschreiben.\nUm dies automatisch zu testen, wenden wir die Inferenz der natürlichen Sprache mit der folgenden Logik an.\nWir betrachten ein Bild als die Prämisse und seine Beschriftung als die daraus abgeleitete Hypothese.\nDarüber hinaus betrachten wir die Beschriftung als Prämisse und die Verfälschung als ihre Hypothese.\nWenn ein NLI-Modell vorhersagt, dass die Verfälschung der Beschriftung widerspricht oder neutral ist, ist das für uns ein Indikator für eine gültige Verfälschung.\nWenn ein NLI vorhersagt, dass die Verfälschung die Beschriftung beinhaltet, kann es keine gute Verfälschung sein, da sie durch Transitivität eine wahrheitsgemäße Beschreibung des Bildes liefert. Wir filtern diese Verfälschungen heraus.\nDieses Verfahren ist aber nicht perfekt, sondern nur ein Indikator für gültige Verfälschungen.\nDaher setzen wir als dritte Maßnahme zur Generierung gültiger Verfälschungen menschliche Annotatoren ein, um die in VALSE verwendeten Daten zu validieren.\nNach dem Filtern und der menschlichen Evaluation haben wir also so viele Testinstanzen, wie in dieser Tabelle beschrieben.\nBeachten Sie hier, dass VALSE keine Trainingsdaten, sondern nur Testdaten liefert.\nDa es sich um einen reinen Zero-Shot-Test-Benchmark handelt, ist er so konzipiert, dass er die bestehenden Fähigkeiten der Seh- und Sprachmodelle nach dem Vortraining nutzt.\nDie Feinabstimmung würde es den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen.\nWir alle wissen, dass diese Modells gerne schummeln und Abkürzungen nehmen.\nWie wir schon erwähnten, sind wir an der Bewertung interessiert, welche Fähigkeiten die Seh- und Sprachmodelle nach dem Vortraining haben.\nWir experimentieren mit fünf Seh- und Sprachmodellen auf VALSE, nämlich mit CLIP, LXMert, ViLBERT, ViLBERT zwölf in einem und VisualBERT.\nZwei unserer wichtigsten Evaluationsmetriken sind die Genauigkeit der Modelle bei der Klassifikation von Bild-Satz-Paaren in Bildbeschreibungen und Verfälschungen.\nVielleicht ist es für dieses Video relevanter, unsere tolerantere Metrik vorzustellen, die paarweise Genauigkeit. Diese misst, ob die Bild-Satz-Ausrichtung für das korrekte Bild-Text-Paar größer ist als für sein Verfälschungspaar.\nWeitere Metriken und Ergebnisse dazu finden Sie in unserem Paper.\nDie Ergebnisse mit paarweiser Genauigkeit werden hier gezeigt und stimmen mit den Ergebnissen überein, die wir von den anderen Metriken erhalten haben, nämlich dass die beste Zero-Shot-Leistung von ViLBERT zwölf zu eins erreicht wird, gefolgt von ViLBERT, LXMert, CLIP und schließlich VisualBERT.\nEs ist bemerkenswert, wie Instrumente, die sich auf die einzelnen Objekte konzentrieren, wie Existenz und Substantivphrasen, von ViLBERT zwölf zu eins fast gelöst werden. Das unterstreicht, dass Modelle in der Lage sind, benannte Objekte und ihre Existenz in Bildern zu identifizieren.\nKeines der verbleibenden Teile kann jedoch in unseren gegnerischen Verfälschungseinstellungen zuverlässig gelöst werden.\nWir sehen an den Instrumenten der Pluralität und der Zählung, dass Seh- und Sprachmodelle Schwierigkeiten haben, Verweise auf einzelne und mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen.\nDer Relationen-Teil zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Relation zwischen Objekten in einem Bild richtig zu klassifizieren.\nSie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie durch Plausibilitätsbias unterstützt werden, wie wir bei dem Handlung-Teil sehen.\nBeim Koreferenz-Teil geht hervor, dass die Verfolgung mehrerer Verweise auf dasselbe Objekt in einem Bild unter Verwendung von Pronomen auch für die Seh- und Sprachmodelle schwierig ist.\nZur Überprüfung der Korrektheit und weil es ein interessantes Experiment ist, vergleichen wir auch zwei reine Textmodelle, GPT 1 und GPT 2. So wollen wir feststellen, ob VALSE durch diese unimodalen Modelle lösbar ist. Wir berechnen die Perplexität der korrekten und der verfälschten Beschriftung, ohne Bild, um den Eintrag mit der niedrigsten Perplexität vorherzusagen.\nWenn die Perplexität bei der Verfälschung höher ist, ist das für uns ein Hinweis, dass die verfälschte Beschriftung möglicherweise unter Plausibilitätsverzerrungen oder anderen sprachlichen Verzerrungen leidet.\nEs ist interessant zu sehen, dass in einigen Fällen die GPT-Modelle mit reinem Text die Plausibilität der Welt besser erfasst haben als die Seh- und Sprachmodelle.\nZusammenfassend lässt sich sagen, dass VALSE ein Benchmark ist, der durch das Objektiv von linguistischen Konstrukten schaut, um die Gemeinschaft bei der Verbesserung von Seh- und Sprachmodellen zu unterstützen, indem er ihre visuelle Erdung hart testet.\nUnsere Experimente zeigen, dass Seh- und Sprachmodelle benannte Objekte und ihre Existenz in Bildern gut erkennen, wie der Existenzteil zeigt. Sie haben aber Schwierigkeiten damit, ihre gegenseitige Abhängigkeit und Beziehungen in visuellen Szenen zu begründen, wenn sie gezwungen sind, sprachliche Indikatoren zu berücksichtigen.\nWir möchten die Community wirklich dazu ermutigen, VALSE für die Messung des Fortschritts von Sprach- Erdung mit Seh- und Sprachmodellen zu nutzen.\nZudem könnte VALSE als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten. So kann kontrolliert werden, ob ein Datensatz dazu beiträgt, dass sich Modelle in einem der von VALSE getesteten Aspekte verbessern.\nWenn Sie Interesse haben, schauen Sie sich die VALSE-Daten auf GitHub an. Wenn Sie irgendwelche Fragen haben, zögern Sie nicht, uns zu kontaktieren.", "src_lang": "en", "tgt_lang": "de", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 420, "src_audio": "/acl6060/audio/en/420.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.\nI'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.\nI'll be explaining in this order.\nFirst, I will introduce automatic release note generation that we are working on in this research.\nA release note is a technical document that summarizes the changes distributed with each release of a software product.\nThe image shows a release note for version two point six point four of the vuejs library.\nRelease notes play an important role in open source development but they're time consuming to prepare manually.\nTherefore, it would be very useful to be able to automatically generate high quality release notes.\nI will defer to two previous researches on automatic release note generation.\nThe first is a system called ARENA released in twenty fourteen.\nIt takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.\nThe most notable feature of this system is the issue extractor in the upper right corner.\nWhich must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.\nIn other words, it cannot be used for many projects on GitHub.\nThe second is Glyph, recently announced in twenty twenty.\nIt is available on the internet and can be installed via pip.\nThis system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.\nThis image is a sample usage that returns a corrective or bug fixes label.\nGlyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.\nThe performance of the text classification model is not high.\nI present two related researches, but their problems are limited applicability and scarce data resources.\nOur paper solves these two problems and automatically generates high quality release notes.\nWith a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.\nThis proposed method can be used for all English repositories.\nFor the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.\nNext, I'll describe our dataset.\nHere is an example of data.\nThe left side is a commit message and the right side is the release notes.\nRelease notes are labeled as improvements or fixes, etc.\nWe have set up a task that takes the commit messages as input and outputs a labeled release notes.\nThis can be regarded as a summarization task.\nWe have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.\nThese were set based on previous research and other factors.\nThe release note on the bottom right is extracted from the release note on the bottom left.\nAt this time, it is necessary to detect the four labels that have been set up in advance.\nBut the labels are not always consistent with each repository.\nFor example, the improvements label includes improvements, enhancements, optimizations, and so on.\nWe prepared a vocabulary list of about thirty labels for each of these notational variations.\nThis is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.\nNext is a commit message.\nCommit messages are not tied to each release.\nAs shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.\nThis is a bit tedious and it is not enough to just get a list of releases and look at the before and after.\nWe created a heuristic matching rule to get the previous and next versions.\nDataset analysis.\nIn the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.\nAlso, the average number of release notes tokens is sixty three, which is quite high for a summarization task.\nAlso, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.\nThis is due to the large number of unique class or method names found in the repository.\nNext, I will explain the proposed method.\nThe classwise extractive then abstractive summarization model consists of two neural modules.\nA classifier using BERT or CodeBERT and a generator using BART.\nFirst, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.\nThe commit messages classified as other are discarded.\nThen CEAS applies the generator to the four labeled documents independently and generates release notes for each class.\nIn this task, the direct correspondences between commit messages and release notes are not known.\nTherefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.\nWe modeled the classwise abstractive summarization approach by two different methods.\nThe first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.\nThe output texts can be divided into classwise segments based on special class-specific endpoint symbols.\nThe second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.\nOkay, let me explain the experiments.\nFive methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.\nRegarding evaluation, in some cases, release notes are output in multiple sentences.\nSince it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.\nThe BLEU is penalized when the system outputs a short sentence.\nThis penalty results in a lower BLEU value in the experiment results described next.\nFinally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.\nA higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.\nHere are the results.\nSince the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.\nCEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.\nIn particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.\nThese results indicate that CEAS and CAS are significantly affected.\nCEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.\nHigh coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.\nCAS-Multi tended to yield higher ROUGE-L than CAS-Single.\nSuggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.\nHere are an error analysis.\nCAS methods tend to output shorter sentences than human reference sentences.\nIn the figure on the right, the reference sentence has three or four sentences, while CAS has only one.\nThe reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.\nFurthermore, CAS methods cannot generate accurate release notes without additional information.\nThe top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.\nThe example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.\nFinally, a conclusion.\nWe have built a new dataset for automatic release note generation.\nWe have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.\nOur experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.\nPlease check out our dataset on GitHub.\nThank you.", "tgt_ref": "Hallo, mein Name ist Kamezawa von der Universität Tokio.\nIch werde folgendes Paper vorstellen: RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.\nIch werde es in dieser Reihenfolge erklären.\nZunächst werde ich die automatische Generierung von Versionshinweisen vorstellen, an der wir in dieser Forschung arbeiten.\nEin Versionshinweis ist ein technisches Dokument, das die Änderungen zusammenfasst, die mit jeder Version eines Softwareprodukts vertrieben werden.\nDas Bild zeigt einen Versionshinweis für Version 2.6.4 der Vuejs-Bibliothek.\nVersionshinweise spielen bei der Open-Source-Entwicklung eine wichtige Rolle, aber ihre Erstellung ist manuell zeitaufwändig.\nDaher wäre es sehr nützlich, wenn man automatisch qualitativ hochwertige Versionshinweise generieren könnte.\nIch werde zwei frühere Untersuchungen zur automatischen Generierung von Versionshinweisen erwähnen.\nZuerst gibt es ein System mit dem Namen ARENA, das im Jahr 2014 veröffentlicht wurde.\nEs verfolgt einen regelbasierten Ansatz, indem es zum Beispiel den Change-Extractor verwendet, um alle Unterschiede, Bibliotheksänderungen und Dokumentänderungen aus den verschiedenen Versionen zu extrahieren und sie schließlich zu kombinieren.\nDie auffälligste Funktion dieses Systems ist der Ausgabe-Extraktor in der oberen rechten Ecke.\nDies muss Jira, dem Fehlerverwaltungssystem, überlassen werden und kann nur auf Projekte angewendet werden, die Jira verwenden.\nMit anderen Worten: Es kann nicht für viele Projekte auf GitHub verwendet werden.\nDas zweite ist Glyph, das kürzlich in 2020 angekündigt wurde.\nEs ist im Internet verfügbar und kann über pip installiert werden.\nDieses System verfügt über ein einfaches, lernbasiertes Textklassifikationsmodell und gibt eine von fünf Markierung wie Funktionen oder Fehlerbehebungen für jede eingegebene Commit-Nachricht aus.\nDieses Bild ist eine Stichprobe, das eine Markierung für eine Korrektur oder Fehlerbehebung zurückgibt.\nDie Trainingsdaten von Glyph sind ziemlich klein, etwa 5.000, und werden in den unten beschriebenen Experimenten gezeigt.\nDie Leistung des Textklassifikationsmodells ist nicht hoch.\nIch stelle zwei verwandte Forschungsarbeiten vor, deren Probleme jedoch die begrenzte Anwendbarkeit und die knappen Datenressourcen sind.\nUnser Paper löst diese zwei Probleme und erzeugt automatisch qualitativ hochwertige Versionshinweise.\nAufgrund des Problems der begrenzten Anwendbarkeit schlagen wir eine qualitativ hochwertige, klassenweise Zusammenfassungsmethode vor, die nur Commit-Nachrichten als Eingabe verwendet.\nDiese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden.\nFür das zweite Problem der knappen Datenressourcen haben wir unseren RNSum-Datensatz erstellt, der aus etwa 82.000 Daten besteht. Diese wurden aus den Daten der öffentlichen GitHub-Repositories mithilfe der GitHub-API gesammelt.\nAls Nächstes werde ich unseren Datensatz beschreiben.\nHier ist ein Beispiel für die Daten.\nAuf der linken Seite ist eine Commit-Nachricht und auf der rechten Seite sind die Versionshinweise.\nVersionshinweise sind als Verbesserungen, Korrekturen usw. markiert.\nWir haben eine Aufgabe eingerichtet, die die Commit-Nachricht als Eingabe aufnimmt und die Ausgabe ist ein markierter Versionshinweis.\nDies kann als eine Zusammenfassungsaufgabe betrachtet werden.\nWir haben vier Markierungen vordefiniert: Funktionen, Verbesserungen, Fehlerbehebungen, Beseitigung von Verwerfungen und bahnbrechende Änderungen.\nDiese wurden auf der Grundlage von früheren Forschungen und anderen Faktoren festgelegt.\nDie Versionshinweise unten rechts wurde aus den Versionshinweisen unten links extrahiert.\nZu diesem Zeitpunkt müssen die vier Markierungen, die im Voraus erstellt wurden, erkannt werden.\nDie Markierungen sind jedoch nicht immer mit jedem Repository konsistent.\nDie Markierung „Verbesserungen“ umfasst beispielsweise Verbesserungen, Erweiterungen, Optimierungen und so weiter.\nWir haben eine Wortschatzliste mit etwa 30 Markierungen für jede dieser Notationsvarianten erstellt.\nDiese dient dazu, die Klasse des Versionshinweises zu erkennen. Sie sammelt den Text des Versionshinweises, der als Versionshinweissatz für die Klasse folgt.\nAls Nächstes folgt eine Commit-Nachricht.\nCommit-Nachrichten sind nicht an die einzelnen Versionen gebunden.\nWenn die aktuelle Version 2.5 bis 19 ist, müssen wir, wie im Bild unten gezeigt, die frühere Version 2.5 bis 18 identifizieren und eine Diff erhalten.\nDies ist etwas mühsam und es reicht nicht aus, nur eine Liste von Versionen zu erstellen und die Vorher-Nachher-Bilanz zu betrachten.\nWir haben eine heuristische Abgleichsregel erstellt, um die vorherige und die nächste Version zu bekommen.\nDie Analyse des Datensatzes.\nAm Ende wurden 7.200 Repositories und 82 Daten gesammelt.\nAußerdem liegt die durchschnittliche Anzahl der Versionshinweis-Token bei 63, was für eine Zusammenfassungsaufgabe recht hoch ist.\nAuch die Anzahl der einzigartigen Token ist mit 830.000 recht groß.\nDies ist auf die große Anzahl von eindeutigen Klassen- oder Methodennamen im Repository zurückzuführen.\nAls Nächstes werde ich die vorgeschlagene Methode erläutern.\nDas klassenweise extraktive und dann abstrahierende Zusammenfassungsmodell besteht aus zwei neuronalen Modulen.\nEin Klassifikator verwendet BERT oder CodeBERT und ein Generator verwendet BART.\nZuerst verwendet CEAS einen Klassifikator, um jede Commit-Nachricht in fünf Klassen von Versionshinweisen zu klassifizieren, die Verbesserungen, Fehlerbehebungen, Verwerfungen und eine weitere Klasse enthalten.\nDie als „Sonstiges“ eingestuften Commit-Nachrichten werden verworfen.\nDann wendet CEAS den Generator auf die vier markierten Dokumente unabhängig voneinander an und erstellt Versionshinweise für jede Klasse.\nIn dieser Aufgabe sind die direkten Korrespondenzen zwischen Commit-Nachrichten und Versionshinweisen nicht bekannt.\nUm den Klassifikator zu trainieren, haben wir daher jeder eingegebenen Commit-Nachricht eine Umfrage zugewiesen und dabei die ersten zehn Zeichen jeder Commit-Nachricht verwendet.\nWir haben den klassenweisen abstrakten Zusammenfassungsansatz durch zwei verschiedene Methoden modelliert.\nDas erste Modell, das wir CAS-Single nennen, besteht aus einem einzigen 6-zu-6-Netzwerk und generiert einen einzigen Versionshinweistext, der eine Verkettung von eingegebenen Commit-Nachrichten ergibt.\nDie ausgegebenen Texte können auf der Grundlage spezieller klassenspezifischer Endpunktsymbole in klassenweise Segmente unterteilt werden.\nDie zweite Methode, die wir CAS-Multi nennen, besteht aus vier verschiedenen seq2seq-Netzwerken, von denen jedes einer der festen Versionshinweisklassen entspricht.\nIch werde nun die Experimente erklären.\nFünf Methoden wurden verglichen: CEAS, CAS-Single, CAS-Multi, Clustering und Glyph aus einer früheren Studie.\nWas die Evaluation betrifft, so werden in einigen Fällen die Versionshinweise in mehreren Sätzen ausgegeben.\nDa es schwierig ist, die Anzahl von Sätzen zu berechnen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt.\nDer BLEU wird benachteiligt, wenn das System einen kurzen Satz ausgibt.\nDieser Abzug führt zu einem niedrigeren BLEU-Wert in den nachfolgend beschriebenen Versuchsergebnissen.\nSchließlich berechnen wir auch die Spezifität, da ROUGE und BLEU nicht berechnet werden können, wenn die Versionshinweise leer sind.\nEine höhere Spezifität bedeutet, dass das Modell in Fällen, in denen die Versionshinweise leer sind, einen leeren Text korrekt ausgibt.\nHier sind die Resultate.\nDa der Datensatz E-Mail-Adressen, Hash-Werte usw. enthält, haben wir auch den bereinigten Datensatz ausgewertet, der diese ausschließt.\nCEAS und CAS erreichten ROUGE-L-Werte, die mehr als zehn Punkte über den Baselines lagen.\nInsbesondere beim bereinigten Testsatz ist der Abstand zwischen der vorgeschlagenen Methode und den Baselines auf mehr als zwanzig Punkte angestiegen.\nDiese Ergebnisse zeigen, dass CEAS und CAS signifikant betroffen sind.\nCEAS erzielte bessere ROUGE-L-Werte als CAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators beim Training des Klassifikators mit Pseudo-Markierungen effektiv ist.\nEine hohe Abdeckung von CEAS kann wahrscheinlich erreicht werden, weil sich der Klassifikator auf die Auswahl relevanter Commit-Nachrichten für jede Klasse konzentrieren kann.\nCAS-Multi ergab tendenziell mehr ROUGE-L-Werte als CAS-Single.\nWir schlagen vor, dass es auch effektiv ist, unterschiedliche abstrakte Zusammenfassungsmodelle, die unabhängig voneinander sind, für jede Klasse von den Versionshinweisen zu entwickeln.\nHier ist eine Fehleranalyse.\nCAS-Methoden geben tendenziell kürzere Sätze aus als menschliche Referenzsätze.\nIn der Abbildung rechts besteht der Referenzsatz aus drei oder vier Sätzen, während CAS aus nur einem besteht.\nDer Grund für die Reluktanz dieses Modells ist, dass in den Trainingsdaten nur 33 Prozent der Sätze in der Markierung Funktionen und 40 Prozent in der Markierung Verbesserung zu finden sind.\nDarüber hinaus können CAS-Methoden ohne zusätzliche Informationen keine genauen Versionshinweise erstellen.\nOben auf der rechten Seite ist ein Beispiel für eine sehr unübersichtliche Commit-Nachricht. Der vollständige Satz kann nicht ohne Bezug auf den entsprechenden Fortschritt oder das Problem generiert werden.\nDas folgende Beispiel zeigt, dass die beiden Commit-Nachrichten in der Eingabe zusammenhängen und zu einem Satz zusammengefasst werden sollten, was jedoch nicht gelingt.\nNun kommen wir zum Fazit.\nWir haben einen neuen Datensatz für die automatische Generierung von Versionshinweisen erstellt.\nWir haben auch eine Aufgabe für die Eingabe von Commit-Nachrichten und deren Zusammenfassung formuliert, sodass sie für alle auf Englisch geschriebene Projekte anwendbar ist.\nUnsere Experimente zeigen, dass die vorgeschlagene Methode weniger verrauschte Versionshinweise mit höherem Abdeckungsgrad als die Baselines erzeugt.\nBitte sehen Sie sich unseren Datensatz auf GitHub an.\nVielen Dank!", "src_lang": "en", "tgt_lang": "de", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 0, "src_audio": "/acl6060/audio/en/0.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo zusammen. Heute werde ich unsere Forschungsarbeit Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction vorstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 1, "src_audio": "/acl6060/audio/en/1.wav", "src_ref": "I'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich bin Allan vom ByteDance AI Lab, und dies ist eine gemeinsame Arbeit mit Jierui Li von der University of Texas in Austin und Wei Lu von SUTD.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 2, "src_audio": "/acl6060/audio/en/2.wav", "src_ref": "First, I'd like to talk about our motivation for reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst möchte ich über unsere Motivation für das Argumentieren sprechen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 3, "src_audio": "/acl6060/audio/en/3.wav", "src_ref": "So here we show an examples where multi-step reasoning is helpful.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deshalb zeigen wir hier ein Beispiel, bei dem mehrstufiges Argumentieren hilfreich ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 4, "src_audio": "/acl6060/audio/en/4.wav", "src_ref": "So this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Abbildung stammt aus dem PaLM-Paper, in dem sie Prompting durchführen, um das Netzwerkproblem in einem Few-Shot-Lernszenario zu lösen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 5, "src_audio": "/acl6060/audio/en/5.wav", "src_ref": "So on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auf der linken Seite können wir also sehen, dass wenn einige Beispiele nur aus Frage und Antworten bestehen, dann bekommen wir nicht die richtigen Antworten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 6, "src_audio": "/acl6060/audio/en/6.wav", "src_ref": "But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir jedoch eine weitere Argumentationsbeschreibung hinzufügen, ist das Modell in der Lage, die Argumentationsbeschreibung vorherzusagen und hier auch eine korrekte Vorhersage zu treffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 7, "src_audio": "/acl6060/audio/en/7.wav", "src_ref": "So it is good to have interpretable multi-step reasoning as output.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist also gut, eine interpretierbare mehrstufige Argumentation als Ergebnis zu haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 8, "src_audio": "/acl6060/audio/en/8.wav", "src_ref": "And we also think math word problem is a straightforward application to evaluate such reasoning abilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir denken auch, dass mathematische Wortprobleme eine einfache Anwendung sind, um solche Argumentationsfähigkeiten zu bewerten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 9, "src_audio": "/acl6060/audio/en/9.wav", "src_ref": "So, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem Problem-Setup müssen wir also angesichts der Fragen diese Frage lösen und die numerischen Antworten erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 10, "src_audio": "/acl6060/audio/en/10.wav", "src_ref": "So in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unseren Datensätzen finden wir auch den mathematischen Ausdruck, der uns zu dieser bestimmten Antwort führt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 11, "src_audio": "/acl6060/audio/en/11.wav", "src_ref": "So, certain assumptions ah also apply as in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es gelten also auch hier bestimmte Annahmen wie in früheren Arbeiten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 12, "src_audio": "/acl6060/audio/en/12.wav", "src_ref": "We assume the precision of quantities are known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir gehen davon aus, dass die Genauigkeit der Mengen bekannt ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 13, "src_audio": "/acl6060/audio/en/13.wav", "src_ref": "And we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und wir betrachten nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponentialrechnung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 14, "src_audio": "/acl6060/audio/en/14.wav", "src_ref": "Furthermore, complicated operators can be actually decomposed into these basic operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus können komplizierte Operatoren in diese Grundoperatoren zerlegt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 15, "src_audio": "/acl6060/audio/en/15.wav", "src_ref": "So, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die früheren Arbeiten zum Lösen von mathematischen Wortproblemen lassen sich also in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modell einteilen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 16, "src_audio": "/acl6060/audio/en/16.wav", "src_ref": "So, traditional sequence to sequence model convert the expression to a specific sequence for generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das traditionelle Sequenz-zu-Sequenz-Modell wandelt also den Ausdruck in eine spezifische Sequenz für die Generierung um.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 17, "src_audio": "/acl6060/audio/en/17.wav", "src_ref": "And it is pretty easy to implement and it can generalize to many different complicated problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist ziemlich einfach zu implementieren und kann für viele verschiedene komplizierte Probleme verallgemeinert werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 18, "src_audio": "/acl6060/audio/en/18.wav", "src_ref": "But the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Nachteile sind jedoch, dass die Leistung im Allgemeinen nicht besser ist als die des strukturierten Modells und dass es an Interpretierbarkeit für Vorhersagen mangelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 19, "src_audio": "/acl6060/audio/en/19.wav", "src_ref": "But actually this direction is still quite popular because of um the transformer model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber eigentlich ist diese Richtung immer noch recht beliebt wegen des Transformermodells.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 20, "src_audio": "/acl6060/audio/en/20.wav", "src_ref": "So, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In baumbasierten Modellen strukturieren wir diese Ausdrücke in Baumform und folgen einem geordneten Traversal in Baumgenerationen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 21, "src_audio": "/acl6060/audio/en/21.wav", "src_ref": "So here we keep generating the operators until we reach the leaves, which are the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier fahren wir also fort, die Operatoren zu generieren, bis wir die Blätter erreichen, die die Mengen darstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 22, "src_audio": "/acl6060/audio/en/22.wav", "src_ref": "So here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Gute daran ist, dass wir dadurch diese binäre Baumstruktur erhalten. Sie ist eigentlich ziemlich kontraintuitiv, weil wir zuerst den Operator und dann am Ende die Mengen generieren müssen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 23, "src_audio": "/acl6060/audio/en/23.wav", "src_ref": "And the second thing is that it also contains some repetitive computations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zudem enthält sie auch einige sich wiederholende Berechnungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 24, "src_audio": "/acl6060/audio/en/24.wav", "src_ref": "So here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir uns also diesen Ausdruck ansehen, wird 8 x 3 + 3 eigentlich zweimal generiert, aber eigentlich sollten wir die Ergebnisse wiederverwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 25, "src_audio": "/acl6060/audio/en/25.wav", "src_ref": "So, in our proposed approach we want to solve those problems in a step by step and interpretable manners.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem vorgeschlagenen Ansatz wollen wir diese Probleme schrittweise und auf interpretierbare Art und Weise lösen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 26, "src_audio": "/acl6060/audio/en/26.wav", "src_ref": "So for example, here in the second step, ah we can obtain these divisors which is twenty seven.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier im zweiten Schritt zum Beispiel können wir diese Teiler erhalten, was 27 ergibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 27, "src_audio": "/acl6060/audio/en/27.wav", "src_ref": "And we can also refer back to the original questions to find the relevant contents.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können auch auf die ursprünglichen Fragen zurückgreifen, um die relevanten Inhalte zu finden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 28, "src_audio": "/acl6060/audio/en/28.wav", "src_ref": "And in these steps we obtain the divisors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesen Schritten erhalten wir die Teiler.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 29, "src_audio": "/acl6060/audio/en/29.wav", "src_ref": "So, ah and then at this third step we actually get the quotient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und in diesem dritten Schritt erhalten wir dann den Quotienten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 30, "src_audio": "/acl6060/audio/en/30.wav", "src_ref": "Alright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Okay. Nach diesen drei Schritten können wir die Ergebnisse des zweiten Schritts wiederverwenden, dann die Ergebnisse des vierten Schritts erhalten und schließlich die Dividenden erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 31, "src_audio": "/acl6060/audio/en/31.wav", "src_ref": "So, here we actually generate the whole expression directly rather than generating a single operators or quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier wird also der gesamte Ausdruck direkt generiert und nicht nur ein einzelner Operator oder eine Menge.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 32, "src_audio": "/acl6060/audio/en/32.wav", "src_ref": "So this makes the process more accurate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dadurch wird der Prozess genauer.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 33, "src_audio": "/acl6060/audio/en/33.wav", "src_ref": "So, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem deduktiven System beginnen wir also zunächst mit einer Reihe von Mengen, die in den Fragen vorgestellt werden, und schließen auch mit einer Konstante als Anfangszustand.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 34, "src_audio": "/acl6060/audio/en/34.wav", "src_ref": "So, the expression is represented by e i j o p.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Ausdruck wird also durch e i j o p dargestellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 35, "src_audio": "/acl6060/audio/en/35.wav", "src_ref": "Where we perform operator from q_i to q_j, and such expression is actually directed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir führen den Operator von q_i nach q_j aus, und dieser Ausdruck ist tatsächlich geregelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 36, "src_audio": "/acl6060/audio/en/36.wav", "src_ref": "So, we also have subtraction with words here to represent the opposite direction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben hier also auch eine Subtraktion mit Wörtern, um die umgekehrte Richtung darzustellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 37, "src_audio": "/acl6060/audio/en/37.wav", "src_ref": "This is quite similar to relation extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies ist der Beziehungsextraktion recht ähnlich.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 38, "src_audio": "/acl6060/audio/en/38.wav", "src_ref": "So in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In einem formalen deduktiven System wenden wir also in einem Zeitschritt t den Operator zwischen dem Paar q_i und q_j an und erhalten dann diesen neuen Ausdruck.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 39, "src_audio": "/acl6060/audio/en/39.wav", "src_ref": "We add it to the next state to become a new quantity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir fügen ihn dem nächsten Zustand hinzu, um eine neue Menge zu erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 40, "src_audio": "/acl6060/audio/en/40.wav", "src_ref": "So, these slides actually visualize the evolution of the state where we keep adding expression to the current state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Folien veranschaulichen also die Entwicklung des Zustands, wobei wir dem aktuellen Zustand immer neue Ausdrücke hinzufügen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 41, "src_audio": "/acl6060/audio/en/41.wav", "src_ref": "So in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unseren Modellimplementierungen verwenden wir also zunächst ein vortrainiertes Sprachmodell, bei dem es sich um BERTs oder Robertas handeln kann, und dann kodieren wir den Satz und erhalten diese Mengendarstellungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 42, "src_audio": "/acl6060/audio/en/42.wav", "src_ref": "So, once we get the quantity representations, we can start to do inference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sobald wir also die Mengendarstellungen haben, können wir mit der Inferenz beginnen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 43, "src_audio": "/acl6060/audio/en/43.wav", "src_ref": "Here we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier zeigen wir ein Beispiel für q_1, um die Darstellung für q_2 geteilt durch q_2 und dann mal q_3 zu erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 44, "src_audio": "/acl6060/audio/en/44.wav", "src_ref": "First we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst erhalten wir die Paardarstellung, die im Grunde nur die Verkettung zwischen q_1 und q_2 ist, und dann wenden wir ein Feedforward-Netzwerk an, das durch den Operator parametrisiert ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 45, "src_audio": "/acl6060/audio/en/45.wav", "src_ref": "And then finally we obtain the expression representation q_1 divided by q_2.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und dann erhalten wir schließlich die Ausdrucksdarstellung q_1 geteilt durch q_2.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 46, "src_audio": "/acl6060/audio/en/46.wav", "src_ref": "But in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber in der Praxis könnten wir in der Inferenzphase auch den falschen Ausdruck erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 47, "src_audio": "/acl6060/audio/en/47.wav", "src_ref": "So, here all the possible expression is equals to three times the number of operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind also alle möglichen Ausdrücke gleich dem Dreifachen der Anzahl der Operatoren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 48, "src_audio": "/acl6060/audio/en/48.wav", "src_ref": "So the nice thing here is that we can easily add constraints to control this search this search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Schöne daran ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu kontrollieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 49, "src_audio": "/acl6060/audio/en/49.wav", "src_ref": "For example, if this expression is not allowed, we can simply remove this expression in our search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn dieser Ausdruck zum Beispiel nicht erlaubt ist, können wir ihn einfach aus unserem Suchraum entfernen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 50, "src_audio": "/acl6060/audio/en/50.wav", "src_ref": "So in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Im zweiten Schritt machen wir das Gleiche, aber der einzige Unterschied ist, dass wir nur eine Menge mehr haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 51, "src_audio": "/acl6060/audio/en/51.wav", "src_ref": "So this quantity come from the previous calculated expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Menge stammt also aus dem vorherigen berechneten Ausdruck.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 52, "src_audio": "/acl6060/audio/en/52.wav", "src_ref": "So finally we can obtain this final expression q_3 times q_4.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "So können wir schließlich diesen endgültigen Ausdruck q_3 mal q_4 erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 53, "src_audio": "/acl6060/audio/en/53.wav", "src_ref": "And we can also see the number of all the possible ah expression is different from the previous step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können auch sehen, dass die Anzahl aller möglichen Ausdrücke anders ist als beim vorherigen Schritt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 54, "src_audio": "/acl6060/audio/en/54.wav", "src_ref": "So, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein solcher Unterschied erschwert die Anwendung von einer Beam Search, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgewogen ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 55, "src_audio": "/acl6060/audio/en/55.wav", "src_ref": "So the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Trainingsverfahren ist also ähnlich dem Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust bei jedem Zeitschritt optimieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 56, "src_audio": "/acl6060/audio/en/56.wav", "src_ref": "And here we also use this tau to represent when we should terminate this generation process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auch hier verwenden wir dieses Tau, um darzustellen, wann wir diesen Generierungsprozess beenden sollten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 57, "src_audio": "/acl6060/audio/en/57.wav", "src_ref": "And here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier unterscheidet sich der Raum von Sequenz zu Sequenz, weil der Raum bei jedem Zeitschritt anders ist, während im traditionellen Sequenz-zu-Sequenz-Modell dies die Anzahl des Vokabulars ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 58, "src_audio": "/acl6060/audio/en/58.wav", "src_ref": "And it also allows us to impose certain constraints from prior from prior knowledge.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es erlaubt uns auch, bestimmte Beschränkungen aus dem Vorwissen aufzuerlegen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 59, "src_audio": "/acl6060/audio/en/59.wav", "src_ref": "So we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher führen wir Experimente mit den häufig verwendeten Datensätzen der mathematischen Wortprobleme durch, MAWPS, Math23K, MathQA und SVAMP.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 60, "src_audio": "/acl6060/audio/en/60.wav", "src_ref": "And here we briefly show the results compared with the previous best approaches.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier zeigen wir kurz die Ergebnisse im Vergleich zu den bisherigen besten Ansätzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 61, "src_audio": "/acl6060/audio/en/61.wav", "src_ref": "So our best performing variant is Roberta-DeductiveReasoner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere leistungsstärkste Variante ist also Roberta-DeductiveReasoner.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 62, "src_audio": "/acl6060/audio/en/62.wav", "src_ref": "And in fact we do not use beam search, in contrast all previous approaches are using beam search.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tatsächlich verwenden wir keine Beam Search, im Gegensatz zu allen früheren Ansätzen, die Beam Search verwendet haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 63, "src_audio": "/acl6060/audio/en/63.wav", "src_ref": "All right. So, the best approaches are often tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Okay. Daher sind die besten Ansätze oft baumbasierte Modelle.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 64, "src_audio": "/acl6060/audio/en/64.wav", "src_ref": "So, overall our reasoner is able to significantl significantly outperform this tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insgesamt ist unser Reasoner also in der Lage, dieses baumbasierte Modell signifikant zu übertreffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 65, "src_audio": "/acl6060/audio/en/65.wav", "src_ref": "But we can see the absolute numbers on MathQA or SVAMP are not really high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wir können sehen, dass die absoluten Zahlen bei MathQA oder SVAMP nicht wirklich hoch sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 66, "src_audio": "/acl6060/audio/en/66.wav", "src_ref": "So we further investigate the results on SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deshalb untersuchen wir die Ergebnisse bei SVAMP weiter.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 67, "src_audio": "/acl6060/audio/en/67.wav", "src_ref": "And this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieser Datensatz ist eine Herausforderung, weil der Autor versucht hat, manuell etwas hinzuzufügen, um das NLP-Modell zu verwirren, wie z. B. das Hinzufügen irrelevanter Informationen und zusätzlicher Mengen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 68, "src_audio": "/acl6060/audio/en/68.wav", "src_ref": "So, in our prediction we find some of the intermediate values are actually negatives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserer Vorhersage stellen wir also fest, dass einige der Zwischenwerte eigentlich negativ sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 69, "src_audio": "/acl6060/audio/en/69.wav", "src_ref": "For example, um, in these questions we are asking how many apples does Jake have?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel geht es in diesen Fragen darum, wie viele Äpfel Jake hat.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 70, "src_audio": "/acl6060/audio/en/70.wav", "src_ref": "But we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wir haben einige zusätzliche Informationen wie 17 Bilder weniger, und Steven hat acht Bilder, was völlig irrelevant ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 71, "src_audio": "/acl6060/audio/en/71.wav", "src_ref": "So, our model makes some prediction like this which is producing negative values.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Modell macht also eine Vorhersage wie diese, die negative Werte ergibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 72, "src_audio": "/acl6060/audio/en/72.wav", "src_ref": "And we observe these two expressions actually have similar scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir stellen fest, dass diese beiden Ausdrücke tatsächlich ähnliche Werte haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 73, "src_audio": "/acl6060/audio/en/73.wav", "src_ref": "So, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können also diesen Suchraum eingrenzen, indem wir die negativen Ergebnisse entfernen, damit die Antwort richtig ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 74, "src_audio": "/acl6060/audio/en/74.wav", "src_ref": "So um we further find such constraint actually improves quite a lot for some models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir stellen also fest, dass eine solche Bedingung für einige Modelle tatsächlich eine erhebliche Verbesserung darstellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 75, "src_audio": "/acl6060/audio/en/75.wav", "src_ref": "For example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei BERT haben wir uns zum Beispiel um sieben Punkte verbessert und beim Roberta-Basismodell haben wir uns sogar um zwei Punkte verbessert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 76, "src_audio": "/acl6060/audio/en/76.wav", "src_ref": "So better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein besseres Sprachmodell hat also bessere Sprachverständnisfähigkeiten, sodass die Zahl hier höher bei Roberta und niedriger bei BERT ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 77, "src_audio": "/acl6060/audio/en/77.wav", "src_ref": "And we also try to analyze the difficulty behind these behind all these datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir versuchen auch, die Schwierigkeiten hinter all diesen Datensätzen zu analysieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 78, "src_audio": "/acl6060/audio/en/78.wav", "src_ref": "We assume the number of unused quantities can be regarded as irrelevant information here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir gehen davon aus, dass die Anzahl der ungenutzten Mengen hier als irrelevante Informationen betrachtet werden kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 79, "src_audio": "/acl6060/audio/en/79.wav", "src_ref": "So ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier können wir also sehen, dass wir den Prozentsatz der Proben mit ungenutzten Mengen haben, und der SVAMP-Datensatz hat den größten Anteil.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 80, "src_audio": "/acl6060/audio/en/80.wav", "src_ref": "And here we also show the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier zeigen wir auch die Gesamtleistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 81, "src_audio": "/acl6060/audio/en/81.wav", "src_ref": "For those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei den Proben ohne ungenutzte Mengen ist die Leistung sogar höher als die Gesamtleistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 82, "src_audio": "/acl6060/audio/en/82.wav", "src_ref": "But with those samples that with unused quantity is actually way worse than the, worse than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber bei diesen Proben mit ungenutzten Mengen ist sie eigentlich viel schlechter als die Gesamtleistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 83, "src_audio": "/acl6060/audio/en/83.wav", "src_ref": "For MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei MAWPS haben wir nicht wirklich viele Testfälle, also ignoriere ich diesen Teil einfach.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 84, "src_audio": "/acl6060/audio/en/84.wav", "src_ref": "So, finally we want to show the interpretability through a question perturbation example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich wollen wir die Interpretierbarkeit anhand eines Frage-Störungsbeispiels zeigen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 85, "src_audio": "/acl6060/audio/en/85.wav", "src_ref": "So here our model actually makes a wrong prediction at the first step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier macht unser Modell also tatsächlich eine falsche Vorhersage im ersten Schritt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 86, "src_audio": "/acl6060/audio/en/86.wav", "src_ref": "So, we can actually correlate this expression with the sentence here. Alright.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können diesen Ausdruck also tatsächlich mit dem Satz hier korrelieren. Okay.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 87, "src_audio": "/acl6060/audio/en/87.wav", "src_ref": "So, we think this sentence might be misleading the model to an incorrect predictions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir denken also, dass dieser Satz das Modell zu falschen Vorhersagen verleiten könnte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 88, "src_audio": "/acl6060/audio/en/88.wav", "src_ref": "So here planting another thirty five makes the model makes the model think it should be an addition operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier führt eine weitere 35 dazu, dass das Modell denkt, dass es ein Additionsoperator sein sollte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 89, "src_audio": "/acl6060/audio/en/89.wav", "src_ref": "So we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir versuchen also den Satz so umzuformulieren, dass die Anzahl der Birnbäume um 35 weniger als die der Apfelbäume ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 90, "src_audio": "/acl6060/audio/en/90.wav", "src_ref": "So, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sorgen also dafür, dass eine genauere Semantik vermittelt wird, sodass das Modell in der Lage ist, die Vorhersage korrekt zu treffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 91, "src_audio": "/acl6060/audio/en/91.wav", "src_ref": "So, this study shows how the interpretable predictions help us understand the model behavior.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Studie zeigt also, wie uns die interpretierbaren Vorhersagen helfen, das Modellverhalten zu verstehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 92, "src_audio": "/acl6060/audio/en/92.wav", "src_ref": "So to conclude our work, so first our model is actually pretty efficient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um unsere Arbeit abzuschließen: Zunächst ist unser Modell tatsächlich ziemlich effizient.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 93, "src_audio": "/acl6060/audio/en/93.wav", "src_ref": "And we are able to provide interpretable solving procedure.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sind in der Lage, ein interpretierbares Lösungsverfahren anzubieten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 94, "src_audio": "/acl6060/audio/en/94.wav", "src_ref": "And we can easily incorporate some prior knowledge as constraint which can help improve the performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir können einfach einiges Vorwissen als Bedingung einbeziehen, was zur Verbesserung der Leistung beitragen kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 95, "src_audio": "/acl6060/audio/en/95.wav", "src_ref": "And the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich gilt der zugrundeliegende Mechanismus nicht nur für das Lösen von Netzwerkproblemen, sondern auch für andere Aufgaben, die mehrstufiges Argumentieren erfordern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 96, "src_audio": "/acl6060/audio/en/96.wav", "src_ref": "We also have certain limitations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auch sind uns gewisse Grenzen gesetzt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 97, "src_audio": "/acl6060/audio/en/97.wav", "src_ref": "Ah, if we have a large number of operators or constants, the memory consumption could be pretty high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir eine große Anzahl von Operatoren oder Konstanten haben, könnte der Speicherverbrauch ziemlich hoch sein.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 98, "src_audio": "/acl6060/audio/en/98.wav", "src_ref": "And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wie bereits erwähnt ist es zudem aufgrund der unausgewogenen Wahrscheinlichkeitsverteilung zwischen den verschiedenen Zeitschritten eine ziemliche Herausforderung, die Beam Search-Strategie anzuwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 99, "src_audio": "/acl6060/audio/en/99.wav", "src_ref": "So this is the end of the talk, and questions are welcomed. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sind am Ende des Vortrags angekommen und freuen uns auf etwaige Fragen. Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 100, "src_audio": "/acl6060/audio/en/100.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo, mein Name ist Antoine und ich bin von der Universität Maastricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 101, "src_audio": "/acl6060/audio/en/101.wav", "src_ref": "I will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde meine gemeinsame Arbeit mit Jerry vorstellen, bei der es um einen neuen Datensatz für das Retrieval von Gesetzesartikeln geht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 102, "src_audio": "/acl6060/audio/en/102.wav", "src_ref": "Legal issues are an integral part of many people's lives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Rechtsfragen sind ein wesentlicher Bestandteil des Lebens vieler Menschen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 103, "src_audio": "/acl6060/audio/en/103.wav", "src_ref": "But the majority of citizens have little to know knowledge about their rights and fundamental legal processes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber die Mehrheit der Bürger verfügt nur über wenig Wissen über ihre Rechte und grundlegende rechtliche Verfahren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 104, "src_audio": "/acl6060/audio/en/104.wav", "src_ref": "As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies hat zur Folge, dass viele schutzbedürftige Bürger, die sich die kostspielige Hilfe eines Rechtsexperten nicht leisten können, schutzlos bleiben oder schlimmstenfalls ausgebeutet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 105, "src_audio": "/acl6060/audio/en/105.wav", "src_ref": "All work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere Arbeit zielt darauf ab, die Kluft zwischen den Menschen und dem Gesetz zu überbrücken, indem ein effektives Retrieval-System für Gesetzesartikel entwickelt wird.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 106, "src_audio": "/acl6060/audio/en/106.wav", "src_ref": "Such a system could provide a free professional legal help service for unskilled humans.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein solches System könnte einen kostenlosen professionellen Rechtshilfedienst für unerfahrene Menschen anbieten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 107, "src_audio": "/acl6060/audio/en/107.wav", "src_ref": "Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bevor wir uns dem Hauptbeitrag dieser Arbeit widmen, wollen wir zunächst das Problem des Retrievals von Gesetzesartikeln beschreiben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 108, "src_audio": "/acl6060/audio/en/108.wav", "src_ref": "Given a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eine einfache Frage zu einer Rechtsangelegenheit wäre: „Was riskiere ich, wenn ich das Berufsgeheimnis verletze?“", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 109, "src_audio": "/acl6060/audio/en/109.wav", "src_ref": "A model is required to retrieve all relevant statutory articles from a large body of legislation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier wird ein Modell benötigt, um alle relevanten Gesetzesartikel aus einem großen Bestand an Rechtsvorschriften abzurufen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 110, "src_audio": "/acl6060/audio/en/110.wav", "src_ref": "This information retrieval task comes with its own set of challenges.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Informationsbeschaffungsaufgabe bringt eine Reihe von Herausforderungen mit sich.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 111, "src_audio": "/acl6060/audio/en/111.wav", "src_ref": "First, it deals with two types of language.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Erstens: Es geht um zwei Arten von Sprache.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 112, "src_audio": "/acl6060/audio/en/112.wav", "src_ref": "Common natural language for the questions and complex legal language for the statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die gemeinsame natürliche Sprache für die Fragen und die komplexe juristische Sprache für die Gesetze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 113, "src_audio": "/acl6060/audio/en/113.wav", "src_ref": "This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieser Unterschied bei den Sprachverteilungen macht es für ein System schwieriger, relevante Kandidaten zu finden, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine rechtliche Frage übersetzen kann, die der Terminologie der Gesetze entspricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 114, "src_audio": "/acl6060/audio/en/114.wav", "src_ref": "Besides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Außerdem ist das Recht keine Ansammlung unabhängiger Artikel, die für sich allein als vollständige Informationsquelle betrachtet werden können, anders als beispielsweise Nachrichten oder Rezepte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 115, "src_audio": "/acl6060/audio/en/115.wav", "src_ref": "Instead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielmehr handelt es sich um eine strukturierte Sammlung von Rechtsvorschriften, die nur im Gesamtkontext, d. h. zusammen mit den ergänzenden Informationen aus den Nachbarartikeln, den Bereichen und Teilbereichen, denen sie angehören, und ihrer Stellung im Aufbau des Gesetzes, eine vollständige Bedeutung haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 116, "src_audio": "/acl6060/audio/en/116.wav", "src_ref": "Lastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich handelt es sich bei den Gesetzesartikeln nicht um kleine Absätze, die in den meisten Retrieval-Arbeiten die typische Retrieval-Einheit darstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 117, "src_audio": "/acl6060/audio/en/117.wav", "src_ref": "Here, there are long documents that may be up to six thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier gibt es lange Dokumente, die bis zu 6.000 Wörter umfassen können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 118, "src_audio": "/acl6060/audio/en/118.wav", "src_ref": "The recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die jüngsten Fortschritte im Bereich NLP haben großes Interesse an vielen juristischen Aufgaben geweckt, z. B. an der Vorhersage juristischer Urteile oder der automatisierten Prüfung von Verträgen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 119, "src_audio": "/acl6060/audio/en/119.wav", "src_ref": "But statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Doch das Retrieval von Gesetzesartikeln ist aufgrund des Mangels an großen und qualitativ hochwertig markierten Datensätzen weitgehend unberührt geblieben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 120, "src_audio": "/acl6060/audio/en/120.wav", "src_ref": "In this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In dieser Arbeit stellen wir einen neuen Datensatz in der französischen Muttersprache vor, mit dem wir Retrievalmodelle untersuchen und analysieren, ob sie bei der Aufgabe der Suche nach Gesetzesartikeln an die Effizienz und Zuverlässigkeit eines Rechtsexperten rankommen können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 121, "src_audio": "/acl6060/audio/en/121.wav", "src_ref": "Our Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser belgischer Datensatz für das Retrieval von Gesetzesartikeln, BSARD, besteht aus mehr als 1.100 juristischen Fragen, die von belgischen Bürgern gestellt wurden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 122, "src_audio": "/acl6060/audio/en/122.wav", "src_ref": "These questions cover a wide range of topics from family, housing, money, to work and social security.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Fragen decken ein breites Spektrum an Themen ab, von Familie, Wohnen, Geld bis hin zu Arbeit und sozialer Sicherheit.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 123, "src_audio": "/acl6060/audio/en/123.wav", "src_ref": "Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jede von ihnen wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als 22.600 Rechtsartikeln aus belgischen Gesetzbüchern markiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 124, "src_audio": "/acl6060/audio/en/124.wav", "src_ref": "Let's now talk about how we collected this dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lassen Sie uns nun darüber sprechen, wie wir diesen Datensatz gesammelt haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 125, "src_audio": "/acl6060/audio/en/125.wav", "src_ref": "First, we started by compiling a large corpus of legal articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst haben wir einen großen Korpus von Rechtsartikeln zusammengestellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 126, "src_audio": "/acl6060/audio/en/126.wav", "src_ref": "We considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben 32 öffentlich zugängliche belgische Gesetzbücher berücksichtigt und alle Artikel sowie die entsprechenden Abschnittsüberschriften extrahiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 127, "src_audio": "/acl6060/audio/en/127.wav", "src_ref": "Then we gathered legal questions with references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann haben wir rechtliche Fragen mit Verweisen auf einschlägige Gesetze zusammengestellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 128, "src_audio": "/acl6060/audio/en/128.wav", "src_ref": "To do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zu diesem Zweck arbeiten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr etwa 4.000 E-Mails von belgischen Bürgern erhält, die um Rat in einer persönlichen Rechtsangelegenheit bitten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 129, "src_audio": "/acl6060/audio/en/129.wav", "src_ref": "We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir hatten das Glück, Zugang zu ihren Websites zu erhalten, auf denen ihr Team aus erfahrenen Juristen die häufigsten Rechtsangelegenheiten der Belgier behandelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 130, "src_audio": "/acl6060/audio/en/130.wav", "src_ref": "We collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben Tausende von Fragen gesammelt und mit Kategorien, Unterkategorien und Verweisen auf einschlägige Gesetze annotiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 131, "src_audio": "/acl6060/audio/en/131.wav", "src_ref": "Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich haben wir die rechtlichen Verweise überprüft und die Fragen herausgefiltert, deren Verweise nicht in einem der von uns berücksichtigten Gesetzbücher enthalten waren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 132, "src_audio": "/acl6060/audio/en/132.wav", "src_ref": "The remaining references were matched and converted to the corresponding article ids from our corpus.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die übrigen Referenzen wurden zusammengeführt und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 133, "src_audio": "/acl6060/audio/en/133.wav", "src_ref": "We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Am Ende hatten wir 1.108 Fragen, jede sorgfältig markiert mit den IDs der relevanten Artikel aus unserem großen Korpus von 22.630 Gesetzesartikeln.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 134, "src_audio": "/acl6060/audio/en/134.wav", "src_ref": "In addition, each question comes with the main category and a concatenation of subcategories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus enthält jede Frage eine Hauptkategorie und eine Verkettung von Unterkategorien.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 135, "src_audio": "/acl6060/audio/en/135.wav", "src_ref": "And each articles comes with a concatenation of the subsequence heading in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jeder Artikel hat eine Verkettung mit der anschließenden Überschrift im Aufbau des Gesetzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 136, "src_audio": "/acl6060/audio/en/136.wav", "src_ref": "This extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für künftige Forschungen zum juristischen Informationen-Retrieval oder zur juristischen Textklassifikation von Interesse sein.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 137, "src_audio": "/acl6060/audio/en/137.wav", "src_ref": "Let's look at some characteristic of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schauen wir uns einige Merkmale unseres Datensatzes an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 138, "src_audio": "/acl6060/audio/en/138.wav", "src_ref": "The questions are between five and forty four words long with a median of fourteen words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Fragen sind zwischen fünf und 24 Wörter lang, mit einem Mittelwert von 14 Wörtern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 139, "src_audio": "/acl6060/audio/en/139.wav", "src_ref": "The articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Artikel sind mit einer durchschnittlichen Länge von 77 Wörtern sehr viel länger, wobei 142 von ihnen mehr als 1.000 Wörter umfassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 140, "src_audio": "/acl6060/audio/en/140.wav", "src_ref": "The lengthiest one being up to five thousand seven hundred and ninety words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der längste von ihnen hat bis zu 5.790 Wörter.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 141, "src_audio": "/acl6060/audio/en/141.wav", "src_ref": "As previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wie bereits erwähnt decken die Fragen ein breites Spektrum an Themen ab, wobei etwa 85 Prozent der Fragen entweder die Familie, das Wohnen, Geld oder die Gerechtigkeit betreffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 142, "src_audio": "/acl6060/audio/en/142.wav", "src_ref": "While the remaining fifteen percent concern either social security, foreigners or work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die restlichen 15 Prozent betreffen entweder soziale Sicherheit, Ausländer oder Arbeit.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 143, "src_audio": "/acl6060/audio/en/143.wav", "src_ref": "The article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Artikel sind auch sehr vielfältig, da sie aus 32 verschiedenen belgischen Gesetzbüchern stammen, die eine große Reihe von Rechtsthemen abdecken.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 144, "src_audio": "/acl6060/audio/en/144.wav", "src_ref": "Here's the total number of articles collected from each of these Belgian codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist die Gesamtzahl der Artikel, die aus diesen belgischen Gesetzbüchern gesammelt wurden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 145, "src_audio": "/acl6060/audio/en/145.wav", "src_ref": "Out of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Von den 22.633 Artikeln werden nur 1.612 als relevant für mindestens eine Frage im Datensatz genannt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 146, "src_audio": "/acl6060/audio/en/146.wav", "src_ref": "And around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Etwa 80 Prozent dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, dem Gerichtsgesetzbuch, dem Gesetzbuch für strafrechtliche Ermittlungen oder dem Strafgesetzbuch.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 147, "src_audio": "/acl6060/audio/en/147.wav", "src_ref": "Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei 18 von 32 Gesetzbüchern werden weniger als fünf Artikel als relevant für mindestens eine Frage bestimmt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 148, "src_audio": "/acl6060/audio/en/148.wav", "src_ref": "Which can be explained by the fact that those codes focused less on individuals and their concerns.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies lässt sich dadurch erklären, dass diese Gesetzbücher weniger auf den Einzelnen und seine Anliegen ausgerichtet waren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 149, "src_audio": "/acl6060/audio/en/149.wav", "src_ref": "Overall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insgesamt liegt der Durchschnitt der Anzahl der Zitate bei zwei, und weniger als 25 Prozent der zitierten Artikel werden mehr als fünfmal zitiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 150, "src_audio": "/acl6060/audio/en/150.wav", "src_ref": "Using all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unter Verwendung aller Datensätze haben wir verschiedene Retrieval-Ansätze, einschließlich lexikalischer und dichter Architektur, einem Benchmarking unterzogen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 151, "src_audio": "/acl6060/audio/en/151.wav", "src_ref": "Given a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei einer Abfrage und einem Artikel weist ein lexikalisches Modell dem Abfrage-Artikel-Paar eine Punktzahl zu, indem es die Summe der Gewichtungen von jedem der Begriffe in diesem Artikel über die Abfrage-Termini berechnet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 152, "src_audio": "/acl6060/audio/en/152.wav", "src_ref": "We experiment with the standard TF-IDF and BM25 ranking functions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit den Standard-Ranking-Funktionen TF-IDF und BM25.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 153, "src_audio": "/acl6060/audio/en/153.wav", "src_ref": "The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Hauptproblem bei diesen Ansätzen ist, dass sie nur Artikel finden können, die Schlüsselwörter enthalten, die in der Anfrage vorkommen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 154, "src_audio": "/acl6060/audio/en/154.wav", "src_ref": "To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um diese Einschränkung zu überwinden, experimentieren wir mit einer neuronal-basierten Architektur, die semantische Beziehungen zwischen Abfragen und Artikeln erfassen kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 155, "src_audio": "/acl6060/audio/en/155.wav", "src_ref": "We use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir verwenden ein Bi-Encoder-Modell, das Abfragen und Artikel in dichten Vektor-Darstellungen abbildet und einen Relevanzwert zwischen einem Abfrage-Artikel-Paar anhand der Ähnlichkeit ihrer Einbettungen berechnet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 156, "src_audio": "/acl6060/audio/en/156.wav", "src_ref": "These embeddings typically result from a pooling operation on the output of a word embedding model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Einbettungen resultieren in der Regel aus einer Pooling-Operation auf der Ausgabe eines Worteinbettungsmodells.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 157, "src_audio": "/acl6060/audio/en/157.wav", "src_ref": "First, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst untersuchen wir die Effektivität von Siamesischen Bi-Encodern in einem Zero-Shot-Evaluation-Setup, d. h. dass vortrainierte Worteinbettungsmodelle ohne zusätzliche Feinabstimmung sofort angewendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 158, "src_audio": "/acl6060/audio/en/158.wav", "src_ref": "We experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit kontextunabhängigen Textencodern, wie word2vec und fastText, und kontextabhängigen Einbettungsmodellen, wie Roberta und insbesondere CamemBERT, einem französischen Roberta-Modell.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 159, "src_audio": "/acl6060/audio/en/159.wav", "src_ref": "Additionally, we train our own CamemBERT based model ah bi-encoders on our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus trainieren wir unser eigenes, auf CamemBERT basierendes Modell, Bi-Encoder, auf unserem Datensatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 160, "src_audio": "/acl6060/audio/en/160.wav", "src_ref": "Note that for training, we experiment with the two flavors of the bi-encoder architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Beachten Sie, dass wir beim Training mit den beiden Varianten der Bi-Encoder-Architektur experimentieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 161, "src_audio": "/acl6060/audio/en/161.wav", "src_ref": "Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Siamese, das ein einzigartiges Worteinbettungsmodell verwendet, das die Abfrage und den Artikel zusammen in einem geteilten dichten Vektorraum abbildet, und Two-Tower, das zwei unabhängige Worteinbettungsmodelle verwendet, die die Abfrage und den Artikel getrennt in verschiedenen Einbettungsräumen kodieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 162, "src_audio": "/acl6060/audio/en/162.wav", "src_ref": "We experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit Mittelwert, Maximalwert und CLS-Pooling sowie mit Produkt und Kosinus für die Berechnung von Ähnlichkeiten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 163, "src_audio": "/acl6060/audio/en/163.wav", "src_ref": "Here are the result of our baseline on the test sets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind die Ergebnisse unserer Baseline auf den Testsätzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 164, "src_audio": "/acl6060/audio/en/164.wav", "src_ref": "With the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die lexikalischen Methoden werden oben, die Siamesischen Bi-Encoder in einem Zero-Shot-Setup in der Mitte und die fein abgestimmten Bi-Encoder werden unten ausgewertet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 165, "src_audio": "/acl6060/audio/en/165.wav", "src_ref": "Overall, the finetuned bi-encoder significantly outperforms all the other baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insgesamt übertrifft der fein abgestimmte Bi-Encoder alle anderen Baselines deutlich.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 166, "src_audio": "/acl6060/audio/en/166.wav", "src_ref": "The two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Two-Tower-Modell verbessert sich gegenüber seinen Siamesischen Varianten beim Recall um 100, schneidet aber bei den anderen Metriken ähnlich gut ab.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 167, "src_audio": "/acl6060/audio/en/167.wav", "src_ref": "Although BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Obwohl BM25 deutlich schlechter abschnitt als der trainierte Bi-Encoder, zeigte seine Leistung, dass er immer noch eine gute Basis für ein domänenspezifisches Retrieval ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 168, "src_audio": "/acl6060/audio/en/168.wav", "src_ref": "Regarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bezüglich der Zero-Shot-Evaluation des Siamesischen Bi-Encoders stellen wir fest, dass die direkte Verwendung der Einbettungen eines vortrainierten CamemBERT-Modells ohne Optimierung für die Informationen-Retrieval-Aufgabe schlechte Ergebnisse liefert, was mit früheren Erkenntnissen übereinstimmt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 169, "src_audio": "/acl6060/audio/en/169.wav", "src_ref": "Furthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus stellen wir fest, dass der word2vec-basierte Bi-Encoder die fastText- und BERT-basierten Modelle signifikant übertrifft, was darauf hindeutet, dass vortrainierte Wort-Ebenen-Einbettungen vielleicht besser für die Aufgabe geeignet sind als Zeichen- oder Unterwort-Ebenen-Einbettungen, wenn sie direkt verwendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 170, "src_audio": "/acl6060/audio/en/170.wav", "src_ref": "Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Obwohl diese Ergebnisse vielversprechend sind, lassen sie im Vergleich zu einem erfahrenen Rechtsexperten auf ein großes Verbesserungspotenzial schließen, der früher oder später alle relevanten Artikel zu jeder Frage abrufen kann und somit perfekte Ergebnisse erzielt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 171, "src_audio": "/acl6060/audio/en/171.wav", "src_ref": "Let's conclude by discussing two limitations of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Abschließend möchte ich auf zwei Einschränkungen unseres Datensatzes eingehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 172, "src_audio": "/acl6060/audio/en/172.wav", "src_ref": "First, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Erstens ist der Artikelkorpus auf die Artikel der 32 herangezogenen belgischen Gesetzbücher beschränkt, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Dekreten, Richtlinien und Verordnungen fehlen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 173, "src_audio": "/acl6060/audio/en/173.wav", "src_ref": "During the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei der Erstellung des Datensatzes werden alle Verweise auf diese nicht erfassten Artikel ignoriert, was dazu führt, dass sich einige Fragen am Ende nur auf einen Bruchteil der ursprünglichen Anzahl der relevanten Artikel beziehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 174, "src_audio": "/acl6060/audio/en/174.wav", "src_ref": "This information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Information impliziert also, dass die Antwort, die in den übrigen relevanten Artikeln enthalten ist, unvollständig sein könnte, obwohl sie immer noch völlig angemessen ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 175, "src_audio": "/acl6060/audio/en/175.wav", "src_ref": "Second, we should note that not all legal questions can be answered with statutes alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zweitens sollten wir beachten, dass nicht alle rechtlichen Fragen allein mit Gesetzen beantwortet werden können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 176, "src_audio": "/acl6060/audio/en/176.wav", "src_ref": "For instance, the question, can I evict my tenants if they make too much noise?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel die Frage: „Kann ich meinen Mietern kündigen, wenn sie zu viel Lärm machen?“", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 177, "src_audio": "/acl6060/audio/en/177.wav", "src_ref": "Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Möglicherweise gibt es im Gesetz keine detaillierte Antwort, die eine bestimmte Lärmschwelle festlegt, ab der eine Räumung zulässig ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 178, "src_audio": "/acl6060/audio/en/178.wav", "src_ref": "Instead, the landlord should probably rely more on case law and find precedents similar to their current situation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Stattdessen sollte sich der Vermieter wahrscheinlich eher auf das Fallrecht stützen und Präzedenzfälle finden, die seiner aktuellen Situation ähnlich sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 179, "src_audio": "/acl6060/audio/en/179.wav", "src_ref": "For example, the tenants makes two parties a week until two AM.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel veranstalten die Mieter zwei Partys pro Woche bis zwei Uhr morgens.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 180, "src_audio": "/acl6060/audio/en/180.wav", "src_ref": "Hence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Einige Fragen eignen sich daher besser als andere für die Aufgabe des Retrievals von Gesetzesartikeln. Die Domäne der weniger geeigneten Fragen muss noch bestimmt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 181, "src_audio": "/acl6060/audio/en/181.wav", "src_ref": "We hope that our work sparks interest in developing practical and reliable statutory article retrieval models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir hoffen, dass unsere Arbeit das Interesse an der Entwicklung praktischer und zuverlässiger Modelle für das Retrieval von Gesetzesartikeln weckt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 182, "src_audio": "/acl6060/audio/en/182.wav", "src_ref": "That can help improve access to justice for all.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie kann dazu beitragen, den Zugang zur Justiz für alle zu verbessern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 183, "src_audio": "/acl6060/audio/en/183.wav", "src_ref": "You can check out our paper, dataset and code at the following links. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie können unser Paper, den Datensatz und die Gesetzbücher unter den folgenden Links einsehen. Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 184, "src_audio": "/acl6060/audio/en/184.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo, wir freuen uns, Ihnen unsere Arbeit über VALSE vorstellen zu können. Das ist ein Aufgaben-unabhängiger Benchmark, der für das Testen von Seh- und Sprachmodellen mit spezifischen sprachlichen Phänomenen konzipiert ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 185, "src_audio": "/acl6060/audio/en/185.wav", "src_ref": "Why did we do the trouble in setting up this benchmark?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Warum haben wir uns die Mühe gemacht, diesen Benchmark einzurichten?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 186, "src_audio": "/acl6060/audio/en/186.wav", "src_ref": "Well, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In den letzten Jahren haben wir eine explosionsartige Zunahme von Transformer-basierten Seh- und Sprachmodellen erlebt, die mit großen Mengen an Bild-Text-Paaren vortrainiert wurden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 187, "src_audio": "/acl6060/audio/en/187.wav", "src_ref": "Each one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jedes dieser Modelle treibt den Fortschritt bei Seh- und Sprachaufgaben voran, wie z. B. die visuelle Fragenbeantwortung, das Argumentieren mit visuellem Menschenverstand, Bild-Retrieval, Phrasen-Erdung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 188, "src_audio": "/acl6060/audio/en/188.wav", "src_ref": "So we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben also die Nachricht erhalten, dass die Genauigkeiten bei diesen Aufgaben und spezifischen Benchmarks stetig steigen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 189, "src_audio": "/acl6060/audio/en/189.wav", "src_ref": "But do we know what the models have actually learned?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wissen wir, was die Modelle tatsächlich gelernt haben?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 190, "src_audio": "/acl6060/audio/en/190.wav", "src_ref": "What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Was hat ein Seh- und Sprach-Transformer verstanden, wenn er für dieses Bild und diesen Satz eine hohe Punktzahl zuweist?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 191, "src_audio": "/acl6060/audio/en/191.wav", "src_ref": "And the low score for this one?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und hier eine niedrige Punktzahl?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 192, "src_audio": "/acl6060/audio/en/192.wav", "src_ref": "Do vision and language models focus on the right thing?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Konzentrieren sich Seh- und Sprachmodelle auf die wesentlichen Punkte?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 193, "src_audio": "/acl6060/audio/en/193.wav", "src_ref": "Or do they focus on biases as shown by previous work?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Oder konzentrieren sie sich auf Verzerrungen, wie in früheren Arbeiten aufgezeigt wurde?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 194, "src_audio": "/acl6060/audio/en/194.wav", "src_ref": "To shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um diesen Aspekt näher zu beleuchten, schlagen wir eine eher aufgabenbezogene agnostische Richtung vor und führen VALSE ein, das die Empfindlichkeit von Seh- und Sprachmodellen für spezifische sprachliche Phänomene testet, die sowohl die sprachlichen als auch die visuellen Modalitäten betreffen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 195, "src_audio": "/acl6060/audio/en/195.wav", "src_ref": "We target existence, plurality, counting, spatial relations, actions and entity coreference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir zielen auf Existenz, Pluralität, Zählung, räumliche Beziehungen, Handlungen und Entity-Koreferenz ab.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 196, "src_audio": "/acl6060/audio/en/196.wav", "src_ref": "But how do we test whether the vision and language models have captured this phenomena?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Doch wie lässt sich überprüfen, ob die Seh- und Sprachmodelle dieses Phänomen erfasst haben?", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 197, "src_audio": "/acl6060/audio/en/197.wav", "src_ref": "By foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Verfälschen einer Methode wurde zuvor von Ravi Shekhar und Mitarbeitern nur für Substantivphrasen in den Seh- und Sprachmodellen und beim Zählen von uns in früheren Arbeiten angewandt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 198, "src_audio": "/acl6060/audio/en/198.wav", "src_ref": "Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Verfälschen bedeutet im Grunde, dass wir die Beschriftung eines Bildes nehmen und eine Verfälschung erzeugen, indem wir die Beschriftung so verändern, dass sie das Bild nicht mehr beschreibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 199, "src_audio": "/acl6060/audio/en/199.wav", "src_ref": "And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir führen diese Veränderungen der Phrase durch, indem wir uns auf sechs spezifische Teile konzentrieren, wie Existenz, Pluralität, Zählung, räumliche Relationen, Handlungen und Entität-Koreferenz, wobei jeder Teil aus einem oder mehreren Instrumenten bestehen kann, falls wir mehr als einen interessanten Weg gefunden haben, um Verfälschungsinstanzen zu erzeugen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 200, "src_audio": "/acl6060/audio/en/200.wav", "src_ref": "For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zum Beispiel haben wir im Fall des Handlungsteils zwei Instrumente, eines, bei dem das Handlungsverb durch eine andere Handlung ersetzt wird, und eines, bei dem die Aktanten ausgetauscht werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 201, "src_audio": "/acl6060/audio/en/201.wav", "src_ref": "Counting and coreference also are pieces that have more than one instrument.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zählung und Koreferenz sind auch Teile, die mehr als ein Instrument haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 202, "src_audio": "/acl6060/audio/en/202.wav", "src_ref": "And we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir erstellen diese Verfälschungen, indem wir sicherstellen, dass sie das Bild nicht beschreiben, dass sie grammatikalisch richtig sind und auch korrekte Sätze bilden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 203, "src_audio": "/acl6060/audio/en/203.wav", "src_ref": "This is not easy to do because a foiled caption may be less likely than the original caption.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das ist nicht einfach zu bewerkstelligen, denn eine verfälschte Beschriftung ist weniger wahrscheinlich als die ursprüngliche Beschriftung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 204, "src_audio": "/acl6060/audio/en/204.wav", "src_ref": "For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist zwar zum Beispiel nicht unmöglich, aber statistisch gesehen ist es unwahrscheinlicher, dass Pflanzen einen Menschen schneiden, als dass ein Mensch Pflanzen schneidet. Große Seh- und Sprachmodelle könnten dies erkennen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 205, "src_audio": "/acl6060/audio/en/205.wav", "src_ref": "Therefore, to obtain valid foils, we must take action.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher müssen wir handeln, wenn wir gültige Verfälschungen erhalten wollen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 206, "src_audio": "/acl6060/audio/en/206.wav", "src_ref": "First, we make use of strong language models to propose foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Erstens nutzen wir starke Sprachmodelle, um Verfälschungen vorzuschlagen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 207, "src_audio": "/acl6060/audio/en/207.wav", "src_ref": "Second, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zweitens verwenden wir die Inferenz der natürlichen Sprache oder kurz NLI, um Verfälschungen herauszufiltern, die das Bild noch beschreiben könnten. Bei der Konstruktion von Verfälschungen müssen wir sicherstellen, dass sie das Bild nicht beschreiben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 208, "src_audio": "/acl6060/audio/en/208.wav", "src_ref": "To test this automatically, we apply natural language inference with the following rationale.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um dies automatisch zu testen, wenden wir die Inferenz der natürlichen Sprache mit der folgenden Logik an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 209, "src_audio": "/acl6060/audio/en/209.wav", "src_ref": "We consider an image to be the premise and its caption its entailed hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir betrachten ein Bild als die Prämisse und seine Beschriftung als die daraus abgeleitete Hypothese.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 210, "src_audio": "/acl6060/audio/en/210.wav", "src_ref": "In addition, we consider the caption to be the premise, and the foil is its hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus betrachten wir die Beschriftung als Prämisse und die Verfälschung als ihre Hypothese.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 211, "src_audio": "/acl6060/audio/en/211.wav", "src_ref": "If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn ein NLI-Modell vorhersagt, dass die Verfälschung der Beschriftung widerspricht oder neutral ist, ist das für uns ein Indikator für eine gültige Verfälschung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 212, "src_audio": "/acl6060/audio/en/212.wav", "src_ref": "If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn ein NLI vorhersagt, dass die Verfälschung die Beschriftung beinhaltet, kann es keine gute Verfälschung sein, da sie durch Transitivität eine wahrheitsgemäße Beschreibung des Bildes liefert. Wir filtern diese Verfälschungen heraus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 213, "src_audio": "/acl6060/audio/en/213.wav", "src_ref": "But this procedure is not perfect, it is just an indicator for valid foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieses Verfahren ist aber nicht perfekt, sondern nur ein Indikator für gültige Verfälschungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 214, "src_audio": "/acl6060/audio/en/214.wav", "src_ref": "Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher setzen wir als dritte Maßnahme zur Generierung gültiger Verfälschungen menschliche Annotatoren ein, um die in VALSE verwendeten Daten zu validieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 215, "src_audio": "/acl6060/audio/en/215.wav", "src_ref": "So, after filtering and human evaluation, we have as many test instances as described in this table.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nach dem Filtern und der menschlichen Evaluation haben wir also so viele Testinstanzen, wie in dieser Tabelle beschrieben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 216, "src_audio": "/acl6060/audio/en/216.wav", "src_ref": "Note that VALSE does not deliver any training data but only test data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Beachten Sie hier, dass VALSE keine Trainingsdaten, sondern nur Testdaten liefert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 217, "src_audio": "/acl6060/audio/en/217.wav", "src_ref": "Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da es sich um einen reinen Zero-Shot-Test-Benchmark handelt, ist er so konzipiert, dass er die bestehenden Fähigkeiten der Seh- und Sprachmodelle nach dem Vortraining nutzt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 218, "src_audio": "/acl6060/audio/en/218.wav", "src_ref": "Finetuning would only enable models to exploit artifacts or statistical biases in the data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Feinabstimmung würde es den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 219, "src_audio": "/acl6060/audio/en/219.wav", "src_ref": "And we all know that these models like to cheat and take shortcuts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir alle wissen, dass diese Modells gerne schummeln und Abkürzungen nehmen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 220, "src_audio": "/acl6060/audio/en/220.wav", "src_ref": "And as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wie wir schon erwähnten, sind wir an der Bewertung interessiert, welche Fähigkeiten die Seh- und Sprachmodelle nach dem Vortraining haben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 221, "src_audio": "/acl6060/audio/en/221.wav", "src_ref": "We experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir experimentieren mit fünf Seh- und Sprachmodellen auf VALSE, nämlich mit CLIP, LXMert, ViLBERT, ViLBERT zwölf in einem und VisualBERT.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 222, "src_audio": "/acl6060/audio/en/222.wav", "src_ref": "Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zwei unserer wichtigsten Evaluationsmetriken sind die Genauigkeit der Modelle bei der Klassifikation von Bild-Satz-Paaren in Bildbeschreibungen und Verfälschungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 223, "src_audio": "/acl6060/audio/en/223.wav", "src_ref": "Perhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielleicht ist es für dieses Video relevanter, unsere tolerantere Metrik vorzustellen, die paarweise Genauigkeit. Diese misst, ob die Bild-Satz-Ausrichtung für das korrekte Bild-Text-Paar größer ist als für sein Verfälschungspaar.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 224, "src_audio": "/acl6060/audio/en/224.wav", "src_ref": "For more metrics and results on them, do check out our paper.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Weitere Metriken und Ergebnisse dazu finden Sie in unserem Paper.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 225, "src_audio": "/acl6060/audio/en/225.wav", "src_ref": "The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Ergebnisse mit paarweiser Genauigkeit werden hier gezeigt und stimmen mit den Ergebnissen überein, die wir von den anderen Metriken erhalten haben, nämlich dass die beste Zero-Shot-Leistung von ViLBERT zwölf zu eins erreicht wird, gefolgt von ViLBERT, LXMert, CLIP und schließlich VisualBERT.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 226, "src_audio": "/acl6060/audio/en/226.wav", "src_ref": "It's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist bemerkenswert, wie Instrumente, die sich auf die einzelnen Objekte konzentrieren, wie Existenz und Substantivphrasen, von ViLBERT zwölf zu eins fast gelöst werden. Das unterstreicht, dass Modelle in der Lage sind, benannte Objekte und ihre Existenz in Bildern zu identifizieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 227, "src_audio": "/acl6060/audio/en/227.wav", "src_ref": "However, none of the remaining pieces can be reliably solved in our adversarial foiling settings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Keines der verbleibenden Teile kann jedoch in unseren gegnerischen Verfälschungseinstellungen zuverlässig gelöst werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 228, "src_audio": "/acl6060/audio/en/228.wav", "src_ref": "We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir sehen an den Instrumenten der Pluralität und der Zählung, dass Seh- und Sprachmodelle Schwierigkeiten haben, Verweise auf einzelne und mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 229, "src_audio": "/acl6060/audio/en/229.wav", "src_ref": "The relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Relationen-Teil zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Relation zwischen Objekten in einem Bild richtig zu klassifizieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 230, "src_audio": "/acl6060/audio/en/230.wav", "src_ref": "They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie durch Plausibilitätsbias unterstützt werden, wie wir bei dem Handlung-Teil sehen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 231, "src_audio": "/acl6060/audio/en/231.wav", "src_ref": "From the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Beim Koreferenz-Teil geht hervor, dass die Verfolgung mehrerer Verweise auf dasselbe Objekt in einem Bild unter Verwendung von Pronomen auch für die Seh- und Sprachmodelle schwierig ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 232, "src_audio": "/acl6060/audio/en/232.wav", "src_ref": "As a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zur Überprüfung der Korrektheit und weil es ein interessantes Experiment ist, vergleichen wir auch zwei reine Textmodelle, GPT 1 und GPT 2. So wollen wir feststellen, ob VALSE durch diese unimodalen Modelle lösbar ist. Wir berechnen die Perplexität der korrekten und der verfälschten Beschriftung, ohne Bild, um den Eintrag mit der niedrigsten Perplexität vorherzusagen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 233, "src_audio": "/acl6060/audio/en/233.wav", "src_ref": "If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die Perplexität bei der Verfälschung höher ist, ist das für uns ein Hinweis, dass die verfälschte Beschriftung möglicherweise unter Plausibilitätsverzerrungen oder anderen sprachlichen Verzerrungen leidet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 234, "src_audio": "/acl6060/audio/en/234.wav", "src_ref": "And it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist interessant zu sehen, dass in einigen Fällen die GPT-Modelle mit reinem Text die Plausibilität der Welt besser erfasst haben als die Seh- und Sprachmodelle.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 235, "src_audio": "/acl6060/audio/en/235.wav", "src_ref": "So to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zusammenfassend lässt sich sagen, dass VALSE ein Benchmark ist, der durch das Objektiv von linguistischen Konstrukten schaut, um die Gemeinschaft bei der Verbesserung von Seh- und Sprachmodellen zu unterstützen, indem er ihre visuelle Erdung hart testet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 236, "src_audio": "/acl6060/audio/en/236.wav", "src_ref": "Our experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere Experimente zeigen, dass Seh- und Sprachmodelle benannte Objekte und ihre Existenz in Bildern gut erkennen, wie der Existenzteil zeigt. Sie haben aber Schwierigkeiten damit, ihre gegenseitige Abhängigkeit und Beziehungen in visuellen Szenen zu begründen, wenn sie gezwungen sind, sprachliche Indikatoren zu berücksichtigen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 237, "src_audio": "/acl6060/audio/en/237.wav", "src_ref": "We would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir möchten die Community wirklich dazu ermutigen, VALSE für die Messung des Fortschritts von Sprach- Erdung mit Seh- und Sprachmodellen zu nutzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 238, "src_audio": "/acl6060/audio/en/238.wav", "src_ref": "And even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zudem könnte VALSE als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten. So kann kontrolliert werden, ob ein Datensatz dazu beiträgt, dass sich Modelle in einem der von VALSE getesteten Aspekte verbessern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 239, "src_audio": "/acl6060/audio/en/239.wav", "src_ref": "If you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn Sie Interesse haben, schauen Sie sich die VALSE-Daten auf GitHub an. Wenn Sie irgendwelche Fragen haben, zögern Sie nicht, uns zu kontaktieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 240, "src_audio": "/acl6060/audio/en/240.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo, mein Name ist Kamezawa von der Universität Tokio.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 241, "src_audio": "/acl6060/audio/en/241.wav", "src_ref": "I'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde folgendes Paper vorstellen: RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 242, "src_audio": "/acl6060/audio/en/242.wav", "src_ref": "I'll be explaining in this order.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde es in dieser Reihenfolge erklären.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 243, "src_audio": "/acl6060/audio/en/243.wav", "src_ref": "First, I will introduce automatic release note generation that we are working on in this research.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst werde ich die automatische Generierung von Versionshinweisen vorstellen, an der wir in dieser Forschung arbeiten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 244, "src_audio": "/acl6060/audio/en/244.wav", "src_ref": "A release note is a technical document that summarizes the changes distributed with each release of a software product.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein Versionshinweis ist ein technisches Dokument, das die Änderungen zusammenfasst, die mit jeder Version eines Softwareprodukts vertrieben werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 245, "src_audio": "/acl6060/audio/en/245.wav", "src_ref": "The image shows a release note for version two point six point four of the vuejs library.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Bild zeigt einen Versionshinweis für Version 2.6.4 der Vuejs-Bibliothek.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 246, "src_audio": "/acl6060/audio/en/246.wav", "src_ref": "Release notes play an important role in open source development but they're time consuming to prepare manually.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Versionshinweise spielen bei der Open-Source-Entwicklung eine wichtige Rolle, aber ihre Erstellung ist manuell zeitaufwändig.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 247, "src_audio": "/acl6060/audio/en/247.wav", "src_ref": "Therefore, it would be very useful to be able to automatically generate high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Daher wäre es sehr nützlich, wenn man automatisch qualitativ hochwertige Versionshinweise generieren könnte.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 248, "src_audio": "/acl6060/audio/en/248.wav", "src_ref": "I will defer to two previous researches on automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde zwei frühere Untersuchungen zur automatischen Generierung von Versionshinweisen erwähnen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 249, "src_audio": "/acl6060/audio/en/249.wav", "src_ref": "The first is a system called ARENA released in twenty fourteen.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zuerst gibt es ein System mit dem Namen ARENA, das im Jahr 2014 veröffentlicht wurde.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 250, "src_audio": "/acl6060/audio/en/250.wav", "src_ref": "It takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es verfolgt einen regelbasierten Ansatz, indem es zum Beispiel den Change-Extractor verwendet, um alle Unterschiede, Bibliotheksänderungen und Dokumentänderungen aus den verschiedenen Versionen zu extrahieren und sie schließlich zu kombinieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 251, "src_audio": "/acl6060/audio/en/251.wav", "src_ref": "The most notable feature of this system is the issue extractor in the upper right corner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die auffälligste Funktion dieses Systems ist der Ausgabe-Extraktor in der oberen rechten Ecke.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 252, "src_audio": "/acl6060/audio/en/252.wav", "src_ref": "Which must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies muss Jira, dem Fehlerverwaltungssystem, überlassen werden und kann nur auf Projekte angewendet werden, die Jira verwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 253, "src_audio": "/acl6060/audio/en/253.wav", "src_ref": "In other words, it cannot be used for many projects on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mit anderen Worten: Es kann nicht für viele Projekte auf GitHub verwendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 254, "src_audio": "/acl6060/audio/en/254.wav", "src_ref": "The second is Glyph, recently announced in twenty twenty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das zweite ist Glyph, das kürzlich in 2020 angekündigt wurde.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 255, "src_audio": "/acl6060/audio/en/255.wav", "src_ref": "It is available on the internet and can be installed via pip.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist im Internet verfügbar und kann über pip installiert werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 256, "src_audio": "/acl6060/audio/en/256.wav", "src_ref": "This system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieses System verfügt über ein einfaches, lernbasiertes Textklassifikationsmodell und gibt eine von fünf Markierung wie Funktionen oder Fehlerbehebungen für jede eingegebene Commit-Nachricht aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 257, "src_audio": "/acl6060/audio/en/257.wav", "src_ref": "This image is a sample usage that returns a corrective or bug fixes label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieses Bild ist eine Stichprobe, das eine Markierung für eine Korrektur oder Fehlerbehebung zurückgibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 258, "src_audio": "/acl6060/audio/en/258.wav", "src_ref": "Glyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Trainingsdaten von Glyph sind ziemlich klein, etwa 5.000, und werden in den unten beschriebenen Experimenten gezeigt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 259, "src_audio": "/acl6060/audio/en/259.wav", "src_ref": "The performance of the text classification model is not high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Leistung des Textklassifikationsmodells ist nicht hoch.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 260, "src_audio": "/acl6060/audio/en/260.wav", "src_ref": "I present two related researches, but their problems are limited applicability and scarce data resources.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich stelle zwei verwandte Forschungsarbeiten vor, deren Probleme jedoch die begrenzte Anwendbarkeit und die knappen Datenressourcen sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 261, "src_audio": "/acl6060/audio/en/261.wav", "src_ref": "Our paper solves these two problems and automatically generates high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Paper löst diese zwei Probleme und erzeugt automatisch qualitativ hochwertige Versionshinweise.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 262, "src_audio": "/acl6060/audio/en/262.wav", "src_ref": "With a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aufgrund des Problems der begrenzten Anwendbarkeit schlagen wir eine qualitativ hochwertige, klassenweise Zusammenfassungsmethode vor, die nur Commit-Nachrichten als Eingabe verwendet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 263, "src_audio": "/acl6060/audio/en/263.wav", "src_ref": "This proposed method can be used for all English repositories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 264, "src_audio": "/acl6060/audio/en/264.wav", "src_ref": "For the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Für das zweite Problem der knappen Datenressourcen haben wir unseren RNSum-Datensatz erstellt, der aus etwa 82.000 Daten besteht. Diese wurden aus den Daten der öffentlichen GitHub-Repositories mithilfe der GitHub-API gesammelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 265, "src_audio": "/acl6060/audio/en/265.wav", "src_ref": "Next, I'll describe our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Nächstes werde ich unseren Datensatz beschreiben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 266, "src_audio": "/acl6060/audio/en/266.wav", "src_ref": "Here is an example of data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist ein Beispiel für die Daten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 267, "src_audio": "/acl6060/audio/en/267.wav", "src_ref": "The left side is a commit message and the right side is the release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auf der linken Seite ist eine Commit-Nachricht und auf der rechten Seite sind die Versionshinweise.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 268, "src_audio": "/acl6060/audio/en/268.wav", "src_ref": "Release notes are labeled as improvements or fixes, etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Versionshinweise sind als Verbesserungen, Korrekturen usw. markiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 269, "src_audio": "/acl6060/audio/en/269.wav", "src_ref": "We have set up a task that takes the commit messages as input and outputs a labeled release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben eine Aufgabe eingerichtet, die die Commit-Nachricht als Eingabe aufnimmt und die Ausgabe ist ein markierter Versionshinweis.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 270, "src_audio": "/acl6060/audio/en/270.wav", "src_ref": "This can be regarded as a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies kann als eine Zusammenfassungsaufgabe betrachtet werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 271, "src_audio": "/acl6060/audio/en/271.wav", "src_ref": "We have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben vier Markierungen vordefiniert: Funktionen, Verbesserungen, Fehlerbehebungen, Beseitigung von Verwerfungen und bahnbrechende Änderungen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 272, "src_audio": "/acl6060/audio/en/272.wav", "src_ref": "These were set based on previous research and other factors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese wurden auf der Grundlage von früheren Forschungen und anderen Faktoren festgelegt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 273, "src_audio": "/acl6060/audio/en/273.wav", "src_ref": "The release note on the bottom right is extracted from the release note on the bottom left.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Versionshinweise unten rechts wurde aus den Versionshinweisen unten links extrahiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 274, "src_audio": "/acl6060/audio/en/274.wav", "src_ref": "At this time, it is necessary to detect the four labels that have been set up in advance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zu diesem Zeitpunkt müssen die vier Markierungen, die im Voraus erstellt wurden, erkannt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 275, "src_audio": "/acl6060/audio/en/275.wav", "src_ref": "But the labels are not always consistent with each repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Markierungen sind jedoch nicht immer mit jedem Repository konsistent.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 276, "src_audio": "/acl6060/audio/en/276.wav", "src_ref": "For example, the improvements label includes improvements, enhancements, optimizations, and so on.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Markierung „Verbesserungen“ umfasst beispielsweise Verbesserungen, Erweiterungen, Optimierungen und so weiter.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 277, "src_audio": "/acl6060/audio/en/277.wav", "src_ref": "We prepared a vocabulary list of about thirty labels for each of these notational variations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben eine Wortschatzliste mit etwa 30 Markierungen für jede dieser Notationsvarianten erstellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 278, "src_audio": "/acl6060/audio/en/278.wav", "src_ref": "This is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese dient dazu, die Klasse des Versionshinweises zu erkennen. Sie sammelt den Text des Versionshinweises, der als Versionshinweissatz für die Klasse folgt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 279, "src_audio": "/acl6060/audio/en/279.wav", "src_ref": "Next is a commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Nächstes folgt eine Commit-Nachricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 280, "src_audio": "/acl6060/audio/en/280.wav", "src_ref": "Commit messages are not tied to each release.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Commit-Nachrichten sind nicht an die einzelnen Versionen gebunden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 281, "src_audio": "/acl6060/audio/en/281.wav", "src_ref": "As shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die aktuelle Version 2.5 bis 19 ist, müssen wir, wie im Bild unten gezeigt, die frühere Version 2.5 bis 18 identifizieren und eine Diff erhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 282, "src_audio": "/acl6060/audio/en/282.wav", "src_ref": "This is a bit tedious and it is not enough to just get a list of releases and look at the before and after.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies ist etwas mühsam und es reicht nicht aus, nur eine Liste von Versionen zu erstellen und die Vorher-Nachher-Bilanz zu betrachten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 283, "src_audio": "/acl6060/audio/en/283.wav", "src_ref": "We created a heuristic matching rule to get the previous and next versions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben eine heuristische Abgleichsregel erstellt, um die vorherige und die nächste Version zu bekommen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 284, "src_audio": "/acl6060/audio/en/284.wav", "src_ref": "Dataset analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Analyse des Datensatzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 285, "src_audio": "/acl6060/audio/en/285.wav", "src_ref": "In the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Am Ende wurden 7.200 Repositories und 82 Daten gesammelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 286, "src_audio": "/acl6060/audio/en/286.wav", "src_ref": "Also, the average number of release notes tokens is sixty three, which is quite high for a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Außerdem liegt die durchschnittliche Anzahl der Versionshinweis-Token bei 63, was für eine Zusammenfassungsaufgabe recht hoch ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 287, "src_audio": "/acl6060/audio/en/287.wav", "src_ref": "Also, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Auch die Anzahl der einzigartigen Token ist mit 830.000 recht groß.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 288, "src_audio": "/acl6060/audio/en/288.wav", "src_ref": "This is due to the large number of unique class or method names found in the repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dies ist auf die große Anzahl von eindeutigen Klassen- oder Methodennamen im Repository zurückzuführen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 289, "src_audio": "/acl6060/audio/en/289.wav", "src_ref": "Next, I will explain the proposed method.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Nächstes werde ich die vorgeschlagene Methode erläutern.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 290, "src_audio": "/acl6060/audio/en/290.wav", "src_ref": "The classwise extractive then abstractive summarization model consists of two neural modules.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das klassenweise extraktive und dann abstrahierende Zusammenfassungsmodell besteht aus zwei neuronalen Modulen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 291, "src_audio": "/acl6060/audio/en/291.wav", "src_ref": "A classifier using BERT or CodeBERT and a generator using BART.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein Klassifikator verwendet BERT oder CodeBERT und ein Generator verwendet BART.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 292, "src_audio": "/acl6060/audio/en/292.wav", "src_ref": "First, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zuerst verwendet CEAS einen Klassifikator, um jede Commit-Nachricht in fünf Klassen von Versionshinweisen zu klassifizieren, die Verbesserungen, Fehlerbehebungen, Verwerfungen und eine weitere Klasse enthalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 293, "src_audio": "/acl6060/audio/en/293.wav", "src_ref": "The commit messages classified as other are discarded.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die als „Sonstiges“ eingestuften Commit-Nachrichten werden verworfen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 294, "src_audio": "/acl6060/audio/en/294.wav", "src_ref": "Then CEAS applies the generator to the four labeled documents independently and generates release notes for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann wendet CEAS den Generator auf die vier markierten Dokumente unabhängig voneinander an und erstellt Versionshinweise für jede Klasse.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 295, "src_audio": "/acl6060/audio/en/295.wav", "src_ref": "In this task, the direct correspondences between commit messages and release notes are not known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In dieser Aufgabe sind die direkten Korrespondenzen zwischen Commit-Nachrichten und Versionshinweisen nicht bekannt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 296, "src_audio": "/acl6060/audio/en/296.wav", "src_ref": "Therefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um den Klassifikator zu trainieren, haben wir daher jeder eingegebenen Commit-Nachricht eine Umfrage zugewiesen und dabei die ersten zehn Zeichen jeder Commit-Nachricht verwendet.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 297, "src_audio": "/acl6060/audio/en/297.wav", "src_ref": "We modeled the classwise abstractive summarization approach by two different methods.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben den klassenweisen abstrakten Zusammenfassungsansatz durch zwei verschiedene Methoden modelliert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 298, "src_audio": "/acl6060/audio/en/298.wav", "src_ref": "The first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das erste Modell, das wir CAS-Single nennen, besteht aus einem einzigen 6-zu-6-Netzwerk und generiert einen einzigen Versionshinweistext, der eine Verkettung von eingegebenen Commit-Nachrichten ergibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 299, "src_audio": "/acl6060/audio/en/299.wav", "src_ref": "The output texts can be divided into classwise segments based on special class-specific endpoint symbols.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die ausgegebenen Texte können auf der Grundlage spezieller klassenspezifischer Endpunktsymbole in klassenweise Segmente unterteilt werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 300, "src_audio": "/acl6060/audio/en/300.wav", "src_ref": "The second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die zweite Methode, die wir CAS-Multi nennen, besteht aus vier verschiedenen seq2seq-Netzwerken, von denen jedes einer der festen Versionshinweisklassen entspricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 301, "src_audio": "/acl6060/audio/en/301.wav", "src_ref": "Okay, let me explain the experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde nun die Experimente erklären.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 302, "src_audio": "/acl6060/audio/en/302.wav", "src_ref": "Five methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Fünf Methoden wurden verglichen: CEAS, CAS-Single, CAS-Multi, Clustering und Glyph aus einer früheren Studie.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 303, "src_audio": "/acl6060/audio/en/303.wav", "src_ref": "Regarding evaluation, in some cases, release notes are output in multiple sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Was die Evaluation betrifft, so werden in einigen Fällen die Versionshinweise in mehreren Sätzen ausgegeben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 304, "src_audio": "/acl6060/audio/en/304.wav", "src_ref": "Since it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da es schwierig ist, die Anzahl von Sätzen zu berechnen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 305, "src_audio": "/acl6060/audio/en/305.wav", "src_ref": "The BLEU is penalized when the system outputs a short sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der BLEU wird benachteiligt, wenn das System einen kurzen Satz ausgibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 306, "src_audio": "/acl6060/audio/en/306.wav", "src_ref": "This penalty results in a lower BLEU value in the experiment results described next.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dieser Abzug führt zu einem niedrigeren BLEU-Wert in den nachfolgend beschriebenen Versuchsergebnissen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 307, "src_audio": "/acl6060/audio/en/307.wav", "src_ref": "Finally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schließlich berechnen wir auch die Spezifität, da ROUGE und BLEU nicht berechnet werden können, wenn die Versionshinweise leer sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 308, "src_audio": "/acl6060/audio/en/308.wav", "src_ref": "A higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eine höhere Spezifität bedeutet, dass das Modell in Fällen, in denen die Versionshinweise leer sind, einen leeren Text korrekt ausgibt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 309, "src_audio": "/acl6060/audio/en/309.wav", "src_ref": "Here are the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind die Resultate.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 310, "src_audio": "/acl6060/audio/en/310.wav", "src_ref": "Since the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da der Datensatz E-Mail-Adressen, Hash-Werte usw. enthält, haben wir auch den bereinigten Datensatz ausgewertet, der diese ausschließt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 311, "src_audio": "/acl6060/audio/en/311.wav", "src_ref": "CEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CEAS und CAS erreichten ROUGE-L-Werte, die mehr als zehn Punkte über den Baselines lagen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 312, "src_audio": "/acl6060/audio/en/312.wav", "src_ref": "In particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Insbesondere beim bereinigten Testsatz ist der Abstand zwischen der vorgeschlagenen Methode und den Baselines auf mehr als zwanzig Punkte angestiegen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 313, "src_audio": "/acl6060/audio/en/313.wav", "src_ref": "These results indicate that CEAS and CAS are significantly affected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Diese Ergebnisse zeigen, dass CEAS und CAS signifikant betroffen sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 314, "src_audio": "/acl6060/audio/en/314.wav", "src_ref": "CEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CEAS erzielte bessere ROUGE-L-Werte als CAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators beim Training des Klassifikators mit Pseudo-Markierungen effektiv ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 315, "src_audio": "/acl6060/audio/en/315.wav", "src_ref": "High coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eine hohe Abdeckung von CEAS kann wahrscheinlich erreicht werden, weil sich der Klassifikator auf die Auswahl relevanter Commit-Nachrichten für jede Klasse konzentrieren kann.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 316, "src_audio": "/acl6060/audio/en/316.wav", "src_ref": "CAS-Multi tended to yield higher ROUGE-L than CAS-Single.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CAS-Multi ergab tendenziell mehr ROUGE-L-Werte als CAS-Single.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 317, "src_audio": "/acl6060/audio/en/317.wav", "src_ref": "Suggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir schlagen vor, dass es auch effektiv ist, unterschiedliche abstrakte Zusammenfassungsmodelle, die unabhängig voneinander sind, für jede Klasse von den Versionshinweisen zu entwickeln.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 318, "src_audio": "/acl6060/audio/en/318.wav", "src_ref": "Here are an error analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist eine Fehleranalyse.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 319, "src_audio": "/acl6060/audio/en/319.wav", "src_ref": "CAS methods tend to output shorter sentences than human reference sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "CAS-Methoden geben tendenziell kürzere Sätze aus als menschliche Referenzsätze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 320, "src_audio": "/acl6060/audio/en/320.wav", "src_ref": "In the figure on the right, the reference sentence has three or four sentences, while CAS has only one.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In der Abbildung rechts besteht der Referenzsatz aus drei oder vier Sätzen, während CAS aus nur einem besteht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 321, "src_audio": "/acl6060/audio/en/321.wav", "src_ref": "The reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Grund für die Reluktanz dieses Modells ist, dass in den Trainingsdaten nur 33 Prozent der Sätze in der Markierung Funktionen und 40 Prozent in der Markierung Verbesserung zu finden sind.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 322, "src_audio": "/acl6060/audio/en/322.wav", "src_ref": "Furthermore, CAS methods cannot generate accurate release notes without additional information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Darüber hinaus können CAS-Methoden ohne zusätzliche Informationen keine genauen Versionshinweise erstellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 323, "src_audio": "/acl6060/audio/en/323.wav", "src_ref": "The top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Oben auf der rechten Seite ist ein Beispiel für eine sehr unübersichtliche Commit-Nachricht. Der vollständige Satz kann nicht ohne Bezug auf den entsprechenden Fortschritt oder das Problem generiert werden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 324, "src_audio": "/acl6060/audio/en/324.wav", "src_ref": "The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das folgende Beispiel zeigt, dass die beiden Commit-Nachrichten in der Eingabe zusammenhängen und zu einem Satz zusammengefasst werden sollten, was jedoch nicht gelingt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 325, "src_audio": "/acl6060/audio/en/325.wav", "src_ref": "Finally, a conclusion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nun kommen wir zum Fazit.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 326, "src_audio": "/acl6060/audio/en/326.wav", "src_ref": "We have built a new dataset for automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben einen neuen Datensatz für die automatische Generierung von Versionshinweisen erstellt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 327, "src_audio": "/acl6060/audio/en/327.wav", "src_ref": "We have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir haben auch eine Aufgabe für die Eingabe von Commit-Nachrichten und deren Zusammenfassung formuliert, sodass sie für alle auf Englisch geschriebene Projekte anwendbar ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 328, "src_audio": "/acl6060/audio/en/328.wav", "src_ref": "Our experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere Experimente zeigen, dass die vorgeschlagene Methode weniger verrauschte Versionshinweise mit höherem Abdeckungsgrad als die Baselines erzeugt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 329, "src_audio": "/acl6060/audio/en/329.wav", "src_ref": "Please check out our dataset on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bitte sehen Sie sich unseren Datensatz auf GitHub an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 330, "src_audio": "/acl6060/audio/en/330.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 331, "src_audio": "/acl6060/audio/en/331.wav", "src_ref": "Hello. My name is Asaf Harari.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hallo! Mein Name ist Asaf Harari.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 332, "src_audio": "/acl6060/audio/en/332.wav", "src_ref": "And I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ich werde unser Paper vorstellen: Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 333, "src_audio": "/acl6060/audio/en/333.wav", "src_ref": "Data scientists analyze data and mainly focus on the manipulating the data's existing features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Datenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der bestehenden Funktionen von Daten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 334, "src_audio": "/acl6060/audio/en/334.wav", "src_ref": "But sometimes, these features are limited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber manchmal sind diese Funktionen begrenzt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 335, "src_audio": "/acl6060/audio/en/335.wav", "src_ref": "Feature generation using another data source may add substantial information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Generierung von Funktionen unter Verwendung einer anderen Datenquelle kann wesentliche Informationen hinzufügen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 336, "src_audio": "/acl6060/audio/en/336.wav", "src_ref": "Our research goal is automatic tabular data enrichment using external sources' free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Forschungsziel ist die automatische Anreicherung tabellarischer Daten mit freiem Text aus externen Quellen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 337, "src_audio": "/acl6060/audio/en/337.wav", "src_ref": "Assume we have a tabular dataset and a knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Angenommen, wir haben einen tabellarischen Datensatz und eine Wissensbasis.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 338, "src_audio": "/acl6060/audio/en/338.wav", "src_ref": "We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir brauchen einen automatischen Prozess, der Entity Linking und eine Textanalyse umfasst, um neue Funktionen aus dem freien Text der Wissensbasis zu extrahieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 339, "src_audio": "/acl6060/audio/en/339.wav", "src_ref": "Our framework FeSTE is exactly this automatic process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Rahmen FeSTE ist genau dieser automatische Prozess.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 340, "src_audio": "/acl6060/audio/en/340.wav", "src_ref": "So let's see an example in a dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sehen wir uns also ein Beispiel in einem Datensatz an, der in FeSTE eingespeist wird.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 341, "src_audio": "/acl6060/audio/en/341.wav", "src_ref": "In this example, the dataset is university dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel ist der Datensatz der Universitätsdatensatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 342, "src_audio": "/acl6060/audio/en/342.wav", "src_ref": "When its goal is to classify universities into low ranking universities and high-ranking universities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Ziel besteht darin, die Universitäten nach niedrigem und hohem Rang einzuteilen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 343, "src_audio": "/acl6060/audio/en/343.wav", "src_ref": "As knowledge base, we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Wissensbasis verwenden wir Wikipedia.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 344, "src_audio": "/acl6060/audio/en/344.wav", "src_ref": "The first phase of FeSTE is entity linking.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die erste Phase von FeSTE ist Entity Linking.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 345, "src_audio": "/acl6060/audio/en/345.wav", "src_ref": "When each entity, in this example the university name, is linked to an entity within the knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jede Entität, in diesem Beispiel der Name der Universität, ist mit einer Entität innerhalb der Wissensbasis verknüpft.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 346, "src_audio": "/acl6060/audio/en/346.wav", "src_ref": "And and the text of the entities of the knowledge base is extracted and added to the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Text der Entitäten aus der Wissensbasis wird extrahiert und dem Datensatz hinzugefügt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 347, "src_audio": "/acl6060/audio/en/347.wav", "src_ref": "In this example, the text is the Wikipedia page's abstract.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel ist der Text das Abstract der Wikipedia-Seite.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 348, "src_audio": "/acl6060/audio/en/348.wav", "src_ref": "Now, we need to generate or extract features from the retrieved text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nun müssen wir Funktionen aus dem abgerufenen Text generieren oder extrahieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 349, "src_audio": "/acl6060/audio/en/349.wav", "src_ref": "So, we need to ah feature extraction phase ah which includes text analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir müssen also eine Phase der Funktionsextraktion einleiten, zu der die Textanalyse gehört.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 350, "src_audio": "/acl6060/audio/en/350.wav", "src_ref": "And this is the main novelty of this paper and I will deep dive into it in the next slides.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das ist die wichtigste Neuerung dieses Papers, auf die ich auf den nächsten Folien näher eingehen werde.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 351, "src_audio": "/acl6060/audio/en/351.wav", "src_ref": "After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nach der Phase der Funktionsextraktion folgt eine Phase der Funktionsgenerierung, in der wir die extrahierten Funktionen verwenden, um eine kleine Anzahl neuer Funktionen zu erzeugen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 352, "src_audio": "/acl6060/audio/en/352.wav", "src_ref": "First generate ah features in the number of classes of the original dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zunächst werden Funktionen in der Anzahl der Klassen des ursprünglichen Datensatzes generiert.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 353, "src_audio": "/acl6060/audio/en/353.wav", "src_ref": "In this example, the original dataset has two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 354, "src_audio": "/acl6060/audio/en/354.wav", "src_ref": "So, FeSTE generates two new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "FeSTE generiert also zwei neue Funktionen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 355, "src_audio": "/acl6060/audio/en/355.wav", "src_ref": "But if the dataset has five classes, FeSTE generates five new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn der Datensatz jedoch fünf Klassen hat, generiert FeSTE fünf neue Funktionen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 356, "src_audio": "/acl6060/audio/en/356.wav", "src_ref": "Each feature represents the likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jede Funktion stellt die Wahrscheinlichkeit für jede Klasse dar.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 357, "src_audio": "/acl6060/audio/en/357.wav", "src_ref": "To analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Um den Text zu analysieren, verwenden wir die aktuellste Textanalyse, die auf Transformer-basierten Sprachmodellen wie BERT, GPT, XLNet und anderen beruht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 358, "src_audio": "/acl6060/audio/en/358.wav", "src_ref": "It is but it is not likely that we can train language models using the input datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es ist aber unwahrscheinlich, dass wir Sprachmodelle mit den eingegebenen Datensätzen trainieren können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 359, "src_audio": "/acl6060/audio/en/359.wav", "src_ref": "So a naive approach will be ah target task finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ein naiver Ansatz ist also eine Feinabstimmung des Zieldatensatzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 360, "src_audio": "/acl6060/audio/en/360.wav", "src_ref": "So, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In der Phase der Funktionsextraktion können wir also vortrainierte Sprachmodelle herunterladen und das Sprachmodell über den Zieldatensatz feinabstimmen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 361, "src_audio": "/acl6060/audio/en/361.wav", "src_ref": "In this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel zur Feinabstimmung des Sprachmodells klassifizieren wir den Text in Klassen, d. h. wir klassifizieren das Abstract in die Klassen niedrig oder hoch.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 362, "src_audio": "/acl6060/audio/en/362.wav", "src_ref": "Receive the language model output, which is the likelihood for each class and use as new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir erhalten die Ausgabe des Sprachmodells, die die Wahrscheinlichkeit für jede Klasse darstellt, und verwenden sie als neue Funktionen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 363, "src_audio": "/acl6060/audio/en/363.wav", "src_ref": "The problem with this approach is datasets may have few distinct entities / texts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Problem bei diesem Ansatz ist, dass Datensätze möglicherweise nur wenige unterschiedliche Entitäten/Texte beinhalten.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 364, "src_audio": "/acl6060/audio/en/364.wav", "src_ref": "In our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem Experiment enthält fast die Hälfte der Datensätze weniger als 400 Stichproben und der kleinste Datensatz enthält 35 Stichproben in einem Trainingssatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 365, "src_audio": "/acl6060/audio/en/365.wav", "src_ref": "So to finetune a language model over ah this dataset will be ineffective.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Feinabstimmung eines Sprachmodells über diesen Datensatz ist also unwirksam.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 366, "src_audio": "/acl6060/audio/en/366.wav", "src_ref": "But we can use prior knowledge about pre-analyzed datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber wir können vorherige Kenntnisse über bereits analysierte Datensätze nutzen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 367, "src_audio": "/acl6060/audio/en/367.wav", "src_ref": "Because FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Da wir FeSTE auf mehrere Datensätze anwenden, können wir die „n minus 1“-Datensätze verwenden, um Informationen über die „n minus 1“-Datensätze zu sammeln, und diese Informationen verwenden, wenn wir den n-ten Datensatz analysieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 368, "src_audio": "/acl6060/audio/en/368.wav", "src_ref": "What we, what we suggest is to add, to add another finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir schlagen vor, eine weitere Feinabstimmungsphase hinzuzufügen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 369, "src_audio": "/acl6060/audio/en/369.wav", "src_ref": "A preliminary multitask finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das war eine vorläufige Multitask-Feinabstimmungsphase.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 370, "src_audio": "/acl6060/audio/en/370.wav", "src_ref": "When you finetune the language model over the n minus one datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die Feinabstimmung des Sprachmodells geht über die „n minus 1“-Datensätze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 371, "src_audio": "/acl6060/audio/en/371.wav", "src_ref": "And, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann führen wir eine weitere Feinabstimmungsphase durch, die eine Feinabstimmung des Zieldatensatzes ist, wenn wir das Sprachmodell über den n-ten Zieldatensatz feinabstimmen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 372, "src_audio": "/acl6060/audio/en/372.wav", "src_ref": "The state-of-the-art in multitask ah multitask finetuning called MTDNN.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Die modernste Multitask-Feinabstimmung wird MTDNN genannt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 373, "src_audio": "/acl6060/audio/en/373.wav", "src_ref": "In MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bei der MTDNN bleiben die Überschriften in der Reihe der Aufgaben im Trainingssatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 374, "src_audio": "/acl6060/audio/en/374.wav", "src_ref": "So, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In diesem Beispiel gibt es also vier Aufgaben im Trainingssatz. Das MTDNN enthält also vier Überschriften, wie Sie auf dem Bild sehen können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 375, "src_audio": "/acl6060/audio/en/375.wav", "src_ref": "And it samples a random batch from ah from the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es nimmt sich eine Zufallsstichprobe aus dem Trainingssatz heraus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 376, "src_audio": "/acl6060/audio/en/376.wav", "src_ref": "And if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die Zufallsstichprobe zum Beispiel zu einer einzelnen Satzklassifikationsaufgabe gehört, führt sie Vorwärts- und Rückwärtspfade durch die erste Überschrift aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 377, "src_audio": "/acl6060/audio/en/377.wav", "src_ref": "And if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn die Zufallsstichprobe zur paarweisen Ranking-Aufgabe gehört, führt sie den Vorwärts- und Rückwärtspfad über die letzte Überschrift aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 378, "src_audio": "/acl6060/audio/en/378.wav", "src_ref": "In our scenario, ah tabular datasets vary in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "In unserem Szenario variieren tabellarische Datensätze in der Anzahl der Klassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 379, "src_audio": "/acl6060/audio/en/379.wav", "src_ref": "So there are many tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es gibt also viele Aufgaben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 380, "src_audio": "/acl6060/audio/en/380.wav", "src_ref": "MTDNN maintained number of classes, heads, output layers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "MTDNN enthält eine Anzahl von Klassen, Überschriften und ausgegebenen Ebenen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 381, "src_audio": "/acl6060/audio/en/381.wav", "src_ref": "And the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und zusätzlich muss MTDNN neue Überschriften für einen neuen Datensatz mit einer neuen Aufgabe initialisieren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 382, "src_audio": "/acl6060/audio/en/382.wav", "src_ref": "Our approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unser Ansatz, genannt Feinabstimmungsreformulierungsaufgabe, besteht darin, dass wir, anstatt mehrere Überschriften beizubehalten, jeden Datensatz in einen Satz pro Klassifikationssproblem umformulieren, was zwei Klassenaufgaben entspricht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 383, "src_audio": "/acl6060/audio/en/383.wav", "src_ref": "So let's see an example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schauen wir uns ein Beispiel an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 384, "src_audio": "/acl6060/audio/en/384.wav", "src_ref": "Here is the our input dataset which consists of entities, features, text and classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier ist unser eingegebener Datensatz, der aus Entitäten, Funktionen, Text und Klassen besteht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 385, "src_audio": "/acl6060/audio/en/385.wav", "src_ref": "And, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir formulieren die Aufgabe von einer Klassifizierung des Textes in niedrig oder hoch zu einer Klassifizierung des Textes, des Abstracts und der Klasse in wahr oder falsch um.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 386, "src_audio": "/acl6060/audio/en/386.wav", "src_ref": "Or in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mit anderen Worten: Wir haben das Sprachmodell so trainiert, dass es ein Abstract und die Klasse als Abstract und die Klasse klassifiziert, wenn das Abstract zur Klasse gehört oder nicht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 387, "src_audio": "/acl6060/audio/en/387.wav", "src_ref": "So the label vector in this case stays always ah which consists always with two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "So bleibt die Vektor-Markierung in diesem Fall bestehen, die immer aus zwei Klassen besteht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 388, "src_audio": "/acl6060/audio/en/388.wav", "src_ref": "And this is the ah algorithm for our fine, reformulated finetuning approach.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das ist der Algorithmus für unseren neu formulierten Finabstimmungsansatz.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 389, "src_audio": "/acl6060/audio/en/389.wav", "src_ref": "So let's see the full framework.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Schauen wir uns also den gesamten Rahmen an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 390, "src_audio": "/acl6060/audio/en/390.wav", "src_ref": "Dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Der Datensatz wird in FeSTE eingespeist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 391, "src_audio": "/acl6060/audio/en/391.wav", "src_ref": "And then ah FeSTE executes entity linking phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann führt FeSTE die Entity-Linking-Phase aus.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 392, "src_audio": "/acl6060/audio/en/392.wav", "src_ref": "It ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es extrahiert den Text aus der Wissensbasis, die in diesem Beispiel das Abstract der Wikipedia-Seite ist.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 393, "src_audio": "/acl6060/audio/en/393.wav", "src_ref": "Then it reformulated the task into a pairwise sentence classification task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann formulierte es die Aufgabe in eine paarweise Satzklassifikationsaufgabe um.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 394, "src_audio": "/acl6060/audio/en/394.wav", "src_ref": "Applied the language model to the new task and the output likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Das Sprachmodell wird auf die neue Aufgabe und die Ausgabewahrscheinlichkeit für jede Klasse angewandt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 395, "src_audio": "/acl6060/audio/en/395.wav", "src_ref": "And now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nun ist das Sprachmodell bereits über den „n minus 1“-Datensatz mithilfe einer vorläufigen Multitask-Feinabstimmung feinabgestimmt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 396, "src_audio": "/acl6060/audio/en/396.wav", "src_ref": "Then we use the output vector of the language model as a newly generated feature in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann verwenden wir den Ausgabe-Vektor des Sprachmodells als eine neu generierte Funktion in der Anzahl der Klassen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 397, "src_audio": "/acl6060/audio/en/397.wav", "src_ref": "To evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Zur Evaluation unseres Rahmens verwenden wir 17 tabellarische Klassifikationsdatensätze, die sich in Größe, Funktionen, Ausgewogenheit, Domäne und anfänglicher Leistung unterscheiden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 398, "src_audio": "/acl6060/audio/en/398.wav", "src_ref": "And as knowledge base we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Als Wissensbasis verwenden wir Wikipedia.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 399, "src_audio": "/acl6060/audio/en/399.wav", "src_ref": "We design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir konzipieren unser Experiment als Leave-One-Out-Evaluation, bei dem wir FeSTe mit 16 Datensätzen trainieren und auf den 17. Datensatz anwenden.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 400, "src_audio": "/acl6060/audio/en/400.wav", "src_ref": "We also, we also split each dataset into four folds and apply four folds cross validation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Außerdem teilen wir jeden Datensatz in vier Brüche auf und wenden eine vierfache Kreuzvalidierung an.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 401, "src_audio": "/acl6060/audio/en/401.wav", "src_ref": "Then, we generate the new features and evaluate them using five evaluation classifiers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dann generieren wir die neuen Funktionen und bewerten sie mit fünf Evaluationssklassifikatoren.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 402, "src_audio": "/acl6060/audio/en/402.wav", "src_ref": "We use in our experiments base BERT base architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wir verwenden in unseren Experimenten die Basisarchitektur BERT.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 403, "src_audio": "/acl6060/audio/en/403.wav", "src_ref": "Here are the results for our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Hier sind die Ergebnisse unserer Experimente.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 404, "src_audio": "/acl6060/audio/en/404.wav", "src_ref": "You can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sie können sehen, dass wir unseren Rahmen mit der Feinabstimmung des Zieldatensatzes, mit der Feinabstimmung der Zielaufgabe und einer vorläufigen MTDNN-Feinabstimmung vergleichen.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 405, "src_audio": "/acl6060/audio/en/405.wav", "src_ref": "And our reformulated finetuning achieves the best result, the best performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Unsere neu formulierte Feinabstimmung erreicht das beste Ergebnis und die beste Leistung.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 406, "src_audio": "/acl6060/audio/en/406.wav", "src_ref": "While MTDNN achieved two percent improvement over the target dataset finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "MTDNN erreichte zwei Prozent Verbesserung gegenüber der Feinabstimmung des Zieldatensatzes.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 407, "src_audio": "/acl6060/audio/en/407.wav", "src_ref": "Our approach achieved six percent improvement.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mit unserem Ansatz haben wir eine Verbesserung von sechs Prozent erreicht.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 408, "src_audio": "/acl6060/audio/en/408.wav", "src_ref": "When we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Wenn wir uns den kleinen Datensatz anschauen, sehen wir, dass die Leistung von MTDNN abnimmt und die Verbesserung der vorläufigen Multitask-Feinabstimmungsphase auf 1,5 Prozent sinkt.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 409, "src_audio": "/acl6060/audio/en/409.wav", "src_ref": "But our performance increased to eleven percent compared to the target task finetuning alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber unsere Leistung stieg auf elf Prozent im Vergleich zur Feinabstimmung des Zieldatensatzes allein.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 410, "src_audio": "/acl6060/audio/en/410.wav", "src_ref": "For summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Für die Summierung ermöglicht FeSTE in unseren Experimenten eine Few-Shot-Anreicherung aus 35 Proben.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 411, "src_audio": "/acl6060/audio/en/411.wav", "src_ref": "It uses one architecture for all tasks and datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es verwendet eine Architektur für alle Aufgaben und Datensätze.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 412, "src_audio": "/acl6060/audio/en/412.wav", "src_ref": "And it keeps the head of ah of the model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Und es behält den Hauptteil des Modells.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 413, "src_audio": "/acl6060/audio/en/413.wav", "src_ref": "But it adds reformulation phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Aber es fügt eine Reformulierungsphase hinzu.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 414, "src_audio": "/acl6060/audio/en/414.wav", "src_ref": "It augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Es erweitert den Trainingssatz und benötigt einen Zielwert mit semantischer Bedeutung, damit wir ihn in das Sprachmodell einspeisen und für das Satzpaar-Klassifikationsproblem verwenden können.", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 415, "src_audio": "/acl6060/audio/en/415.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vielen Dank!", "tgt_lang": "de"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 416, "src_audio": "/acl6060/audio/en/416.wav", "src_ref": "Hello. My name is Asaf Harari.\nAnd I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.\nData scientists analyze data and mainly focus on the manipulating the data's existing features.\nBut sometimes, these features are limited.\nFeature generation using another data source may add substantial information.\nOur research goal is automatic tabular data enrichment using external sources' free text.\nAssume we have a tabular dataset and a knowledge base.\nWe need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.\nOur framework FeSTE is exactly this automatic process.\nSo let's see an example in a dataset fed into FeSTE.\nIn this example, the dataset is university dataset.\nWhen its goal is to classify universities into low ranking universities and high-ranking universities.\nAs knowledge base, we use Wikipedia.\nThe first phase of FeSTE is entity linking.\nWhen each entity, in this example the university name, is linked to an entity within the knowledge base.\nAnd and the text of the entities of the knowledge base is extracted and added to the dataset.\nIn this example, the text is the Wikipedia page's abstract.\nNow, we need to generate or extract features from the retrieved text.\nSo, we need to ah feature extraction phase ah which includes text analysis.\nAnd this is the main novelty of this paper and I will deep dive into it in the next slides.\nAfter the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.\nFirst generate ah features in the number of classes of the original dataset.\nIn this example, the original dataset has two classes.\nSo, FeSTE generates two new features.\nBut if the dataset has five classes, FeSTE generates five new features.\nEach feature represents the likelihood for each class.\nTo analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.\nIt is but it is not likely that we can train language models using the input datasets.\nSo a naive approach will be ah target task finetuning.\nSo, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.\nIn this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.\nReceive the language model output, which is the likelihood for each class and use as new features.\nThe problem with this approach is datasets may have few distinct entities / texts.\nIn our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.\nSo to finetune a language model over ah this dataset will be ineffective.\nBut we can use prior knowledge about pre-analyzed datasets.\nBecause FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.\nWhat we, what we suggest is to add, to add another finetuning phase.\nA preliminary multitask finetuning phase.\nWhen you finetune the language model over the n minus one datasets.\nAnd, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.\nThe state-of-the-art in multitask ah multitask finetuning called MTDNN.\nIn MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.\nSo, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.\nAnd it samples a random batch from ah from the training set.\nAnd if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.\nAnd if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.\nIn our scenario, ah tabular datasets vary in the number of classes.\nSo there are many tasks.\nMTDNN maintained number of classes, heads, output layers.\nAnd the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.\nOur approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.\nSo let's see an example.\nHere is the our input dataset which consists of entities, features, text and classes.\nAnd, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.\nOr in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.\nSo the label vector in this case stays always ah which consists always with two classes.\nAnd this is the ah algorithm for our fine, reformulated finetuning approach.\nSo let's see the full framework.\nDataset fed into FeSTE.\nAnd then ah FeSTE executes entity linking phase.\nIt ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.\nThen it reformulated the task into a pairwise sentence classification task.\nApplied the language model to the new task and the output likelihood for each class.\nAnd now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.\nThen we use the output vector of the language model as a newly generated feature in the number of classes.\nTo evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.\nAnd as knowledge base we use Wikipedia.\nWe design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.\nWe also, we also split each dataset into four folds and apply four folds cross validation.\nThen, we generate the new features and evaluate them using five evaluation classifiers.\nWe use in our experiments base BERT base architecture.\nHere are the results for our experiments.\nYou can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.\nAnd our reformulated finetuning achieves the best result, the best performance.\nWhile MTDNN achieved two percent improvement over the target dataset finetuning.\nOur approach achieved six percent improvement.\nWhen we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.\nBut our performance increased to eleven percent compared to the target task finetuning alone.\nFor summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.\nIt uses one architecture for all tasks and datasets.\nAnd it keeps the head of ah of the model.\nBut it adds reformulation phase.\nIt augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.\nThank you.", "tgt_ref": "Hallo! Mein Name ist Asaf Harari.\nIch werde unser Paper vorstellen: Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.\nDatenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der bestehenden Funktionen von Daten.\nAber manchmal sind diese Funktionen begrenzt.\nDie Generierung von Funktionen unter Verwendung einer anderen Datenquelle kann wesentliche Informationen hinzufügen.\nUnser Forschungsziel ist die automatische Anreicherung tabellarischer Daten mit freiem Text aus externen Quellen.\nAngenommen, wir haben einen tabellarischen Datensatz und eine Wissensbasis.\nWir brauchen einen automatischen Prozess, der Entity Linking und eine Textanalyse umfasst, um neue Funktionen aus dem freien Text der Wissensbasis zu extrahieren.\nUnser Rahmen FeSTE ist genau dieser automatische Prozess.\nSehen wir uns also ein Beispiel in einem Datensatz an, der in FeSTE eingespeist wird.\nIn diesem Beispiel ist der Datensatz der Universitätsdatensatz.\nDas Ziel besteht darin, die Universitäten nach niedrigem und hohem Rang einzuteilen.\nAls Wissensbasis verwenden wir Wikipedia.\nDie erste Phase von FeSTE ist Entity Linking.\nJede Entität, in diesem Beispiel der Name der Universität, ist mit einer Entität innerhalb der Wissensbasis verknüpft.\nDer Text der Entitäten aus der Wissensbasis wird extrahiert und dem Datensatz hinzugefügt.\nIn diesem Beispiel ist der Text das Abstract der Wikipedia-Seite.\nNun müssen wir Funktionen aus dem abgerufenen Text generieren oder extrahieren.\nWir müssen also eine Phase der Funktionsextraktion einleiten, zu der die Textanalyse gehört.\nDas ist die wichtigste Neuerung dieses Papers, auf die ich auf den nächsten Folien näher eingehen werde.\nNach der Phase der Funktionsextraktion folgt eine Phase der Funktionsgenerierung, in der wir die extrahierten Funktionen verwenden, um eine kleine Anzahl neuer Funktionen zu erzeugen.\nZunächst werden Funktionen in der Anzahl der Klassen des ursprünglichen Datensatzes generiert.\nIn diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen.\nFeSTE generiert also zwei neue Funktionen.\nWenn der Datensatz jedoch fünf Klassen hat, generiert FeSTE fünf neue Funktionen.\nJede Funktion stellt die Wahrscheinlichkeit für jede Klasse dar.\nUm den Text zu analysieren, verwenden wir die aktuellste Textanalyse, die auf Transformer-basierten Sprachmodellen wie BERT, GPT, XLNet und anderen beruht.\nEs ist aber unwahrscheinlich, dass wir Sprachmodelle mit den eingegebenen Datensätzen trainieren können.\nEin naiver Ansatz ist also eine Feinabstimmung des Zieldatensatzes.\nIn der Phase der Funktionsextraktion können wir also vortrainierte Sprachmodelle herunterladen und das Sprachmodell über den Zieldatensatz feinabstimmen.\nIn diesem Beispiel zur Feinabstimmung des Sprachmodells klassifizieren wir den Text in Klassen, d. h. wir klassifizieren das Abstract in die Klassen niedrig oder hoch.\nWir erhalten die Ausgabe des Sprachmodells, die die Wahrscheinlichkeit für jede Klasse darstellt, und verwenden sie als neue Funktionen.\nDas Problem bei diesem Ansatz ist, dass Datensätze möglicherweise nur wenige unterschiedliche Entitäten/Texte beinhalten.\nIn unserem Experiment enthält fast die Hälfte der Datensätze weniger als 400 Stichproben und der kleinste Datensatz enthält 35 Stichproben in einem Trainingssatz.\nDie Feinabstimmung eines Sprachmodells über diesen Datensatz ist also unwirksam.\nAber wir können vorherige Kenntnisse über bereits analysierte Datensätze nutzen.\nDa wir FeSTE auf mehrere Datensätze anwenden, können wir die „n minus 1“-Datensätze verwenden, um Informationen über die „n minus 1“-Datensätze zu sammeln, und diese Informationen verwenden, wenn wir den n-ten Datensatz analysieren.\nWir schlagen vor, eine weitere Feinabstimmungsphase hinzuzufügen.\nDas war eine vorläufige Multitask-Feinabstimmungsphase.\nDie Feinabstimmung des Sprachmodells geht über die „n minus 1“-Datensätze.\nDann führen wir eine weitere Feinabstimmungsphase durch, die eine Feinabstimmung des Zieldatensatzes ist, wenn wir das Sprachmodell über den n-ten Zieldatensatz feinabstimmen.\nDie modernste Multitask-Feinabstimmung wird MTDNN genannt.\nBei der MTDNN bleiben die Überschriften in der Reihe der Aufgaben im Trainingssatz.\nIn diesem Beispiel gibt es also vier Aufgaben im Trainingssatz. Das MTDNN enthält also vier Überschriften, wie Sie auf dem Bild sehen können.\nEs nimmt sich eine Zufallsstichprobe aus dem Trainingssatz heraus.\nWenn die Zufallsstichprobe zum Beispiel zu einer einzelnen Satzklassifikationsaufgabe gehört, führt sie Vorwärts- und Rückwärtspfade durch die erste Überschrift aus.\nWenn die Zufallsstichprobe zur paarweisen Ranking-Aufgabe gehört, führt sie den Vorwärts- und Rückwärtspfad über die letzte Überschrift aus.\nIn unserem Szenario variieren tabellarische Datensätze in der Anzahl der Klassen.\nEs gibt also viele Aufgaben.\nMTDNN enthält eine Anzahl von Klassen, Überschriften und ausgegebenen Ebenen.\nUnd zusätzlich muss MTDNN neue Überschriften für einen neuen Datensatz mit einer neuen Aufgabe initialisieren.\nUnser Ansatz, genannt Feinabstimmungsreformulierungsaufgabe, besteht darin, dass wir, anstatt mehrere Überschriften beizubehalten, jeden Datensatz in einen Satz pro Klassifikationssproblem umformulieren, was zwei Klassenaufgaben entspricht.\nSchauen wir uns ein Beispiel an.\nHier ist unser eingegebener Datensatz, der aus Entitäten, Funktionen, Text und Klassen besteht.\nWir formulieren die Aufgabe von einer Klassifizierung des Textes in niedrig oder hoch zu einer Klassifizierung des Textes, des Abstracts und der Klasse in wahr oder falsch um.\nMit anderen Worten: Wir haben das Sprachmodell so trainiert, dass es ein Abstract und die Klasse als Abstract und die Klasse klassifiziert, wenn das Abstract zur Klasse gehört oder nicht.\nSo bleibt die Vektor-Markierung in diesem Fall bestehen, die immer aus zwei Klassen besteht.\nDas ist der Algorithmus für unseren neu formulierten Finabstimmungsansatz.\nSchauen wir uns also den gesamten Rahmen an.\nDer Datensatz wird in FeSTE eingespeist.\nDann führt FeSTE die Entity-Linking-Phase aus.\nEs extrahiert den Text aus der Wissensbasis, die in diesem Beispiel das Abstract der Wikipedia-Seite ist.\nDann formulierte es die Aufgabe in eine paarweise Satzklassifikationsaufgabe um.\nDas Sprachmodell wird auf die neue Aufgabe und die Ausgabewahrscheinlichkeit für jede Klasse angewandt.\nNun ist das Sprachmodell bereits über den „n minus 1“-Datensatz mithilfe einer vorläufigen Multitask-Feinabstimmung feinabgestimmt.\nDann verwenden wir den Ausgabe-Vektor des Sprachmodells als eine neu generierte Funktion in der Anzahl der Klassen.\nZur Evaluation unseres Rahmens verwenden wir 17 tabellarische Klassifikationsdatensätze, die sich in Größe, Funktionen, Ausgewogenheit, Domäne und anfänglicher Leistung unterscheiden.\nAls Wissensbasis verwenden wir Wikipedia.\nWir konzipieren unser Experiment als Leave-One-Out-Evaluation, bei dem wir FeSTe mit 16 Datensätzen trainieren und auf den 17. Datensatz anwenden.\nAußerdem teilen wir jeden Datensatz in vier Brüche auf und wenden eine vierfache Kreuzvalidierung an.\nDann generieren wir die neuen Funktionen und bewerten sie mit fünf Evaluationssklassifikatoren.\nWir verwenden in unseren Experimenten die Basisarchitektur BERT.\nHier sind die Ergebnisse unserer Experimente.\nSie können sehen, dass wir unseren Rahmen mit der Feinabstimmung des Zieldatensatzes, mit der Feinabstimmung der Zielaufgabe und einer vorläufigen MTDNN-Feinabstimmung vergleichen.\nUnsere neu formulierte Feinabstimmung erreicht das beste Ergebnis und die beste Leistung.\nMTDNN erreichte zwei Prozent Verbesserung gegenüber der Feinabstimmung des Zieldatensatzes.\nMit unserem Ansatz haben wir eine Verbesserung von sechs Prozent erreicht.\nWenn wir uns den kleinen Datensatz anschauen, sehen wir, dass die Leistung von MTDNN abnimmt und die Verbesserung der vorläufigen Multitask-Feinabstimmungsphase auf 1,5 Prozent sinkt.\nAber unsere Leistung stieg auf elf Prozent im Vergleich zur Feinabstimmung des Zieldatensatzes allein.\nFür die Summierung ermöglicht FeSTE in unseren Experimenten eine Few-Shot-Anreicherung aus 35 Proben.\nEs verwendet eine Architektur für alle Aufgaben und Datensätze.\nUnd es behält den Hauptteil des Modells.\nAber es fügt eine Reformulierungsphase hinzu.\nEs erweitert den Trainingssatz und benötigt einen Zielwert mit semantischer Bedeutung, damit wir ihn in das Sprachmodell einspeisen und für das Satzpaar-Klassifikationsproblem verwenden können.\nVielen Dank!", "src_lang": "en", "tgt_lang": "de", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 417, "src_audio": "/acl6060/audio/en/417.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.\nI'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.\nFirst, I'd like to talk about our motivation for reasoning.\nSo here we show an examples where multi-step reasoning is helpful.\nSo this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.\nSo on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.\nBut if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.\nSo it is good to have interpretable multi-step reasoning as output.\nAnd we also think math word problem is a straightforward application to evaluate such reasoning abilities.\nSo, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.\nSo in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.\nSo, certain assumptions ah also apply as in previous work.\nWe assume the precision of quantities are known.\nAnd we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.\nFurthermore, complicated operators can be actually decomposed into these basic operators.\nSo, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.\nSo, traditional sequence to sequence model convert the expression to a specific sequence for generation.\nAnd it is pretty easy to implement and it can generalize to many different complicated problem.\nBut the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.\nBut actually this direction is still quite popular because of um the transformer model.\nSo, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.\nSo here we keep generating the operators until we reach the leaves, which are the quantities.\nSo here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.\nAnd the second thing is that it also contains some repetitive computations.\nSo here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.\nSo, in our proposed approach we want to solve those problems in a step by step and interpretable manners.\nSo for example, here in the second step, ah we can obtain these divisors which is twenty seven.\nAnd we can also refer back to the original questions to find the relevant contents.\nAnd in these steps we obtain the divisors.\nSo, ah and then at this third step we actually get the quotient.\nAlright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.\nSo, here we actually generate the whole expression directly rather than generating a single operators or quantities.\nSo this makes the process more accurate.\nSo, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.\nSo, the expression is represented by e i j o p.\nWhere we perform operator from q_i to q_j, and such expression is actually directed.\nSo, we also have subtraction with words here to represent the opposite direction.\nThis is quite similar to relation extraction.\nSo in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.\nWe add it to the next state to become a new quantity.\nSo, these slides actually visualize the evolution of the state where we keep adding expression to the current state.\nSo in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.\nSo, once we get the quantity representations, we can start to do inference.\nHere we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.\nFirst we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.\nAnd then finally we obtain the expression representation q_1 divided by q_2.\nBut in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.\nSo, here all the possible expression is equals to three times the number of operators.\nSo the nice thing here is that we can easily add constraints to control this search this search space.\nFor example, if this expression is not allowed, we can simply remove this expression in our search space.\nSo in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.\nSo this quantity come from the previous calculated expression.\nSo finally we can obtain this final expression q_3 times q_4.\nAnd we can also see the number of all the possible ah expression is different from the previous step.\nSo, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.\nSo the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.\nAnd here we also use this tau to represent when we should terminate this generation process.\nAnd here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.\nAnd it also allows us to impose certain constraints from prior from prior knowledge.\nSo we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.\nAnd here we briefly show the results compared with the previous best approaches.\nSo our best performing variant is Roberta-DeductiveReasoner.\nAnd in fact we do not use beam search, in contrast all previous approaches are using beam search.\nAll right. So, the best approaches are often tree based model.\nSo, overall our reasoner is able to significantl significantly outperform this tree based model.\nBut we can see the absolute numbers on MathQA or SVAMP are not really high.\nSo we further investigate the results on SVAMP.\nAnd this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.\nSo, in our prediction we find some of the intermediate values are actually negatives.\nFor example, um, in these questions we are asking how many apples does Jake have?\nBut we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.\nSo, our model makes some prediction like this which is producing negative values.\nAnd we observe these two expressions actually have similar scores.\nSo, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.\nSo um we further find such constraint actually improves quite a lot for some models.\nFor example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.\nSo better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.\nAnd we also try to analyze the difficulty behind these behind all these datasets.\nWe assume the number of unused quantities can be regarded as irrelevant information here.\nSo ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.\nAnd here we also show the overall performance.\nFor those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.\nBut with those samples that with unused quantity is actually way worse than the, worse than the overall performance.\nFor MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.\nSo, finally we want to show the interpretability through a question perturbation example.\nSo here our model actually makes a wrong prediction at the first step.\nSo, we can actually correlate this expression with the sentence here. Alright.\nSo, we think this sentence might be misleading the model to an incorrect predictions.\nSo here planting another thirty five makes the model makes the model think it should be an addition operator.\nSo we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.\nSo, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.\nSo, this study shows how the interpretable predictions help us understand the model behavior.\nSo to conclude our work, so first our model is actually pretty efficient.\nAnd we are able to provide interpretable solving procedure.\nAnd we can easily incorporate some prior knowledge as constraint which can help improve the performance.\nAnd the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.\nWe also have certain limitations.\nAh, if we have a large number of operators or constants, the memory consumption could be pretty high.\nAnd the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.\nSo this is the end of the talk, and questions are welcomed. Thank you.", "tgt_ref": "Hallo zusammen. Heute werde ich unsere Forschungsarbeit Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction vorstellen.\nIch bin Allan vom ByteDance AI Lab, und dies ist eine gemeinsame Arbeit mit Jierui Li von der University of Texas in Austin und Wei Lu von SUTD.\nZunächst möchte ich über unsere Motivation für das Argumentieren sprechen.\nDeshalb zeigen wir hier ein Beispiel, bei dem mehrstufiges Argumentieren hilfreich ist.\nDiese Abbildung stammt aus dem PaLM-Paper, in dem sie Prompting durchführen, um das Netzwerkproblem in einem Few-Shot-Lernszenario zu lösen.\nAuf der linken Seite können wir also sehen, dass wenn einige Beispiele nur aus Frage und Antworten bestehen, dann bekommen wir nicht die richtigen Antworten.\nWenn wir jedoch eine weitere Argumentationsbeschreibung hinzufügen, ist das Modell in der Lage, die Argumentationsbeschreibung vorherzusagen und hier auch eine korrekte Vorhersage zu treffen.\nEs ist also gut, eine interpretierbare mehrstufige Argumentation als Ergebnis zu haben.\nWir denken auch, dass mathematische Wortprobleme eine einfache Anwendung sind, um solche Argumentationsfähigkeiten zu bewerten.\nIn unserem Problem-Setup müssen wir also angesichts der Fragen diese Frage lösen und die numerischen Antworten erhalten.\nIn unseren Datensätzen finden wir auch den mathematischen Ausdruck, der uns zu dieser bestimmten Antwort führt.\nEs gelten also auch hier bestimmte Annahmen wie in früheren Arbeiten.\nWir gehen davon aus, dass die Genauigkeit der Mengen bekannt ist.\nUnd wir betrachten nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponentialrechnung.\nDarüber hinaus können komplizierte Operatoren in diese Grundoperatoren zerlegt werden.\nDie früheren Arbeiten zum Lösen von mathematischen Wortproblemen lassen sich also in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modell einteilen.\nDas traditionelle Sequenz-zu-Sequenz-Modell wandelt also den Ausdruck in eine spezifische Sequenz für die Generierung um.\nEs ist ziemlich einfach zu implementieren und kann für viele verschiedene komplizierte Probleme verallgemeinert werden.\nDie Nachteile sind jedoch, dass die Leistung im Allgemeinen nicht besser ist als die des strukturierten Modells und dass es an Interpretierbarkeit für Vorhersagen mangelt.\nAber eigentlich ist diese Richtung immer noch recht beliebt wegen des Transformermodells.\nIn baumbasierten Modellen strukturieren wir diese Ausdrücke in Baumform und folgen einem geordneten Traversal in Baumgenerationen.\nHier fahren wir also fort, die Operatoren zu generieren, bis wir die Blätter erreichen, die die Mengen darstellen.\nDas Gute daran ist, dass wir dadurch diese binäre Baumstruktur erhalten. Sie ist eigentlich ziemlich kontraintuitiv, weil wir zuerst den Operator und dann am Ende die Mengen generieren müssen.\nZudem enthält sie auch einige sich wiederholende Berechnungen.\nWenn wir uns also diesen Ausdruck ansehen, wird 8 x 3 + 3 eigentlich zweimal generiert, aber eigentlich sollten wir die Ergebnisse wiederverwenden.\nIn unserem vorgeschlagenen Ansatz wollen wir diese Probleme schrittweise und auf interpretierbare Art und Weise lösen.\nHier im zweiten Schritt zum Beispiel können wir diese Teiler erhalten, was 27 ergibt.\nWir können auch auf die ursprünglichen Fragen zurückgreifen, um die relevanten Inhalte zu finden.\nIn diesen Schritten erhalten wir die Teiler.\nUnd in diesem dritten Schritt erhalten wir dann den Quotienten.\nOkay. Nach diesen drei Schritten können wir die Ergebnisse des zweiten Schritts wiederverwenden, dann die Ergebnisse des vierten Schritts erhalten und schließlich die Dividenden erhalten.\nHier wird also der gesamte Ausdruck direkt generiert und nicht nur ein einzelner Operator oder eine Menge.\nDadurch wird der Prozess genauer.\nIn unserem deduktiven System beginnen wir also zunächst mit einer Reihe von Mengen, die in den Fragen vorgestellt werden, und schließen auch mit einer Konstante als Anfangszustand.\nDer Ausdruck wird also durch e i j o p dargestellt.\nWir führen den Operator von q_i nach q_j aus, und dieser Ausdruck ist tatsächlich geregelt.\nWir haben hier also auch eine Subtraktion mit Wörtern, um die umgekehrte Richtung darzustellen.\nDies ist der Beziehungsextraktion recht ähnlich.\nIn einem formalen deduktiven System wenden wir also in einem Zeitschritt t den Operator zwischen dem Paar q_i und q_j an und erhalten dann diesen neuen Ausdruck.\nWir fügen ihn dem nächsten Zustand hinzu, um eine neue Menge zu erhalten.\nDiese Folien veranschaulichen also die Entwicklung des Zustands, wobei wir dem aktuellen Zustand immer neue Ausdrücke hinzufügen.\nIn unseren Modellimplementierungen verwenden wir also zunächst ein vortrainiertes Sprachmodell, bei dem es sich um BERTs oder Robertas handeln kann, und dann kodieren wir den Satz und erhalten diese Mengendarstellungen.\nSobald wir also die Mengendarstellungen haben, können wir mit der Inferenz beginnen.\nHier zeigen wir ein Beispiel für q_1, um die Darstellung für q_2 geteilt durch q_2 und dann mal q_3 zu erhalten.\nZunächst erhalten wir die Paardarstellung, die im Grunde nur die Verkettung zwischen q_1 und q_2 ist, und dann wenden wir ein Feedforward-Netzwerk an, das durch den Operator parametrisiert ist.\nUnd dann erhalten wir schließlich die Ausdrucksdarstellung q_1 geteilt durch q_2.\nAber in der Praxis könnten wir in der Inferenzphase auch den falschen Ausdruck erhalten.\nHier sind also alle möglichen Ausdrücke gleich dem Dreifachen der Anzahl der Operatoren.\nDas Schöne daran ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu kontrollieren.\nWenn dieser Ausdruck zum Beispiel nicht erlaubt ist, können wir ihn einfach aus unserem Suchraum entfernen.\nIm zweiten Schritt machen wir das Gleiche, aber der einzige Unterschied ist, dass wir nur eine Menge mehr haben.\nDiese Menge stammt also aus dem vorherigen berechneten Ausdruck.\nSo können wir schließlich diesen endgültigen Ausdruck q_3 mal q_4 erhalten.\nWir können auch sehen, dass die Anzahl aller möglichen Ausdrücke anders ist als beim vorherigen Schritt.\nEin solcher Unterschied erschwert die Anwendung von einer Beam Search, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgewogen ist.\nDas Trainingsverfahren ist also ähnlich dem Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust bei jedem Zeitschritt optimieren.\nAuch hier verwenden wir dieses Tau, um darzustellen, wann wir diesen Generierungsprozess beenden sollten.\nHier unterscheidet sich der Raum von Sequenz zu Sequenz, weil der Raum bei jedem Zeitschritt anders ist, während im traditionellen Sequenz-zu-Sequenz-Modell dies die Anzahl des Vokabulars ist.\nEs erlaubt uns auch, bestimmte Beschränkungen aus dem Vorwissen aufzuerlegen.\nDaher führen wir Experimente mit den häufig verwendeten Datensätzen der mathematischen Wortprobleme durch, MAWPS, Math23K, MathQA und SVAMP.\nHier zeigen wir kurz die Ergebnisse im Vergleich zu den bisherigen besten Ansätzen.\nUnsere leistungsstärkste Variante ist also Roberta-DeductiveReasoner.\nTatsächlich verwenden wir keine Beam Search, im Gegensatz zu allen früheren Ansätzen, die Beam Search verwendet haben.\nOkay. Daher sind die besten Ansätze oft baumbasierte Modelle.\nInsgesamt ist unser Reasoner also in der Lage, dieses baumbasierte Modell signifikant zu übertreffen.\nAber wir können sehen, dass die absoluten Zahlen bei MathQA oder SVAMP nicht wirklich hoch sind.\nDeshalb untersuchen wir die Ergebnisse bei SVAMP weiter.\nDieser Datensatz ist eine Herausforderung, weil der Autor versucht hat, manuell etwas hinzuzufügen, um das NLP-Modell zu verwirren, wie z. B. das Hinzufügen irrelevanter Informationen und zusätzlicher Mengen.\nIn unserer Vorhersage stellen wir also fest, dass einige der Zwischenwerte eigentlich negativ sind.\nZum Beispiel geht es in diesen Fragen darum, wie viele Äpfel Jake hat.\nAber wir haben einige zusätzliche Informationen wie 17 Bilder weniger, und Steven hat acht Bilder, was völlig irrelevant ist.\nUnser Modell macht also eine Vorhersage wie diese, die negative Werte ergibt.\nWir stellen fest, dass diese beiden Ausdrücke tatsächlich ähnliche Werte haben.\nWir können also diesen Suchraum eingrenzen, indem wir die negativen Ergebnisse entfernen, damit die Antwort richtig ist.\nWir stellen also fest, dass eine solche Bedingung für einige Modelle tatsächlich eine erhebliche Verbesserung darstellt.\nBei BERT haben wir uns zum Beispiel um sieben Punkte verbessert und beim Roberta-Basismodell haben wir uns sogar um zwei Punkte verbessert.\nEin besseres Sprachmodell hat also bessere Sprachverständnisfähigkeiten, sodass die Zahl hier höher bei Roberta und niedriger bei BERT ist.\nWir versuchen auch, die Schwierigkeiten hinter all diesen Datensätzen zu analysieren.\nWir gehen davon aus, dass die Anzahl der ungenutzten Mengen hier als irrelevante Informationen betrachtet werden kann.\nHier können wir also sehen, dass wir den Prozentsatz der Proben mit ungenutzten Mengen haben, und der SVAMP-Datensatz hat den größten Anteil.\nHier zeigen wir auch die Gesamtleistung.\nBei den Proben ohne ungenutzte Mengen ist die Leistung sogar höher als die Gesamtleistung.\nAber bei diesen Proben mit ungenutzten Mengen ist sie eigentlich viel schlechter als die Gesamtleistung.\nBei MAWPS haben wir nicht wirklich viele Testfälle, also ignoriere ich diesen Teil einfach.\nSchließlich wollen wir die Interpretierbarkeit anhand eines Frage-Störungsbeispiels zeigen.\nHier macht unser Modell also tatsächlich eine falsche Vorhersage im ersten Schritt.\nWir können diesen Ausdruck also tatsächlich mit dem Satz hier korrelieren. Okay.\nWir denken also, dass dieser Satz das Modell zu falschen Vorhersagen verleiten könnte.\nHier führt eine weitere 35 dazu, dass das Modell denkt, dass es ein Additionsoperator sein sollte.\nWir versuchen also den Satz so umzuformulieren, dass die Anzahl der Birnbäume um 35 weniger als die der Apfelbäume ist.\nWir sorgen also dafür, dass eine genauere Semantik vermittelt wird, sodass das Modell in der Lage ist, die Vorhersage korrekt zu treffen.\nDiese Studie zeigt also, wie uns die interpretierbaren Vorhersagen helfen, das Modellverhalten zu verstehen.\nUm unsere Arbeit abzuschließen: Zunächst ist unser Modell tatsächlich ziemlich effizient.\nWir sind in der Lage, ein interpretierbares Lösungsverfahren anzubieten.\nWir können einfach einiges Vorwissen als Bedingung einbeziehen, was zur Verbesserung der Leistung beitragen kann.\nSchließlich gilt der zugrundeliegende Mechanismus nicht nur für das Lösen von Netzwerkproblemen, sondern auch für andere Aufgaben, die mehrstufiges Argumentieren erfordern.\nAuch sind uns gewisse Grenzen gesetzt.\nWenn wir eine große Anzahl von Operatoren oder Konstanten haben, könnte der Speicherverbrauch ziemlich hoch sein.\nWie bereits erwähnt ist es zudem aufgrund der unausgewogenen Wahrscheinlichkeitsverteilung zwischen den verschiedenen Zeitschritten eine ziemliche Herausforderung, die Beam Search-Strategie anzuwenden.\nWir sind am Ende des Vortrags angekommen und freuen uns auf etwaige Fragen. Vielen Dank!", "src_lang": "en", "tgt_lang": "de", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 418, "src_audio": "/acl6060/audio/en/418.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.\nI will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.\nLegal issues are an integral part of many people's lives.\nBut the majority of citizens have little to know knowledge about their rights and fundamental legal processes.\nAs a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.\nAll work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.\nSuch a system could provide a free professional legal help service for unskilled humans.\nBefore diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.\nGiven a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?\nA model is required to retrieve all relevant statutory articles from a large body of legislation.\nThis information retrieval task comes with its own set of challenges.\nFirst, it deals with two types of language.\nCommon natural language for the questions and complex legal language for the statutes.\nThis difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.\nBesides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.\nInstead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.\nLastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.\nHere, there are long documents that may be up to six thousand words.\nThe recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.\nBut statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.\nIn this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.\nOur Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.\nThese questions cover a wide range of topics from family, housing, money, to work and social security.\nEach of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.\nLet's now talk about how we collected this dataset.\nFirst, we started by compiling a large corpus of legal articles.\nWe considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.\nThen we gathered legal questions with references to relevant statutes.\nTo do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.\nWe were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.\nWe collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.\nLastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.\nThe remaining references were matched and converted to the corresponding article ids from our corpus.\nWe eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.\nIn addition, each question comes with the main category and a concatenation of subcategories.\nAnd each articles comes with a concatenation of the subsequence heading in the structure of the law.\nThis extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.\nLet's look at some characteristic of our dataset.\nThe questions are between five and forty four words long with a median of fourteen words.\nThe articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.\nThe lengthiest one being up to five thousand seven hundred and ninety words.\nAs previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.\nWhile the remaining fifteen percent concern either social security, foreigners or work.\nThe article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.\nHere's the total number of articles collected from each of these Belgian codes.\nOut of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.\nAnd around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.\nMeanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.\nWhich can be explained by the fact that those codes focused less on individuals and their concerns.\nOverall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.\nUsing all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.\nGiven a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.\nWe experiment with the standard TF-IDF and BM25 ranking functions.\nThe main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.\nTo overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.\nWe use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.\nThese embeddings typically result from a pooling operation on the output of a word embedding model.\nFirst, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.\nWe experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.\nAdditionally, we train our own CamemBERT based model ah bi-encoders on our dataset.\nNote that for training, we experiment with the two flavors of the bi-encoder architecture.\nSiamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.\nWe experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.\nHere are the result of our baseline on the test sets.\nWith the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.\nOverall, the finetuned bi-encoder significantly outperforms all the other baselines.\nThe two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.\nAlthough BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.\nRegarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.\nFurthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.\nAlthough promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.\nLet's conclude by discussing two limitations of our dataset.\nFirst, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.\nDuring the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.\nThis information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.\nSecond, we should note that not all legal questions can be answered with statutes alone.\nFor instance, the question, can I evict my tenants if they make too much noise?\nMight not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.\nInstead, the landlord should probably rely more on case law and find precedents similar to their current situation.\nFor example, the tenants makes two parties a week until two AM.\nHence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.\nWe hope that our work sparks interest in developing practical and reliable statutory article retrieval models.\nThat can help improve access to justice for all.\nYou can check out our paper, dataset and code at the following links. Thank you.", "tgt_ref": "Hallo, mein Name ist Antoine und ich bin von der Universität Maastricht.\nIch werde meine gemeinsame Arbeit mit Jerry vorstellen, bei der es um einen neuen Datensatz für das Retrieval von Gesetzesartikeln geht.\nRechtsfragen sind ein wesentlicher Bestandteil des Lebens vieler Menschen.\nAber die Mehrheit der Bürger verfügt nur über wenig Wissen über ihre Rechte und grundlegende rechtliche Verfahren.\nDies hat zur Folge, dass viele schutzbedürftige Bürger, die sich die kostspielige Hilfe eines Rechtsexperten nicht leisten können, schutzlos bleiben oder schlimmstenfalls ausgebeutet werden.\nUnsere Arbeit zielt darauf ab, die Kluft zwischen den Menschen und dem Gesetz zu überbrücken, indem ein effektives Retrieval-System für Gesetzesartikel entwickelt wird.\nEin solches System könnte einen kostenlosen professionellen Rechtshilfedienst für unerfahrene Menschen anbieten.\nBevor wir uns dem Hauptbeitrag dieser Arbeit widmen, wollen wir zunächst das Problem des Retrievals von Gesetzesartikeln beschreiben.\nEine einfache Frage zu einer Rechtsangelegenheit wäre: „Was riskiere ich, wenn ich das Berufsgeheimnis verletze?“\nHier wird ein Modell benötigt, um alle relevanten Gesetzesartikel aus einem großen Bestand an Rechtsvorschriften abzurufen.\nDiese Informationsbeschaffungsaufgabe bringt eine Reihe von Herausforderungen mit sich.\nErstens: Es geht um zwei Arten von Sprache.\nDie gemeinsame natürliche Sprache für die Fragen und die komplexe juristische Sprache für die Gesetze.\nDieser Unterschied bei den Sprachverteilungen macht es für ein System schwieriger, relevante Kandidaten zu finden, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine rechtliche Frage übersetzen kann, die der Terminologie der Gesetze entspricht.\nAußerdem ist das Recht keine Ansammlung unabhängiger Artikel, die für sich allein als vollständige Informationsquelle betrachtet werden können, anders als beispielsweise Nachrichten oder Rezepte.\nVielmehr handelt es sich um eine strukturierte Sammlung von Rechtsvorschriften, die nur im Gesamtkontext, d. h. zusammen mit den ergänzenden Informationen aus den Nachbarartikeln, den Bereichen und Teilbereichen, denen sie angehören, und ihrer Stellung im Aufbau des Gesetzes, eine vollständige Bedeutung haben.\nSchließlich handelt es sich bei den Gesetzesartikeln nicht um kleine Absätze, die in den meisten Retrieval-Arbeiten die typische Retrieval-Einheit darstellen.\nHier gibt es lange Dokumente, die bis zu 6.000 Wörter umfassen können.\nDie jüngsten Fortschritte im Bereich NLP haben großes Interesse an vielen juristischen Aufgaben geweckt, z. B. an der Vorhersage juristischer Urteile oder der automatisierten Prüfung von Verträgen.\nDoch das Retrieval von Gesetzesartikeln ist aufgrund des Mangels an großen und qualitativ hochwertig markierten Datensätzen weitgehend unberührt geblieben.\nIn dieser Arbeit stellen wir einen neuen Datensatz in der französischen Muttersprache vor, mit dem wir Retrievalmodelle untersuchen und analysieren, ob sie bei der Aufgabe der Suche nach Gesetzesartikeln an die Effizienz und Zuverlässigkeit eines Rechtsexperten rankommen können.\nUnser belgischer Datensatz für das Retrieval von Gesetzesartikeln, BSARD, besteht aus mehr als 1.100 juristischen Fragen, die von belgischen Bürgern gestellt wurden.\nDiese Fragen decken ein breites Spektrum an Themen ab, von Familie, Wohnen, Geld bis hin zu Arbeit und sozialer Sicherheit.\nJede von ihnen wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als 22.600 Rechtsartikeln aus belgischen Gesetzbüchern markiert.\nLassen Sie uns nun darüber sprechen, wie wir diesen Datensatz gesammelt haben.\nZunächst haben wir einen großen Korpus von Rechtsartikeln zusammengestellt.\nWir haben 32 öffentlich zugängliche belgische Gesetzbücher berücksichtigt und alle Artikel sowie die entsprechenden Abschnittsüberschriften extrahiert.\nDann haben wir rechtliche Fragen mit Verweisen auf einschlägige Gesetze zusammengestellt.\nZu diesem Zweck arbeiten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr etwa 4.000 E-Mails von belgischen Bürgern erhält, die um Rat in einer persönlichen Rechtsangelegenheit bitten.\nWir hatten das Glück, Zugang zu ihren Websites zu erhalten, auf denen ihr Team aus erfahrenen Juristen die häufigsten Rechtsangelegenheiten der Belgier behandelt.\nWir haben Tausende von Fragen gesammelt und mit Kategorien, Unterkategorien und Verweisen auf einschlägige Gesetze annotiert.\nSchließlich haben wir die rechtlichen Verweise überprüft und die Fragen herausgefiltert, deren Verweise nicht in einem der von uns berücksichtigten Gesetzbücher enthalten waren.\nDie übrigen Referenzen wurden zusammengeführt und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt.\nAm Ende hatten wir 1.108 Fragen, jede sorgfältig markiert mit den IDs der relevanten Artikel aus unserem großen Korpus von 22.630 Gesetzesartikeln.\nDarüber hinaus enthält jede Frage eine Hauptkategorie und eine Verkettung von Unterkategorien.\nJeder Artikel hat eine Verkettung mit der anschließenden Überschrift im Aufbau des Gesetzes.\nDiese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für künftige Forschungen zum juristischen Informationen-Retrieval oder zur juristischen Textklassifikation von Interesse sein.\nSchauen wir uns einige Merkmale unseres Datensatzes an.\nDie Fragen sind zwischen fünf und 24 Wörter lang, mit einem Mittelwert von 14 Wörtern.\nDie Artikel sind mit einer durchschnittlichen Länge von 77 Wörtern sehr viel länger, wobei 142 von ihnen mehr als 1.000 Wörter umfassen.\nDer längste von ihnen hat bis zu 5.790 Wörter.\nWie bereits erwähnt decken die Fragen ein breites Spektrum an Themen ab, wobei etwa 85 Prozent der Fragen entweder die Familie, das Wohnen, Geld oder die Gerechtigkeit betreffen.\nDie restlichen 15 Prozent betreffen entweder soziale Sicherheit, Ausländer oder Arbeit.\nDie Artikel sind auch sehr vielfältig, da sie aus 32 verschiedenen belgischen Gesetzbüchern stammen, die eine große Reihe von Rechtsthemen abdecken.\nHier ist die Gesamtzahl der Artikel, die aus diesen belgischen Gesetzbüchern gesammelt wurden.\nVon den 22.633 Artikeln werden nur 1.612 als relevant für mindestens eine Frage im Datensatz genannt.\nEtwa 80 Prozent dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, dem Gerichtsgesetzbuch, dem Gesetzbuch für strafrechtliche Ermittlungen oder dem Strafgesetzbuch.\nBei 18 von 32 Gesetzbüchern werden weniger als fünf Artikel als relevant für mindestens eine Frage bestimmt.\nDies lässt sich dadurch erklären, dass diese Gesetzbücher weniger auf den Einzelnen und seine Anliegen ausgerichtet waren.\nInsgesamt liegt der Durchschnitt der Anzahl der Zitate bei zwei, und weniger als 25 Prozent der zitierten Artikel werden mehr als fünfmal zitiert.\nUnter Verwendung aller Datensätze haben wir verschiedene Retrieval-Ansätze, einschließlich lexikalischer und dichter Architektur, einem Benchmarking unterzogen.\nBei einer Abfrage und einem Artikel weist ein lexikalisches Modell dem Abfrage-Artikel-Paar eine Punktzahl zu, indem es die Summe der Gewichtungen von jedem der Begriffe in diesem Artikel über die Abfrage-Termini berechnet.\nWir experimentieren mit den Standard-Ranking-Funktionen TF-IDF und BM25.\nDas Hauptproblem bei diesen Ansätzen ist, dass sie nur Artikel finden können, die Schlüsselwörter enthalten, die in der Anfrage vorkommen.\nUm diese Einschränkung zu überwinden, experimentieren wir mit einer neuronal-basierten Architektur, die semantische Beziehungen zwischen Abfragen und Artikeln erfassen kann.\nWir verwenden ein Bi-Encoder-Modell, das Abfragen und Artikel in dichten Vektor-Darstellungen abbildet und einen Relevanzwert zwischen einem Abfrage-Artikel-Paar anhand der Ähnlichkeit ihrer Einbettungen berechnet.\nDiese Einbettungen resultieren in der Regel aus einer Pooling-Operation auf der Ausgabe eines Worteinbettungsmodells.\nZunächst untersuchen wir die Effektivität von Siamesischen Bi-Encodern in einem Zero-Shot-Evaluation-Setup, d. h. dass vortrainierte Worteinbettungsmodelle ohne zusätzliche Feinabstimmung sofort angewendet werden.\nWir experimentieren mit kontextunabhängigen Textencodern, wie word2vec und fastText, und kontextabhängigen Einbettungsmodellen, wie Roberta und insbesondere CamemBERT, einem französischen Roberta-Modell.\nDarüber hinaus trainieren wir unser eigenes, auf CamemBERT basierendes Modell, Bi-Encoder, auf unserem Datensatz.\nBeachten Sie, dass wir beim Training mit den beiden Varianten der Bi-Encoder-Architektur experimentieren.\nSiamese, das ein einzigartiges Worteinbettungsmodell verwendet, das die Abfrage und den Artikel zusammen in einem geteilten dichten Vektorraum abbildet, und Two-Tower, das zwei unabhängige Worteinbettungsmodelle verwendet, die die Abfrage und den Artikel getrennt in verschiedenen Einbettungsräumen kodieren.\nWir experimentieren mit Mittelwert, Maximalwert und CLS-Pooling sowie mit Produkt und Kosinus für die Berechnung von Ähnlichkeiten.\nHier sind die Ergebnisse unserer Baseline auf den Testsätzen.\nDie lexikalischen Methoden werden oben, die Siamesischen Bi-Encoder in einem Zero-Shot-Setup in der Mitte und die fein abgestimmten Bi-Encoder werden unten ausgewertet.\nInsgesamt übertrifft der fein abgestimmte Bi-Encoder alle anderen Baselines deutlich.\nDas Two-Tower-Modell verbessert sich gegenüber seinen Siamesischen Varianten beim Recall um 100, schneidet aber bei den anderen Metriken ähnlich gut ab.\nObwohl BM25 deutlich schlechter abschnitt als der trainierte Bi-Encoder, zeigte seine Leistung, dass er immer noch eine gute Basis für ein domänenspezifisches Retrieval ist.\nBezüglich der Zero-Shot-Evaluation des Siamesischen Bi-Encoders stellen wir fest, dass die direkte Verwendung der Einbettungen eines vortrainierten CamemBERT-Modells ohne Optimierung für die Informationen-Retrieval-Aufgabe schlechte Ergebnisse liefert, was mit früheren Erkenntnissen übereinstimmt.\nDarüber hinaus stellen wir fest, dass der word2vec-basierte Bi-Encoder die fastText- und BERT-basierten Modelle signifikant übertrifft, was darauf hindeutet, dass vortrainierte Wort-Ebenen-Einbettungen vielleicht besser für die Aufgabe geeignet sind als Zeichen- oder Unterwort-Ebenen-Einbettungen, wenn sie direkt verwendet werden.\nObwohl diese Ergebnisse vielversprechend sind, lassen sie im Vergleich zu einem erfahrenen Rechtsexperten auf ein großes Verbesserungspotenzial schließen, der früher oder später alle relevanten Artikel zu jeder Frage abrufen kann und somit perfekte Ergebnisse erzielt.\nAbschließend möchte ich auf zwei Einschränkungen unseres Datensatzes eingehen.\nErstens ist der Artikelkorpus auf die Artikel der 32 herangezogenen belgischen Gesetzbücher beschränkt, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Dekreten, Richtlinien und Verordnungen fehlen.\nBei der Erstellung des Datensatzes werden alle Verweise auf diese nicht erfassten Artikel ignoriert, was dazu führt, dass sich einige Fragen am Ende nur auf einen Bruchteil der ursprünglichen Anzahl der relevanten Artikel beziehen.\nDiese Information impliziert also, dass die Antwort, die in den übrigen relevanten Artikeln enthalten ist, unvollständig sein könnte, obwohl sie immer noch völlig angemessen ist.\nZweitens sollten wir beachten, dass nicht alle rechtlichen Fragen allein mit Gesetzen beantwortet werden können.\nZum Beispiel die Frage: „Kann ich meinen Mietern kündigen, wenn sie zu viel Lärm machen?“\nMöglicherweise gibt es im Gesetz keine detaillierte Antwort, die eine bestimmte Lärmschwelle festlegt, ab der eine Räumung zulässig ist.\nStattdessen sollte sich der Vermieter wahrscheinlich eher auf das Fallrecht stützen und Präzedenzfälle finden, die seiner aktuellen Situation ähnlich sind.\nZum Beispiel veranstalten die Mieter zwei Partys pro Woche bis zwei Uhr morgens.\nEinige Fragen eignen sich daher besser als andere für die Aufgabe des Retrievals von Gesetzesartikeln. Die Domäne der weniger geeigneten Fragen muss noch bestimmt werden.\nWir hoffen, dass unsere Arbeit das Interesse an der Entwicklung praktischer und zuverlässiger Modelle für das Retrieval von Gesetzesartikeln weckt.\nSie kann dazu beitragen, den Zugang zur Justiz für alle zu verbessern.\nSie können unser Paper, den Datensatz und die Gesetzbücher unter den folgenden Links einsehen. Vielen Dank!", "src_lang": "en", "tgt_lang": "de", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 419, "src_audio": "/acl6060/audio/en/419.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.\nWhy did we do the trouble in setting up this benchmark?\nWell, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.\nEach one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.\nSo we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.\nBut do we know what the models have actually learned?\nWhat is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?\nAnd the low score for this one?\nDo vision and language models focus on the right thing?\nOr do they focus on biases as shown by previous work?\nTo shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.\nWe target existence, plurality, counting, spatial relations, actions and entity coreference.\nBut how do we test whether the vision and language models have captured this phenomena?\nBy foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.\nFoiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.\nAnd we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.\nFor example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.\nCounting and coreference also are pieces that have more than one instrument.\nAnd we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.\nThis is not easy to do because a foiled caption may be less likely than the original caption.\nFor example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.\nTherefore, to obtain valid foils, we must take action.\nFirst, we make use of strong language models to propose foils.\nSecond, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.\nTo test this automatically, we apply natural language inference with the following rationale.\nWe consider an image to be the premise and its caption its entailed hypothesis.\nIn addition, we consider the caption to be the premise, and the foil is its hypothesis.\nIf an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.\nIf an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.\nBut this procedure is not perfect, it is just an indicator for valid foils.\nTherefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.\nSo, after filtering and human evaluation, we have as many test instances as described in this table.\nNote that VALSE does not deliver any training data but only test data.\nSince it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.\nFinetuning would only enable models to exploit artifacts or statistical biases in the data.\nAnd we all know that these models like to cheat and take shortcuts.\nAnd as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.\nWe experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.\nTwo of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.\nPerhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.\nFor more metrics and results on them, do check out our paper.\nThe results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.\nIt's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.\nHowever, none of the remaining pieces can be reliably solved in our adversarial foiling settings.\nWe see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.\nThe relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.\nThey also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.\nFrom the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.\nAs a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.\nIf the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.\nAnd it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.\nSo to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.\nOur experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.\nWe would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.\nAnd even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.\nIf you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "tgt_ref": "Hallo, wir freuen uns, Ihnen unsere Arbeit über VALSE vorstellen zu können. Das ist ein Aufgaben-unabhängiger Benchmark, der für das Testen von Seh- und Sprachmodellen mit spezifischen sprachlichen Phänomenen konzipiert ist.\nWarum haben wir uns die Mühe gemacht, diesen Benchmark einzurichten?\nIn den letzten Jahren haben wir eine explosionsartige Zunahme von Transformer-basierten Seh- und Sprachmodellen erlebt, die mit großen Mengen an Bild-Text-Paaren vortrainiert wurden.\nJedes dieser Modelle treibt den Fortschritt bei Seh- und Sprachaufgaben voran, wie z. B. die visuelle Fragenbeantwortung, das Argumentieren mit visuellem Menschenverstand, Bild-Retrieval, Phrasen-Erdung.\nWir haben also die Nachricht erhalten, dass die Genauigkeiten bei diesen Aufgaben und spezifischen Benchmarks stetig steigen.\nAber wissen wir, was die Modelle tatsächlich gelernt haben?\nWas hat ein Seh- und Sprach-Transformer verstanden, wenn er für dieses Bild und diesen Satz eine hohe Punktzahl zuweist?\nUnd hier eine niedrige Punktzahl?\nKonzentrieren sich Seh- und Sprachmodelle auf die wesentlichen Punkte?\nOder konzentrieren sie sich auf Verzerrungen, wie in früheren Arbeiten aufgezeigt wurde?\nUm diesen Aspekt näher zu beleuchten, schlagen wir eine eher aufgabenbezogene agnostische Richtung vor und führen VALSE ein, das die Empfindlichkeit von Seh- und Sprachmodellen für spezifische sprachliche Phänomene testet, die sowohl die sprachlichen als auch die visuellen Modalitäten betreffen.\nWir zielen auf Existenz, Pluralität, Zählung, räumliche Beziehungen, Handlungen und Entity-Koreferenz ab.\nDoch wie lässt sich überprüfen, ob die Seh- und Sprachmodelle dieses Phänomen erfasst haben?\nDas Verfälschen einer Methode wurde zuvor von Ravi Shekhar und Mitarbeitern nur für Substantivphrasen in den Seh- und Sprachmodellen und beim Zählen von uns in früheren Arbeiten angewandt.\nVerfälschen bedeutet im Grunde, dass wir die Beschriftung eines Bildes nehmen und eine Verfälschung erzeugen, indem wir die Beschriftung so verändern, dass sie das Bild nicht mehr beschreibt.\nWir führen diese Veränderungen der Phrase durch, indem wir uns auf sechs spezifische Teile konzentrieren, wie Existenz, Pluralität, Zählung, räumliche Relationen, Handlungen und Entität-Koreferenz, wobei jeder Teil aus einem oder mehreren Instrumenten bestehen kann, falls wir mehr als einen interessanten Weg gefunden haben, um Verfälschungsinstanzen zu erzeugen.\nZum Beispiel haben wir im Fall des Handlungsteils zwei Instrumente, eines, bei dem das Handlungsverb durch eine andere Handlung ersetzt wird, und eines, bei dem die Aktanten ausgetauscht werden.\nZählung und Koreferenz sind auch Teile, die mehr als ein Instrument haben.\nWir erstellen diese Verfälschungen, indem wir sicherstellen, dass sie das Bild nicht beschreiben, dass sie grammatikalisch richtig sind und auch korrekte Sätze bilden.\nDas ist nicht einfach zu bewerkstelligen, denn eine verfälschte Beschriftung ist weniger wahrscheinlich als die ursprüngliche Beschriftung.\nEs ist zwar zum Beispiel nicht unmöglich, aber statistisch gesehen ist es unwahrscheinlicher, dass Pflanzen einen Menschen schneiden, als dass ein Mensch Pflanzen schneidet. Große Seh- und Sprachmodelle könnten dies erkennen.\nDaher müssen wir handeln, wenn wir gültige Verfälschungen erhalten wollen.\nErstens nutzen wir starke Sprachmodelle, um Verfälschungen vorzuschlagen.\nZweitens verwenden wir die Inferenz der natürlichen Sprache oder kurz NLI, um Verfälschungen herauszufiltern, die das Bild noch beschreiben könnten. Bei der Konstruktion von Verfälschungen müssen wir sicherstellen, dass sie das Bild nicht beschreiben.\nUm dies automatisch zu testen, wenden wir die Inferenz der natürlichen Sprache mit der folgenden Logik an.\nWir betrachten ein Bild als die Prämisse und seine Beschriftung als die daraus abgeleitete Hypothese.\nDarüber hinaus betrachten wir die Beschriftung als Prämisse und die Verfälschung als ihre Hypothese.\nWenn ein NLI-Modell vorhersagt, dass die Verfälschung der Beschriftung widerspricht oder neutral ist, ist das für uns ein Indikator für eine gültige Verfälschung.\nWenn ein NLI vorhersagt, dass die Verfälschung die Beschriftung beinhaltet, kann es keine gute Verfälschung sein, da sie durch Transitivität eine wahrheitsgemäße Beschreibung des Bildes liefert. Wir filtern diese Verfälschungen heraus.\nDieses Verfahren ist aber nicht perfekt, sondern nur ein Indikator für gültige Verfälschungen.\nDaher setzen wir als dritte Maßnahme zur Generierung gültiger Verfälschungen menschliche Annotatoren ein, um die in VALSE verwendeten Daten zu validieren.\nNach dem Filtern und der menschlichen Evaluation haben wir also so viele Testinstanzen, wie in dieser Tabelle beschrieben.\nBeachten Sie hier, dass VALSE keine Trainingsdaten, sondern nur Testdaten liefert.\nDa es sich um einen reinen Zero-Shot-Test-Benchmark handelt, ist er so konzipiert, dass er die bestehenden Fähigkeiten der Seh- und Sprachmodelle nach dem Vortraining nutzt.\nDie Feinabstimmung würde es den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen.\nWir alle wissen, dass diese Modells gerne schummeln und Abkürzungen nehmen.\nWie wir schon erwähnten, sind wir an der Bewertung interessiert, welche Fähigkeiten die Seh- und Sprachmodelle nach dem Vortraining haben.\nWir experimentieren mit fünf Seh- und Sprachmodellen auf VALSE, nämlich mit CLIP, LXMert, ViLBERT, ViLBERT zwölf in einem und VisualBERT.\nZwei unserer wichtigsten Evaluationsmetriken sind die Genauigkeit der Modelle bei der Klassifikation von Bild-Satz-Paaren in Bildbeschreibungen und Verfälschungen.\nVielleicht ist es für dieses Video relevanter, unsere tolerantere Metrik vorzustellen, die paarweise Genauigkeit. Diese misst, ob die Bild-Satz-Ausrichtung für das korrekte Bild-Text-Paar größer ist als für sein Verfälschungspaar.\nWeitere Metriken und Ergebnisse dazu finden Sie in unserem Paper.\nDie Ergebnisse mit paarweiser Genauigkeit werden hier gezeigt und stimmen mit den Ergebnissen überein, die wir von den anderen Metriken erhalten haben, nämlich dass die beste Zero-Shot-Leistung von ViLBERT zwölf zu eins erreicht wird, gefolgt von ViLBERT, LXMert, CLIP und schließlich VisualBERT.\nEs ist bemerkenswert, wie Instrumente, die sich auf die einzelnen Objekte konzentrieren, wie Existenz und Substantivphrasen, von ViLBERT zwölf zu eins fast gelöst werden. Das unterstreicht, dass Modelle in der Lage sind, benannte Objekte und ihre Existenz in Bildern zu identifizieren.\nKeines der verbleibenden Teile kann jedoch in unseren gegnerischen Verfälschungseinstellungen zuverlässig gelöst werden.\nWir sehen an den Instrumenten der Pluralität und der Zählung, dass Seh- und Sprachmodelle Schwierigkeiten haben, Verweise auf einzelne und mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen.\nDer Relationen-Teil zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Relation zwischen Objekten in einem Bild richtig zu klassifizieren.\nSie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie durch Plausibilitätsbias unterstützt werden, wie wir bei dem Handlung-Teil sehen.\nBeim Koreferenz-Teil geht hervor, dass die Verfolgung mehrerer Verweise auf dasselbe Objekt in einem Bild unter Verwendung von Pronomen auch für die Seh- und Sprachmodelle schwierig ist.\nZur Überprüfung der Korrektheit und weil es ein interessantes Experiment ist, vergleichen wir auch zwei reine Textmodelle, GPT 1 und GPT 2. So wollen wir feststellen, ob VALSE durch diese unimodalen Modelle lösbar ist. Wir berechnen die Perplexität der korrekten und der verfälschten Beschriftung, ohne Bild, um den Eintrag mit der niedrigsten Perplexität vorherzusagen.\nWenn die Perplexität bei der Verfälschung höher ist, ist das für uns ein Hinweis, dass die verfälschte Beschriftung möglicherweise unter Plausibilitätsverzerrungen oder anderen sprachlichen Verzerrungen leidet.\nEs ist interessant zu sehen, dass in einigen Fällen die GPT-Modelle mit reinem Text die Plausibilität der Welt besser erfasst haben als die Seh- und Sprachmodelle.\nZusammenfassend lässt sich sagen, dass VALSE ein Benchmark ist, der durch das Objektiv von linguistischen Konstrukten schaut, um die Gemeinschaft bei der Verbesserung von Seh- und Sprachmodellen zu unterstützen, indem er ihre visuelle Erdung hart testet.\nUnsere Experimente zeigen, dass Seh- und Sprachmodelle benannte Objekte und ihre Existenz in Bildern gut erkennen, wie der Existenzteil zeigt. Sie haben aber Schwierigkeiten damit, ihre gegenseitige Abhängigkeit und Beziehungen in visuellen Szenen zu begründen, wenn sie gezwungen sind, sprachliche Indikatoren zu berücksichtigen.\nWir möchten die Community wirklich dazu ermutigen, VALSE für die Messung des Fortschritts von Sprach- Erdung mit Seh- und Sprachmodellen zu nutzen.\nZudem könnte VALSE als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten. So kann kontrolliert werden, ob ein Datensatz dazu beiträgt, dass sich Modelle in einem der von VALSE getesteten Aspekte verbessern.\nWenn Sie Interesse haben, schauen Sie sich die VALSE-Daten auf GitHub an. Wenn Sie irgendwelche Fragen haben, zögern Sie nicht, uns zu kontaktieren.", "src_lang": "en", "tgt_lang": "de", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 420, "src_audio": "/acl6060/audio/en/420.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.\nI'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.\nI'll be explaining in this order.\nFirst, I will introduce automatic release note generation that we are working on in this research.\nA release note is a technical document that summarizes the changes distributed with each release of a software product.\nThe image shows a release note for version two point six point four of the vuejs library.\nRelease notes play an important role in open source development but they're time consuming to prepare manually.\nTherefore, it would be very useful to be able to automatically generate high quality release notes.\nI will defer to two previous researches on automatic release note generation.\nThe first is a system called ARENA released in twenty fourteen.\nIt takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.\nThe most notable feature of this system is the issue extractor in the upper right corner.\nWhich must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.\nIn other words, it cannot be used for many projects on GitHub.\nThe second is Glyph, recently announced in twenty twenty.\nIt is available on the internet and can be installed via pip.\nThis system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.\nThis image is a sample usage that returns a corrective or bug fixes label.\nGlyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.\nThe performance of the text classification model is not high.\nI present two related researches, but their problems are limited applicability and scarce data resources.\nOur paper solves these two problems and automatically generates high quality release notes.\nWith a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.\nThis proposed method can be used for all English repositories.\nFor the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.\nNext, I'll describe our dataset.\nHere is an example of data.\nThe left side is a commit message and the right side is the release notes.\nRelease notes are labeled as improvements or fixes, etc.\nWe have set up a task that takes the commit messages as input and outputs a labeled release notes.\nThis can be regarded as a summarization task.\nWe have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.\nThese were set based on previous research and other factors.\nThe release note on the bottom right is extracted from the release note on the bottom left.\nAt this time, it is necessary to detect the four labels that have been set up in advance.\nBut the labels are not always consistent with each repository.\nFor example, the improvements label includes improvements, enhancements, optimizations, and so on.\nWe prepared a vocabulary list of about thirty labels for each of these notational variations.\nThis is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.\nNext is a commit message.\nCommit messages are not tied to each release.\nAs shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.\nThis is a bit tedious and it is not enough to just get a list of releases and look at the before and after.\nWe created a heuristic matching rule to get the previous and next versions.\nDataset analysis.\nIn the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.\nAlso, the average number of release notes tokens is sixty three, which is quite high for a summarization task.\nAlso, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.\nThis is due to the large number of unique class or method names found in the repository.\nNext, I will explain the proposed method.\nThe classwise extractive then abstractive summarization model consists of two neural modules.\nA classifier using BERT or CodeBERT and a generator using BART.\nFirst, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.\nThe commit messages classified as other are discarded.\nThen CEAS applies the generator to the four labeled documents independently and generates release notes for each class.\nIn this task, the direct correspondences between commit messages and release notes are not known.\nTherefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.\nWe modeled the classwise abstractive summarization approach by two different methods.\nThe first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.\nThe output texts can be divided into classwise segments based on special class-specific endpoint symbols.\nThe second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.\nOkay, let me explain the experiments.\nFive methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.\nRegarding evaluation, in some cases, release notes are output in multiple sentences.\nSince it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.\nThe BLEU is penalized when the system outputs a short sentence.\nThis penalty results in a lower BLEU value in the experiment results described next.\nFinally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.\nA higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.\nHere are the results.\nSince the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.\nCEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.\nIn particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.\nThese results indicate that CEAS and CAS are significantly affected.\nCEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.\nHigh coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.\nCAS-Multi tended to yield higher ROUGE-L than CAS-Single.\nSuggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.\nHere are an error analysis.\nCAS methods tend to output shorter sentences than human reference sentences.\nIn the figure on the right, the reference sentence has three or four sentences, while CAS has only one.\nThe reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.\nFurthermore, CAS methods cannot generate accurate release notes without additional information.\nThe top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.\nThe example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.\nFinally, a conclusion.\nWe have built a new dataset for automatic release note generation.\nWe have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.\nOur experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.\nPlease check out our dataset on GitHub.\nThank you.", "tgt_ref": "Hallo, mein Name ist Kamezawa von der Universität Tokio.\nIch werde folgendes Paper vorstellen: RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.\nIch werde es in dieser Reihenfolge erklären.\nZunächst werde ich die automatische Generierung von Versionshinweisen vorstellen, an der wir in dieser Forschung arbeiten.\nEin Versionshinweis ist ein technisches Dokument, das die Änderungen zusammenfasst, die mit jeder Version eines Softwareprodukts vertrieben werden.\nDas Bild zeigt einen Versionshinweis für Version 2.6.4 der Vuejs-Bibliothek.\nVersionshinweise spielen bei der Open-Source-Entwicklung eine wichtige Rolle, aber ihre Erstellung ist manuell zeitaufwändig.\nDaher wäre es sehr nützlich, wenn man automatisch qualitativ hochwertige Versionshinweise generieren könnte.\nIch werde zwei frühere Untersuchungen zur automatischen Generierung von Versionshinweisen erwähnen.\nZuerst gibt es ein System mit dem Namen ARENA, das im Jahr 2014 veröffentlicht wurde.\nEs verfolgt einen regelbasierten Ansatz, indem es zum Beispiel den Change-Extractor verwendet, um alle Unterschiede, Bibliotheksänderungen und Dokumentänderungen aus den verschiedenen Versionen zu extrahieren und sie schließlich zu kombinieren.\nDie auffälligste Funktion dieses Systems ist der Ausgabe-Extraktor in der oberen rechten Ecke.\nDies muss Jira, dem Fehlerverwaltungssystem, überlassen werden und kann nur auf Projekte angewendet werden, die Jira verwenden.\nMit anderen Worten: Es kann nicht für viele Projekte auf GitHub verwendet werden.\nDas zweite ist Glyph, das kürzlich in 2020 angekündigt wurde.\nEs ist im Internet verfügbar und kann über pip installiert werden.\nDieses System verfügt über ein einfaches, lernbasiertes Textklassifikationsmodell und gibt eine von fünf Markierung wie Funktionen oder Fehlerbehebungen für jede eingegebene Commit-Nachricht aus.\nDieses Bild ist eine Stichprobe, das eine Markierung für eine Korrektur oder Fehlerbehebung zurückgibt.\nDie Trainingsdaten von Glyph sind ziemlich klein, etwa 5.000, und werden in den unten beschriebenen Experimenten gezeigt.\nDie Leistung des Textklassifikationsmodells ist nicht hoch.\nIch stelle zwei verwandte Forschungsarbeiten vor, deren Probleme jedoch die begrenzte Anwendbarkeit und die knappen Datenressourcen sind.\nUnser Paper löst diese zwei Probleme und erzeugt automatisch qualitativ hochwertige Versionshinweise.\nAufgrund des Problems der begrenzten Anwendbarkeit schlagen wir eine qualitativ hochwertige, klassenweise Zusammenfassungsmethode vor, die nur Commit-Nachrichten als Eingabe verwendet.\nDiese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden.\nFür das zweite Problem der knappen Datenressourcen haben wir unseren RNSum-Datensatz erstellt, der aus etwa 82.000 Daten besteht. Diese wurden aus den Daten der öffentlichen GitHub-Repositories mithilfe der GitHub-API gesammelt.\nAls Nächstes werde ich unseren Datensatz beschreiben.\nHier ist ein Beispiel für die Daten.\nAuf der linken Seite ist eine Commit-Nachricht und auf der rechten Seite sind die Versionshinweise.\nVersionshinweise sind als Verbesserungen, Korrekturen usw. markiert.\nWir haben eine Aufgabe eingerichtet, die die Commit-Nachricht als Eingabe aufnimmt und die Ausgabe ist ein markierter Versionshinweis.\nDies kann als eine Zusammenfassungsaufgabe betrachtet werden.\nWir haben vier Markierungen vordefiniert: Funktionen, Verbesserungen, Fehlerbehebungen, Beseitigung von Verwerfungen und bahnbrechende Änderungen.\nDiese wurden auf der Grundlage von früheren Forschungen und anderen Faktoren festgelegt.\nDie Versionshinweise unten rechts wurde aus den Versionshinweisen unten links extrahiert.\nZu diesem Zeitpunkt müssen die vier Markierungen, die im Voraus erstellt wurden, erkannt werden.\nDie Markierungen sind jedoch nicht immer mit jedem Repository konsistent.\nDie Markierung „Verbesserungen“ umfasst beispielsweise Verbesserungen, Erweiterungen, Optimierungen und so weiter.\nWir haben eine Wortschatzliste mit etwa 30 Markierungen für jede dieser Notationsvarianten erstellt.\nDiese dient dazu, die Klasse des Versionshinweises zu erkennen. Sie sammelt den Text des Versionshinweises, der als Versionshinweissatz für die Klasse folgt.\nAls Nächstes folgt eine Commit-Nachricht.\nCommit-Nachrichten sind nicht an die einzelnen Versionen gebunden.\nWenn die aktuelle Version 2.5 bis 19 ist, müssen wir, wie im Bild unten gezeigt, die frühere Version 2.5 bis 18 identifizieren und eine Diff erhalten.\nDies ist etwas mühsam und es reicht nicht aus, nur eine Liste von Versionen zu erstellen und die Vorher-Nachher-Bilanz zu betrachten.\nWir haben eine heuristische Abgleichsregel erstellt, um die vorherige und die nächste Version zu bekommen.\nDie Analyse des Datensatzes.\nAm Ende wurden 7.200 Repositories und 82 Daten gesammelt.\nAußerdem liegt die durchschnittliche Anzahl der Versionshinweis-Token bei 63, was für eine Zusammenfassungsaufgabe recht hoch ist.\nAuch die Anzahl der einzigartigen Token ist mit 830.000 recht groß.\nDies ist auf die große Anzahl von eindeutigen Klassen- oder Methodennamen im Repository zurückzuführen.\nAls Nächstes werde ich die vorgeschlagene Methode erläutern.\nDas klassenweise extraktive und dann abstrahierende Zusammenfassungsmodell besteht aus zwei neuronalen Modulen.\nEin Klassifikator verwendet BERT oder CodeBERT und ein Generator verwendet BART.\nZuerst verwendet CEAS einen Klassifikator, um jede Commit-Nachricht in fünf Klassen von Versionshinweisen zu klassifizieren, die Verbesserungen, Fehlerbehebungen, Verwerfungen und eine weitere Klasse enthalten.\nDie als „Sonstiges“ eingestuften Commit-Nachrichten werden verworfen.\nDann wendet CEAS den Generator auf die vier markierten Dokumente unabhängig voneinander an und erstellt Versionshinweise für jede Klasse.\nIn dieser Aufgabe sind die direkten Korrespondenzen zwischen Commit-Nachrichten und Versionshinweisen nicht bekannt.\nUm den Klassifikator zu trainieren, haben wir daher jeder eingegebenen Commit-Nachricht eine Umfrage zugewiesen und dabei die ersten zehn Zeichen jeder Commit-Nachricht verwendet.\nWir haben den klassenweisen abstrakten Zusammenfassungsansatz durch zwei verschiedene Methoden modelliert.\nDas erste Modell, das wir CAS-Single nennen, besteht aus einem einzigen 6-zu-6-Netzwerk und generiert einen einzigen Versionshinweistext, der eine Verkettung von eingegebenen Commit-Nachrichten ergibt.\nDie ausgegebenen Texte können auf der Grundlage spezieller klassenspezifischer Endpunktsymbole in klassenweise Segmente unterteilt werden.\nDie zweite Methode, die wir CAS-Multi nennen, besteht aus vier verschiedenen seq2seq-Netzwerken, von denen jedes einer der festen Versionshinweisklassen entspricht.\nIch werde nun die Experimente erklären.\nFünf Methoden wurden verglichen: CEAS, CAS-Single, CAS-Multi, Clustering und Glyph aus einer früheren Studie.\nWas die Evaluation betrifft, so werden in einigen Fällen die Versionshinweise in mehreren Sätzen ausgegeben.\nDa es schwierig ist, die Anzahl von Sätzen zu berechnen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt.\nDer BLEU wird benachteiligt, wenn das System einen kurzen Satz ausgibt.\nDieser Abzug führt zu einem niedrigeren BLEU-Wert in den nachfolgend beschriebenen Versuchsergebnissen.\nSchließlich berechnen wir auch die Spezifität, da ROUGE und BLEU nicht berechnet werden können, wenn die Versionshinweise leer sind.\nEine höhere Spezifität bedeutet, dass das Modell in Fällen, in denen die Versionshinweise leer sind, einen leeren Text korrekt ausgibt.\nHier sind die Resultate.\nDa der Datensatz E-Mail-Adressen, Hash-Werte usw. enthält, haben wir auch den bereinigten Datensatz ausgewertet, der diese ausschließt.\nCEAS und CAS erreichten ROUGE-L-Werte, die mehr als zehn Punkte über den Baselines lagen.\nInsbesondere beim bereinigten Testsatz ist der Abstand zwischen der vorgeschlagenen Methode und den Baselines auf mehr als zwanzig Punkte angestiegen.\nDiese Ergebnisse zeigen, dass CEAS und CAS signifikant betroffen sind.\nCEAS erzielte bessere ROUGE-L-Werte als CAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators beim Training des Klassifikators mit Pseudo-Markierungen effektiv ist.\nEine hohe Abdeckung von CEAS kann wahrscheinlich erreicht werden, weil sich der Klassifikator auf die Auswahl relevanter Commit-Nachrichten für jede Klasse konzentrieren kann.\nCAS-Multi ergab tendenziell mehr ROUGE-L-Werte als CAS-Single.\nWir schlagen vor, dass es auch effektiv ist, unterschiedliche abstrakte Zusammenfassungsmodelle, die unabhängig voneinander sind, für jede Klasse von den Versionshinweisen zu entwickeln.\nHier ist eine Fehleranalyse.\nCAS-Methoden geben tendenziell kürzere Sätze aus als menschliche Referenzsätze.\nIn der Abbildung rechts besteht der Referenzsatz aus drei oder vier Sätzen, während CAS aus nur einem besteht.\nDer Grund für die Reluktanz dieses Modells ist, dass in den Trainingsdaten nur 33 Prozent der Sätze in der Markierung Funktionen und 40 Prozent in der Markierung Verbesserung zu finden sind.\nDarüber hinaus können CAS-Methoden ohne zusätzliche Informationen keine genauen Versionshinweise erstellen.\nOben auf der rechten Seite ist ein Beispiel für eine sehr unübersichtliche Commit-Nachricht. Der vollständige Satz kann nicht ohne Bezug auf den entsprechenden Fortschritt oder das Problem generiert werden.\nDas folgende Beispiel zeigt, dass die beiden Commit-Nachrichten in der Eingabe zusammenhängen und zu einem Satz zusammengefasst werden sollten, was jedoch nicht gelingt.\nNun kommen wir zum Fazit.\nWir haben einen neuen Datensatz für die automatische Generierung von Versionshinweisen erstellt.\nWir haben auch eine Aufgabe für die Eingabe von Commit-Nachrichten und deren Zusammenfassung formuliert, sodass sie für alle auf Englisch geschriebene Projekte anwendbar ist.\nUnsere Experimente zeigen, dass die vorgeschlagene Methode weniger verrauschte Versionshinweise mit höherem Abdeckungsgrad als die Baselines erzeugt.\nBitte sehen Sie sich unseren Datensatz auf GitHub an.\nVielen Dank!", "src_lang": "en", "tgt_lang": "de", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
