{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 0, "src_audio": "/acl6060/audio/en/0.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour tout le monde. Aujourd'hui, je vais vous présenter notre travail de recherche « Apprendre à raisonner par déduction » : résolution de problèmes de mots mathématiques comme extraction de relation complexe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 1, "src_audio": "/acl6060/audio/en/1.wav", "src_ref": "I'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'appelle Allan du laboratoire d'intelligence artificielle ByteDance, et ceci est un travail en commun avec Jierui Li de l'Université du Texas à Austin et Wei Lu du SUTD.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 2, "src_audio": "/acl6060/audio/en/2.wav", "src_ref": "First, I'd like to talk about our motivation for reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, j'aimerais parler de notre motivation pour le raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 3, "src_audio": "/acl6060/audio/en/3.wav", "src_ref": "So here we show an examples where multi-step reasoning is helpful.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous montrons un exemple où le raisonnement en plusieurs étapes est utile.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 4, "src_audio": "/acl6060/audio/en/4.wav", "src_ref": "So this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce chiffre est donc tiré de l'article PaLM où ils incitent à résoudre le problème de réseau dans le scénario d'apprentissage de quelques prises de vue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 5, "src_audio": "/acl6060/audio/en/5.wav", "src_ref": "So on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, sur le côté gauche, nous pouvons voir que si nous fournissons quelques exemples avec juste des questions et des réponses, nous pourrions ne pas être en mesure d'obtenir les bonnes réponses.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 6, "src_audio": "/acl6060/audio/en/6.wav", "src_ref": "But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si nous fournissons plus de description du raisonnement, le modèle est capable de prévenir la description du raisonnement, mais aussi d'effectuer ici une prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 7, "src_audio": "/acl6060/audio/en/7.wav", "src_ref": "So it is good to have interpretable multi-step reasoning as output.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est alors bon d'avoir comme résultat un raisonnement à plusieurs étapes interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 8, "src_audio": "/acl6060/audio/en/8.wav", "src_ref": "And we also think math word problem is a straightforward application to evaluate such reasoning abilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pensons également que les problèmes de mots mathématiques sont une application directe pour évaluer de telles capacités de raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 9, "src_audio": "/acl6060/audio/en/9.wav", "src_ref": "So, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 10, "src_audio": "/acl6060/audio/en/10.wav", "src_ref": "So in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans nos données, nous recevons également l‘expression mathématique qui conduit à cette réponse particulière.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 11, "src_audio": "/acl6060/audio/en/11.wav", "src_ref": "So, certain assumptions ah also apply as in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, certaines hypothèses s'appliquent également comme dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 12, "src_audio": "/acl6060/audio/en/12.wav", "src_ref": "We assume the precision of quantities are known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que la précision des quantités est connue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 13, "src_audio": "/acl6060/audio/en/13.wav", "src_ref": "And we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 14, "src_audio": "/acl6060/audio/en/14.wav", "src_ref": "Furthermore, complicated operators can be actually decomposed into these basic operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les opérateurs compliqués peuvent effectivement être décomposés en ces opérateurs de base.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 15, "src_audio": "/acl6060/audio/en/15.wav", "src_ref": "So, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, le travail antérieur dans la résolution de problèmes de mots mathématiques peut en effet être classé dans un modèle séquence à séquence et séquence à arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 16, "src_audio": "/acl6060/audio/en/16.wav", "src_ref": "So, traditional sequence to sequence model convert the expression to a specific sequence for generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle séquence à séquence traditionnel convertit donc l'expression en une séquence spécifique pour la génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 17, "src_audio": "/acl6060/audio/en/17.wav", "src_ref": "And it is pretty easy to implement and it can generalize to many different complicated problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est assez facile à mettre en œuvre et peut être généralisé à de nombreux problèmes compliqués différents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 18, "src_audio": "/acl6060/audio/en/18.wav", "src_ref": "But the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle structuré et son manque d'interopérabilité pour la prévention.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 19, "src_audio": "/acl6060/audio/en/19.wav", "src_ref": "But actually this direction is still quite popular because of um the transformer model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, cette direction est encore très populaire en raison du modèle de conversion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 20, "src_audio": "/acl6060/audio/en/20.wav", "src_ref": "So, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans les modèles à base d'arbre, nous structurons en réalité ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 21, "src_audio": "/acl6060/audio/en/21.wav", "src_ref": "So here we keep generating the operators until we reach the leaves, which are the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous continuons à produire les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 22, "src_audio": "/acl6060/audio/en/22.wav", "src_ref": "So here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, la bonne chose est qu'en fait, cela nous donne cette structure d'arbre binaire. Mais en réalité, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur et ensuite, à la fin, nous générons les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 23, "src_audio": "/acl6060/audio/en/23.wav", "src_ref": "And the second thing is that it also contains some repetitive computations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que cela contient également des calculs répétitifs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 24, "src_audio": "/acl6060/audio/en/24.wav", "src_ref": "So here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, si nous regardons cette expression, huit fois trois plus trois est en fait généré deux fois, mais en réalité, nous devrions réutiliser les résultats.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 25, "src_audio": "/acl6060/audio/en/25.wav", "src_ref": "So, in our proposed approach we want to solve those problems in a step by step and interpretable manners.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 26, "src_audio": "/acl6060/audio/en/26.wav", "src_ref": "So for example, here in the second step, ah we can obtain these divisors which is twenty seven.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, ici dans la deuxième étape, nous pouvons obtenir ces diviseurs qui sont vingt-sept.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 27, "src_audio": "/acl6060/audio/en/27.wav", "src_ref": "And we can also refer back to the original questions to find the relevant contents.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 28, "src_audio": "/acl6060/audio/en/28.wav", "src_ref": "And in these steps we obtain the divisors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et dans ces étapes, nous obtenons les diviseurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 29, "src_audio": "/acl6060/audio/en/29.wav", "src_ref": "So, ah and then at this third step we actually get the quotient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis, à cette troisième étape, nous obtenons alors le quotient.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 30, "src_audio": "/acl6060/audio/en/30.wav", "src_ref": "Alright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Et après ces trois étapes, nous pouvons alors réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, et enfin, nous pouvons obtenir les dividendes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 31, "src_audio": "/acl6060/audio/en/31.wav", "src_ref": "So, here we actually generate the whole expression directly rather than generating a single operators or quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous générons en réalité l'expression entière directement plutôt que de produire un seul opérateur ou une seule quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 32, "src_audio": "/acl6060/audio/en/32.wav", "src_ref": "So this makes the process more accurate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela rend le processus plus précis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 33, "src_audio": "/acl6060/audio/en/33.wav", "src_ref": "So, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et incluant également une certaine constante comme notre état initial.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 34, "src_audio": "/acl6060/audio/en/34.wav", "src_ref": "So, the expression is represented by e i j o p.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, l'expression est représentée par e i j o p.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 35, "src_audio": "/acl6060/audio/en/35.wav", "src_ref": "Where we perform operator from q_i to q_j, and such expression is actually directed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Où nous effectuons l'opérateur de q_i à q_j, et cette expression est en fait dirigée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 36, "src_audio": "/acl6060/audio/en/36.wav", "src_ref": "So, we also have subtraction with words here to represent the opposite direction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, nous avons également ici la soustraction avec des mots pour représenter la direction opposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 37, "src_audio": "/acl6060/audio/en/37.wav", "src_ref": "This is quite similar to relation extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est tout à fait similaire à l'extraction de relation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 38, "src_audio": "/acl6060/audio/en/38.wav", "src_ref": "So in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans un système déductif formel, à un pas de temps t, on applique l'opérateur entre la paire q_i et q_j, puis on obtient cette nouvelle expression.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 39, "src_audio": "/acl6060/audio/en/39.wav", "src_ref": "We add it to the next state to become a new quantity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous l'ajoutons à l'état suivant pour devenir une nouvelle quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 40, "src_audio": "/acl6060/audio/en/40.wav", "src_ref": "So, these slides actually visualize the evolution of the state where we keep adding expression to the current state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, ces diapositives visualisent en réalité l'évolution de l'état où nous continuons à ajouter de l'expression à l'état actuel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 41, "src_audio": "/acl6060/audio/en/41.wav", "src_ref": "So in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langue préformée qui peut être BERT ou Roberta, puis nous encodons la phrase et obtenons ensuite ces représentations de quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 42, "src_audio": "/acl6060/audio/en/42.wav", "src_ref": "So, once we get the quantity representations, we can start to do inference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire l'inférence.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 43, "src_audio": "/acl6060/audio/en/43.wav", "src_ref": "Here we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous montrons ici un exemple de q_1 pour obtenir la représentation pour q_2 divisée par q_2 multipliée par q_3.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 44, "src_audio": "/acl6060/audio/en/44.wav", "src_ref": "First we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous obtenons la représentation de paire, qui n'est essentiellement que l'enchaînement entre q_1 et q_2, puis nous appliquons un réseau prédictif qui est paramétré par l'opérateur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 45, "src_audio": "/acl6060/audio/en/45.wav", "src_ref": "And then finally we obtain the expression representation q_1 divided by q_2.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et enfin, nous obtenons la représentation de l'expression q_1 divisée par q_2.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 46, "src_audio": "/acl6060/audio/en/46.wav", "src_ref": "But in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, dans la pratique, au stade de l'inférence, nous pourrions également obtenir une expression incorrecte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 47, "src_audio": "/acl6060/audio/en/47.wav", "src_ref": "So, here all the possible expression is equals to three times the number of operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, toute l'expression possible est égale à trois fois le nombre d'opérateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 48, "src_audio": "/acl6060/audio/en/48.wav", "src_ref": "So the nice thing here is that we can easily add constraints to control this search this search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors, ce qui est bien ici, c'est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 49, "src_audio": "/acl6060/audio/en/49.wav", "src_ref": "For example, if this expression is not allowed, we can simply remove this expression in our search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 50, "src_audio": "/acl6060/audio/en/50.wav", "src_ref": "So in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans la deuxième étape, nous faisons la même chose, mais la seule différence est que nous avons une quantité de plus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 51, "src_audio": "/acl6060/audio/en/51.wav", "src_ref": "So this quantity come from the previous calculated expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette quantité provient donc de l'expression calculée antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 52, "src_audio": "/acl6060/audio/en/52.wav", "src_ref": "So finally we can obtain this final expression q_3 times q_4.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous pouvons obtenir cette expression finale q_3 multipliée par q_4.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 53, "src_audio": "/acl6060/audio/en/53.wav", "src_ref": "And we can also see the number of all the possible ah expression is different from the previous step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 54, "src_audio": "/acl6060/audio/en/54.wav", "src_ref": "So, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une telle différence rend difficile l'application de la beam search car la distribution de probabilité entre ces deux étapes est déséquilibrée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 55, "src_audio": "/acl6060/audio/en/55.wav", "src_ref": "So the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La procédure de formation est donc similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 56, "src_audio": "/acl6060/audio/en/56.wav", "src_ref": "And here we also use this tau to represent when we should terminate this generation process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous utilisons également ce tau pour représenter le moment où nous devrions mettre fin à ce processus de génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 57, "src_audio": "/acl6060/audio/en/57.wav", "src_ref": "And here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, l'espace est différent de séquence à séquence car il est différent à chaque pas de temps, alors que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 58, "src_audio": "/acl6060/audio/en/58.wav", "src_ref": "And it also allows us to impose certain constraints from prior from prior knowledge.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 59, "src_audio": "/acl6060/audio/en/59.wav", "src_ref": "So we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous menons donc des expériences sur les données de problèmes de mots mathématiques, MAWPS, Math23K, MathQA et SVAMP couramment utilisées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 60, "src_audio": "/acl6060/audio/en/60.wav", "src_ref": "And here we briefly show the results compared with the previous best approaches.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons brièvement les résultats comparés aux meilleures approches antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 61, "src_audio": "/acl6060/audio/en/61.wav", "src_ref": "So our best performing variant is Roberta-DeductiveReasoner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, notre variante la plus performante est Roberta-DeuctiveReasoner.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 62, "src_audio": "/acl6060/audio/en/62.wav", "src_ref": "And in fact we do not use beam search, in contrast all previous approaches are using beam search.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en réalité, nous n'utilisons pas la beam search ; au contraire, toutes les approches antérieures utilisent la beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 63, "src_audio": "/acl6060/audio/en/63.wav", "src_ref": "All right. So, the best approaches are often tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Ainsi, les meilleures approches sont souvent un modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 64, "src_audio": "/acl6060/audio/en/64.wav", "src_ref": "So, overall our reasoner is able to significantl significantly outperform this tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans l'ensemble, notre raisonneur est capable de dépasser significativement ce modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 65, "src_audio": "/acl6060/audio/en/65.wav", "src_ref": "But we can see the absolute numbers on MathQA or SVAMP are not really high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons voir que les nombres absolus sur MathQA ou SVAMP ne sont pas vraiment élevés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 66, "src_audio": "/acl6060/audio/en/66.wav", "src_ref": "So we further investigate the results on SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous étudions donc plus en détail les résultats sur SVAMP.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 67, "src_audio": "/acl6060/audio/en/67.wav", "src_ref": "And this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ces données sont difficiles parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle TAL traitement automatique du langage naturel comme ajouter des informations non pertinentes et des quantités supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 68, "src_audio": "/acl6060/audio/en/68.wav", "src_ref": "So, in our prediction we find some of the intermediate values are actually negatives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans notre prévention, nous trouvons que certaines des valeurs intermédiaires sont en réalité négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 69, "src_audio": "/acl6060/audio/en/69.wav", "src_ref": "For example, um, in these questions we are asking how many apples does Jake have?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans ces questions, nous demandons « combien de pommes a Jake ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 70, "src_audio": "/acl6060/audio/en/70.wav", "src_ref": "But we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous avons des informations supplémentaires comme « dix-sept photos de moins », et « Steven en a huit », ce qui est totalement hors de propos.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 71, "src_audio": "/acl6060/audio/en/71.wav", "src_ref": "So, our model makes some prediction like this which is producing negative values.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, notre modèle effectue une prévention comme celle-ci qui produit des valeurs négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 72, "src_audio": "/acl6060/audio/en/72.wav", "src_ref": "And we observe these two expressions actually have similar scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous constatons que ces deux expressions ont en réalité des scores similaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 73, "src_audio": "/acl6060/audio/en/73.wav", "src_ref": "So, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pouvons alors limiter cet espace de recherche en supprimant les résultats négatifs afin que nous puissions rendre la réponse correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 74, "src_audio": "/acl6060/audio/en/74.wav", "src_ref": "So um we further find such constraint actually improves quite a lot for some models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous trouvons donc que cette contrainte améliore en réalité beaucoup pour certains modèles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 75, "src_audio": "/acl6060/audio/en/75.wav", "src_ref": "For example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous améliorons sept points, puis pour le modèle de base Roberta, nous avons amélioré deux points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 76, "src_audio": "/acl6060/audio/en/76.wav", "src_ref": "So better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le meilleur modèle de langue a donc de meilleures capacités de compréhension de la langue de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 77, "src_audio": "/acl6060/audio/en/77.wav", "src_ref": "And we also try to analyze the difficulty behind these behind all these datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous essayons également d'analyser la difficulté derrière toutes ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 78, "src_audio": "/acl6060/audio/en/78.wav", "src_ref": "We assume the number of unused quantities can be regarded as irrelevant information here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que le nombre de quantités inutilisées peut être considéré ici comme une information non pertinente.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 79, "src_audio": "/acl6060/audio/en/79.wav", "src_ref": "So ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et les données SVAMP ont la plus grande partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 80, "src_audio": "/acl6060/audio/en/80.wav", "src_ref": "And here we also show the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons également la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 81, "src_audio": "/acl6060/audio/en/81.wav", "src_ref": "For those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ces échantillons sans quantités inutilisées, la performance est en réalité supérieure à la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 82, "src_audio": "/acl6060/audio/en/82.wav", "src_ref": "But with those samples that with unused quantity is actually way worse than the, worse than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais avec ces échantillons ayant une quantité inutilisée, c'est en fait bien pire que la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 83, "src_audio": "/acl6060/audio/en/83.wav", "src_ref": "For MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour MAWPS, nous n'avons pas vraiment trop de cas de test, alors j'ignore simplement cette partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 84, "src_audio": "/acl6060/audio/en/84.wav", "src_ref": "So, finally we want to show the interpretability through a question perturbation example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous voulons montrer l'interopérabilité à travers un exemple de perturbation de question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 85, "src_audio": "/acl6060/audio/en/85.wav", "src_ref": "So here our model actually makes a wrong prediction at the first step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, notre modèle effectue en réalité une prévention erronée à la première étape.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 86, "src_audio": "/acl6060/audio/en/86.wav", "src_ref": "So, we can actually correlate this expression with the sentence here. Alright.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, nous pouvons effectivement corréler cette expression avec la phrase ici. Très bien.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 87, "src_audio": "/acl6060/audio/en/87.wav", "src_ref": "So, we think this sentence might be misleading the model to an incorrect predictions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pensons donc que cette phrase pourrait induire le modèle en erreur avec des préventions incorrectes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 88, "src_audio": "/acl6060/audio/en/88.wav", "src_ref": "So here planting another thirty five makes the model makes the model think it should be an addition operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, en planter trente-cinq autres fait penser au modèle qu'il devrait être un opérateur d'addition.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 89, "src_audio": "/acl6060/audio/en/89.wav", "src_ref": "So we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous essayons alors de réviser la phrase pour que cela soit quelque chose comme « le nombre de poiriers est inférieur de trente-cinq à celui des pommiers ».", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 90, "src_audio": "/acl6060/audio/en/90.wav", "src_ref": "So, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous faisons en sorte de transmettre une sémantique plus précise afin que le modèle soit capable de rendre la prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 91, "src_audio": "/acl6060/audio/en/91.wav", "src_ref": "So, this study shows how the interpretable predictions help us understand the model behavior.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, cette étude montre comment les préventions interprétables nous aident à comprendre le comportement du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 92, "src_audio": "/acl6060/audio/en/92.wav", "src_ref": "So to conclude our work, so first our model is actually pretty efficient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour conclure notre travail, notre modèle est en réalité assez efficace.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 93, "src_audio": "/acl6060/audio/en/93.wav", "src_ref": "And we are able to provide interpretable solving procedure.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous sommes en mesure de fournir une procédure de résolution interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 94, "src_audio": "/acl6060/audio/en/94.wav", "src_ref": "And we can easily incorporate some prior knowledge as constraint which can help improve the performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons facilement incorporer des connaissances antérieures en tant que contraintes qui peuvent aider à améliorer la performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 95, "src_audio": "/acl6060/audio/en/95.wav", "src_ref": "And the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution des problèmes de réseau, mais aussi aux autres tâches qui impliquent un raisonnement à plusieurs étapes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 96, "src_audio": "/acl6060/audio/en/96.wav", "src_ref": "We also have certain limitations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également certaines limites.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 97, "src_audio": "/acl6060/audio/en/97.wav", "src_ref": "Ah, if we have a large number of operators or constants, the memory consumption could be pretty high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 98, "src_audio": "/acl6060/audio/en/98.wav", "src_ref": "And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que, comme mentionné, puisque la distribution de probabilité est déséquilibrée entre les différentes étapes de temps, il est donc également assez difficile d'appliquer la stratégie de beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 99, "src_audio": "/acl6060/audio/en/99.wav", "src_ref": "So this is the end of the talk, and questions are welcomed. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous arrivons donc à la fin de la discussion, et vos questions sont les bienvenues. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 100, "src_audio": "/acl6060/audio/en/100.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 101, "src_audio": "/acl6060/audio/en/101.wav", "src_ref": "I will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter mon travail en commun avec Jerry, qui porte sur de nouvelles données pour l'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 102, "src_audio": "/acl6060/audio/en/102.wav", "src_ref": "Legal issues are an integral part of many people's lives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions juridiques font partie intégrante de la vie de nombreuses personnes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 103, "src_audio": "/acl6060/audio/en/103.wav", "src_ref": "But the majority of citizens have little to know knowledge about their rights and fundamental legal processes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais la majorité des citoyens ont peu de connaissances sur leurs droits et leurs processus juridiques fondamentaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 104, "src_audio": "/acl6060/audio/en/104.wav", "src_ref": "As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En conséquence, de nombreux citoyens vulnérables qui n'ont pas les moyens de se payer l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 105, "src_audio": "/acl6060/audio/en/105.wav", "src_ref": "All work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tous les travaux visent à combler le fossé entre les personnes et la loi en développant un système d'extraction efficace pour les articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 106, "src_audio": "/acl6060/audio/en/106.wav", "src_ref": "Such a system could provide a free professional legal help service for unskilled humans.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un tel système pourrait fournir un service d'aide juridique professionnel gratuit aux personnes non qualifiées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 107, "src_audio": "/acl6060/audio/en/107.wav", "src_ref": "Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avant de plonger dans la principale contribution de ce travail, nous allons d'abord décrire le problème d'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 108, "src_audio": "/acl6060/audio/en/108.wav", "src_ref": "Given a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Compte tenu d'une simple question sur une question juridique telle que : « qu'est-ce que je risque si je viole la confidentialité professionnelle ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 109, "src_audio": "/acl6060/audio/en/109.wav", "src_ref": "A model is required to retrieve all relevant statutory articles from a large body of legislation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un modèle est nécessaire pour extraire tous les articles de droit pertinents d'un grand corpus législatif.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 110, "src_audio": "/acl6060/audio/en/110.wav", "src_ref": "This information retrieval task comes with its own set of challenges.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette tâche d'extraction des informations comporte son propre ensemble de défis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 111, "src_audio": "/acl6060/audio/en/111.wav", "src_ref": "First, it deals with two types of language.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, elle traite de deux types de langue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 112, "src_audio": "/acl6060/audio/en/112.wav", "src_ref": "Common natural language for the questions and complex legal language for the statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La langue naturelle commune pour les questions et la langue juridique complexe pour les lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 113, "src_audio": "/acl6060/audio/en/113.wav", "src_ref": "This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette différence dans les distributions de langue rend plus difficile pour un système d'extraire les candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 114, "src_audio": "/acl6060/audio/en/114.wav", "src_ref": "Besides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le droit écrit n'est pas une pile d'articles indépendants qui peuvent être traités comme une source complète d'informations par eux-mêmes, contrairement aux nouvelles ou aux recettes, par exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 115, "src_audio": "/acl6060/audio/en/115.wav", "src_ref": "Instead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, il s'agit d'un ensemble structuré de dispositions juridiques qui n'ont un sens entier que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les domaines et sous-domaines auxquels elles appartiennent et leur place dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 116, "src_audio": "/acl6060/audio/en/116.wav", "src_ref": "Lastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, les articles de droit ne sont pas de petits paragraphes constituant généralement l'unité d'extraction typique dans la plupart des travaux d'extraction.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 117, "src_audio": "/acl6060/audio/en/117.wav", "src_ref": "Here, there are long documents that may be up to six thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ici, il y a de longs documents qui peuvent aller jusqu'à six mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 118, "src_audio": "/acl6060/audio/en/118.wav", "src_ref": "The recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les recent advances en matière de TAL traitement automatique du langage naturel ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prévention du jugement juridique ou l'examen des contrats de contacts automatisés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 119, "src_audio": "/acl6060/audio/en/119.wav", "src_ref": "But statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais l'extraction de l'article de droit est restée essentiellement inchangée en raison du manque de grandes données étiquetées de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 120, "src_audio": "/acl6060/audio/en/120.wav", "src_ref": "In this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans ce travail, nous présentons de nouvelles données centrées sur le citoyen natif français pour étudier si l'extraction des modèles peut se rapprocher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche d'extraction de l'article de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 121, "src_audio": "/acl6060/audio/en/121.wav", "src_ref": "Our Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos données d'extraction de l'article de droit belge BSARD se composent de plus de mille cent questions juridiques posées par des citoyens belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 122, "src_audio": "/acl6060/audio/en/122.wav", "src_ref": "These questions cover a wide range of topics from family, housing, money, to work and social security.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces questions couvrent un wide range de sujets allant de la famille au logement, à l'argent, au travail et à la sécurité sociale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 123, "src_audio": "/acl6060/audio/en/123.wav", "src_ref": "Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques de codes de droit belge.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 124, "src_audio": "/acl6060/audio/en/124.wav", "src_ref": "Let's now talk about how we collected this dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Parlons maintenant de la façon dont nous avons collecté ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 125, "src_audio": "/acl6060/audio/en/125.wav", "src_ref": "First, we started by compiling a large corpus of legal articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 126, "src_audio": "/acl6060/audio/en/126.wav", "src_ref": "We considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons examiné trente-deux codes belges accessibles au public et extrait tous les articles ainsi que les titres de section correspondants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 127, "src_audio": "/acl6060/audio/en/127.wav", "src_ref": "Then we gathered legal questions with references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous avons rassemblé les questions juridiques avec les références aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 128, "src_audio": "/acl6060/audio/en/128.wav", "src_ref": "To do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ce faire, nous nous associons au cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur une question juridique personnelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 129, "src_audio": "/acl6060/audio/en/129.wav", "src_ref": "We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons eu la chance d'avoir accès à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 130, "src_audio": "/acl6060/audio/en/130.wav", "src_ref": "We collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons recueilli des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 131, "src_audio": "/acl6060/audio/en/131.wav", "src_ref": "Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous avons fait passer les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avons considérés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 132, "src_audio": "/acl6060/audio/en/132.wav", "src_ref": "The remaining references were matched and converted to the corresponding article ids from our corpus.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les références restantes ont été appariées et converties aux identifiants d'article correspondants de notre corpus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 133, "src_audio": "/acl6060/audio/en/133.wav", "src_ref": "We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous nous sommes finalement retrouvés avec mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 134, "src_audio": "/acl6060/audio/en/134.wav", "src_ref": "In addition, each question comes with the main category and a concatenation of subcategories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, chaque question est accompagnée de la catégorie principale et d'un enchaînement de sous-catégories.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 135, "src_audio": "/acl6060/audio/en/135.wav", "src_ref": "And each articles comes with a concatenation of the subsequence heading in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et chaque article comporte un enchaînement de la rubrique de sous-séquence dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 136, "src_audio": "/acl6060/audio/en/136.wav", "src_ref": "This extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour des recherches futures sur l'extraction d'informations juridiques ou la classification de textes juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 137, "src_audio": "/acl6060/audio/en/137.wav", "src_ref": "Let's look at some characteristic of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jetons un coup d'œil à certaines caractéristiques de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 138, "src_audio": "/acl6060/audio/en/138.wav", "src_ref": "The questions are between five and forty four words long with a median of fourteen words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions comportent entre cinq et quarante-quatre mots avec une moyenne de quatorze mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 139, "src_audio": "/acl6060/audio/en/139.wav", "src_ref": "The articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les articles sont beaucoup plus longs avec une longueur moyenne de soixante-dix-sept mots, avec cent quarante-deux d'entre eux dépassant les mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 140, "src_audio": "/acl6060/audio/en/140.wav", "src_ref": "The lengthiest one being up to five thousand seven hundred and ninety words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 141, "src_audio": "/acl6060/audio/en/141.wav", "src_ref": "As previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme mentionné précédemment, les questions couvrent un wide range de sujets, dont environ quatre-vingt-cinq pour cent concernent la famille, le logement, l'argent ou la justice.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 142, "src_audio": "/acl6060/audio/en/142.wav", "src_ref": "While the remaining fifteen percent concern either social security, foreigners or work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 143, "src_audio": "/acl6060/audio/en/143.wav", "src_ref": "The article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'article est également très diversifié car il provient de trente-deux codes belges différents qui couvrent un grand nombre de sujets juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 144, "src_audio": "/acl6060/audio/en/144.wav", "src_ref": "Here's the total number of articles collected from each of these Belgian codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le nombre total d'articles collectés à partir de chacun de ces codes belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 145, "src_audio": "/acl6060/audio/en/145.wav", "src_ref": "Out of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sur les vingt-deux mille six cent trente-trois articles, seuls mille six cent douze sont désignés comme pertinents à au moins une question dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 146, "src_audio": "/acl6060/audio/en/146.wav", "src_ref": "And around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et environ quatre-vingts pour cent de ces articles cités proviennent du code civil, des codes judiciaires, des codes d'enquête criminelle ou des codes pénaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 147, "src_audio": "/acl6060/audio/en/147.wav", "src_ref": "Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 148, "src_audio": "/acl6060/audio/en/148.wav", "src_ref": "Which can be explained by the fact that those codes focused less on individuals and their concerns.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce qui peut s'expliquer par le fait que ces codes se concentraient moins sur les individus et leurs préoccupations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 149, "src_audio": "/acl6060/audio/en/149.wav", "src_ref": "Overall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le nombre moyen de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 150, "src_audio": "/acl6060/audio/en/150.wav", "src_ref": "Using all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En utilisant toutes les données, nous avons comparé plusieurs approches d'extraction, y compris l'architecture lexicale et dense.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 151, "src_audio": "/acl6060/audio/en/151.wav", "src_ref": "Given a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné une requête et un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de requête des poids de chacun de ces termes dans cet article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 152, "src_audio": "/acl6060/audio/en/152.wav", "src_ref": "We experiment with the standard TF-IDF and BM25 ranking functions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec les fonctions de classement TF-IDF et BM25 standard.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 153, "src_audio": "/acl6060/audio/en/153.wav", "src_ref": "The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le principal problème de ces approches est qu'elles ne peuvent extraire que les articles contenant des mots-clés présents dans la requête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 154, "src_audio": "/acl6060/audio/en/154.wav", "src_ref": "To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour surmonter cette limitation, nous expérimentons une architecture neuronale qui peut capturer les relations sémantiques entre les requêtes et l'article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 155, "src_audio": "/acl6060/audio/en/155.wav", "src_ref": "We use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons un modèle bi-encodeur qui cartographie les requêtes et les articles en représentations vecteurs denses et calculons un score de pertinence entre une paire d'articles de requête par la similitude de leurs intégrations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 156, "src_audio": "/acl6060/audio/en/156.wav", "src_ref": "These embeddings typically result from a pooling operation on the output of a word embedding model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces intégrations résultent typiquement d'une opération de pooling sur la sortie d'un modèle d'intégration de mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 157, "src_audio": "/acl6060/audio/en/157.wav", "src_ref": "First, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous étudions l'efficacité des bi-encodeurs siamois dans une configuration d'évaluation « zero shot », ce qui signifie que des modèles d'intégration de mots préformés sont appliqués immédiatement sans aucun raffinement supplémentaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 158, "src_audio": "/acl6060/audio/en/158.wav", "src_ref": "We experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec l'encodeur de texte indépendant du contexte, à savoir word2vec et fastText, et des modèles d'intégration dépendants du contexte, à savoir Roberta et plus précisément CamemBERT qui est un modèle Roberta français.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 159, "src_audio": "/acl6060/audio/en/159.wav", "src_ref": "Additionally, we train our own CamemBERT based model ah bi-encoders on our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, nous formons nos propres bi-encodeurs de modèles basés sur CamemBERT sur nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 160, "src_audio": "/acl6060/audio/en/160.wav", "src_ref": "Note that for training, we experiment with the two flavors of the bi-encoder architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que pour la formation, nous expérimentons avec les deux modèles de l'architecture bi-encodeur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 161, "src_audio": "/acl6060/audio/en/161.wav", "src_ref": "Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le siamois, qui utilise un modèle d'intégration de mots unique cartographiant la requête et l'article ensemble dans un vector space dense partagé, et le modèle à deux tours, qui utilise deux modèles d'intégration de mots indépendants encodant la requête et l'article séparément dans différents espaces d'intégration.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 162, "src_audio": "/acl6060/audio/en/162.wav", "src_ref": "We experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons la mise en commun moyenne, maximale et CLS, ainsi que le produit et cosinus pour calculer les similitudes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 163, "src_audio": "/acl6060/audio/en/163.wav", "src_ref": "Here are the result of our baseline on the test sets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le résultat de notre base sur les ensembles de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 164, "src_audio": "/acl6060/audio/en/164.wav", "src_ref": "With the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec les méthodes lexicales ci-dessus, les bi-encodeurs siamois ont évalué dans une configuration zero shot au milieu, et les bi-encodeurs raffinés ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 165, "src_audio": "/acl6060/audio/en/165.wav", "src_ref": "Overall, the finetuned bi-encoder significantly outperforms all the other baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le bi-encodeur raffiné dépasse de manière significative toutes les autres bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 166, "src_audio": "/acl6060/audio/en/166.wav", "src_ref": "The two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle à deux tours s'améliore par rapport à ses variantes siamoises lors du rappel à cent, mais fonctionne de la même manière sur les autres indicateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 167, "src_audio": "/acl6060/audio/en/167.wav", "src_ref": "Although BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que BM25 ait sous-performé le bi-encodeur formé de manière significative, ses performances indiquent qu'il s'agit toujours d'une base solide pour l'extraction spécifique au domaine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 168, "src_audio": "/acl6060/audio/en/168.wav", "src_ref": "Regarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation zero shot du bi-encodeur siamois, nous constatons que l'utilisation directe des intégrations d'un modèle CamemBERT préformé sans optimiser pour la tâche d'extraction d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 169, "src_audio": "/acl6060/audio/en/169.wav", "src_ref": "Furthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous observons que le bi-encodeur basé sur word2vec a dépassé de manière significative les modèles basés sur fastText et les Représentations d'encodeurs bidirectionnels à partir de transformateurs, suggérant que peut-être les intégrations de niveau mot préformées sont plus appropriées pour la tâche que les intégrations de niveau caractère ou sous-mot lorsqu'elles sont utilisées immédiatement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 170, "src_audio": "/acl6060/audio/en/170.wav", "src_ref": "Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que prometteurs, ces résultats suggèrent de nombreuses possibilités d'amélioration comparé à un expert juridique qualifié qui peut éventuellement extraire tous les articles pertinents à n'importe quelle question, et ainsi obtenir des scores parfaits.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 171, "src_audio": "/acl6060/audio/en/171.wav", "src_ref": "Let's conclude by discussing two limitations of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Concluons en discutant de deux limites de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 172, "src_audio": "/acl6060/audio/en/172.wav", "src_ref": "First, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Premièrement, le corpus d'article est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 173, "src_audio": "/acl6060/audio/en/173.wav", "src_ref": "During the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au cours de la construction des données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions ne se retrouvent qu'avec une fraction du nombre initial d'articles pertinents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 174, "src_audio": "/acl6060/audio/en/174.wav", "src_ref": "This information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette information implique donc que la réponse contenue dans les autres articles pertinents pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 175, "src_audio": "/acl6060/audio/en/175.wav", "src_ref": "Second, we should note that not all legal questions can be answered with statutes alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, il convient de noter qu'on ne peut pas répondre à toutes les questions juridiques uniquement par des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 176, "src_audio": "/acl6060/audio/en/176.wav", "src_ref": "For instance, the question, can I evict my tenants if they make too much noise?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, la question « puis-je expulser mes locataires s'ils font trop de bruit ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 177, "src_audio": "/acl6060/audio/en/177.wav", "src_ref": "Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle peut ne pas avoir de réponse détaillée dans le droit écrit qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 178, "src_audio": "/acl6060/audio/en/178.wav", "src_ref": "Instead, the landlord should probably rely more on case law and find precedents similar to their current situation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des antécédents similaires à sa situation actuelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 179, "src_audio": "/acl6060/audio/en/179.wav", "src_ref": "For example, the tenants makes two parties a week until two AM.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, les locataires font la fête deux fois par semaine jusqu'à deux heures du matin.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 180, "src_audio": "/acl6060/audio/en/180.wav", "src_ref": "Hence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche d'extraction de l'article de droit, et le domaine des moins adaptées reste à déterminer.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 181, "src_audio": "/acl6060/audio/en/181.wav", "src_ref": "We hope that our work sparks interest in developing practical and reliable statutory article retrieval models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous espérons que nos travaux susciteront l'intérêt pour l'élaboration de modèles d'extraction d'article de droit pratiques et fiables.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 182, "src_audio": "/acl6060/audio/en/182.wav", "src_ref": "That can help improve access to justice for all.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut aider à améliorer l'accès à la justice pour tous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 183, "src_audio": "/acl6060/audio/en/183.wav", "src_ref": "You can check out our paper, dataset and code at the following links. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez consulter notre article, nos données et notre code aux liens suivants. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 184, "src_audio": "/acl6060/audio/en/184.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE ; un indice de référence indépendant de la tâche destiné à tester les modèles de langue et de vision avec des phénomènes linguistiques spécifiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 185, "src_audio": "/acl6060/audio/en/185.wav", "src_ref": "Why did we do the trouble in setting up this benchmark?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pourquoi avons-nous pris la peine de mettre en place cet indice de référence ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 186, "src_audio": "/acl6060/audio/en/186.wav", "src_ref": "Well, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eh bien, au cours des dernières années, nous avons vu une explosion de la vision basée sur le transformateur et des modèles de langue préformés sur de grandes quantités de paires de textes d'images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 187, "src_audio": "/acl6060/audio/en/187.wav", "src_ref": "Each one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacun de ces modèles pousse les tâches de pointe en matière de vision et de langue telles que la réponse aux questions visuelles, le raisonnement de bon sens visuel, l'extraction d'images et les bases de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 188, "src_audio": "/acl6060/audio/en/188.wav", "src_ref": "So we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc reçu un message : les précisions sur ces tâches et les indices de référence spécifiques augmentent régulièrement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 189, "src_audio": "/acl6060/audio/en/189.wav", "src_ref": "But do we know what the models have actually learned?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais savons-nous ce que les modèles ont réellement appris ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 190, "src_audio": "/acl6060/audio/en/190.wav", "src_ref": "What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Qu'est-ce qu'une vision et un transformateur de langue ont compris lors de l'attribution d'un score élevé pour que cette image et cette phrase correspondent ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 191, "src_audio": "/acl6060/audio/en/191.wav", "src_ref": "And the low score for this one?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le score bas pour celle-ci ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 192, "src_audio": "/acl6060/audio/en/192.wav", "src_ref": "Do vision and language models focus on the right thing?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Est-ce que les modèles de langue et de vision se concentrent sur la bonne chose ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 193, "src_audio": "/acl6060/audio/en/193.wav", "src_ref": "Or do they focus on biases as shown by previous work?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou se concentrent-ils sur les biais comme le montre le travail antérieur ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 194, "src_audio": "/acl6060/audio/en/194.wav", "src_ref": "To shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour éclairer davantage cet aspect, nous proposons une direction plus indépendante des tâches et introduisons VALSE qui teste la sensibilité des modèles de langue et de vision à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistiques et visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 195, "src_audio": "/acl6060/audio/en/195.wav", "src_ref": "We target existence, plurality, counting, spatial relations, actions and entity coreference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 196, "src_audio": "/acl6060/audio/en/196.wav", "src_ref": "But how do we test whether the vision and language models have captured this phenomena?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais comment tester si les modèles de langue et de vision ont capturé ce phénomène ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 197, "src_audio": "/acl6060/audio/en/197.wav", "src_ref": "By foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En pratiquant le foiling sur une méthode précédemment appliquée pour les modèles de langue et de vision seulement pour des phrases nominales de Ravi Shekhar et de ses collaborateurs, et en comptant par nous dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 198, "src_audio": "/acl6060/audio/en/198.wav", "src_ref": "Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Effectuer un foiling signifie essentiellement que nous prenons la légende d'une image et produisons un foil en modifiant la légende de sorte qu'elle ne décrive plus l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 199, "src_audio": "/acl6060/audio/en/199.wav", "src_ref": "And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous apportons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous aurions trouvé plus d'une façon intéressante de créer des instances de foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 200, "src_audio": "/acl6060/audio/en/200.wav", "src_ref": "For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans le cas de l'élément d'actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié avec une action différente, et un dans lequel les acteurs sont échangés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 201, "src_audio": "/acl6060/audio/en/201.wav", "src_ref": "Counting and coreference also are pieces that have more than one instrument.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le comptage et la coréférence sont également des éléments qui ont plusieurs instruments.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 202, "src_audio": "/acl6060/audio/en/202.wav", "src_ref": "And we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous créons ces foils en nous assurant qu'ils ne décrivent pas l'image et que ce sont des phrases grammaticales et autrement valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 203, "src_audio": "/acl6060/audio/en/203.wav", "src_ref": "This is not easy to do because a foiled caption may be less likely than the original caption.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce n'est pas facile à faire car une légende ayant subi un foil peut être moins probable que la légende originale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 204, "src_audio": "/acl6060/audio/en/204.wav", "src_ref": "For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que les plantes coupent un homme qu'un homme coupe les plantes, et la grande vision et les modèles de langue pourraient capter cela.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 205, "src_audio": "/acl6060/audio/en/205.wav", "src_ref": "Therefore, to obtain valid foils, we must take action.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour obtenir des foils valides, nous devons agir.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 206, "src_audio": "/acl6060/audio/en/206.wav", "src_ref": "First, we make use of strong language models to propose foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous utilisons des modèles de langue forts pour proposer des foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 207, "src_audio": "/acl6060/audio/en/207.wav", "src_ref": "Second, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, nous utilisons l'inférence de la langue naturelle ou la NLI courte pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 208, "src_audio": "/acl6060/audio/en/208.wav", "src_ref": "To test this automatically, we apply natural language inference with the following rationale.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour tester cela automatiquement, nous appliquons l'inférence de la langue naturelle avec la justification suivante.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 209, "src_audio": "/acl6060/audio/en/209.wav", "src_ref": "We consider an image to be the premise and its caption its entailed hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous considérons qu'une image est la prémisse et sa légende son hypothèse implicite.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 210, "src_audio": "/acl6060/audio/en/210.wav", "src_ref": "In addition, we consider the caption to be the premise, and the foil is its hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous considérons la légende comme la prémisse, et le foil est son hypothèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 211, "src_audio": "/acl6060/audio/en/211.wav", "src_ref": "If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si un modèle NLI prévient que le foil est en contradiction ou neutre par rapport à la légende, nous considérons cela comme un indicateur d'un foil valide.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 212, "src_audio": "/acl6060/audio/en/212.wav", "src_ref": "If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si une NLI prévient le foil à engendrer par la légende, cela ne peut pas être un bon foil, car par transitivité, cela donnera une description véridique de l'image et nous filtrons ces foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 213, "src_audio": "/acl6060/audio/en/213.wav", "src_ref": "But this procedure is not perfect, it is just an indicator for valid foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cette procédure n'est pas parfaite, il s'agit simplement d'un indicateur pour les foils valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 214, "src_audio": "/acl6060/audio/en/214.wav", "src_ref": "Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, comme troisième mesure pour produire des foils valides, nous utilisons des annotateurs humains pour valider les données utilisées dans VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 215, "src_audio": "/acl6060/audio/en/215.wav", "src_ref": "So, after filtering and human evaluation, we have as many test instances as described in this table.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrit dans ce tableau.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 216, "src_audio": "/acl6060/audio/en/216.wav", "src_ref": "Note that VALSE does not deliver any training data but only test data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que VALSE ne fournit pas de données de formation mais seulement des données de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 217, "src_audio": "/acl6060/audio/en/217.wav", "src_ref": "Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme il s'agit uniquement d'un indicateur de référence de test zero shot, il est conçu pour tirer parti des capacités existantes des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 218, "src_audio": "/acl6060/audio/en/218.wav", "src_ref": "Finetuning would only enable models to exploit artifacts or statistical biases in the data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement permettrait seulement aux modèles d'exploiter des artefacts ou des biais statistiques dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 219, "src_audio": "/acl6060/audio/en/219.wav", "src_ref": "And we all know that these models like to cheat and take shortcuts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 220, "src_audio": "/acl6060/audio/en/220.wav", "src_ref": "And as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et comme nous l'avons dit, cela nous intéresse d'évaluer les capacités des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 221, "src_audio": "/acl6060/audio/en/221.wav", "src_ref": "We experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec cinq modèles de langue et de vision sur VALSE, à savoir avec CLIP, LXMert, ViLBERT, ViLBERT douze en un, et VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 222, "src_audio": "/acl6060/audio/en/222.wav", "src_ref": "Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deux de nos indicateurs d'évaluation les plus importants sont la précision des modèles dans la classification de paires de phrases images en légendes et foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 223, "src_audio": "/acl6060/audio/en/223.wav", "src_ref": "Perhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Peut-être plus pertinent pour cette vidéo, nous présenterons notre indicateur plus permissif, la précision par paire, qui mesure si le score de sentence alignment d'image est plus élevé pour la bonne paire de texte image que pour sa paire ayant subi un foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 224, "src_audio": "/acl6060/audio/en/224.wav", "src_ref": "For more metrics and results on them, do check out our paper.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour plus d'indicateurs et de résultats, consultez notre article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 225, "src_audio": "/acl6060/audio/en/225.wav", "src_ref": "The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats avec une précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres indicateurs. La meilleure performance zero shot est obtenue par ViLBERT douze en un, suivi de ViLBERT, LXMert, CLIP, et enfin VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 226, "src_audio": "/acl6060/audio/en/226.wav", "src_ref": "It's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est remarquable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par ViLBERT douze en un, en soulignant que les modèles sont capables d'identifier des objets nommés et leur présence dans les images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 227, "src_audio": "/acl6060/audio/en/227.wav", "src_ref": "However, none of the remaining pieces can be reliably solved in our adversarial foiling settings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de foiling antagonistes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 228, "src_audio": "/acl6060/audio/en/228.wav", "src_ref": "We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous voyons à partir des instruments de pluralité et de comptage que les modèles de langue et de vision ont du mal à distinguer les références à des objets uniques par rapport à plusieurs, ou à les compter dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 229, "src_audio": "/acl6060/audio/en/229.wav", "src_ref": "The relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 230, "src_audio": "/acl6060/audio/en/230.wav", "src_ref": "They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ils ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité comme nous le voyons dans l'élément d'actions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 231, "src_audio": "/acl6060/audio/en/231.wav", "src_ref": "From the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À partir de l'élément de coréférence, nous découvrons que relever plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 232, "src_audio": "/acl6060/audio/en/232.wav", "src_ref": "As a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour vérifier la santé mentale, et parce que c'est une expérience intéressante, nous comparons également deux modèles à texte seulement, GPT un et GPT deux, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et ayant subi un foil, sans image ici, et en prévenant l'entrée avec la plus faible perplexité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 233, "src_audio": "/acl6060/audio/en/233.wav", "src_ref": "If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si la perplexité est plus élevée pour le foil, nous considérons cela comme une indication que la légende ayant subi un foil peut souffrir de biais de plausibilité ou d'autres biais linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 234, "src_audio": "/acl6060/audio/en/234.wav", "src_ref": "And it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il est intéressant de voir que dans certains cas, les modèles GPT à texte seulement ont capturé la plausibilité du monde mieux que les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 235, "src_audio": "/acl6060/audio/en/235.wav", "src_ref": "So to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour résumer, VALSE est un indicateur de référence qui utilise l'objectif des constructions linguistiques pour aider la communauté à améliorer les modèles de langue et de vision en testant durement leurs capacités de bases visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 236, "src_audio": "/acl6060/audio/en/236.wav", "src_ref": "Our experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que les modèles de langue et de vision identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais luttent pour ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 237, "src_audio": "/acl6060/audio/en/237.wav", "src_ref": "We would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers les bases de la langue avec des modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 238, "src_audio": "/acl6060/audio/en/238.wav", "src_ref": "And even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et plus encore, VALSE pourrait être utilisé comme une évaluation indirecte des données, car les modèles pourraient être évalués avant et après la formation ou le raffinement pour voir si des données aident les modèles à améliorer l'un des aspects testés par VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 239, "src_audio": "/acl6060/audio/en/239.wav", "src_ref": "If you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 240, "src_audio": "/acl6060/audio/en/240.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Kamezawa de l'Université de Tokyo.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 241, "src_audio": "/acl6060/audio/en/241.wav", "src_ref": "I'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter un article intitulé RNSum : des données à grande échelle pour la génération automatique de note de version via la synthèse des journaux de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 242, "src_audio": "/acl6060/audio/en/242.wav", "src_ref": "I'll be explaining in this order.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous expliquer dans cet ordre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 243, "src_audio": "/acl6060/audio/en/243.wav", "src_ref": "First, I will introduce automatic release note generation that we are working on in this research.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, je vais présenter la génération de note de version automatique sur laquelle nous travaillons dans cette recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 244, "src_audio": "/acl6060/audio/en/244.wav", "src_ref": "A release note is a technical document that summarizes the changes distributed with each release of a software product.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une note de version est un document technique qui résume les modifications distribuées à chaque version d'un produit logiciel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 245, "src_audio": "/acl6060/audio/en/245.wav", "src_ref": "The image shows a release note for version two point six point four of the vuejs library.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'image montre une note de version pour la version deux point six point quatre de la bibliothèque Vuejs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 246, "src_audio": "/acl6060/audio/en/246.wav", "src_ref": "Release notes play an important role in open source development but they're time consuming to prepare manually.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version jouent un rôle important dans le développement open source, mais elles mettent du temps à être préparées manuellement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 247, "src_audio": "/acl6060/audio/en/247.wav", "src_ref": "Therefore, it would be very useful to be able to automatically generate high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 248, "src_audio": "/acl6060/audio/en/248.wav", "src_ref": "I will defer to two previous researches on automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'en remettrai à deux recherches antérieures sur la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 249, "src_audio": "/acl6060/audio/en/249.wav", "src_ref": "The first is a system called ARENA released in twenty fourteen.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première est un système appelé ARENA sorti en deux-mille-quatorze.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 250, "src_audio": "/acl6060/audio/en/250.wav", "src_ref": "It takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il faut une approche basée sur des règles, par exemple, en utilisant l'extracteur de changement pour extraire toutes les différences, les changements de bibliothèque et les changements de document à partir des différences entre les versions, et enfin, en les combinant.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 251, "src_audio": "/acl6060/audio/en/251.wav", "src_ref": "The most notable feature of this system is the issue extractor in the upper right corner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 252, "src_audio": "/acl6060/audio/en/252.wav", "src_ref": "Which must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le système de suivi des problèmes, qui doit être laissé à Jira, ne peut être appliqué qu'aux projets qui utilisent Jira.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 253, "src_audio": "/acl6060/audio/en/253.wav", "src_ref": "In other words, it cannot be used for many projects on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En d'autres mots, il ne peut pas être utilisé pour de nombreux projets sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 254, "src_audio": "/acl6060/audio/en/254.wav", "src_ref": "The second is Glyph, recently announced in twenty twenty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde est Glyphe, récemment annoncée en deux-mille-vingt.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 255, "src_audio": "/acl6060/audio/en/255.wav", "src_ref": "It is available on the internet and can be installed via pip.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle est disponible sur internet et peut être installée via pip.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 256, "src_audio": "/acl6060/audio/en/256.wav", "src_ref": "This system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'une des cinq étiquettes telles que les fonctions ou corrections de bugs pour chaque message de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 257, "src_audio": "/acl6060/audio/en/257.wav", "src_ref": "This image is a sample usage that returns a corrective or bug fixes label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette image est un exemple d'utilisation qui renvoie une étiquette de correction ou de débogage.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 258, "src_audio": "/acl6060/audio/en/258.wav", "src_ref": "Glyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données de formation de Glyphe sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 259, "src_audio": "/acl6060/audio/en/259.wav", "src_ref": "The performance of the text classification model is not high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les performances du modèle de classification de texte ne sont pas élevées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 260, "src_audio": "/acl6060/audio/en/260.wav", "src_ref": "I present two related researches, but their problems are limited applicability and scarce data resources.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je présente deux recherches connexes, mais leurs problèmes sont l'applicabilité limitée et les ressources de données rares.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 261, "src_audio": "/acl6060/audio/en/261.wav", "src_ref": "Our paper solves these two problems and automatically generates high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 262, "src_audio": "/acl6060/audio/en/262.wav", "src_ref": "With a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec un problème d'applicabilité limitée, nous proposons une méthode de synthèse par classe de haute qualité en utilisant uniquement des messages de validation comme saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 263, "src_audio": "/acl6060/audio/en/263.wav", "src_ref": "This proposed method can be used for all English repositories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette méthode proposée peut être utilisée pour tous les référentiels anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 264, "src_audio": "/acl6060/audio/en/264.wav", "src_ref": "For the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour le deuxième problème de ressources de données rares, nous avons construit nos données RNSum composées d'environ quatre-vingt-deux mille éléments de données en collectant des données à partir de référentiels GitHub publics en utilisant l'API GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 265, "src_audio": "/acl6060/audio/en/265.wav", "src_ref": "Next, I'll describe our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais décrire nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 266, "src_audio": "/acl6060/audio/en/266.wav", "src_ref": "Here is an example of data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici un exemple de données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 267, "src_audio": "/acl6060/audio/en/267.wav", "src_ref": "The left side is a commit message and the right side is the release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le côté gauche est un message de validation et le côté droit représente les notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 268, "src_audio": "/acl6060/audio/en/268.wav", "src_ref": "Release notes are labeled as improvements or fixes, etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version sont étiquetées comme améliorations ou correctifs, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 269, "src_audio": "/acl6060/audio/en/269.wav", "src_ref": "We have set up a task that takes the commit messages as input and outputs a labeled release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons mis en place une tâche qui prend les messages de validation comme saisie et sorties des notes de version étiquetées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 270, "src_audio": "/acl6060/audio/en/270.wav", "src_ref": "This can be regarded as a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut être considéré comme une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 271, "src_audio": "/acl6060/audio/en/271.wav", "src_ref": "We have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons prédéfini quatre étiquettes : fonctions, améliorations, corrections de bugs, suppressions de dépréciations et modifications de rupture.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 272, "src_audio": "/acl6060/audio/en/272.wav", "src_ref": "These were set based on previous research and other factors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Celles-ci ont été établies sur la base de la recherche antérieure et d'autres facteurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 273, "src_audio": "/acl6060/audio/en/273.wav", "src_ref": "The release note on the bottom right is extracted from the release note on the bottom left.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La note de version en bas à droite est extraite de la note de version en bas à gauche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 274, "src_audio": "/acl6060/audio/en/274.wav", "src_ref": "At this time, it is necessary to detect the four labels that have been set up in advance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été mises en place à l'avance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 275, "src_audio": "/acl6060/audio/en/275.wav", "src_ref": "But the labels are not always consistent with each repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 276, "src_audio": "/acl6060/audio/en/276.wav", "src_ref": "For example, the improvements label includes improvements, enhancements, optimizations, and so on.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, l'étiquette d'améliorations inclut des améliorations, des perfectionnements, des optimisations, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 277, "src_audio": "/acl6060/audio/en/277.wav", "src_ref": "We prepared a vocabulary list of about thirty labels for each of these notational variations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons préparé une liste de vocabulaire d'une trentaine d'étiquettes pour chacune de ces variations de notation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 278, "src_audio": "/acl6060/audio/en/278.wav", "src_ref": "This is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de détecter la classe de note de version et de collecter le texte de la version qui suit en tant que phrase de note de version pour la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 279, "src_audio": "/acl6060/audio/en/279.wav", "src_ref": "Next is a commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il y a un message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 280, "src_audio": "/acl6060/audio/en/280.wav", "src_ref": "Commit messages are not tied to each release.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation ne sont pas liés à chaque version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 281, "src_audio": "/acl6060/audio/en/281.wav", "src_ref": "As shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme le montre l'image ci-dessous, si la version actuelle est la version deux point cinq à dix-neuf, nous devons identifier la version deux point cinq à dix-huit antérieure et obtenir un diff.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 282, "src_audio": "/acl6060/audio/en/282.wav", "src_ref": "This is a bit tedious and it is not enough to just get a list of releases and look at the before and after.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est un peu fastidieux et il ne suffit pas d'obtenir une liste de versions et de regarder l'avant et l'après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 283, "src_audio": "/acl6060/audio/en/283.wav", "src_ref": "We created a heuristic matching rule to get the previous and next versions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons créé une règle de correspondance heuristique pour obtenir les versions antérieures et suivantes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 284, "src_audio": "/acl6060/audio/en/284.wav", "src_ref": "Dataset analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Analyse des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 285, "src_audio": "/acl6060/audio/en/285.wav", "src_ref": "In the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En fin de compte, sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données ont été recueillis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 286, "src_audio": "/acl6060/audio/en/286.wav", "src_ref": "Also, the average number of release notes tokens is sixty three, which is quite high for a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le nombre moyen de gages de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 287, "src_audio": "/acl6060/audio/en/287.wav", "src_ref": "Also, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De même, le nombre de gages uniques est assez grand, s'élevant à huit mille huit cent trente mille.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 288, "src_audio": "/acl6060/audio/en/288.wav", "src_ref": "This is due to the large number of unique class or method names found in the repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela est dû au grand nombre de noms de méthode ou de classe unique trouvés dans le référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 289, "src_audio": "/acl6060/audio/en/289.wav", "src_ref": "Next, I will explain the proposed method.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais expliquer la méthode proposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 290, "src_audio": "/acl6060/audio/en/290.wav", "src_ref": "The classwise extractive then abstractive summarization model consists of two neural modules.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle de synthèse extractive puis abstractive par classe se compose de deux modules neuronaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 291, "src_audio": "/acl6060/audio/en/291.wav", "src_ref": "A classifier using BERT or CodeBERT and a generator using BART.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un classificateur utilisant BERT ou CodeBERT et un générateur utilisant BART.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 292, "src_audio": "/acl6060/audio/en/292.wav", "src_ref": "First, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, CEAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, qui utilisent des améliorations, des corrections de bogues, des dépréciations et d'autres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 293, "src_audio": "/acl6060/audio/en/293.wav", "src_ref": "The commit messages classified as other are discarded.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation classés comme autres sont supprimés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 294, "src_audio": "/acl6060/audio/en/294.wav", "src_ref": "Then CEAS applies the generator to the four labeled documents independently and generates release notes for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 295, "src_audio": "/acl6060/audio/en/295.wav", "src_ref": "In this task, the direct correspondences between commit messages and release notes are not known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 296, "src_audio": "/acl6060/audio/en/296.wav", "src_ref": "Therefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour former le classificateur, nous avons réaffecté les enquêtes à chaque message de validation de saisie en utilisant les dix premiers caractères de chaque message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 297, "src_audio": "/acl6060/audio/en/297.wav", "src_ref": "We modeled the classwise abstractive summarization approach by two different methods.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons modélisé l'approche de synthèse abstractive par classe à travers deux méthodes différentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 298, "src_audio": "/acl6060/audio/en/298.wav", "src_ref": "The first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le premier modèle, que nous appelons CAS unique, se compose d'un réseau de six à six unique et génère un seul texte de note de version donnant un enchaînement de messages de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 299, "src_audio": "/acl6060/audio/en/299.wav", "src_ref": "The output texts can be divided into classwise segments based on special class-specific endpoint symbols.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les textes de sortie peuvent être divisés en segments par classe sur la base de symboles de point d'extrémité particuliers, spécifiques à la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 300, "src_audio": "/acl6060/audio/en/300.wav", "src_ref": "The second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde méthode, que nous appelons CAS multiple, se compose de quatre réseaux seq2seq différents, dont chacun correspond à l'une des classes de notes de version fixes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 301, "src_audio": "/acl6060/audio/en/301.wav", "src_ref": "Okay, let me explain the experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ok, je vais vous expliquer les expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 302, "src_audio": "/acl6060/audio/en/302.wav", "src_ref": "Five methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cinq méthodes ont été comparées : CEAS, CAS unique, CAS multiple, regroupement et étude antérieure, Glyph.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 303, "src_audio": "/acl6060/audio/en/303.wav", "src_ref": "Regarding evaluation, in some cases, release notes are output in multiple sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 304, "src_audio": "/acl6060/audio/en/304.wav", "src_ref": "Since it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisqu'il est difficile de calculer le nombre de phrases telles qu'elles sont, elles sont combinées avec des espaces et traitées comme une seule phrase longue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 305, "src_audio": "/acl6060/audio/en/305.wav", "src_ref": "The BLEU is penalized when the system outputs a short sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le BLEU est pénalisé lorsque le système produit une phrase courte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 306, "src_audio": "/acl6060/audio/en/306.wav", "src_ref": "This penalty results in a lower BLEU value in the experiment results described next.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette pénalité se traduit par une valeur BLEU inférieure dans les résultats de l'expérience décrits ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 307, "src_audio": "/acl6060/audio/en/307.wav", "src_ref": "Finally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous calculons également la spécificité car ROUGE et BLEU ne peuvent pas être calculés si les notes de version sont vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 308, "src_audio": "/acl6060/audio/en/308.wav", "src_ref": "A higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une spécificité plus élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées être vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 309, "src_audio": "/acl6060/audio/en/309.wav", "src_ref": "Here are the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats sont indiqués ci-après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 310, "src_audio": "/acl6060/audio/en/310.wav", "src_ref": "Since the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné que les données contiennent des adresses électroniques, des valeurs hachées, etc., nous avons également évalué les données nettoyées, ce qui les exclut.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 311, "src_audio": "/acl6060/audio/en/311.wav", "src_ref": "CEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS et le CAS ont obtenu des scores de ROUGE-L supérieurs de plus de dix points par rapport aux bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 312, "src_audio": "/acl6060/audio/en/312.wav", "src_ref": "In particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les bases a atteint plus de vingt points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 313, "src_audio": "/acl6060/audio/en/313.wav", "src_ref": "These results indicate that CEAS and CAS are significantly affected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces résultats indiquent que le CEAS et le CAS sont considérablement touchés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 314, "src_audio": "/acl6060/audio/en/314.wav", "src_ref": "CEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS a obtenu un meilleur score ROUGE-L que le CAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour former le classificateur à l'aide de pseudo-étiquettes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 315, "src_audio": "/acl6060/audio/en/315.wav", "src_ref": "High coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une couverture élevée du CEAS peut être obtenue probablement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 316, "src_audio": "/acl6060/audio/en/316.wav", "src_ref": "CAS-Multi tended to yield higher ROUGE-L than CAS-Single.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CAS multiple tendait à produire plus de ROUGE-L que le CAS unique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 317, "src_audio": "/acl6060/audio/en/317.wav", "src_ref": "Suggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En suggérant qu'il est également efficace de développer indépendamment et différemment des modèles de abstractive summarization pour chaque classe de note de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 318, "src_audio": "/acl6060/audio/en/318.wav", "src_ref": "Here are an error analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici une analyse d'erreur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 319, "src_audio": "/acl6060/audio/en/319.wav", "src_ref": "CAS methods tend to output shorter sentences than human reference sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 320, "src_audio": "/acl6060/audio/en/320.wav", "src_ref": "In the figure on the right, the reference sentence has three or four sentences, while CAS has only one.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que le CAS n'en a qu'une.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 321, "src_audio": "/acl6060/audio/en/321.wav", "src_ref": "The reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La raison de la réticence de ce modèle est que dans les données de formation, seulement trente-trois pour cent des phrases sont présentes dans l'étiquette des fonctions et quarante pour cent dans l'étiquette des améliorations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 322, "src_audio": "/acl6060/audio/en/322.wav", "src_ref": "Furthermore, CAS methods cannot generate accurate release notes without additional information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les méthodes CAS ne peuvent pas générer des notes de version précises sans informations supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 323, "src_audio": "/acl6060/audio/en/323.wav", "src_ref": "The top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple en haut à droite est un exemple de message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la progression ou au problème correspondant(e).", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 324, "src_audio": "/acl6060/audio/en/324.wav", "src_ref": "The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple ci-dessous montre que les deux messages de validation dans la saisie sont liés et doivent être combinés en une phrase, mais cela n'est pas fait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 325, "src_audio": "/acl6060/audio/en/325.wav", "src_ref": "Finally, a conclusion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, passons à la conclusion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 326, "src_audio": "/acl6060/audio/en/326.wav", "src_ref": "We have built a new dataset for automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons construit de nouvelles données pour la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 327, "src_audio": "/acl6060/audio/en/327.wav", "src_ref": "We have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également formulé une tâche consistant à saisir des messages de validation et à les résumer afin qu'ils soient applicables à tous les projets écrits en anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 328, "src_audio": "/acl6060/audio/en/328.wav", "src_ref": "Our experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 329, "src_audio": "/acl6060/audio/en/329.wav", "src_ref": "Please check out our dataset on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Veuillez consulter nos données sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 330, "src_audio": "/acl6060/audio/en/330.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 331, "src_audio": "/acl6060/audio/en/331.wav", "src_ref": "Hello. My name is Asaf Harari.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour. Je m'appelle Asaf Harari.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 332, "src_audio": "/acl6060/audio/en/332.wav", "src_ref": "And I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et je vais vous présenter notre article : enrichissement de données tabulaires en quelques coups à l'aide d'architectures de transformateurs raffinées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 333, "src_audio": "/acl6060/audio/en/333.wav", "src_ref": "Data scientists analyze data and mainly focus on the manipulating the data's existing features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les scientifiques analysent les données et se concentrent principalement sur la manipulation des fonctions existantes des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 334, "src_audio": "/acl6060/audio/en/334.wav", "src_ref": "But sometimes, these features are limited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais parfois, ces fonctions sont limitées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 335, "src_audio": "/acl6060/audio/en/335.wav", "src_ref": "Feature generation using another data source may add substantial information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La génération de fonctions en utilisant une autre source de données peut ajouter des informations substantielles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 336, "src_audio": "/acl6060/audio/en/336.wav", "src_ref": "Our research goal is automatic tabular data enrichment using external sources' free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des textes libres de sources externes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 337, "src_audio": "/acl6060/audio/en/337.wav", "src_ref": "Assume we have a tabular dataset and a knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Supposons que nous ayons des données tabulaires et une base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 338, "src_audio": "/acl6060/audio/en/338.wav", "src_ref": "We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles fonctions du texte libre de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 339, "src_audio": "/acl6060/audio/en/339.wav", "src_ref": "Our framework FeSTE is exactly this automatic process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre cadre FeSTE est exactement ce processus automatique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 340, "src_audio": "/acl6060/audio/en/340.wav", "src_ref": "So let's see an example in a dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons donc un exemple dans des données introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 341, "src_audio": "/acl6060/audio/en/341.wav", "src_ref": "In this example, the dataset is university dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données sont des données universitaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 342, "src_audio": "/acl6060/audio/en/342.wav", "src_ref": "When its goal is to classify universities into low ranking universities and high-ranking universities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Quand l'objectif est de classer les universités en universités de bas rang et en universités de haut rang.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 343, "src_audio": "/acl6060/audio/en/343.wav", "src_ref": "As knowledge base, we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 344, "src_audio": "/acl6060/audio/en/344.wav", "src_ref": "The first phase of FeSTE is entity linking.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première phase de FeSTE est la liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 345, "src_audio": "/acl6060/audio/en/345.wav", "src_ref": "When each entity, in this example the university name, is linked to an entity within the knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 346, "src_audio": "/acl6060/audio/en/346.wav", "src_ref": "And and the text of the entities of the knowledge base is extracted and added to the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le texte des entités de la base de connaissances est extrait et ajouté aux données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 347, "src_audio": "/acl6060/audio/en/347.wav", "src_ref": "In this example, the text is the Wikipedia page's abstract.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, le texte est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 348, "src_audio": "/acl6060/audio/en/348.wav", "src_ref": "Now, we need to generate or extract features from the retrieved text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Maintenant, nous devons générer ou extraire des fonctions à partir du texte extrait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 349, "src_audio": "/acl6060/audio/en/349.wav", "src_ref": "So, we need to ah feature extraction phase ah which includes text analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc besoin de la phase d'extraction des fonctions qui comprend l'analyse de texte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 350, "src_audio": "/acl6060/audio/en/350.wav", "src_ref": "And this is the main novelty of this paper and I will deep dive into it in the next slides.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de la principale nouveauté de cet article et je vais mieux l'expliquer dans les prochaines diapositives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 351, "src_audio": "/acl6060/audio/en/351.wav", "src_ref": "After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Après la phase d'extraction des fonctions, il y a une phase de génération de fonctions lorsque nous utilisons les fonctions extraites pour générer un petit nombre de nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 352, "src_audio": "/acl6060/audio/en/352.wav", "src_ref": "First generate ah features in the number of classes of the original dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Générez d'abord les fonctions dans le nombre de classes des données d'origine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 353, "src_audio": "/acl6060/audio/en/353.wav", "src_ref": "In this example, the original dataset has two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données d'origine ont deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 354, "src_audio": "/acl6060/audio/en/354.wav", "src_ref": "So, FeSTE generates two new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, FeSTE génère deux nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 355, "src_audio": "/acl6060/audio/en/355.wav", "src_ref": "But if the dataset has five classes, FeSTE generates five new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si les données ont cinq classes, FeSTE génère cinq nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 356, "src_audio": "/acl6060/audio/en/356.wav", "src_ref": "Each feature represents the likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chaque fonction représente la probabilité pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 357, "src_audio": "/acl6060/audio/en/357.wav", "src_ref": "To analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour analyser le texte, nous utilisons les analyses de texte de pointe, qui sont des modèles de langue basés sur la conversion comme BERT, GPT, XLNet etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 358, "src_audio": "/acl6060/audio/en/358.wav", "src_ref": "It is but it is not likely that we can train language models using the input datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais il est peu probable que nous puissions former des modèles de langue en utilisant les données de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 359, "src_audio": "/acl6060/audio/en/359.wav", "src_ref": "So a naive approach will be ah target task finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une approche naïve sera le raffinement de la tâche cible.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 360, "src_audio": "/acl6060/audio/en/360.wav", "src_ref": "So, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la phase d'extraction des fonctions, nous pouvons télécharger des modèles de langue préformée et raffiner le modèle de langue sur les données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 361, "src_audio": "/acl6060/audio/en/361.wav", "src_ref": "In this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, raffiner le modèle de langue, classer le texte en classes, résumer en classes, basses ou hautes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 362, "src_audio": "/acl6060/audio/en/362.wav", "src_ref": "Receive the language model output, which is the likelihood for each class and use as new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Recevoir les résultats du modèle de langue, qui sont la probabilité pour chaque classe et les utiliser comme nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 363, "src_audio": "/acl6060/audio/en/363.wav", "src_ref": "The problem with this approach is datasets may have few distinct entities / texts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le problème avec cette approche est que les données peuvent avoir peu d'entités / de textes distinct(e)s.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 364, "src_audio": "/acl6060/audio/en/364.wav", "src_ref": "In our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre expérience, près de la moitié des données contiennent moins de quatre cents échantillons et les plus petites données contiennent trente-cinq échantillons dans un ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 365, "src_audio": "/acl6060/audio/en/365.wav", "src_ref": "So to finetune a language model over ah this dataset will be ineffective.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour raffiner un modèle de langue sur cela, les données seront inefficaces.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 366, "src_audio": "/acl6060/audio/en/366.wav", "src_ref": "But we can use prior knowledge about pre-analyzed datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons utiliser des connaissances préalables sur des données préanalysées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 367, "src_audio": "/acl6060/audio/en/367.wav", "src_ref": "Because FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisque nous appliquons FeSTE sur des données multiples, nous pouvons utiliser les données n moins un pour recueillir des informations sur les données n moins un, et utiliser ces informations lorsque nous analysons les nièmes données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 368, "src_audio": "/acl6060/audio/en/368.wav", "src_ref": "What we, what we suggest is to add, to add another finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce que nous suggérons, c'est d'ajouter une autre phase de raffinement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 369, "src_audio": "/acl6060/audio/en/369.wav", "src_ref": "A preliminary multitask finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une phase de raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 370, "src_audio": "/acl6060/audio/en/370.wav", "src_ref": "When you finetune the language model over the n minus one datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque vous raffinez le modèle de langue sur les données n moins un.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 371, "src_audio": "/acl6060/audio/en/371.wav", "src_ref": "And, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ensuite, nous exécutons une autre phase de raffinement qui est un raffinement de tâche cible, lorsque nous raffinons le modèle de langue sur les nièmes données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 372, "src_audio": "/acl6060/audio/en/372.wav", "src_ref": "The state-of-the-art in multitask ah multitask finetuning called MTDNN.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement multitâche de pointe appelé MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 373, "src_audio": "/acl6060/audio/en/373.wav", "src_ref": "In MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN maintient les têtes dans le nombre de tâches dans l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 374, "src_audio": "/acl6060/audio/en/374.wav", "src_ref": "So, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans cet exemple, il y a quatre tâches dans l'ensemble de formation. Le MTDNN maintient alors quatre têtes comme vous pouvez le voir sur l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 375, "src_audio": "/acl6060/audio/en/375.wav", "src_ref": "And it samples a random batch from ah from the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il échantillonne un lot aléatoire de l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 376, "src_audio": "/acl6060/audio/en/376.wav", "src_ref": "And if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si elles appartiennent à un lot aléatoire, par exemple, une seule tâche de classification de phrases, il exécute des chemins d'aller et retour à travers la première tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 377, "src_audio": "/acl6060/audio/en/377.wav", "src_ref": "And if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si le lot aléatoire appartient à la tâche de classement par paire, il exécute un chemin d'aller et retour dans la dernière tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 378, "src_audio": "/acl6060/audio/en/378.wav", "src_ref": "In our scenario, ah tabular datasets vary in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre scénario, les données tabulaires varient dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 379, "src_audio": "/acl6060/audio/en/379.wav", "src_ref": "So there are many tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il y a donc beaucoup de tâches.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 380, "src_audio": "/acl6060/audio/en/380.wav", "src_ref": "MTDNN maintained number of classes, heads, output layers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN a maintenu le nombre de classes, de têtes et de couches de sortie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 381, "src_audio": "/acl6060/audio/en/381.wav", "src_ref": "And the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en outre, le MTDNN doit initialiser de nouvelles têtes pour de nouvelles données avec une nouvelle tâche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 382, "src_audio": "/acl6060/audio/en/382.wav", "src_ref": "Our approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche, appelée raffinement de reformulation de tâche, est dans notre raffinement de reformulation de tâche d'approche. Au lieu de maintenir plusieurs têtes, nous reformulons chaque donnée dans une phrase par problème de classification, étant des tâches de deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 383, "src_audio": "/acl6060/audio/en/383.wav", "src_ref": "So let's see an example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Prenons donc un exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 384, "src_audio": "/acl6060/audio/en/384.wav", "src_ref": "Here is the our input dataset which consists of entities, features, text and classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici nos données de saisie qui se composent d'entités, de fonctions, de texte et de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 385, "src_audio": "/acl6060/audio/en/385.wav", "src_ref": "And, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous reformulons la tâche à partir d'une classification du texte en bas ou haut pour classer le texte, le résumé et la classe en vrai ou faux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 386, "src_audio": "/acl6060/audio/en/386.wav", "src_ref": "Or in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou en d'autres mots, nous avons formé le modèle de langue pour classer un résumé et une classe en résumé et classe, si le résumé appartient à la classe ou non.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 387, "src_audio": "/acl6060/audio/en/387.wav", "src_ref": "So the label vector in this case stays always ah which consists always with two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc dans ce cas, le vecteur d'étiquette consiste toujours en deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 388, "src_audio": "/acl6060/audio/en/388.wav", "src_ref": "And this is the ah algorithm for our fine, reformulated finetuning approach.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il s'agit de l'algorithme pour notre très bonne approche de raffinement reformulée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 389, "src_audio": "/acl6060/audio/en/389.wav", "src_ref": "So let's see the full framework.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons le cadre complet.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 390, "src_audio": "/acl6060/audio/en/390.wav", "src_ref": "Dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données sont introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 391, "src_audio": "/acl6060/audio/en/391.wav", "src_ref": "And then ah FeSTE executes entity linking phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis FeSTE exécute la phase de liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 392, "src_audio": "/acl6060/audio/en/392.wav", "src_ref": "It ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 393, "src_audio": "/acl6060/audio/en/393.wav", "src_ref": "Then it reformulated the task into a pairwise sentence classification task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il a reformulé la tâche en une tâche de classification des phrases par paire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 394, "src_audio": "/acl6060/audio/en/394.wav", "src_ref": "Applied the language model to the new task and the output likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il a appliqué le modèle de langue à la nouvelle tâche et la probabilité de sortie pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 395, "src_audio": "/acl6060/audio/en/395.wav", "src_ref": "And now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et maintenant, le modèle de langue est déjà raffiné sur des données n moins un en utilisant un raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 396, "src_audio": "/acl6060/audio/en/396.wav", "src_ref": "Then we use the output vector of the language model as a newly generated feature in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous utilisons le vecteur de sortie du modèle de langue comme une fonction nouvellement générée dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 397, "src_audio": "/acl6060/audio/en/397.wav", "src_ref": "To evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour évaluer notre cadre, nous utilisons dix-sept données de classification tabulaires qui varient en taille, fonctions, équilibre, domaine et performance initiale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 398, "src_audio": "/acl6060/audio/en/398.wav", "src_ref": "And as knowledge base we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 399, "src_audio": "/acl6060/audio/en/399.wav", "src_ref": "We design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous concevons notre expérience comme une évaluation où nous formons FeSTe sur seize données et l'appliquons à la dix-septième donnée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 400, "src_audio": "/acl6060/audio/en/400.wav", "src_ref": "We also, we also split each dataset into four folds and apply four folds cross validation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous divisons également chaque donnée en quatre plis et appliquons une validation croisée de quatre plis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 401, "src_audio": "/acl6060/audio/en/401.wav", "src_ref": "Then, we generate the new features and evaluate them using five evaluation classifiers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous générons les nouvelles fonctions et les évaluons en utilisant cinq classificateurs d'évaluation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 402, "src_audio": "/acl6060/audio/en/402.wav", "src_ref": "We use in our experiments base BERT base architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons dans nos expériences l'architecture de base des Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 403, "src_audio": "/acl6060/audio/en/403.wav", "src_ref": "Here are the results for our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici les résultats de nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 404, "src_audio": "/acl6060/audio/en/404.wav", "src_ref": "You can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez voir que nous comparons notre cadre au raffinement des données cibles, au raffinement de la tâche cible et à un raffinement préliminaire MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 405, "src_audio": "/acl6060/audio/en/405.wav", "src_ref": "And our reformulated finetuning achieves the best result, the best performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et notre raffinement reformulé atteint le meilleur résultat et la meilleure performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 406, "src_audio": "/acl6060/audio/en/406.wav", "src_ref": "While MTDNN achieved two percent improvement over the target dataset finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que le MTDNN a obtenu une amélioration de deux pour cent par rapport au raffinement des données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 407, "src_audio": "/acl6060/audio/en/407.wav", "src_ref": "Our approach achieved six percent improvement.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche a obtenu une amélioration de six pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 408, "src_audio": "/acl6060/audio/en/408.wav", "src_ref": "When we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque nous regardons les petites données, nous pouvons voir que la performance du MTDNN diminue et l'amélioration de la phase de raffinement multitâche préliminaire diminue aussi à un virgule cinq pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 409, "src_audio": "/acl6060/audio/en/409.wav", "src_ref": "But our performance increased to eleven percent compared to the target task finetuning alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais notre performance a augmenté à onze pour cent comparé au raffinement de tâche cible seul.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 410, "src_audio": "/acl6060/audio/en/410.wav", "src_ref": "For summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour résumer, FeSTE permet un enrichissement en quelques coups à partir de trente-cinq échantillons dans nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 411, "src_audio": "/acl6060/audio/en/411.wav", "src_ref": "It uses one architecture for all tasks and datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il utilise une architecture pour toutes les tâches et données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 412, "src_audio": "/acl6060/audio/en/412.wav", "src_ref": "And it keeps the head of ah of the model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il garde la tête du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 413, "src_audio": "/acl6060/audio/en/413.wav", "src_ref": "But it adds reformulation phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cela ajoute une phase de reformulation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 414, "src_audio": "/acl6060/audio/en/414.wav", "src_ref": "It augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il augmente l'ensemble de formation et a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'introduire dans le modèle de langue et l'utiliser dans le problème de classification par paire de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 415, "src_audio": "/acl6060/audio/en/415.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 0, "src_audio": "/acl6060/audio/en/0.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour tout le monde. Aujourd'hui, je vais vous présenter notre travail de recherche « Apprendre à raisonner par déduction » : résolution de problèmes de mots mathématiques comme extraction de relation complexe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 1, "src_audio": "/acl6060/audio/en/1.wav", "src_ref": "I'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'appelle Allan du laboratoire d'intelligence artificielle ByteDance, et ceci est un travail en commun avec Jierui Li de l'Université du Texas à Austin et Wei Lu du SUTD.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 2, "src_audio": "/acl6060/audio/en/2.wav", "src_ref": "First, I'd like to talk about our motivation for reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, j'aimerais parler de notre motivation pour le raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 3, "src_audio": "/acl6060/audio/en/3.wav", "src_ref": "So here we show an examples where multi-step reasoning is helpful.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous montrons un exemple où le raisonnement en plusieurs étapes est utile.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 4, "src_audio": "/acl6060/audio/en/4.wav", "src_ref": "So this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce chiffre est donc tiré de l'article PaLM où ils incitent à résoudre le problème de réseau dans le scénario d'apprentissage de quelques prises de vue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 5, "src_audio": "/acl6060/audio/en/5.wav", "src_ref": "So on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, sur le côté gauche, nous pouvons voir que si nous fournissons quelques exemples avec juste des questions et des réponses, nous pourrions ne pas être en mesure d'obtenir les bonnes réponses.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 6, "src_audio": "/acl6060/audio/en/6.wav", "src_ref": "But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si nous fournissons plus de description du raisonnement, le modèle est capable de prévenir la description du raisonnement, mais aussi d'effectuer ici une prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 7, "src_audio": "/acl6060/audio/en/7.wav", "src_ref": "So it is good to have interpretable multi-step reasoning as output.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est alors bon d'avoir comme résultat un raisonnement à plusieurs étapes interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 8, "src_audio": "/acl6060/audio/en/8.wav", "src_ref": "And we also think math word problem is a straightforward application to evaluate such reasoning abilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pensons également que les problèmes de mots mathématiques sont une application directe pour évaluer de telles capacités de raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 9, "src_audio": "/acl6060/audio/en/9.wav", "src_ref": "So, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 10, "src_audio": "/acl6060/audio/en/10.wav", "src_ref": "So in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans nos données, nous recevons également l‘expression mathématique qui conduit à cette réponse particulière.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 11, "src_audio": "/acl6060/audio/en/11.wav", "src_ref": "So, certain assumptions ah also apply as in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, certaines hypothèses s'appliquent également comme dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 12, "src_audio": "/acl6060/audio/en/12.wav", "src_ref": "We assume the precision of quantities are known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que la précision des quantités est connue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 13, "src_audio": "/acl6060/audio/en/13.wav", "src_ref": "And we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 14, "src_audio": "/acl6060/audio/en/14.wav", "src_ref": "Furthermore, complicated operators can be actually decomposed into these basic operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les opérateurs compliqués peuvent effectivement être décomposés en ces opérateurs de base.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 15, "src_audio": "/acl6060/audio/en/15.wav", "src_ref": "So, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, le travail antérieur dans la résolution de problèmes de mots mathématiques peut en effet être classé dans un modèle séquence à séquence et séquence à arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 16, "src_audio": "/acl6060/audio/en/16.wav", "src_ref": "So, traditional sequence to sequence model convert the expression to a specific sequence for generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle séquence à séquence traditionnel convertit donc l'expression en une séquence spécifique pour la génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 17, "src_audio": "/acl6060/audio/en/17.wav", "src_ref": "And it is pretty easy to implement and it can generalize to many different complicated problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est assez facile à mettre en œuvre et peut être généralisé à de nombreux problèmes compliqués différents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 18, "src_audio": "/acl6060/audio/en/18.wav", "src_ref": "But the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle structuré et son manque d'interopérabilité pour la prévention.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 19, "src_audio": "/acl6060/audio/en/19.wav", "src_ref": "But actually this direction is still quite popular because of um the transformer model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, cette direction est encore très populaire en raison du modèle de conversion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 20, "src_audio": "/acl6060/audio/en/20.wav", "src_ref": "So, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans les modèles à base d'arbre, nous structurons en réalité ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 21, "src_audio": "/acl6060/audio/en/21.wav", "src_ref": "So here we keep generating the operators until we reach the leaves, which are the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous continuons à produire les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 22, "src_audio": "/acl6060/audio/en/22.wav", "src_ref": "So here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, la bonne chose est qu'en fait, cela nous donne cette structure d'arbre binaire. Mais en réalité, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur et ensuite, à la fin, nous générons les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 23, "src_audio": "/acl6060/audio/en/23.wav", "src_ref": "And the second thing is that it also contains some repetitive computations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que cela contient également des calculs répétitifs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 24, "src_audio": "/acl6060/audio/en/24.wav", "src_ref": "So here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, si nous regardons cette expression, huit fois trois plus trois est en fait généré deux fois, mais en réalité, nous devrions réutiliser les résultats.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 25, "src_audio": "/acl6060/audio/en/25.wav", "src_ref": "So, in our proposed approach we want to solve those problems in a step by step and interpretable manners.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 26, "src_audio": "/acl6060/audio/en/26.wav", "src_ref": "So for example, here in the second step, ah we can obtain these divisors which is twenty seven.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, ici dans la deuxième étape, nous pouvons obtenir ces diviseurs qui sont vingt-sept.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 27, "src_audio": "/acl6060/audio/en/27.wav", "src_ref": "And we can also refer back to the original questions to find the relevant contents.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 28, "src_audio": "/acl6060/audio/en/28.wav", "src_ref": "And in these steps we obtain the divisors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et dans ces étapes, nous obtenons les diviseurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 29, "src_audio": "/acl6060/audio/en/29.wav", "src_ref": "So, ah and then at this third step we actually get the quotient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis, à cette troisième étape, nous obtenons alors le quotient.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 30, "src_audio": "/acl6060/audio/en/30.wav", "src_ref": "Alright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Et après ces trois étapes, nous pouvons alors réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, et enfin, nous pouvons obtenir les dividendes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 31, "src_audio": "/acl6060/audio/en/31.wav", "src_ref": "So, here we actually generate the whole expression directly rather than generating a single operators or quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous générons en réalité l'expression entière directement plutôt que de produire un seul opérateur ou une seule quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 32, "src_audio": "/acl6060/audio/en/32.wav", "src_ref": "So this makes the process more accurate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela rend le processus plus précis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 33, "src_audio": "/acl6060/audio/en/33.wav", "src_ref": "So, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et incluant également une certaine constante comme notre état initial.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 34, "src_audio": "/acl6060/audio/en/34.wav", "src_ref": "So, the expression is represented by e i j o p.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, l'expression est représentée par e i j o p.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 35, "src_audio": "/acl6060/audio/en/35.wav", "src_ref": "Where we perform operator from q_i to q_j, and such expression is actually directed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Où nous effectuons l'opérateur de q_i à q_j, et cette expression est en fait dirigée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 36, "src_audio": "/acl6060/audio/en/36.wav", "src_ref": "So, we also have subtraction with words here to represent the opposite direction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, nous avons également ici la soustraction avec des mots pour représenter la direction opposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 37, "src_audio": "/acl6060/audio/en/37.wav", "src_ref": "This is quite similar to relation extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est tout à fait similaire à l'extraction de relation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 38, "src_audio": "/acl6060/audio/en/38.wav", "src_ref": "So in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans un système déductif formel, à un pas de temps t, on applique l'opérateur entre la paire q_i et q_j, puis on obtient cette nouvelle expression.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 39, "src_audio": "/acl6060/audio/en/39.wav", "src_ref": "We add it to the next state to become a new quantity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous l'ajoutons à l'état suivant pour devenir une nouvelle quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 40, "src_audio": "/acl6060/audio/en/40.wav", "src_ref": "So, these slides actually visualize the evolution of the state where we keep adding expression to the current state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, ces diapositives visualisent en réalité l'évolution de l'état où nous continuons à ajouter de l'expression à l'état actuel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 41, "src_audio": "/acl6060/audio/en/41.wav", "src_ref": "So in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langue préformée qui peut être BERT ou Roberta, puis nous encodons la phrase et obtenons ensuite ces représentations de quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 42, "src_audio": "/acl6060/audio/en/42.wav", "src_ref": "So, once we get the quantity representations, we can start to do inference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire l'inférence.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 43, "src_audio": "/acl6060/audio/en/43.wav", "src_ref": "Here we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous montrons ici un exemple de q_1 pour obtenir la représentation pour q_2 divisée par q_2 multipliée par q_3.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 44, "src_audio": "/acl6060/audio/en/44.wav", "src_ref": "First we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous obtenons la représentation de paire, qui n'est essentiellement que l'enchaînement entre q_1 et q_2, puis nous appliquons un réseau prédictif qui est paramétré par l'opérateur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 45, "src_audio": "/acl6060/audio/en/45.wav", "src_ref": "And then finally we obtain the expression representation q_1 divided by q_2.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et enfin, nous obtenons la représentation de l'expression q_1 divisée par q_2.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 46, "src_audio": "/acl6060/audio/en/46.wav", "src_ref": "But in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, dans la pratique, au stade de l'inférence, nous pourrions également obtenir une expression incorrecte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 47, "src_audio": "/acl6060/audio/en/47.wav", "src_ref": "So, here all the possible expression is equals to three times the number of operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, toute l'expression possible est égale à trois fois le nombre d'opérateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 48, "src_audio": "/acl6060/audio/en/48.wav", "src_ref": "So the nice thing here is that we can easily add constraints to control this search this search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors, ce qui est bien ici, c'est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 49, "src_audio": "/acl6060/audio/en/49.wav", "src_ref": "For example, if this expression is not allowed, we can simply remove this expression in our search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 50, "src_audio": "/acl6060/audio/en/50.wav", "src_ref": "So in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans la deuxième étape, nous faisons la même chose, mais la seule différence est que nous avons une quantité de plus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 51, "src_audio": "/acl6060/audio/en/51.wav", "src_ref": "So this quantity come from the previous calculated expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette quantité provient donc de l'expression calculée antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 52, "src_audio": "/acl6060/audio/en/52.wav", "src_ref": "So finally we can obtain this final expression q_3 times q_4.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous pouvons obtenir cette expression finale q_3 multipliée par q_4.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 53, "src_audio": "/acl6060/audio/en/53.wav", "src_ref": "And we can also see the number of all the possible ah expression is different from the previous step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 54, "src_audio": "/acl6060/audio/en/54.wav", "src_ref": "So, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une telle différence rend difficile l'application de la beam search car la distribution de probabilité entre ces deux étapes est déséquilibrée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 55, "src_audio": "/acl6060/audio/en/55.wav", "src_ref": "So the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La procédure de formation est donc similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 56, "src_audio": "/acl6060/audio/en/56.wav", "src_ref": "And here we also use this tau to represent when we should terminate this generation process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous utilisons également ce tau pour représenter le moment où nous devrions mettre fin à ce processus de génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 57, "src_audio": "/acl6060/audio/en/57.wav", "src_ref": "And here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, l'espace est différent de séquence à séquence car il est différent à chaque pas de temps, alors que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 58, "src_audio": "/acl6060/audio/en/58.wav", "src_ref": "And it also allows us to impose certain constraints from prior from prior knowledge.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 59, "src_audio": "/acl6060/audio/en/59.wav", "src_ref": "So we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous menons donc des expériences sur les données de problèmes de mots mathématiques, MAWPS, Math23K, MathQA et SVAMP couramment utilisées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 60, "src_audio": "/acl6060/audio/en/60.wav", "src_ref": "And here we briefly show the results compared with the previous best approaches.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons brièvement les résultats comparés aux meilleures approches antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 61, "src_audio": "/acl6060/audio/en/61.wav", "src_ref": "So our best performing variant is Roberta-DeductiveReasoner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, notre variante la plus performante est Roberta-DeuctiveReasoner.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 62, "src_audio": "/acl6060/audio/en/62.wav", "src_ref": "And in fact we do not use beam search, in contrast all previous approaches are using beam search.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en réalité, nous n'utilisons pas la beam search ; au contraire, toutes les approches antérieures utilisent la beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 63, "src_audio": "/acl6060/audio/en/63.wav", "src_ref": "All right. So, the best approaches are often tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Ainsi, les meilleures approches sont souvent un modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 64, "src_audio": "/acl6060/audio/en/64.wav", "src_ref": "So, overall our reasoner is able to significantl significantly outperform this tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans l'ensemble, notre raisonneur est capable de dépasser significativement ce modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 65, "src_audio": "/acl6060/audio/en/65.wav", "src_ref": "But we can see the absolute numbers on MathQA or SVAMP are not really high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons voir que les nombres absolus sur MathQA ou SVAMP ne sont pas vraiment élevés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 66, "src_audio": "/acl6060/audio/en/66.wav", "src_ref": "So we further investigate the results on SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous étudions donc plus en détail les résultats sur SVAMP.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 67, "src_audio": "/acl6060/audio/en/67.wav", "src_ref": "And this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ces données sont difficiles parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle TAL traitement automatique du langage naturel comme ajouter des informations non pertinentes et des quantités supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 68, "src_audio": "/acl6060/audio/en/68.wav", "src_ref": "So, in our prediction we find some of the intermediate values are actually negatives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans notre prévention, nous trouvons que certaines des valeurs intermédiaires sont en réalité négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 69, "src_audio": "/acl6060/audio/en/69.wav", "src_ref": "For example, um, in these questions we are asking how many apples does Jake have?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans ces questions, nous demandons « combien de pommes a Jake ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 70, "src_audio": "/acl6060/audio/en/70.wav", "src_ref": "But we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous avons des informations supplémentaires comme « dix-sept photos de moins », et « Steven en a huit », ce qui est totalement hors de propos.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 71, "src_audio": "/acl6060/audio/en/71.wav", "src_ref": "So, our model makes some prediction like this which is producing negative values.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, notre modèle effectue une prévention comme celle-ci qui produit des valeurs négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 72, "src_audio": "/acl6060/audio/en/72.wav", "src_ref": "And we observe these two expressions actually have similar scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous constatons que ces deux expressions ont en réalité des scores similaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 73, "src_audio": "/acl6060/audio/en/73.wav", "src_ref": "So, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pouvons alors limiter cet espace de recherche en supprimant les résultats négatifs afin que nous puissions rendre la réponse correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 74, "src_audio": "/acl6060/audio/en/74.wav", "src_ref": "So um we further find such constraint actually improves quite a lot for some models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous trouvons donc que cette contrainte améliore en réalité beaucoup pour certains modèles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 75, "src_audio": "/acl6060/audio/en/75.wav", "src_ref": "For example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous améliorons sept points, puis pour le modèle de base Roberta, nous avons amélioré deux points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 76, "src_audio": "/acl6060/audio/en/76.wav", "src_ref": "So better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le meilleur modèle de langue a donc de meilleures capacités de compréhension de la langue de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 77, "src_audio": "/acl6060/audio/en/77.wav", "src_ref": "And we also try to analyze the difficulty behind these behind all these datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous essayons également d'analyser la difficulté derrière toutes ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 78, "src_audio": "/acl6060/audio/en/78.wav", "src_ref": "We assume the number of unused quantities can be regarded as irrelevant information here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que le nombre de quantités inutilisées peut être considéré ici comme une information non pertinente.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 79, "src_audio": "/acl6060/audio/en/79.wav", "src_ref": "So ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et les données SVAMP ont la plus grande partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 80, "src_audio": "/acl6060/audio/en/80.wav", "src_ref": "And here we also show the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons également la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 81, "src_audio": "/acl6060/audio/en/81.wav", "src_ref": "For those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ces échantillons sans quantités inutilisées, la performance est en réalité supérieure à la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 82, "src_audio": "/acl6060/audio/en/82.wav", "src_ref": "But with those samples that with unused quantity is actually way worse than the, worse than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais avec ces échantillons ayant une quantité inutilisée, c'est en fait bien pire que la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 83, "src_audio": "/acl6060/audio/en/83.wav", "src_ref": "For MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour MAWPS, nous n'avons pas vraiment trop de cas de test, alors j'ignore simplement cette partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 84, "src_audio": "/acl6060/audio/en/84.wav", "src_ref": "So, finally we want to show the interpretability through a question perturbation example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous voulons montrer l'interopérabilité à travers un exemple de perturbation de question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 85, "src_audio": "/acl6060/audio/en/85.wav", "src_ref": "So here our model actually makes a wrong prediction at the first step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, notre modèle effectue en réalité une prévention erronée à la première étape.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 86, "src_audio": "/acl6060/audio/en/86.wav", "src_ref": "So, we can actually correlate this expression with the sentence here. Alright.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, nous pouvons effectivement corréler cette expression avec la phrase ici. Très bien.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 87, "src_audio": "/acl6060/audio/en/87.wav", "src_ref": "So, we think this sentence might be misleading the model to an incorrect predictions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pensons donc que cette phrase pourrait induire le modèle en erreur avec des préventions incorrectes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 88, "src_audio": "/acl6060/audio/en/88.wav", "src_ref": "So here planting another thirty five makes the model makes the model think it should be an addition operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, en planter trente-cinq autres fait penser au modèle qu'il devrait être un opérateur d'addition.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 89, "src_audio": "/acl6060/audio/en/89.wav", "src_ref": "So we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous essayons alors de réviser la phrase pour que cela soit quelque chose comme « le nombre de poiriers est inférieur de trente-cinq à celui des pommiers ».", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 90, "src_audio": "/acl6060/audio/en/90.wav", "src_ref": "So, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous faisons en sorte de transmettre une sémantique plus précise afin que le modèle soit capable de rendre la prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 91, "src_audio": "/acl6060/audio/en/91.wav", "src_ref": "So, this study shows how the interpretable predictions help us understand the model behavior.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, cette étude montre comment les préventions interprétables nous aident à comprendre le comportement du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 92, "src_audio": "/acl6060/audio/en/92.wav", "src_ref": "So to conclude our work, so first our model is actually pretty efficient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour conclure notre travail, notre modèle est en réalité assez efficace.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 93, "src_audio": "/acl6060/audio/en/93.wav", "src_ref": "And we are able to provide interpretable solving procedure.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous sommes en mesure de fournir une procédure de résolution interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 94, "src_audio": "/acl6060/audio/en/94.wav", "src_ref": "And we can easily incorporate some prior knowledge as constraint which can help improve the performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons facilement incorporer des connaissances antérieures en tant que contraintes qui peuvent aider à améliorer la performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 95, "src_audio": "/acl6060/audio/en/95.wav", "src_ref": "And the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution des problèmes de réseau, mais aussi aux autres tâches qui impliquent un raisonnement à plusieurs étapes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 96, "src_audio": "/acl6060/audio/en/96.wav", "src_ref": "We also have certain limitations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également certaines limites.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 97, "src_audio": "/acl6060/audio/en/97.wav", "src_ref": "Ah, if we have a large number of operators or constants, the memory consumption could be pretty high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 98, "src_audio": "/acl6060/audio/en/98.wav", "src_ref": "And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que, comme mentionné, puisque la distribution de probabilité est déséquilibrée entre les différentes étapes de temps, il est donc également assez difficile d'appliquer la stratégie de beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 99, "src_audio": "/acl6060/audio/en/99.wav", "src_ref": "So this is the end of the talk, and questions are welcomed. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous arrivons donc à la fin de la discussion, et vos questions sont les bienvenues. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 100, "src_audio": "/acl6060/audio/en/100.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 101, "src_audio": "/acl6060/audio/en/101.wav", "src_ref": "I will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter mon travail en commun avec Jerry, qui porte sur de nouvelles données pour l'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 102, "src_audio": "/acl6060/audio/en/102.wav", "src_ref": "Legal issues are an integral part of many people's lives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions juridiques font partie intégrante de la vie de nombreuses personnes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 103, "src_audio": "/acl6060/audio/en/103.wav", "src_ref": "But the majority of citizens have little to know knowledge about their rights and fundamental legal processes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais la majorité des citoyens ont peu de connaissances sur leurs droits et leurs processus juridiques fondamentaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 104, "src_audio": "/acl6060/audio/en/104.wav", "src_ref": "As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En conséquence, de nombreux citoyens vulnérables qui n'ont pas les moyens de se payer l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 105, "src_audio": "/acl6060/audio/en/105.wav", "src_ref": "All work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tous les travaux visent à combler le fossé entre les personnes et la loi en développant un système d'extraction efficace pour les articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 106, "src_audio": "/acl6060/audio/en/106.wav", "src_ref": "Such a system could provide a free professional legal help service for unskilled humans.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un tel système pourrait fournir un service d'aide juridique professionnel gratuit aux personnes non qualifiées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 107, "src_audio": "/acl6060/audio/en/107.wav", "src_ref": "Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avant de plonger dans la principale contribution de ce travail, nous allons d'abord décrire le problème d'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 108, "src_audio": "/acl6060/audio/en/108.wav", "src_ref": "Given a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Compte tenu d'une simple question sur une question juridique telle que : « qu'est-ce que je risque si je viole la confidentialité professionnelle ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 109, "src_audio": "/acl6060/audio/en/109.wav", "src_ref": "A model is required to retrieve all relevant statutory articles from a large body of legislation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un modèle est nécessaire pour extraire tous les articles de droit pertinents d'un grand corpus législatif.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 110, "src_audio": "/acl6060/audio/en/110.wav", "src_ref": "This information retrieval task comes with its own set of challenges.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette tâche d'extraction des informations comporte son propre ensemble de défis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 111, "src_audio": "/acl6060/audio/en/111.wav", "src_ref": "First, it deals with two types of language.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, elle traite de deux types de langue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 112, "src_audio": "/acl6060/audio/en/112.wav", "src_ref": "Common natural language for the questions and complex legal language for the statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La langue naturelle commune pour les questions et la langue juridique complexe pour les lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 113, "src_audio": "/acl6060/audio/en/113.wav", "src_ref": "This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette différence dans les distributions de langue rend plus difficile pour un système d'extraire les candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 114, "src_audio": "/acl6060/audio/en/114.wav", "src_ref": "Besides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le droit écrit n'est pas une pile d'articles indépendants qui peuvent être traités comme une source complète d'informations par eux-mêmes, contrairement aux nouvelles ou aux recettes, par exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 115, "src_audio": "/acl6060/audio/en/115.wav", "src_ref": "Instead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, il s'agit d'un ensemble structuré de dispositions juridiques qui n'ont un sens entier que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les domaines et sous-domaines auxquels elles appartiennent et leur place dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 116, "src_audio": "/acl6060/audio/en/116.wav", "src_ref": "Lastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, les articles de droit ne sont pas de petits paragraphes constituant généralement l'unité d'extraction typique dans la plupart des travaux d'extraction.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 117, "src_audio": "/acl6060/audio/en/117.wav", "src_ref": "Here, there are long documents that may be up to six thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ici, il y a de longs documents qui peuvent aller jusqu'à six mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 118, "src_audio": "/acl6060/audio/en/118.wav", "src_ref": "The recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les recent advances en matière de TAL traitement automatique du langage naturel ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prévention du jugement juridique ou l'examen des contrats de contacts automatisés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 119, "src_audio": "/acl6060/audio/en/119.wav", "src_ref": "But statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais l'extraction de l'article de droit est restée essentiellement inchangée en raison du manque de grandes données étiquetées de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 120, "src_audio": "/acl6060/audio/en/120.wav", "src_ref": "In this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans ce travail, nous présentons de nouvelles données centrées sur le citoyen natif français pour étudier si l'extraction des modèles peut se rapprocher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche d'extraction de l'article de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 121, "src_audio": "/acl6060/audio/en/121.wav", "src_ref": "Our Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos données d'extraction de l'article de droit belge BSARD se composent de plus de mille cent questions juridiques posées par des citoyens belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 122, "src_audio": "/acl6060/audio/en/122.wav", "src_ref": "These questions cover a wide range of topics from family, housing, money, to work and social security.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces questions couvrent un wide range de sujets allant de la famille au logement, à l'argent, au travail et à la sécurité sociale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 123, "src_audio": "/acl6060/audio/en/123.wav", "src_ref": "Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques de codes de droit belge.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 124, "src_audio": "/acl6060/audio/en/124.wav", "src_ref": "Let's now talk about how we collected this dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Parlons maintenant de la façon dont nous avons collecté ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 125, "src_audio": "/acl6060/audio/en/125.wav", "src_ref": "First, we started by compiling a large corpus of legal articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 126, "src_audio": "/acl6060/audio/en/126.wav", "src_ref": "We considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons examiné trente-deux codes belges accessibles au public et extrait tous les articles ainsi que les titres de section correspondants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 127, "src_audio": "/acl6060/audio/en/127.wav", "src_ref": "Then we gathered legal questions with references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous avons rassemblé les questions juridiques avec les références aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 128, "src_audio": "/acl6060/audio/en/128.wav", "src_ref": "To do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ce faire, nous nous associons au cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur une question juridique personnelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 129, "src_audio": "/acl6060/audio/en/129.wav", "src_ref": "We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons eu la chance d'avoir accès à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 130, "src_audio": "/acl6060/audio/en/130.wav", "src_ref": "We collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons recueilli des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 131, "src_audio": "/acl6060/audio/en/131.wav", "src_ref": "Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous avons fait passer les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avons considérés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 132, "src_audio": "/acl6060/audio/en/132.wav", "src_ref": "The remaining references were matched and converted to the corresponding article ids from our corpus.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les références restantes ont été appariées et converties aux identifiants d'article correspondants de notre corpus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 133, "src_audio": "/acl6060/audio/en/133.wav", "src_ref": "We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous nous sommes finalement retrouvés avec mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 134, "src_audio": "/acl6060/audio/en/134.wav", "src_ref": "In addition, each question comes with the main category and a concatenation of subcategories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, chaque question est accompagnée de la catégorie principale et d'un enchaînement de sous-catégories.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 135, "src_audio": "/acl6060/audio/en/135.wav", "src_ref": "And each articles comes with a concatenation of the subsequence heading in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et chaque article comporte un enchaînement de la rubrique de sous-séquence dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 136, "src_audio": "/acl6060/audio/en/136.wav", "src_ref": "This extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour des recherches futures sur l'extraction d'informations juridiques ou la classification de textes juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 137, "src_audio": "/acl6060/audio/en/137.wav", "src_ref": "Let's look at some characteristic of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jetons un coup d'œil à certaines caractéristiques de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 138, "src_audio": "/acl6060/audio/en/138.wav", "src_ref": "The questions are between five and forty four words long with a median of fourteen words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions comportent entre cinq et quarante-quatre mots avec une moyenne de quatorze mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 139, "src_audio": "/acl6060/audio/en/139.wav", "src_ref": "The articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les articles sont beaucoup plus longs avec une longueur moyenne de soixante-dix-sept mots, avec cent quarante-deux d'entre eux dépassant les mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 140, "src_audio": "/acl6060/audio/en/140.wav", "src_ref": "The lengthiest one being up to five thousand seven hundred and ninety words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 141, "src_audio": "/acl6060/audio/en/141.wav", "src_ref": "As previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme mentionné précédemment, les questions couvrent un wide range de sujets, dont environ quatre-vingt-cinq pour cent concernent la famille, le logement, l'argent ou la justice.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 142, "src_audio": "/acl6060/audio/en/142.wav", "src_ref": "While the remaining fifteen percent concern either social security, foreigners or work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 143, "src_audio": "/acl6060/audio/en/143.wav", "src_ref": "The article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'article est également très diversifié car il provient de trente-deux codes belges différents qui couvrent un grand nombre de sujets juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 144, "src_audio": "/acl6060/audio/en/144.wav", "src_ref": "Here's the total number of articles collected from each of these Belgian codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le nombre total d'articles collectés à partir de chacun de ces codes belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 145, "src_audio": "/acl6060/audio/en/145.wav", "src_ref": "Out of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sur les vingt-deux mille six cent trente-trois articles, seuls mille six cent douze sont désignés comme pertinents à au moins une question dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 146, "src_audio": "/acl6060/audio/en/146.wav", "src_ref": "And around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et environ quatre-vingts pour cent de ces articles cités proviennent du code civil, des codes judiciaires, des codes d'enquête criminelle ou des codes pénaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 147, "src_audio": "/acl6060/audio/en/147.wav", "src_ref": "Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 148, "src_audio": "/acl6060/audio/en/148.wav", "src_ref": "Which can be explained by the fact that those codes focused less on individuals and their concerns.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce qui peut s'expliquer par le fait que ces codes se concentraient moins sur les individus et leurs préoccupations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 149, "src_audio": "/acl6060/audio/en/149.wav", "src_ref": "Overall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le nombre moyen de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 150, "src_audio": "/acl6060/audio/en/150.wav", "src_ref": "Using all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En utilisant toutes les données, nous avons comparé plusieurs approches d'extraction, y compris l'architecture lexicale et dense.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 151, "src_audio": "/acl6060/audio/en/151.wav", "src_ref": "Given a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné une requête et un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de requête des poids de chacun de ces termes dans cet article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 152, "src_audio": "/acl6060/audio/en/152.wav", "src_ref": "We experiment with the standard TF-IDF and BM25 ranking functions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec les fonctions de classement TF-IDF et BM25 standard.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 153, "src_audio": "/acl6060/audio/en/153.wav", "src_ref": "The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le principal problème de ces approches est qu'elles ne peuvent extraire que les articles contenant des mots-clés présents dans la requête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 154, "src_audio": "/acl6060/audio/en/154.wav", "src_ref": "To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour surmonter cette limitation, nous expérimentons une architecture neuronale qui peut capturer les relations sémantiques entre les requêtes et l'article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 155, "src_audio": "/acl6060/audio/en/155.wav", "src_ref": "We use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons un modèle bi-encodeur qui cartographie les requêtes et les articles en représentations vecteurs denses et calculons un score de pertinence entre une paire d'articles de requête par la similitude de leurs intégrations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 156, "src_audio": "/acl6060/audio/en/156.wav", "src_ref": "These embeddings typically result from a pooling operation on the output of a word embedding model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces intégrations résultent typiquement d'une opération de pooling sur la sortie d'un modèle d'intégration de mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 157, "src_audio": "/acl6060/audio/en/157.wav", "src_ref": "First, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous étudions l'efficacité des bi-encodeurs siamois dans une configuration d'évaluation « zero shot », ce qui signifie que des modèles d'intégration de mots préformés sont appliqués immédiatement sans aucun raffinement supplémentaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 158, "src_audio": "/acl6060/audio/en/158.wav", "src_ref": "We experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec l'encodeur de texte indépendant du contexte, à savoir word2vec et fastText, et des modèles d'intégration dépendants du contexte, à savoir Roberta et plus précisément CamemBERT qui est un modèle Roberta français.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 159, "src_audio": "/acl6060/audio/en/159.wav", "src_ref": "Additionally, we train our own CamemBERT based model ah bi-encoders on our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, nous formons nos propres bi-encodeurs de modèles basés sur CamemBERT sur nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 160, "src_audio": "/acl6060/audio/en/160.wav", "src_ref": "Note that for training, we experiment with the two flavors of the bi-encoder architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que pour la formation, nous expérimentons avec les deux modèles de l'architecture bi-encodeur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 161, "src_audio": "/acl6060/audio/en/161.wav", "src_ref": "Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le siamois, qui utilise un modèle d'intégration de mots unique cartographiant la requête et l'article ensemble dans un vector space dense partagé, et le modèle à deux tours, qui utilise deux modèles d'intégration de mots indépendants encodant la requête et l'article séparément dans différents espaces d'intégration.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 162, "src_audio": "/acl6060/audio/en/162.wav", "src_ref": "We experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons la mise en commun moyenne, maximale et CLS, ainsi que le produit et cosinus pour calculer les similitudes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 163, "src_audio": "/acl6060/audio/en/163.wav", "src_ref": "Here are the result of our baseline on the test sets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le résultat de notre base sur les ensembles de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 164, "src_audio": "/acl6060/audio/en/164.wav", "src_ref": "With the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec les méthodes lexicales ci-dessus, les bi-encodeurs siamois ont évalué dans une configuration zero shot au milieu, et les bi-encodeurs raffinés ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 165, "src_audio": "/acl6060/audio/en/165.wav", "src_ref": "Overall, the finetuned bi-encoder significantly outperforms all the other baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le bi-encodeur raffiné dépasse de manière significative toutes les autres bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 166, "src_audio": "/acl6060/audio/en/166.wav", "src_ref": "The two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle à deux tours s'améliore par rapport à ses variantes siamoises lors du rappel à cent, mais fonctionne de la même manière sur les autres indicateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 167, "src_audio": "/acl6060/audio/en/167.wav", "src_ref": "Although BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que BM25 ait sous-performé le bi-encodeur formé de manière significative, ses performances indiquent qu'il s'agit toujours d'une base solide pour l'extraction spécifique au domaine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 168, "src_audio": "/acl6060/audio/en/168.wav", "src_ref": "Regarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation zero shot du bi-encodeur siamois, nous constatons que l'utilisation directe des intégrations d'un modèle CamemBERT préformé sans optimiser pour la tâche d'extraction d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 169, "src_audio": "/acl6060/audio/en/169.wav", "src_ref": "Furthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous observons que le bi-encodeur basé sur word2vec a dépassé de manière significative les modèles basés sur fastText et les Représentations d'encodeurs bidirectionnels à partir de transformateurs, suggérant que peut-être les intégrations de niveau mot préformées sont plus appropriées pour la tâche que les intégrations de niveau caractère ou sous-mot lorsqu'elles sont utilisées immédiatement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 170, "src_audio": "/acl6060/audio/en/170.wav", "src_ref": "Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que prometteurs, ces résultats suggèrent de nombreuses possibilités d'amélioration comparé à un expert juridique qualifié qui peut éventuellement extraire tous les articles pertinents à n'importe quelle question, et ainsi obtenir des scores parfaits.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 171, "src_audio": "/acl6060/audio/en/171.wav", "src_ref": "Let's conclude by discussing two limitations of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Concluons en discutant de deux limites de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 172, "src_audio": "/acl6060/audio/en/172.wav", "src_ref": "First, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Premièrement, le corpus d'article est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 173, "src_audio": "/acl6060/audio/en/173.wav", "src_ref": "During the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au cours de la construction des données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions ne se retrouvent qu'avec une fraction du nombre initial d'articles pertinents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 174, "src_audio": "/acl6060/audio/en/174.wav", "src_ref": "This information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette information implique donc que la réponse contenue dans les autres articles pertinents pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 175, "src_audio": "/acl6060/audio/en/175.wav", "src_ref": "Second, we should note that not all legal questions can be answered with statutes alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, il convient de noter qu'on ne peut pas répondre à toutes les questions juridiques uniquement par des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 176, "src_audio": "/acl6060/audio/en/176.wav", "src_ref": "For instance, the question, can I evict my tenants if they make too much noise?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, la question « puis-je expulser mes locataires s'ils font trop de bruit ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 177, "src_audio": "/acl6060/audio/en/177.wav", "src_ref": "Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle peut ne pas avoir de réponse détaillée dans le droit écrit qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 178, "src_audio": "/acl6060/audio/en/178.wav", "src_ref": "Instead, the landlord should probably rely more on case law and find precedents similar to their current situation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des antécédents similaires à sa situation actuelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 179, "src_audio": "/acl6060/audio/en/179.wav", "src_ref": "For example, the tenants makes two parties a week until two AM.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, les locataires font la fête deux fois par semaine jusqu'à deux heures du matin.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 180, "src_audio": "/acl6060/audio/en/180.wav", "src_ref": "Hence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche d'extraction de l'article de droit, et le domaine des moins adaptées reste à déterminer.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 181, "src_audio": "/acl6060/audio/en/181.wav", "src_ref": "We hope that our work sparks interest in developing practical and reliable statutory article retrieval models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous espérons que nos travaux susciteront l'intérêt pour l'élaboration de modèles d'extraction d'article de droit pratiques et fiables.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 182, "src_audio": "/acl6060/audio/en/182.wav", "src_ref": "That can help improve access to justice for all.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut aider à améliorer l'accès à la justice pour tous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 183, "src_audio": "/acl6060/audio/en/183.wav", "src_ref": "You can check out our paper, dataset and code at the following links. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez consulter notre article, nos données et notre code aux liens suivants. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 184, "src_audio": "/acl6060/audio/en/184.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE ; un indice de référence indépendant de la tâche destiné à tester les modèles de langue et de vision avec des phénomènes linguistiques spécifiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 185, "src_audio": "/acl6060/audio/en/185.wav", "src_ref": "Why did we do the trouble in setting up this benchmark?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pourquoi avons-nous pris la peine de mettre en place cet indice de référence ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 186, "src_audio": "/acl6060/audio/en/186.wav", "src_ref": "Well, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eh bien, au cours des dernières années, nous avons vu une explosion de la vision basée sur le transformateur et des modèles de langue préformés sur de grandes quantités de paires de textes d'images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 187, "src_audio": "/acl6060/audio/en/187.wav", "src_ref": "Each one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacun de ces modèles pousse les tâches de pointe en matière de vision et de langue telles que la réponse aux questions visuelles, le raisonnement de bon sens visuel, l'extraction d'images et les bases de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 188, "src_audio": "/acl6060/audio/en/188.wav", "src_ref": "So we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc reçu un message : les précisions sur ces tâches et les indices de référence spécifiques augmentent régulièrement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 189, "src_audio": "/acl6060/audio/en/189.wav", "src_ref": "But do we know what the models have actually learned?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais savons-nous ce que les modèles ont réellement appris ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 190, "src_audio": "/acl6060/audio/en/190.wav", "src_ref": "What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Qu'est-ce qu'une vision et un transformateur de langue ont compris lors de l'attribution d'un score élevé pour que cette image et cette phrase correspondent ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 191, "src_audio": "/acl6060/audio/en/191.wav", "src_ref": "And the low score for this one?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le score bas pour celle-ci ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 192, "src_audio": "/acl6060/audio/en/192.wav", "src_ref": "Do vision and language models focus on the right thing?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Est-ce que les modèles de langue et de vision se concentrent sur la bonne chose ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 193, "src_audio": "/acl6060/audio/en/193.wav", "src_ref": "Or do they focus on biases as shown by previous work?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou se concentrent-ils sur les biais comme le montre le travail antérieur ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 194, "src_audio": "/acl6060/audio/en/194.wav", "src_ref": "To shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour éclairer davantage cet aspect, nous proposons une direction plus indépendante des tâches et introduisons VALSE qui teste la sensibilité des modèles de langue et de vision à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistiques et visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 195, "src_audio": "/acl6060/audio/en/195.wav", "src_ref": "We target existence, plurality, counting, spatial relations, actions and entity coreference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 196, "src_audio": "/acl6060/audio/en/196.wav", "src_ref": "But how do we test whether the vision and language models have captured this phenomena?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais comment tester si les modèles de langue et de vision ont capturé ce phénomène ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 197, "src_audio": "/acl6060/audio/en/197.wav", "src_ref": "By foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En pratiquant le foiling sur une méthode précédemment appliquée pour les modèles de langue et de vision seulement pour des phrases nominales de Ravi Shekhar et de ses collaborateurs, et en comptant par nous dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 198, "src_audio": "/acl6060/audio/en/198.wav", "src_ref": "Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Effectuer un foiling signifie essentiellement que nous prenons la légende d'une image et produisons un foil en modifiant la légende de sorte qu'elle ne décrive plus l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 199, "src_audio": "/acl6060/audio/en/199.wav", "src_ref": "And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous apportons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous aurions trouvé plus d'une façon intéressante de créer des instances de foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 200, "src_audio": "/acl6060/audio/en/200.wav", "src_ref": "For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans le cas de l'élément d'actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié avec une action différente, et un dans lequel les acteurs sont échangés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 201, "src_audio": "/acl6060/audio/en/201.wav", "src_ref": "Counting and coreference also are pieces that have more than one instrument.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le comptage et la coréférence sont également des éléments qui ont plusieurs instruments.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 202, "src_audio": "/acl6060/audio/en/202.wav", "src_ref": "And we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous créons ces foils en nous assurant qu'ils ne décrivent pas l'image et que ce sont des phrases grammaticales et autrement valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 203, "src_audio": "/acl6060/audio/en/203.wav", "src_ref": "This is not easy to do because a foiled caption may be less likely than the original caption.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce n'est pas facile à faire car une légende ayant subi un foil peut être moins probable que la légende originale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 204, "src_audio": "/acl6060/audio/en/204.wav", "src_ref": "For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que les plantes coupent un homme qu'un homme coupe les plantes, et la grande vision et les modèles de langue pourraient capter cela.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 205, "src_audio": "/acl6060/audio/en/205.wav", "src_ref": "Therefore, to obtain valid foils, we must take action.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour obtenir des foils valides, nous devons agir.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 206, "src_audio": "/acl6060/audio/en/206.wav", "src_ref": "First, we make use of strong language models to propose foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous utilisons des modèles de langue forts pour proposer des foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 207, "src_audio": "/acl6060/audio/en/207.wav", "src_ref": "Second, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, nous utilisons l'inférence de la langue naturelle ou la NLI courte pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 208, "src_audio": "/acl6060/audio/en/208.wav", "src_ref": "To test this automatically, we apply natural language inference with the following rationale.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour tester cela automatiquement, nous appliquons l'inférence de la langue naturelle avec la justification suivante.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 209, "src_audio": "/acl6060/audio/en/209.wav", "src_ref": "We consider an image to be the premise and its caption its entailed hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous considérons qu'une image est la prémisse et sa légende son hypothèse implicite.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 210, "src_audio": "/acl6060/audio/en/210.wav", "src_ref": "In addition, we consider the caption to be the premise, and the foil is its hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous considérons la légende comme la prémisse, et le foil est son hypothèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 211, "src_audio": "/acl6060/audio/en/211.wav", "src_ref": "If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si un modèle NLI prévient que le foil est en contradiction ou neutre par rapport à la légende, nous considérons cela comme un indicateur d'un foil valide.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 212, "src_audio": "/acl6060/audio/en/212.wav", "src_ref": "If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si une NLI prévient le foil à engendrer par la légende, cela ne peut pas être un bon foil, car par transitivité, cela donnera une description véridique de l'image et nous filtrons ces foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 213, "src_audio": "/acl6060/audio/en/213.wav", "src_ref": "But this procedure is not perfect, it is just an indicator for valid foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cette procédure n'est pas parfaite, il s'agit simplement d'un indicateur pour les foils valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 214, "src_audio": "/acl6060/audio/en/214.wav", "src_ref": "Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, comme troisième mesure pour produire des foils valides, nous utilisons des annotateurs humains pour valider les données utilisées dans VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 215, "src_audio": "/acl6060/audio/en/215.wav", "src_ref": "So, after filtering and human evaluation, we have as many test instances as described in this table.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrit dans ce tableau.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 216, "src_audio": "/acl6060/audio/en/216.wav", "src_ref": "Note that VALSE does not deliver any training data but only test data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que VALSE ne fournit pas de données de formation mais seulement des données de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 217, "src_audio": "/acl6060/audio/en/217.wav", "src_ref": "Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme il s'agit uniquement d'un indicateur de référence de test zero shot, il est conçu pour tirer parti des capacités existantes des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 218, "src_audio": "/acl6060/audio/en/218.wav", "src_ref": "Finetuning would only enable models to exploit artifacts or statistical biases in the data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement permettrait seulement aux modèles d'exploiter des artefacts ou des biais statistiques dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 219, "src_audio": "/acl6060/audio/en/219.wav", "src_ref": "And we all know that these models like to cheat and take shortcuts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 220, "src_audio": "/acl6060/audio/en/220.wav", "src_ref": "And as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et comme nous l'avons dit, cela nous intéresse d'évaluer les capacités des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 221, "src_audio": "/acl6060/audio/en/221.wav", "src_ref": "We experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec cinq modèles de langue et de vision sur VALSE, à savoir avec CLIP, LXMert, ViLBERT, ViLBERT douze en un, et VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 222, "src_audio": "/acl6060/audio/en/222.wav", "src_ref": "Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deux de nos indicateurs d'évaluation les plus importants sont la précision des modèles dans la classification de paires de phrases images en légendes et foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 223, "src_audio": "/acl6060/audio/en/223.wav", "src_ref": "Perhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Peut-être plus pertinent pour cette vidéo, nous présenterons notre indicateur plus permissif, la précision par paire, qui mesure si le score de sentence alignment d'image est plus élevé pour la bonne paire de texte image que pour sa paire ayant subi un foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 224, "src_audio": "/acl6060/audio/en/224.wav", "src_ref": "For more metrics and results on them, do check out our paper.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour plus d'indicateurs et de résultats, consultez notre article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 225, "src_audio": "/acl6060/audio/en/225.wav", "src_ref": "The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats avec une précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres indicateurs. La meilleure performance zero shot est obtenue par ViLBERT douze en un, suivi de ViLBERT, LXMert, CLIP, et enfin VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 226, "src_audio": "/acl6060/audio/en/226.wav", "src_ref": "It's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est remarquable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par ViLBERT douze en un, en soulignant que les modèles sont capables d'identifier des objets nommés et leur présence dans les images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 227, "src_audio": "/acl6060/audio/en/227.wav", "src_ref": "However, none of the remaining pieces can be reliably solved in our adversarial foiling settings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de foiling antagonistes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 228, "src_audio": "/acl6060/audio/en/228.wav", "src_ref": "We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous voyons à partir des instruments de pluralité et de comptage que les modèles de langue et de vision ont du mal à distinguer les références à des objets uniques par rapport à plusieurs, ou à les compter dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 229, "src_audio": "/acl6060/audio/en/229.wav", "src_ref": "The relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 230, "src_audio": "/acl6060/audio/en/230.wav", "src_ref": "They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ils ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité comme nous le voyons dans l'élément d'actions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 231, "src_audio": "/acl6060/audio/en/231.wav", "src_ref": "From the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À partir de l'élément de coréférence, nous découvrons que relever plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 232, "src_audio": "/acl6060/audio/en/232.wav", "src_ref": "As a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour vérifier la santé mentale, et parce que c'est une expérience intéressante, nous comparons également deux modèles à texte seulement, GPT un et GPT deux, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et ayant subi un foil, sans image ici, et en prévenant l'entrée avec la plus faible perplexité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 233, "src_audio": "/acl6060/audio/en/233.wav", "src_ref": "If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si la perplexité est plus élevée pour le foil, nous considérons cela comme une indication que la légende ayant subi un foil peut souffrir de biais de plausibilité ou d'autres biais linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 234, "src_audio": "/acl6060/audio/en/234.wav", "src_ref": "And it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il est intéressant de voir que dans certains cas, les modèles GPT à texte seulement ont capturé la plausibilité du monde mieux que les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 235, "src_audio": "/acl6060/audio/en/235.wav", "src_ref": "So to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour résumer, VALSE est un indicateur de référence qui utilise l'objectif des constructions linguistiques pour aider la communauté à améliorer les modèles de langue et de vision en testant durement leurs capacités de bases visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 236, "src_audio": "/acl6060/audio/en/236.wav", "src_ref": "Our experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que les modèles de langue et de vision identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais luttent pour ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 237, "src_audio": "/acl6060/audio/en/237.wav", "src_ref": "We would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers les bases de la langue avec des modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 238, "src_audio": "/acl6060/audio/en/238.wav", "src_ref": "And even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et plus encore, VALSE pourrait être utilisé comme une évaluation indirecte des données, car les modèles pourraient être évalués avant et après la formation ou le raffinement pour voir si des données aident les modèles à améliorer l'un des aspects testés par VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 239, "src_audio": "/acl6060/audio/en/239.wav", "src_ref": "If you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 240, "src_audio": "/acl6060/audio/en/240.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Kamezawa de l'Université de Tokyo.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 241, "src_audio": "/acl6060/audio/en/241.wav", "src_ref": "I'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter un article intitulé RNSum : des données à grande échelle pour la génération automatique de note de version via la synthèse des journaux de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 242, "src_audio": "/acl6060/audio/en/242.wav", "src_ref": "I'll be explaining in this order.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous expliquer dans cet ordre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 243, "src_audio": "/acl6060/audio/en/243.wav", "src_ref": "First, I will introduce automatic release note generation that we are working on in this research.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, je vais présenter la génération de note de version automatique sur laquelle nous travaillons dans cette recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 244, "src_audio": "/acl6060/audio/en/244.wav", "src_ref": "A release note is a technical document that summarizes the changes distributed with each release of a software product.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une note de version est un document technique qui résume les modifications distribuées à chaque version d'un produit logiciel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 245, "src_audio": "/acl6060/audio/en/245.wav", "src_ref": "The image shows a release note for version two point six point four of the vuejs library.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'image montre une note de version pour la version deux point six point quatre de la bibliothèque Vuejs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 246, "src_audio": "/acl6060/audio/en/246.wav", "src_ref": "Release notes play an important role in open source development but they're time consuming to prepare manually.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version jouent un rôle important dans le développement open source, mais elles mettent du temps à être préparées manuellement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 247, "src_audio": "/acl6060/audio/en/247.wav", "src_ref": "Therefore, it would be very useful to be able to automatically generate high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 248, "src_audio": "/acl6060/audio/en/248.wav", "src_ref": "I will defer to two previous researches on automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'en remettrai à deux recherches antérieures sur la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 249, "src_audio": "/acl6060/audio/en/249.wav", "src_ref": "The first is a system called ARENA released in twenty fourteen.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première est un système appelé ARENA sorti en deux-mille-quatorze.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 250, "src_audio": "/acl6060/audio/en/250.wav", "src_ref": "It takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il faut une approche basée sur des règles, par exemple, en utilisant l'extracteur de changement pour extraire toutes les différences, les changements de bibliothèque et les changements de document à partir des différences entre les versions, et enfin, en les combinant.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 251, "src_audio": "/acl6060/audio/en/251.wav", "src_ref": "The most notable feature of this system is the issue extractor in the upper right corner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 252, "src_audio": "/acl6060/audio/en/252.wav", "src_ref": "Which must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le système de suivi des problèmes, qui doit être laissé à Jira, ne peut être appliqué qu'aux projets qui utilisent Jira.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 253, "src_audio": "/acl6060/audio/en/253.wav", "src_ref": "In other words, it cannot be used for many projects on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En d'autres mots, il ne peut pas être utilisé pour de nombreux projets sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 254, "src_audio": "/acl6060/audio/en/254.wav", "src_ref": "The second is Glyph, recently announced in twenty twenty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde est Glyphe, récemment annoncée en deux-mille-vingt.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 255, "src_audio": "/acl6060/audio/en/255.wav", "src_ref": "It is available on the internet and can be installed via pip.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle est disponible sur internet et peut être installée via pip.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 256, "src_audio": "/acl6060/audio/en/256.wav", "src_ref": "This system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'une des cinq étiquettes telles que les fonctions ou corrections de bugs pour chaque message de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 257, "src_audio": "/acl6060/audio/en/257.wav", "src_ref": "This image is a sample usage that returns a corrective or bug fixes label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette image est un exemple d'utilisation qui renvoie une étiquette de correction ou de débogage.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 258, "src_audio": "/acl6060/audio/en/258.wav", "src_ref": "Glyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données de formation de Glyphe sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 259, "src_audio": "/acl6060/audio/en/259.wav", "src_ref": "The performance of the text classification model is not high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les performances du modèle de classification de texte ne sont pas élevées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 260, "src_audio": "/acl6060/audio/en/260.wav", "src_ref": "I present two related researches, but their problems are limited applicability and scarce data resources.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je présente deux recherches connexes, mais leurs problèmes sont l'applicabilité limitée et les ressources de données rares.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 261, "src_audio": "/acl6060/audio/en/261.wav", "src_ref": "Our paper solves these two problems and automatically generates high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 262, "src_audio": "/acl6060/audio/en/262.wav", "src_ref": "With a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec un problème d'applicabilité limitée, nous proposons une méthode de synthèse par classe de haute qualité en utilisant uniquement des messages de validation comme saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 263, "src_audio": "/acl6060/audio/en/263.wav", "src_ref": "This proposed method can be used for all English repositories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette méthode proposée peut être utilisée pour tous les référentiels anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 264, "src_audio": "/acl6060/audio/en/264.wav", "src_ref": "For the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour le deuxième problème de ressources de données rares, nous avons construit nos données RNSum composées d'environ quatre-vingt-deux mille éléments de données en collectant des données à partir de référentiels GitHub publics en utilisant l'API GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 265, "src_audio": "/acl6060/audio/en/265.wav", "src_ref": "Next, I'll describe our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais décrire nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 266, "src_audio": "/acl6060/audio/en/266.wav", "src_ref": "Here is an example of data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici un exemple de données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 267, "src_audio": "/acl6060/audio/en/267.wav", "src_ref": "The left side is a commit message and the right side is the release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le côté gauche est un message de validation et le côté droit représente les notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 268, "src_audio": "/acl6060/audio/en/268.wav", "src_ref": "Release notes are labeled as improvements or fixes, etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version sont étiquetées comme améliorations ou correctifs, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 269, "src_audio": "/acl6060/audio/en/269.wav", "src_ref": "We have set up a task that takes the commit messages as input and outputs a labeled release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons mis en place une tâche qui prend les messages de validation comme saisie et sorties des notes de version étiquetées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 270, "src_audio": "/acl6060/audio/en/270.wav", "src_ref": "This can be regarded as a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut être considéré comme une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 271, "src_audio": "/acl6060/audio/en/271.wav", "src_ref": "We have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons prédéfini quatre étiquettes : fonctions, améliorations, corrections de bugs, suppressions de dépréciations et modifications de rupture.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 272, "src_audio": "/acl6060/audio/en/272.wav", "src_ref": "These were set based on previous research and other factors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Celles-ci ont été établies sur la base de la recherche antérieure et d'autres facteurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 273, "src_audio": "/acl6060/audio/en/273.wav", "src_ref": "The release note on the bottom right is extracted from the release note on the bottom left.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La note de version en bas à droite est extraite de la note de version en bas à gauche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 274, "src_audio": "/acl6060/audio/en/274.wav", "src_ref": "At this time, it is necessary to detect the four labels that have been set up in advance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été mises en place à l'avance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 275, "src_audio": "/acl6060/audio/en/275.wav", "src_ref": "But the labels are not always consistent with each repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 276, "src_audio": "/acl6060/audio/en/276.wav", "src_ref": "For example, the improvements label includes improvements, enhancements, optimizations, and so on.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, l'étiquette d'améliorations inclut des améliorations, des perfectionnements, des optimisations, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 277, "src_audio": "/acl6060/audio/en/277.wav", "src_ref": "We prepared a vocabulary list of about thirty labels for each of these notational variations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons préparé une liste de vocabulaire d'une trentaine d'étiquettes pour chacune de ces variations de notation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 278, "src_audio": "/acl6060/audio/en/278.wav", "src_ref": "This is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de détecter la classe de note de version et de collecter le texte de la version qui suit en tant que phrase de note de version pour la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 279, "src_audio": "/acl6060/audio/en/279.wav", "src_ref": "Next is a commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il y a un message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 280, "src_audio": "/acl6060/audio/en/280.wav", "src_ref": "Commit messages are not tied to each release.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation ne sont pas liés à chaque version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 281, "src_audio": "/acl6060/audio/en/281.wav", "src_ref": "As shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme le montre l'image ci-dessous, si la version actuelle est la version deux point cinq à dix-neuf, nous devons identifier la version deux point cinq à dix-huit antérieure et obtenir un diff.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 282, "src_audio": "/acl6060/audio/en/282.wav", "src_ref": "This is a bit tedious and it is not enough to just get a list of releases and look at the before and after.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est un peu fastidieux et il ne suffit pas d'obtenir une liste de versions et de regarder l'avant et l'après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 283, "src_audio": "/acl6060/audio/en/283.wav", "src_ref": "We created a heuristic matching rule to get the previous and next versions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons créé une règle de correspondance heuristique pour obtenir les versions antérieures et suivantes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 284, "src_audio": "/acl6060/audio/en/284.wav", "src_ref": "Dataset analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Analyse des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 285, "src_audio": "/acl6060/audio/en/285.wav", "src_ref": "In the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En fin de compte, sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données ont été recueillis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 286, "src_audio": "/acl6060/audio/en/286.wav", "src_ref": "Also, the average number of release notes tokens is sixty three, which is quite high for a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le nombre moyen de gages de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 287, "src_audio": "/acl6060/audio/en/287.wav", "src_ref": "Also, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De même, le nombre de gages uniques est assez grand, s'élevant à huit mille huit cent trente mille.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 288, "src_audio": "/acl6060/audio/en/288.wav", "src_ref": "This is due to the large number of unique class or method names found in the repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela est dû au grand nombre de noms de méthode ou de classe unique trouvés dans le référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 289, "src_audio": "/acl6060/audio/en/289.wav", "src_ref": "Next, I will explain the proposed method.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais expliquer la méthode proposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 290, "src_audio": "/acl6060/audio/en/290.wav", "src_ref": "The classwise extractive then abstractive summarization model consists of two neural modules.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle de synthèse extractive puis abstractive par classe se compose de deux modules neuronaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 291, "src_audio": "/acl6060/audio/en/291.wav", "src_ref": "A classifier using BERT or CodeBERT and a generator using BART.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un classificateur utilisant BERT ou CodeBERT et un générateur utilisant BART.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 292, "src_audio": "/acl6060/audio/en/292.wav", "src_ref": "First, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, CEAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, qui utilisent des améliorations, des corrections de bogues, des dépréciations et d'autres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 293, "src_audio": "/acl6060/audio/en/293.wav", "src_ref": "The commit messages classified as other are discarded.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation classés comme autres sont supprimés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 294, "src_audio": "/acl6060/audio/en/294.wav", "src_ref": "Then CEAS applies the generator to the four labeled documents independently and generates release notes for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 295, "src_audio": "/acl6060/audio/en/295.wav", "src_ref": "In this task, the direct correspondences between commit messages and release notes are not known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 296, "src_audio": "/acl6060/audio/en/296.wav", "src_ref": "Therefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour former le classificateur, nous avons réaffecté les enquêtes à chaque message de validation de saisie en utilisant les dix premiers caractères de chaque message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 297, "src_audio": "/acl6060/audio/en/297.wav", "src_ref": "We modeled the classwise abstractive summarization approach by two different methods.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons modélisé l'approche de synthèse abstractive par classe à travers deux méthodes différentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 298, "src_audio": "/acl6060/audio/en/298.wav", "src_ref": "The first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le premier modèle, que nous appelons CAS unique, se compose d'un réseau de six à six unique et génère un seul texte de note de version donnant un enchaînement de messages de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 299, "src_audio": "/acl6060/audio/en/299.wav", "src_ref": "The output texts can be divided into classwise segments based on special class-specific endpoint symbols.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les textes de sortie peuvent être divisés en segments par classe sur la base de symboles de point d'extrémité particuliers, spécifiques à la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 300, "src_audio": "/acl6060/audio/en/300.wav", "src_ref": "The second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde méthode, que nous appelons CAS multiple, se compose de quatre réseaux seq2seq différents, dont chacun correspond à l'une des classes de notes de version fixes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 301, "src_audio": "/acl6060/audio/en/301.wav", "src_ref": "Okay, let me explain the experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ok, je vais vous expliquer les expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 302, "src_audio": "/acl6060/audio/en/302.wav", "src_ref": "Five methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cinq méthodes ont été comparées : CEAS, CAS unique, CAS multiple, regroupement et étude antérieure, Glyph.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 303, "src_audio": "/acl6060/audio/en/303.wav", "src_ref": "Regarding evaluation, in some cases, release notes are output in multiple sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 304, "src_audio": "/acl6060/audio/en/304.wav", "src_ref": "Since it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisqu'il est difficile de calculer le nombre de phrases telles qu'elles sont, elles sont combinées avec des espaces et traitées comme une seule phrase longue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 305, "src_audio": "/acl6060/audio/en/305.wav", "src_ref": "The BLEU is penalized when the system outputs a short sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le BLEU est pénalisé lorsque le système produit une phrase courte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 306, "src_audio": "/acl6060/audio/en/306.wav", "src_ref": "This penalty results in a lower BLEU value in the experiment results described next.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette pénalité se traduit par une valeur BLEU inférieure dans les résultats de l'expérience décrits ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 307, "src_audio": "/acl6060/audio/en/307.wav", "src_ref": "Finally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous calculons également la spécificité car ROUGE et BLEU ne peuvent pas être calculés si les notes de version sont vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 308, "src_audio": "/acl6060/audio/en/308.wav", "src_ref": "A higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une spécificité plus élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées être vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 309, "src_audio": "/acl6060/audio/en/309.wav", "src_ref": "Here are the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats sont indiqués ci-après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 310, "src_audio": "/acl6060/audio/en/310.wav", "src_ref": "Since the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné que les données contiennent des adresses électroniques, des valeurs hachées, etc., nous avons également évalué les données nettoyées, ce qui les exclut.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 311, "src_audio": "/acl6060/audio/en/311.wav", "src_ref": "CEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS et le CAS ont obtenu des scores de ROUGE-L supérieurs de plus de dix points par rapport aux bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 312, "src_audio": "/acl6060/audio/en/312.wav", "src_ref": "In particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les bases a atteint plus de vingt points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 313, "src_audio": "/acl6060/audio/en/313.wav", "src_ref": "These results indicate that CEAS and CAS are significantly affected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces résultats indiquent que le CEAS et le CAS sont considérablement touchés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 314, "src_audio": "/acl6060/audio/en/314.wav", "src_ref": "CEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS a obtenu un meilleur score ROUGE-L que le CAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour former le classificateur à l'aide de pseudo-étiquettes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 315, "src_audio": "/acl6060/audio/en/315.wav", "src_ref": "High coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une couverture élevée du CEAS peut être obtenue probablement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 316, "src_audio": "/acl6060/audio/en/316.wav", "src_ref": "CAS-Multi tended to yield higher ROUGE-L than CAS-Single.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CAS multiple tendait à produire plus de ROUGE-L que le CAS unique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 317, "src_audio": "/acl6060/audio/en/317.wav", "src_ref": "Suggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En suggérant qu'il est également efficace de développer indépendamment et différemment des modèles de abstractive summarization pour chaque classe de note de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 318, "src_audio": "/acl6060/audio/en/318.wav", "src_ref": "Here are an error analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici une analyse d'erreur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 319, "src_audio": "/acl6060/audio/en/319.wav", "src_ref": "CAS methods tend to output shorter sentences than human reference sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 320, "src_audio": "/acl6060/audio/en/320.wav", "src_ref": "In the figure on the right, the reference sentence has three or four sentences, while CAS has only one.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que le CAS n'en a qu'une.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 321, "src_audio": "/acl6060/audio/en/321.wav", "src_ref": "The reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La raison de la réticence de ce modèle est que dans les données de formation, seulement trente-trois pour cent des phrases sont présentes dans l'étiquette des fonctions et quarante pour cent dans l'étiquette des améliorations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 322, "src_audio": "/acl6060/audio/en/322.wav", "src_ref": "Furthermore, CAS methods cannot generate accurate release notes without additional information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les méthodes CAS ne peuvent pas générer des notes de version précises sans informations supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 323, "src_audio": "/acl6060/audio/en/323.wav", "src_ref": "The top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple en haut à droite est un exemple de message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la progression ou au problème correspondant(e).", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 324, "src_audio": "/acl6060/audio/en/324.wav", "src_ref": "The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple ci-dessous montre que les deux messages de validation dans la saisie sont liés et doivent être combinés en une phrase, mais cela n'est pas fait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 325, "src_audio": "/acl6060/audio/en/325.wav", "src_ref": "Finally, a conclusion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, passons à la conclusion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 326, "src_audio": "/acl6060/audio/en/326.wav", "src_ref": "We have built a new dataset for automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons construit de nouvelles données pour la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 327, "src_audio": "/acl6060/audio/en/327.wav", "src_ref": "We have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également formulé une tâche consistant à saisir des messages de validation et à les résumer afin qu'ils soient applicables à tous les projets écrits en anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 328, "src_audio": "/acl6060/audio/en/328.wav", "src_ref": "Our experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 329, "src_audio": "/acl6060/audio/en/329.wav", "src_ref": "Please check out our dataset on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Veuillez consulter nos données sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 330, "src_audio": "/acl6060/audio/en/330.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 331, "src_audio": "/acl6060/audio/en/331.wav", "src_ref": "Hello. My name is Asaf Harari.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour. Je m'appelle Asaf Harari.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 332, "src_audio": "/acl6060/audio/en/332.wav", "src_ref": "And I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et je vais vous présenter notre article : enrichissement de données tabulaires en quelques coups à l'aide d'architectures de transformateurs raffinées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 333, "src_audio": "/acl6060/audio/en/333.wav", "src_ref": "Data scientists analyze data and mainly focus on the manipulating the data's existing features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les scientifiques analysent les données et se concentrent principalement sur la manipulation des fonctions existantes des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 334, "src_audio": "/acl6060/audio/en/334.wav", "src_ref": "But sometimes, these features are limited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais parfois, ces fonctions sont limitées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 335, "src_audio": "/acl6060/audio/en/335.wav", "src_ref": "Feature generation using another data source may add substantial information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La génération de fonctions en utilisant une autre source de données peut ajouter des informations substantielles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 336, "src_audio": "/acl6060/audio/en/336.wav", "src_ref": "Our research goal is automatic tabular data enrichment using external sources' free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des textes libres de sources externes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 337, "src_audio": "/acl6060/audio/en/337.wav", "src_ref": "Assume we have a tabular dataset and a knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Supposons que nous ayons des données tabulaires et une base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 338, "src_audio": "/acl6060/audio/en/338.wav", "src_ref": "We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles fonctions du texte libre de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 339, "src_audio": "/acl6060/audio/en/339.wav", "src_ref": "Our framework FeSTE is exactly this automatic process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre cadre FeSTE est exactement ce processus automatique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 340, "src_audio": "/acl6060/audio/en/340.wav", "src_ref": "So let's see an example in a dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons donc un exemple dans des données introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 341, "src_audio": "/acl6060/audio/en/341.wav", "src_ref": "In this example, the dataset is university dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données sont des données universitaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 342, "src_audio": "/acl6060/audio/en/342.wav", "src_ref": "When its goal is to classify universities into low ranking universities and high-ranking universities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Quand l'objectif est de classer les universités en universités de bas rang et en universités de haut rang.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 343, "src_audio": "/acl6060/audio/en/343.wav", "src_ref": "As knowledge base, we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 344, "src_audio": "/acl6060/audio/en/344.wav", "src_ref": "The first phase of FeSTE is entity linking.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première phase de FeSTE est la liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 345, "src_audio": "/acl6060/audio/en/345.wav", "src_ref": "When each entity, in this example the university name, is linked to an entity within the knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 346, "src_audio": "/acl6060/audio/en/346.wav", "src_ref": "And and the text of the entities of the knowledge base is extracted and added to the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le texte des entités de la base de connaissances est extrait et ajouté aux données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 347, "src_audio": "/acl6060/audio/en/347.wav", "src_ref": "In this example, the text is the Wikipedia page's abstract.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, le texte est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 348, "src_audio": "/acl6060/audio/en/348.wav", "src_ref": "Now, we need to generate or extract features from the retrieved text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Maintenant, nous devons générer ou extraire des fonctions à partir du texte extrait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 349, "src_audio": "/acl6060/audio/en/349.wav", "src_ref": "So, we need to ah feature extraction phase ah which includes text analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc besoin de la phase d'extraction des fonctions qui comprend l'analyse de texte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 350, "src_audio": "/acl6060/audio/en/350.wav", "src_ref": "And this is the main novelty of this paper and I will deep dive into it in the next slides.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de la principale nouveauté de cet article et je vais mieux l'expliquer dans les prochaines diapositives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 351, "src_audio": "/acl6060/audio/en/351.wav", "src_ref": "After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Après la phase d'extraction des fonctions, il y a une phase de génération de fonctions lorsque nous utilisons les fonctions extraites pour générer un petit nombre de nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 352, "src_audio": "/acl6060/audio/en/352.wav", "src_ref": "First generate ah features in the number of classes of the original dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Générez d'abord les fonctions dans le nombre de classes des données d'origine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 353, "src_audio": "/acl6060/audio/en/353.wav", "src_ref": "In this example, the original dataset has two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données d'origine ont deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 354, "src_audio": "/acl6060/audio/en/354.wav", "src_ref": "So, FeSTE generates two new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, FeSTE génère deux nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 355, "src_audio": "/acl6060/audio/en/355.wav", "src_ref": "But if the dataset has five classes, FeSTE generates five new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si les données ont cinq classes, FeSTE génère cinq nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 356, "src_audio": "/acl6060/audio/en/356.wav", "src_ref": "Each feature represents the likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chaque fonction représente la probabilité pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 357, "src_audio": "/acl6060/audio/en/357.wav", "src_ref": "To analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour analyser le texte, nous utilisons les analyses de texte de pointe, qui sont des modèles de langue basés sur la conversion comme BERT, GPT, XLNet etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 358, "src_audio": "/acl6060/audio/en/358.wav", "src_ref": "It is but it is not likely that we can train language models using the input datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais il est peu probable que nous puissions former des modèles de langue en utilisant les données de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 359, "src_audio": "/acl6060/audio/en/359.wav", "src_ref": "So a naive approach will be ah target task finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une approche naïve sera le raffinement de la tâche cible.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 360, "src_audio": "/acl6060/audio/en/360.wav", "src_ref": "So, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la phase d'extraction des fonctions, nous pouvons télécharger des modèles de langue préformée et raffiner le modèle de langue sur les données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 361, "src_audio": "/acl6060/audio/en/361.wav", "src_ref": "In this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, raffiner le modèle de langue, classer le texte en classes, résumer en classes, basses ou hautes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 362, "src_audio": "/acl6060/audio/en/362.wav", "src_ref": "Receive the language model output, which is the likelihood for each class and use as new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Recevoir les résultats du modèle de langue, qui sont la probabilité pour chaque classe et les utiliser comme nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 363, "src_audio": "/acl6060/audio/en/363.wav", "src_ref": "The problem with this approach is datasets may have few distinct entities / texts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le problème avec cette approche est que les données peuvent avoir peu d'entités / de textes distinct(e)s.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 364, "src_audio": "/acl6060/audio/en/364.wav", "src_ref": "In our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre expérience, près de la moitié des données contiennent moins de quatre cents échantillons et les plus petites données contiennent trente-cinq échantillons dans un ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 365, "src_audio": "/acl6060/audio/en/365.wav", "src_ref": "So to finetune a language model over ah this dataset will be ineffective.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour raffiner un modèle de langue sur cela, les données seront inefficaces.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 366, "src_audio": "/acl6060/audio/en/366.wav", "src_ref": "But we can use prior knowledge about pre-analyzed datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons utiliser des connaissances préalables sur des données préanalysées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 367, "src_audio": "/acl6060/audio/en/367.wav", "src_ref": "Because FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisque nous appliquons FeSTE sur des données multiples, nous pouvons utiliser les données n moins un pour recueillir des informations sur les données n moins un, et utiliser ces informations lorsque nous analysons les nièmes données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 368, "src_audio": "/acl6060/audio/en/368.wav", "src_ref": "What we, what we suggest is to add, to add another finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce que nous suggérons, c'est d'ajouter une autre phase de raffinement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 369, "src_audio": "/acl6060/audio/en/369.wav", "src_ref": "A preliminary multitask finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une phase de raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 370, "src_audio": "/acl6060/audio/en/370.wav", "src_ref": "When you finetune the language model over the n minus one datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque vous raffinez le modèle de langue sur les données n moins un.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 371, "src_audio": "/acl6060/audio/en/371.wav", "src_ref": "And, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ensuite, nous exécutons une autre phase de raffinement qui est un raffinement de tâche cible, lorsque nous raffinons le modèle de langue sur les nièmes données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 372, "src_audio": "/acl6060/audio/en/372.wav", "src_ref": "The state-of-the-art in multitask ah multitask finetuning called MTDNN.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement multitâche de pointe appelé MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 373, "src_audio": "/acl6060/audio/en/373.wav", "src_ref": "In MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN maintient les têtes dans le nombre de tâches dans l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 374, "src_audio": "/acl6060/audio/en/374.wav", "src_ref": "So, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans cet exemple, il y a quatre tâches dans l'ensemble de formation. Le MTDNN maintient alors quatre têtes comme vous pouvez le voir sur l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 375, "src_audio": "/acl6060/audio/en/375.wav", "src_ref": "And it samples a random batch from ah from the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il échantillonne un lot aléatoire de l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 376, "src_audio": "/acl6060/audio/en/376.wav", "src_ref": "And if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si elles appartiennent à un lot aléatoire, par exemple, une seule tâche de classification de phrases, il exécute des chemins d'aller et retour à travers la première tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 377, "src_audio": "/acl6060/audio/en/377.wav", "src_ref": "And if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si le lot aléatoire appartient à la tâche de classement par paire, il exécute un chemin d'aller et retour dans la dernière tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 378, "src_audio": "/acl6060/audio/en/378.wav", "src_ref": "In our scenario, ah tabular datasets vary in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre scénario, les données tabulaires varient dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 379, "src_audio": "/acl6060/audio/en/379.wav", "src_ref": "So there are many tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il y a donc beaucoup de tâches.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 380, "src_audio": "/acl6060/audio/en/380.wav", "src_ref": "MTDNN maintained number of classes, heads, output layers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN a maintenu le nombre de classes, de têtes et de couches de sortie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 381, "src_audio": "/acl6060/audio/en/381.wav", "src_ref": "And the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en outre, le MTDNN doit initialiser de nouvelles têtes pour de nouvelles données avec une nouvelle tâche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 382, "src_audio": "/acl6060/audio/en/382.wav", "src_ref": "Our approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche, appelée raffinement de reformulation de tâche, est dans notre raffinement de reformulation de tâche d'approche. Au lieu de maintenir plusieurs têtes, nous reformulons chaque donnée dans une phrase par problème de classification, étant des tâches de deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 383, "src_audio": "/acl6060/audio/en/383.wav", "src_ref": "So let's see an example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Prenons donc un exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 384, "src_audio": "/acl6060/audio/en/384.wav", "src_ref": "Here is the our input dataset which consists of entities, features, text and classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici nos données de saisie qui se composent d'entités, de fonctions, de texte et de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 385, "src_audio": "/acl6060/audio/en/385.wav", "src_ref": "And, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous reformulons la tâche à partir d'une classification du texte en bas ou haut pour classer le texte, le résumé et la classe en vrai ou faux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 386, "src_audio": "/acl6060/audio/en/386.wav", "src_ref": "Or in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou en d'autres mots, nous avons formé le modèle de langue pour classer un résumé et une classe en résumé et classe, si le résumé appartient à la classe ou non.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 387, "src_audio": "/acl6060/audio/en/387.wav", "src_ref": "So the label vector in this case stays always ah which consists always with two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc dans ce cas, le vecteur d'étiquette consiste toujours en deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 388, "src_audio": "/acl6060/audio/en/388.wav", "src_ref": "And this is the ah algorithm for our fine, reformulated finetuning approach.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il s'agit de l'algorithme pour notre très bonne approche de raffinement reformulée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 389, "src_audio": "/acl6060/audio/en/389.wav", "src_ref": "So let's see the full framework.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons le cadre complet.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 390, "src_audio": "/acl6060/audio/en/390.wav", "src_ref": "Dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données sont introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 391, "src_audio": "/acl6060/audio/en/391.wav", "src_ref": "And then ah FeSTE executes entity linking phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis FeSTE exécute la phase de liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 392, "src_audio": "/acl6060/audio/en/392.wav", "src_ref": "It ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 393, "src_audio": "/acl6060/audio/en/393.wav", "src_ref": "Then it reformulated the task into a pairwise sentence classification task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il a reformulé la tâche en une tâche de classification des phrases par paire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 394, "src_audio": "/acl6060/audio/en/394.wav", "src_ref": "Applied the language model to the new task and the output likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il a appliqué le modèle de langue à la nouvelle tâche et la probabilité de sortie pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 395, "src_audio": "/acl6060/audio/en/395.wav", "src_ref": "And now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et maintenant, le modèle de langue est déjà raffiné sur des données n moins un en utilisant un raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 396, "src_audio": "/acl6060/audio/en/396.wav", "src_ref": "Then we use the output vector of the language model as a newly generated feature in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous utilisons le vecteur de sortie du modèle de langue comme une fonction nouvellement générée dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 397, "src_audio": "/acl6060/audio/en/397.wav", "src_ref": "To evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour évaluer notre cadre, nous utilisons dix-sept données de classification tabulaires qui varient en taille, fonctions, équilibre, domaine et performance initiale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 398, "src_audio": "/acl6060/audio/en/398.wav", "src_ref": "And as knowledge base we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 399, "src_audio": "/acl6060/audio/en/399.wav", "src_ref": "We design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous concevons notre expérience comme une évaluation où nous formons FeSTe sur seize données et l'appliquons à la dix-septième donnée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 400, "src_audio": "/acl6060/audio/en/400.wav", "src_ref": "We also, we also split each dataset into four folds and apply four folds cross validation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous divisons également chaque donnée en quatre plis et appliquons une validation croisée de quatre plis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 401, "src_audio": "/acl6060/audio/en/401.wav", "src_ref": "Then, we generate the new features and evaluate them using five evaluation classifiers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous générons les nouvelles fonctions et les évaluons en utilisant cinq classificateurs d'évaluation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 402, "src_audio": "/acl6060/audio/en/402.wav", "src_ref": "We use in our experiments base BERT base architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons dans nos expériences l'architecture de base des Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 403, "src_audio": "/acl6060/audio/en/403.wav", "src_ref": "Here are the results for our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici les résultats de nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 404, "src_audio": "/acl6060/audio/en/404.wav", "src_ref": "You can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez voir que nous comparons notre cadre au raffinement des données cibles, au raffinement de la tâche cible et à un raffinement préliminaire MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 405, "src_audio": "/acl6060/audio/en/405.wav", "src_ref": "And our reformulated finetuning achieves the best result, the best performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et notre raffinement reformulé atteint le meilleur résultat et la meilleure performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 406, "src_audio": "/acl6060/audio/en/406.wav", "src_ref": "While MTDNN achieved two percent improvement over the target dataset finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que le MTDNN a obtenu une amélioration de deux pour cent par rapport au raffinement des données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 407, "src_audio": "/acl6060/audio/en/407.wav", "src_ref": "Our approach achieved six percent improvement.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche a obtenu une amélioration de six pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 408, "src_audio": "/acl6060/audio/en/408.wav", "src_ref": "When we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque nous regardons les petites données, nous pouvons voir que la performance du MTDNN diminue et l'amélioration de la phase de raffinement multitâche préliminaire diminue aussi à un virgule cinq pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 409, "src_audio": "/acl6060/audio/en/409.wav", "src_ref": "But our performance increased to eleven percent compared to the target task finetuning alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais notre performance a augmenté à onze pour cent comparé au raffinement de tâche cible seul.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 410, "src_audio": "/acl6060/audio/en/410.wav", "src_ref": "For summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour résumer, FeSTE permet un enrichissement en quelques coups à partir de trente-cinq échantillons dans nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 411, "src_audio": "/acl6060/audio/en/411.wav", "src_ref": "It uses one architecture for all tasks and datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il utilise une architecture pour toutes les tâches et données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 412, "src_audio": "/acl6060/audio/en/412.wav", "src_ref": "And it keeps the head of ah of the model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il garde la tête du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 413, "src_audio": "/acl6060/audio/en/413.wav", "src_ref": "But it adds reformulation phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cela ajoute une phase de reformulation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 414, "src_audio": "/acl6060/audio/en/414.wav", "src_ref": "It augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il augmente l'ensemble de formation et a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'introduire dans le modèle de langue et l'utiliser dans le problème de classification par paire de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 415, "src_audio": "/acl6060/audio/en/415.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 0, "src_audio": "/acl6060/audio/en/0.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour tout le monde. Aujourd'hui, je vais vous présenter notre travail de recherche « Apprendre à raisonner par déduction » : résolution de problèmes de mots mathématiques comme extraction de relation complexe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 1, "src_audio": "/acl6060/audio/en/1.wav", "src_ref": "I'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'appelle Allan du laboratoire d'intelligence artificielle ByteDance, et ceci est un travail en commun avec Jierui Li de l'Université du Texas à Austin et Wei Lu du SUTD.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 2, "src_audio": "/acl6060/audio/en/2.wav", "src_ref": "First, I'd like to talk about our motivation for reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, j'aimerais parler de notre motivation pour le raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 3, "src_audio": "/acl6060/audio/en/3.wav", "src_ref": "So here we show an examples where multi-step reasoning is helpful.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous montrons un exemple où le raisonnement en plusieurs étapes est utile.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 4, "src_audio": "/acl6060/audio/en/4.wav", "src_ref": "So this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce chiffre est donc tiré de l'article PaLM où ils incitent à résoudre le problème de réseau dans le scénario d'apprentissage de quelques prises de vue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 5, "src_audio": "/acl6060/audio/en/5.wav", "src_ref": "So on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, sur le côté gauche, nous pouvons voir que si nous fournissons quelques exemples avec juste des questions et des réponses, nous pourrions ne pas être en mesure d'obtenir les bonnes réponses.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 6, "src_audio": "/acl6060/audio/en/6.wav", "src_ref": "But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si nous fournissons plus de description du raisonnement, le modèle est capable de prévenir la description du raisonnement, mais aussi d'effectuer ici une prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 7, "src_audio": "/acl6060/audio/en/7.wav", "src_ref": "So it is good to have interpretable multi-step reasoning as output.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est alors bon d'avoir comme résultat un raisonnement à plusieurs étapes interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 8, "src_audio": "/acl6060/audio/en/8.wav", "src_ref": "And we also think math word problem is a straightforward application to evaluate such reasoning abilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pensons également que les problèmes de mots mathématiques sont une application directe pour évaluer de telles capacités de raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 9, "src_audio": "/acl6060/audio/en/9.wav", "src_ref": "So, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 10, "src_audio": "/acl6060/audio/en/10.wav", "src_ref": "So in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans nos données, nous recevons également l‘expression mathématique qui conduit à cette réponse particulière.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 11, "src_audio": "/acl6060/audio/en/11.wav", "src_ref": "So, certain assumptions ah also apply as in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, certaines hypothèses s'appliquent également comme dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 12, "src_audio": "/acl6060/audio/en/12.wav", "src_ref": "We assume the precision of quantities are known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que la précision des quantités est connue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 13, "src_audio": "/acl6060/audio/en/13.wav", "src_ref": "And we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 14, "src_audio": "/acl6060/audio/en/14.wav", "src_ref": "Furthermore, complicated operators can be actually decomposed into these basic operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les opérateurs compliqués peuvent effectivement être décomposés en ces opérateurs de base.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 15, "src_audio": "/acl6060/audio/en/15.wav", "src_ref": "So, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, le travail antérieur dans la résolution de problèmes de mots mathématiques peut en effet être classé dans un modèle séquence à séquence et séquence à arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 16, "src_audio": "/acl6060/audio/en/16.wav", "src_ref": "So, traditional sequence to sequence model convert the expression to a specific sequence for generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle séquence à séquence traditionnel convertit donc l'expression en une séquence spécifique pour la génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 17, "src_audio": "/acl6060/audio/en/17.wav", "src_ref": "And it is pretty easy to implement and it can generalize to many different complicated problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est assez facile à mettre en œuvre et peut être généralisé à de nombreux problèmes compliqués différents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 18, "src_audio": "/acl6060/audio/en/18.wav", "src_ref": "But the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle structuré et son manque d'interopérabilité pour la prévention.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 19, "src_audio": "/acl6060/audio/en/19.wav", "src_ref": "But actually this direction is still quite popular because of um the transformer model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, cette direction est encore très populaire en raison du modèle de conversion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 20, "src_audio": "/acl6060/audio/en/20.wav", "src_ref": "So, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans les modèles à base d'arbre, nous structurons en réalité ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 21, "src_audio": "/acl6060/audio/en/21.wav", "src_ref": "So here we keep generating the operators until we reach the leaves, which are the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous continuons à produire les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 22, "src_audio": "/acl6060/audio/en/22.wav", "src_ref": "So here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, la bonne chose est qu'en fait, cela nous donne cette structure d'arbre binaire. Mais en réalité, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur et ensuite, à la fin, nous générons les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 23, "src_audio": "/acl6060/audio/en/23.wav", "src_ref": "And the second thing is that it also contains some repetitive computations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que cela contient également des calculs répétitifs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 24, "src_audio": "/acl6060/audio/en/24.wav", "src_ref": "So here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, si nous regardons cette expression, huit fois trois plus trois est en fait généré deux fois, mais en réalité, nous devrions réutiliser les résultats.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 25, "src_audio": "/acl6060/audio/en/25.wav", "src_ref": "So, in our proposed approach we want to solve those problems in a step by step and interpretable manners.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 26, "src_audio": "/acl6060/audio/en/26.wav", "src_ref": "So for example, here in the second step, ah we can obtain these divisors which is twenty seven.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, ici dans la deuxième étape, nous pouvons obtenir ces diviseurs qui sont vingt-sept.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 27, "src_audio": "/acl6060/audio/en/27.wav", "src_ref": "And we can also refer back to the original questions to find the relevant contents.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 28, "src_audio": "/acl6060/audio/en/28.wav", "src_ref": "And in these steps we obtain the divisors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et dans ces étapes, nous obtenons les diviseurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 29, "src_audio": "/acl6060/audio/en/29.wav", "src_ref": "So, ah and then at this third step we actually get the quotient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis, à cette troisième étape, nous obtenons alors le quotient.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 30, "src_audio": "/acl6060/audio/en/30.wav", "src_ref": "Alright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Et après ces trois étapes, nous pouvons alors réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, et enfin, nous pouvons obtenir les dividendes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 31, "src_audio": "/acl6060/audio/en/31.wav", "src_ref": "So, here we actually generate the whole expression directly rather than generating a single operators or quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous générons en réalité l'expression entière directement plutôt que de produire un seul opérateur ou une seule quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 32, "src_audio": "/acl6060/audio/en/32.wav", "src_ref": "So this makes the process more accurate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela rend le processus plus précis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 33, "src_audio": "/acl6060/audio/en/33.wav", "src_ref": "So, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et incluant également une certaine constante comme notre état initial.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 34, "src_audio": "/acl6060/audio/en/34.wav", "src_ref": "So, the expression is represented by e i j o p.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, l'expression est représentée par e i j o p.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 35, "src_audio": "/acl6060/audio/en/35.wav", "src_ref": "Where we perform operator from q_i to q_j, and such expression is actually directed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Où nous effectuons l'opérateur de q_i à q_j, et cette expression est en fait dirigée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 36, "src_audio": "/acl6060/audio/en/36.wav", "src_ref": "So, we also have subtraction with words here to represent the opposite direction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, nous avons également ici la soustraction avec des mots pour représenter la direction opposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 37, "src_audio": "/acl6060/audio/en/37.wav", "src_ref": "This is quite similar to relation extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est tout à fait similaire à l'extraction de relation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 38, "src_audio": "/acl6060/audio/en/38.wav", "src_ref": "So in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans un système déductif formel, à un pas de temps t, on applique l'opérateur entre la paire q_i et q_j, puis on obtient cette nouvelle expression.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 39, "src_audio": "/acl6060/audio/en/39.wav", "src_ref": "We add it to the next state to become a new quantity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous l'ajoutons à l'état suivant pour devenir une nouvelle quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 40, "src_audio": "/acl6060/audio/en/40.wav", "src_ref": "So, these slides actually visualize the evolution of the state where we keep adding expression to the current state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, ces diapositives visualisent en réalité l'évolution de l'état où nous continuons à ajouter de l'expression à l'état actuel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 41, "src_audio": "/acl6060/audio/en/41.wav", "src_ref": "So in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langue préformée qui peut être BERT ou Roberta, puis nous encodons la phrase et obtenons ensuite ces représentations de quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 42, "src_audio": "/acl6060/audio/en/42.wav", "src_ref": "So, once we get the quantity representations, we can start to do inference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire l'inférence.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 43, "src_audio": "/acl6060/audio/en/43.wav", "src_ref": "Here we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous montrons ici un exemple de q_1 pour obtenir la représentation pour q_2 divisée par q_2 multipliée par q_3.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 44, "src_audio": "/acl6060/audio/en/44.wav", "src_ref": "First we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous obtenons la représentation de paire, qui n'est essentiellement que l'enchaînement entre q_1 et q_2, puis nous appliquons un réseau prédictif qui est paramétré par l'opérateur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 45, "src_audio": "/acl6060/audio/en/45.wav", "src_ref": "And then finally we obtain the expression representation q_1 divided by q_2.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et enfin, nous obtenons la représentation de l'expression q_1 divisée par q_2.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 46, "src_audio": "/acl6060/audio/en/46.wav", "src_ref": "But in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, dans la pratique, au stade de l'inférence, nous pourrions également obtenir une expression incorrecte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 47, "src_audio": "/acl6060/audio/en/47.wav", "src_ref": "So, here all the possible expression is equals to three times the number of operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, toute l'expression possible est égale à trois fois le nombre d'opérateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 48, "src_audio": "/acl6060/audio/en/48.wav", "src_ref": "So the nice thing here is that we can easily add constraints to control this search this search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors, ce qui est bien ici, c'est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 49, "src_audio": "/acl6060/audio/en/49.wav", "src_ref": "For example, if this expression is not allowed, we can simply remove this expression in our search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 50, "src_audio": "/acl6060/audio/en/50.wav", "src_ref": "So in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans la deuxième étape, nous faisons la même chose, mais la seule différence est que nous avons une quantité de plus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 51, "src_audio": "/acl6060/audio/en/51.wav", "src_ref": "So this quantity come from the previous calculated expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette quantité provient donc de l'expression calculée antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 52, "src_audio": "/acl6060/audio/en/52.wav", "src_ref": "So finally we can obtain this final expression q_3 times q_4.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous pouvons obtenir cette expression finale q_3 multipliée par q_4.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 53, "src_audio": "/acl6060/audio/en/53.wav", "src_ref": "And we can also see the number of all the possible ah expression is different from the previous step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 54, "src_audio": "/acl6060/audio/en/54.wav", "src_ref": "So, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une telle différence rend difficile l'application de la beam search car la distribution de probabilité entre ces deux étapes est déséquilibrée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 55, "src_audio": "/acl6060/audio/en/55.wav", "src_ref": "So the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La procédure de formation est donc similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 56, "src_audio": "/acl6060/audio/en/56.wav", "src_ref": "And here we also use this tau to represent when we should terminate this generation process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous utilisons également ce tau pour représenter le moment où nous devrions mettre fin à ce processus de génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 57, "src_audio": "/acl6060/audio/en/57.wav", "src_ref": "And here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, l'espace est différent de séquence à séquence car il est différent à chaque pas de temps, alors que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 58, "src_audio": "/acl6060/audio/en/58.wav", "src_ref": "And it also allows us to impose certain constraints from prior from prior knowledge.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 59, "src_audio": "/acl6060/audio/en/59.wav", "src_ref": "So we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous menons donc des expériences sur les données de problèmes de mots mathématiques, MAWPS, Math23K, MathQA et SVAMP couramment utilisées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 60, "src_audio": "/acl6060/audio/en/60.wav", "src_ref": "And here we briefly show the results compared with the previous best approaches.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons brièvement les résultats comparés aux meilleures approches antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 61, "src_audio": "/acl6060/audio/en/61.wav", "src_ref": "So our best performing variant is Roberta-DeductiveReasoner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, notre variante la plus performante est Roberta-DeuctiveReasoner.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 62, "src_audio": "/acl6060/audio/en/62.wav", "src_ref": "And in fact we do not use beam search, in contrast all previous approaches are using beam search.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en réalité, nous n'utilisons pas la beam search ; au contraire, toutes les approches antérieures utilisent la beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 63, "src_audio": "/acl6060/audio/en/63.wav", "src_ref": "All right. So, the best approaches are often tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Ainsi, les meilleures approches sont souvent un modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 64, "src_audio": "/acl6060/audio/en/64.wav", "src_ref": "So, overall our reasoner is able to significantl significantly outperform this tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans l'ensemble, notre raisonneur est capable de dépasser significativement ce modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 65, "src_audio": "/acl6060/audio/en/65.wav", "src_ref": "But we can see the absolute numbers on MathQA or SVAMP are not really high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons voir que les nombres absolus sur MathQA ou SVAMP ne sont pas vraiment élevés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 66, "src_audio": "/acl6060/audio/en/66.wav", "src_ref": "So we further investigate the results on SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous étudions donc plus en détail les résultats sur SVAMP.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 67, "src_audio": "/acl6060/audio/en/67.wav", "src_ref": "And this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ces données sont difficiles parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle TAL traitement automatique du langage naturel comme ajouter des informations non pertinentes et des quantités supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 68, "src_audio": "/acl6060/audio/en/68.wav", "src_ref": "So, in our prediction we find some of the intermediate values are actually negatives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans notre prévention, nous trouvons que certaines des valeurs intermédiaires sont en réalité négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 69, "src_audio": "/acl6060/audio/en/69.wav", "src_ref": "For example, um, in these questions we are asking how many apples does Jake have?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans ces questions, nous demandons « combien de pommes a Jake ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 70, "src_audio": "/acl6060/audio/en/70.wav", "src_ref": "But we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous avons des informations supplémentaires comme « dix-sept photos de moins », et « Steven en a huit », ce qui est totalement hors de propos.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 71, "src_audio": "/acl6060/audio/en/71.wav", "src_ref": "So, our model makes some prediction like this which is producing negative values.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, notre modèle effectue une prévention comme celle-ci qui produit des valeurs négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 72, "src_audio": "/acl6060/audio/en/72.wav", "src_ref": "And we observe these two expressions actually have similar scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous constatons que ces deux expressions ont en réalité des scores similaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 73, "src_audio": "/acl6060/audio/en/73.wav", "src_ref": "So, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pouvons alors limiter cet espace de recherche en supprimant les résultats négatifs afin que nous puissions rendre la réponse correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 74, "src_audio": "/acl6060/audio/en/74.wav", "src_ref": "So um we further find such constraint actually improves quite a lot for some models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous trouvons donc que cette contrainte améliore en réalité beaucoup pour certains modèles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 75, "src_audio": "/acl6060/audio/en/75.wav", "src_ref": "For example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous améliorons sept points, puis pour le modèle de base Roberta, nous avons amélioré deux points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 76, "src_audio": "/acl6060/audio/en/76.wav", "src_ref": "So better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le meilleur modèle de langue a donc de meilleures capacités de compréhension de la langue de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 77, "src_audio": "/acl6060/audio/en/77.wav", "src_ref": "And we also try to analyze the difficulty behind these behind all these datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous essayons également d'analyser la difficulté derrière toutes ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 78, "src_audio": "/acl6060/audio/en/78.wav", "src_ref": "We assume the number of unused quantities can be regarded as irrelevant information here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que le nombre de quantités inutilisées peut être considéré ici comme une information non pertinente.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 79, "src_audio": "/acl6060/audio/en/79.wav", "src_ref": "So ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et les données SVAMP ont la plus grande partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 80, "src_audio": "/acl6060/audio/en/80.wav", "src_ref": "And here we also show the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons également la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 81, "src_audio": "/acl6060/audio/en/81.wav", "src_ref": "For those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ces échantillons sans quantités inutilisées, la performance est en réalité supérieure à la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 82, "src_audio": "/acl6060/audio/en/82.wav", "src_ref": "But with those samples that with unused quantity is actually way worse than the, worse than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais avec ces échantillons ayant une quantité inutilisée, c'est en fait bien pire que la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 83, "src_audio": "/acl6060/audio/en/83.wav", "src_ref": "For MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour MAWPS, nous n'avons pas vraiment trop de cas de test, alors j'ignore simplement cette partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 84, "src_audio": "/acl6060/audio/en/84.wav", "src_ref": "So, finally we want to show the interpretability through a question perturbation example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous voulons montrer l'interopérabilité à travers un exemple de perturbation de question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 85, "src_audio": "/acl6060/audio/en/85.wav", "src_ref": "So here our model actually makes a wrong prediction at the first step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, notre modèle effectue en réalité une prévention erronée à la première étape.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 86, "src_audio": "/acl6060/audio/en/86.wav", "src_ref": "So, we can actually correlate this expression with the sentence here. Alright.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, nous pouvons effectivement corréler cette expression avec la phrase ici. Très bien.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 87, "src_audio": "/acl6060/audio/en/87.wav", "src_ref": "So, we think this sentence might be misleading the model to an incorrect predictions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pensons donc que cette phrase pourrait induire le modèle en erreur avec des préventions incorrectes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 88, "src_audio": "/acl6060/audio/en/88.wav", "src_ref": "So here planting another thirty five makes the model makes the model think it should be an addition operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, en planter trente-cinq autres fait penser au modèle qu'il devrait être un opérateur d'addition.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 89, "src_audio": "/acl6060/audio/en/89.wav", "src_ref": "So we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous essayons alors de réviser la phrase pour que cela soit quelque chose comme « le nombre de poiriers est inférieur de trente-cinq à celui des pommiers ».", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 90, "src_audio": "/acl6060/audio/en/90.wav", "src_ref": "So, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous faisons en sorte de transmettre une sémantique plus précise afin que le modèle soit capable de rendre la prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 91, "src_audio": "/acl6060/audio/en/91.wav", "src_ref": "So, this study shows how the interpretable predictions help us understand the model behavior.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, cette étude montre comment les préventions interprétables nous aident à comprendre le comportement du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 92, "src_audio": "/acl6060/audio/en/92.wav", "src_ref": "So to conclude our work, so first our model is actually pretty efficient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour conclure notre travail, notre modèle est en réalité assez efficace.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 93, "src_audio": "/acl6060/audio/en/93.wav", "src_ref": "And we are able to provide interpretable solving procedure.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous sommes en mesure de fournir une procédure de résolution interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 94, "src_audio": "/acl6060/audio/en/94.wav", "src_ref": "And we can easily incorporate some prior knowledge as constraint which can help improve the performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons facilement incorporer des connaissances antérieures en tant que contraintes qui peuvent aider à améliorer la performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 95, "src_audio": "/acl6060/audio/en/95.wav", "src_ref": "And the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution des problèmes de réseau, mais aussi aux autres tâches qui impliquent un raisonnement à plusieurs étapes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 96, "src_audio": "/acl6060/audio/en/96.wav", "src_ref": "We also have certain limitations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également certaines limites.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 97, "src_audio": "/acl6060/audio/en/97.wav", "src_ref": "Ah, if we have a large number of operators or constants, the memory consumption could be pretty high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 98, "src_audio": "/acl6060/audio/en/98.wav", "src_ref": "And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que, comme mentionné, puisque la distribution de probabilité est déséquilibrée entre les différentes étapes de temps, il est donc également assez difficile d'appliquer la stratégie de beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 99, "src_audio": "/acl6060/audio/en/99.wav", "src_ref": "So this is the end of the talk, and questions are welcomed. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous arrivons donc à la fin de la discussion, et vos questions sont les bienvenues. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 100, "src_audio": "/acl6060/audio/en/100.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 101, "src_audio": "/acl6060/audio/en/101.wav", "src_ref": "I will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter mon travail en commun avec Jerry, qui porte sur de nouvelles données pour l'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 102, "src_audio": "/acl6060/audio/en/102.wav", "src_ref": "Legal issues are an integral part of many people's lives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions juridiques font partie intégrante de la vie de nombreuses personnes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 103, "src_audio": "/acl6060/audio/en/103.wav", "src_ref": "But the majority of citizens have little to know knowledge about their rights and fundamental legal processes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais la majorité des citoyens ont peu de connaissances sur leurs droits et leurs processus juridiques fondamentaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 104, "src_audio": "/acl6060/audio/en/104.wav", "src_ref": "As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En conséquence, de nombreux citoyens vulnérables qui n'ont pas les moyens de se payer l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 105, "src_audio": "/acl6060/audio/en/105.wav", "src_ref": "All work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tous les travaux visent à combler le fossé entre les personnes et la loi en développant un système d'extraction efficace pour les articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 106, "src_audio": "/acl6060/audio/en/106.wav", "src_ref": "Such a system could provide a free professional legal help service for unskilled humans.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un tel système pourrait fournir un service d'aide juridique professionnel gratuit aux personnes non qualifiées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 107, "src_audio": "/acl6060/audio/en/107.wav", "src_ref": "Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avant de plonger dans la principale contribution de ce travail, nous allons d'abord décrire le problème d'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 108, "src_audio": "/acl6060/audio/en/108.wav", "src_ref": "Given a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Compte tenu d'une simple question sur une question juridique telle que : « qu'est-ce que je risque si je viole la confidentialité professionnelle ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 109, "src_audio": "/acl6060/audio/en/109.wav", "src_ref": "A model is required to retrieve all relevant statutory articles from a large body of legislation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un modèle est nécessaire pour extraire tous les articles de droit pertinents d'un grand corpus législatif.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 110, "src_audio": "/acl6060/audio/en/110.wav", "src_ref": "This information retrieval task comes with its own set of challenges.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette tâche d'extraction des informations comporte son propre ensemble de défis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 111, "src_audio": "/acl6060/audio/en/111.wav", "src_ref": "First, it deals with two types of language.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, elle traite de deux types de langue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 112, "src_audio": "/acl6060/audio/en/112.wav", "src_ref": "Common natural language for the questions and complex legal language for the statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La langue naturelle commune pour les questions et la langue juridique complexe pour les lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 113, "src_audio": "/acl6060/audio/en/113.wav", "src_ref": "This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette différence dans les distributions de langue rend plus difficile pour un système d'extraire les candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 114, "src_audio": "/acl6060/audio/en/114.wav", "src_ref": "Besides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le droit écrit n'est pas une pile d'articles indépendants qui peuvent être traités comme une source complète d'informations par eux-mêmes, contrairement aux nouvelles ou aux recettes, par exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 115, "src_audio": "/acl6060/audio/en/115.wav", "src_ref": "Instead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, il s'agit d'un ensemble structuré de dispositions juridiques qui n'ont un sens entier que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les domaines et sous-domaines auxquels elles appartiennent et leur place dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 116, "src_audio": "/acl6060/audio/en/116.wav", "src_ref": "Lastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, les articles de droit ne sont pas de petits paragraphes constituant généralement l'unité d'extraction typique dans la plupart des travaux d'extraction.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 117, "src_audio": "/acl6060/audio/en/117.wav", "src_ref": "Here, there are long documents that may be up to six thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ici, il y a de longs documents qui peuvent aller jusqu'à six mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 118, "src_audio": "/acl6060/audio/en/118.wav", "src_ref": "The recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les recent advances en matière de TAL traitement automatique du langage naturel ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prévention du jugement juridique ou l'examen des contrats de contacts automatisés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 119, "src_audio": "/acl6060/audio/en/119.wav", "src_ref": "But statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais l'extraction de l'article de droit est restée essentiellement inchangée en raison du manque de grandes données étiquetées de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 120, "src_audio": "/acl6060/audio/en/120.wav", "src_ref": "In this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans ce travail, nous présentons de nouvelles données centrées sur le citoyen natif français pour étudier si l'extraction des modèles peut se rapprocher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche d'extraction de l'article de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 121, "src_audio": "/acl6060/audio/en/121.wav", "src_ref": "Our Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos données d'extraction de l'article de droit belge BSARD se composent de plus de mille cent questions juridiques posées par des citoyens belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 122, "src_audio": "/acl6060/audio/en/122.wav", "src_ref": "These questions cover a wide range of topics from family, housing, money, to work and social security.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces questions couvrent un wide range de sujets allant de la famille au logement, à l'argent, au travail et à la sécurité sociale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 123, "src_audio": "/acl6060/audio/en/123.wav", "src_ref": "Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques de codes de droit belge.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 124, "src_audio": "/acl6060/audio/en/124.wav", "src_ref": "Let's now talk about how we collected this dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Parlons maintenant de la façon dont nous avons collecté ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 125, "src_audio": "/acl6060/audio/en/125.wav", "src_ref": "First, we started by compiling a large corpus of legal articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 126, "src_audio": "/acl6060/audio/en/126.wav", "src_ref": "We considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons examiné trente-deux codes belges accessibles au public et extrait tous les articles ainsi que les titres de section correspondants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 127, "src_audio": "/acl6060/audio/en/127.wav", "src_ref": "Then we gathered legal questions with references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous avons rassemblé les questions juridiques avec les références aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 128, "src_audio": "/acl6060/audio/en/128.wav", "src_ref": "To do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ce faire, nous nous associons au cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur une question juridique personnelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 129, "src_audio": "/acl6060/audio/en/129.wav", "src_ref": "We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons eu la chance d'avoir accès à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 130, "src_audio": "/acl6060/audio/en/130.wav", "src_ref": "We collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons recueilli des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 131, "src_audio": "/acl6060/audio/en/131.wav", "src_ref": "Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous avons fait passer les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avons considérés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 132, "src_audio": "/acl6060/audio/en/132.wav", "src_ref": "The remaining references were matched and converted to the corresponding article ids from our corpus.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les références restantes ont été appariées et converties aux identifiants d'article correspondants de notre corpus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 133, "src_audio": "/acl6060/audio/en/133.wav", "src_ref": "We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous nous sommes finalement retrouvés avec mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 134, "src_audio": "/acl6060/audio/en/134.wav", "src_ref": "In addition, each question comes with the main category and a concatenation of subcategories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, chaque question est accompagnée de la catégorie principale et d'un enchaînement de sous-catégories.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 135, "src_audio": "/acl6060/audio/en/135.wav", "src_ref": "And each articles comes with a concatenation of the subsequence heading in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et chaque article comporte un enchaînement de la rubrique de sous-séquence dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 136, "src_audio": "/acl6060/audio/en/136.wav", "src_ref": "This extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour des recherches futures sur l'extraction d'informations juridiques ou la classification de textes juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 137, "src_audio": "/acl6060/audio/en/137.wav", "src_ref": "Let's look at some characteristic of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jetons un coup d'œil à certaines caractéristiques de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 138, "src_audio": "/acl6060/audio/en/138.wav", "src_ref": "The questions are between five and forty four words long with a median of fourteen words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions comportent entre cinq et quarante-quatre mots avec une moyenne de quatorze mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 139, "src_audio": "/acl6060/audio/en/139.wav", "src_ref": "The articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les articles sont beaucoup plus longs avec une longueur moyenne de soixante-dix-sept mots, avec cent quarante-deux d'entre eux dépassant les mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 140, "src_audio": "/acl6060/audio/en/140.wav", "src_ref": "The lengthiest one being up to five thousand seven hundred and ninety words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 141, "src_audio": "/acl6060/audio/en/141.wav", "src_ref": "As previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme mentionné précédemment, les questions couvrent un wide range de sujets, dont environ quatre-vingt-cinq pour cent concernent la famille, le logement, l'argent ou la justice.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 142, "src_audio": "/acl6060/audio/en/142.wav", "src_ref": "While the remaining fifteen percent concern either social security, foreigners or work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 143, "src_audio": "/acl6060/audio/en/143.wav", "src_ref": "The article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'article est également très diversifié car il provient de trente-deux codes belges différents qui couvrent un grand nombre de sujets juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 144, "src_audio": "/acl6060/audio/en/144.wav", "src_ref": "Here's the total number of articles collected from each of these Belgian codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le nombre total d'articles collectés à partir de chacun de ces codes belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 145, "src_audio": "/acl6060/audio/en/145.wav", "src_ref": "Out of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sur les vingt-deux mille six cent trente-trois articles, seuls mille six cent douze sont désignés comme pertinents à au moins une question dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 146, "src_audio": "/acl6060/audio/en/146.wav", "src_ref": "And around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et environ quatre-vingts pour cent de ces articles cités proviennent du code civil, des codes judiciaires, des codes d'enquête criminelle ou des codes pénaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 147, "src_audio": "/acl6060/audio/en/147.wav", "src_ref": "Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 148, "src_audio": "/acl6060/audio/en/148.wav", "src_ref": "Which can be explained by the fact that those codes focused less on individuals and their concerns.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce qui peut s'expliquer par le fait que ces codes se concentraient moins sur les individus et leurs préoccupations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 149, "src_audio": "/acl6060/audio/en/149.wav", "src_ref": "Overall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le nombre moyen de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 150, "src_audio": "/acl6060/audio/en/150.wav", "src_ref": "Using all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En utilisant toutes les données, nous avons comparé plusieurs approches d'extraction, y compris l'architecture lexicale et dense.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 151, "src_audio": "/acl6060/audio/en/151.wav", "src_ref": "Given a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné une requête et un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de requête des poids de chacun de ces termes dans cet article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 152, "src_audio": "/acl6060/audio/en/152.wav", "src_ref": "We experiment with the standard TF-IDF and BM25 ranking functions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec les fonctions de classement TF-IDF et BM25 standard.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 153, "src_audio": "/acl6060/audio/en/153.wav", "src_ref": "The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le principal problème de ces approches est qu'elles ne peuvent extraire que les articles contenant des mots-clés présents dans la requête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 154, "src_audio": "/acl6060/audio/en/154.wav", "src_ref": "To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour surmonter cette limitation, nous expérimentons une architecture neuronale qui peut capturer les relations sémantiques entre les requêtes et l'article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 155, "src_audio": "/acl6060/audio/en/155.wav", "src_ref": "We use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons un modèle bi-encodeur qui cartographie les requêtes et les articles en représentations vecteurs denses et calculons un score de pertinence entre une paire d'articles de requête par la similitude de leurs intégrations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 156, "src_audio": "/acl6060/audio/en/156.wav", "src_ref": "These embeddings typically result from a pooling operation on the output of a word embedding model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces intégrations résultent typiquement d'une opération de pooling sur la sortie d'un modèle d'intégration de mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 157, "src_audio": "/acl6060/audio/en/157.wav", "src_ref": "First, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous étudions l'efficacité des bi-encodeurs siamois dans une configuration d'évaluation « zero shot », ce qui signifie que des modèles d'intégration de mots préformés sont appliqués immédiatement sans aucun raffinement supplémentaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 158, "src_audio": "/acl6060/audio/en/158.wav", "src_ref": "We experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec l'encodeur de texte indépendant du contexte, à savoir word2vec et fastText, et des modèles d'intégration dépendants du contexte, à savoir Roberta et plus précisément CamemBERT qui est un modèle Roberta français.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 159, "src_audio": "/acl6060/audio/en/159.wav", "src_ref": "Additionally, we train our own CamemBERT based model ah bi-encoders on our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, nous formons nos propres bi-encodeurs de modèles basés sur CamemBERT sur nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 160, "src_audio": "/acl6060/audio/en/160.wav", "src_ref": "Note that for training, we experiment with the two flavors of the bi-encoder architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que pour la formation, nous expérimentons avec les deux modèles de l'architecture bi-encodeur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 161, "src_audio": "/acl6060/audio/en/161.wav", "src_ref": "Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le siamois, qui utilise un modèle d'intégration de mots unique cartographiant la requête et l'article ensemble dans un vector space dense partagé, et le modèle à deux tours, qui utilise deux modèles d'intégration de mots indépendants encodant la requête et l'article séparément dans différents espaces d'intégration.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 162, "src_audio": "/acl6060/audio/en/162.wav", "src_ref": "We experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons la mise en commun moyenne, maximale et CLS, ainsi que le produit et cosinus pour calculer les similitudes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 163, "src_audio": "/acl6060/audio/en/163.wav", "src_ref": "Here are the result of our baseline on the test sets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le résultat de notre base sur les ensembles de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 164, "src_audio": "/acl6060/audio/en/164.wav", "src_ref": "With the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec les méthodes lexicales ci-dessus, les bi-encodeurs siamois ont évalué dans une configuration zero shot au milieu, et les bi-encodeurs raffinés ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 165, "src_audio": "/acl6060/audio/en/165.wav", "src_ref": "Overall, the finetuned bi-encoder significantly outperforms all the other baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le bi-encodeur raffiné dépasse de manière significative toutes les autres bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 166, "src_audio": "/acl6060/audio/en/166.wav", "src_ref": "The two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle à deux tours s'améliore par rapport à ses variantes siamoises lors du rappel à cent, mais fonctionne de la même manière sur les autres indicateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 167, "src_audio": "/acl6060/audio/en/167.wav", "src_ref": "Although BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que BM25 ait sous-performé le bi-encodeur formé de manière significative, ses performances indiquent qu'il s'agit toujours d'une base solide pour l'extraction spécifique au domaine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 168, "src_audio": "/acl6060/audio/en/168.wav", "src_ref": "Regarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation zero shot du bi-encodeur siamois, nous constatons que l'utilisation directe des intégrations d'un modèle CamemBERT préformé sans optimiser pour la tâche d'extraction d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 169, "src_audio": "/acl6060/audio/en/169.wav", "src_ref": "Furthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous observons que le bi-encodeur basé sur word2vec a dépassé de manière significative les modèles basés sur fastText et les Représentations d'encodeurs bidirectionnels à partir de transformateurs, suggérant que peut-être les intégrations de niveau mot préformées sont plus appropriées pour la tâche que les intégrations de niveau caractère ou sous-mot lorsqu'elles sont utilisées immédiatement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 170, "src_audio": "/acl6060/audio/en/170.wav", "src_ref": "Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que prometteurs, ces résultats suggèrent de nombreuses possibilités d'amélioration comparé à un expert juridique qualifié qui peut éventuellement extraire tous les articles pertinents à n'importe quelle question, et ainsi obtenir des scores parfaits.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 171, "src_audio": "/acl6060/audio/en/171.wav", "src_ref": "Let's conclude by discussing two limitations of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Concluons en discutant de deux limites de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 172, "src_audio": "/acl6060/audio/en/172.wav", "src_ref": "First, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Premièrement, le corpus d'article est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 173, "src_audio": "/acl6060/audio/en/173.wav", "src_ref": "During the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au cours de la construction des données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions ne se retrouvent qu'avec une fraction du nombre initial d'articles pertinents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 174, "src_audio": "/acl6060/audio/en/174.wav", "src_ref": "This information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette information implique donc que la réponse contenue dans les autres articles pertinents pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 175, "src_audio": "/acl6060/audio/en/175.wav", "src_ref": "Second, we should note that not all legal questions can be answered with statutes alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, il convient de noter qu'on ne peut pas répondre à toutes les questions juridiques uniquement par des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 176, "src_audio": "/acl6060/audio/en/176.wav", "src_ref": "For instance, the question, can I evict my tenants if they make too much noise?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, la question « puis-je expulser mes locataires s'ils font trop de bruit ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 177, "src_audio": "/acl6060/audio/en/177.wav", "src_ref": "Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle peut ne pas avoir de réponse détaillée dans le droit écrit qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 178, "src_audio": "/acl6060/audio/en/178.wav", "src_ref": "Instead, the landlord should probably rely more on case law and find precedents similar to their current situation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des antécédents similaires à sa situation actuelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 179, "src_audio": "/acl6060/audio/en/179.wav", "src_ref": "For example, the tenants makes two parties a week until two AM.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, les locataires font la fête deux fois par semaine jusqu'à deux heures du matin.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 180, "src_audio": "/acl6060/audio/en/180.wav", "src_ref": "Hence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche d'extraction de l'article de droit, et le domaine des moins adaptées reste à déterminer.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 181, "src_audio": "/acl6060/audio/en/181.wav", "src_ref": "We hope that our work sparks interest in developing practical and reliable statutory article retrieval models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous espérons que nos travaux susciteront l'intérêt pour l'élaboration de modèles d'extraction d'article de droit pratiques et fiables.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 182, "src_audio": "/acl6060/audio/en/182.wav", "src_ref": "That can help improve access to justice for all.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut aider à améliorer l'accès à la justice pour tous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 183, "src_audio": "/acl6060/audio/en/183.wav", "src_ref": "You can check out our paper, dataset and code at the following links. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez consulter notre article, nos données et notre code aux liens suivants. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 184, "src_audio": "/acl6060/audio/en/184.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE ; un indice de référence indépendant de la tâche destiné à tester les modèles de langue et de vision avec des phénomènes linguistiques spécifiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 185, "src_audio": "/acl6060/audio/en/185.wav", "src_ref": "Why did we do the trouble in setting up this benchmark?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pourquoi avons-nous pris la peine de mettre en place cet indice de référence ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 186, "src_audio": "/acl6060/audio/en/186.wav", "src_ref": "Well, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eh bien, au cours des dernières années, nous avons vu une explosion de la vision basée sur le transformateur et des modèles de langue préformés sur de grandes quantités de paires de textes d'images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 187, "src_audio": "/acl6060/audio/en/187.wav", "src_ref": "Each one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacun de ces modèles pousse les tâches de pointe en matière de vision et de langue telles que la réponse aux questions visuelles, le raisonnement de bon sens visuel, l'extraction d'images et les bases de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 188, "src_audio": "/acl6060/audio/en/188.wav", "src_ref": "So we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc reçu un message : les précisions sur ces tâches et les indices de référence spécifiques augmentent régulièrement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 189, "src_audio": "/acl6060/audio/en/189.wav", "src_ref": "But do we know what the models have actually learned?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais savons-nous ce que les modèles ont réellement appris ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 190, "src_audio": "/acl6060/audio/en/190.wav", "src_ref": "What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Qu'est-ce qu'une vision et un transformateur de langue ont compris lors de l'attribution d'un score élevé pour que cette image et cette phrase correspondent ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 191, "src_audio": "/acl6060/audio/en/191.wav", "src_ref": "And the low score for this one?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le score bas pour celle-ci ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 192, "src_audio": "/acl6060/audio/en/192.wav", "src_ref": "Do vision and language models focus on the right thing?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Est-ce que les modèles de langue et de vision se concentrent sur la bonne chose ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 193, "src_audio": "/acl6060/audio/en/193.wav", "src_ref": "Or do they focus on biases as shown by previous work?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou se concentrent-ils sur les biais comme le montre le travail antérieur ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 194, "src_audio": "/acl6060/audio/en/194.wav", "src_ref": "To shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour éclairer davantage cet aspect, nous proposons une direction plus indépendante des tâches et introduisons VALSE qui teste la sensibilité des modèles de langue et de vision à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistiques et visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 195, "src_audio": "/acl6060/audio/en/195.wav", "src_ref": "We target existence, plurality, counting, spatial relations, actions and entity coreference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 196, "src_audio": "/acl6060/audio/en/196.wav", "src_ref": "But how do we test whether the vision and language models have captured this phenomena?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais comment tester si les modèles de langue et de vision ont capturé ce phénomène ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 197, "src_audio": "/acl6060/audio/en/197.wav", "src_ref": "By foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En pratiquant le foiling sur une méthode précédemment appliquée pour les modèles de langue et de vision seulement pour des phrases nominales de Ravi Shekhar et de ses collaborateurs, et en comptant par nous dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 198, "src_audio": "/acl6060/audio/en/198.wav", "src_ref": "Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Effectuer un foiling signifie essentiellement que nous prenons la légende d'une image et produisons un foil en modifiant la légende de sorte qu'elle ne décrive plus l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 199, "src_audio": "/acl6060/audio/en/199.wav", "src_ref": "And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous apportons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous aurions trouvé plus d'une façon intéressante de créer des instances de foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 200, "src_audio": "/acl6060/audio/en/200.wav", "src_ref": "For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans le cas de l'élément d'actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié avec une action différente, et un dans lequel les acteurs sont échangés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 201, "src_audio": "/acl6060/audio/en/201.wav", "src_ref": "Counting and coreference also are pieces that have more than one instrument.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le comptage et la coréférence sont également des éléments qui ont plusieurs instruments.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 202, "src_audio": "/acl6060/audio/en/202.wav", "src_ref": "And we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous créons ces foils en nous assurant qu'ils ne décrivent pas l'image et que ce sont des phrases grammaticales et autrement valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 203, "src_audio": "/acl6060/audio/en/203.wav", "src_ref": "This is not easy to do because a foiled caption may be less likely than the original caption.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce n'est pas facile à faire car une légende ayant subi un foil peut être moins probable que la légende originale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 204, "src_audio": "/acl6060/audio/en/204.wav", "src_ref": "For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que les plantes coupent un homme qu'un homme coupe les plantes, et la grande vision et les modèles de langue pourraient capter cela.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 205, "src_audio": "/acl6060/audio/en/205.wav", "src_ref": "Therefore, to obtain valid foils, we must take action.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour obtenir des foils valides, nous devons agir.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 206, "src_audio": "/acl6060/audio/en/206.wav", "src_ref": "First, we make use of strong language models to propose foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous utilisons des modèles de langue forts pour proposer des foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 207, "src_audio": "/acl6060/audio/en/207.wav", "src_ref": "Second, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, nous utilisons l'inférence de la langue naturelle ou la NLI courte pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 208, "src_audio": "/acl6060/audio/en/208.wav", "src_ref": "To test this automatically, we apply natural language inference with the following rationale.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour tester cela automatiquement, nous appliquons l'inférence de la langue naturelle avec la justification suivante.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 209, "src_audio": "/acl6060/audio/en/209.wav", "src_ref": "We consider an image to be the premise and its caption its entailed hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous considérons qu'une image est la prémisse et sa légende son hypothèse implicite.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 210, "src_audio": "/acl6060/audio/en/210.wav", "src_ref": "In addition, we consider the caption to be the premise, and the foil is its hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous considérons la légende comme la prémisse, et le foil est son hypothèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 211, "src_audio": "/acl6060/audio/en/211.wav", "src_ref": "If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si un modèle NLI prévient que le foil est en contradiction ou neutre par rapport à la légende, nous considérons cela comme un indicateur d'un foil valide.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 212, "src_audio": "/acl6060/audio/en/212.wav", "src_ref": "If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si une NLI prévient le foil à engendrer par la légende, cela ne peut pas être un bon foil, car par transitivité, cela donnera une description véridique de l'image et nous filtrons ces foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 213, "src_audio": "/acl6060/audio/en/213.wav", "src_ref": "But this procedure is not perfect, it is just an indicator for valid foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cette procédure n'est pas parfaite, il s'agit simplement d'un indicateur pour les foils valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 214, "src_audio": "/acl6060/audio/en/214.wav", "src_ref": "Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, comme troisième mesure pour produire des foils valides, nous utilisons des annotateurs humains pour valider les données utilisées dans VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 215, "src_audio": "/acl6060/audio/en/215.wav", "src_ref": "So, after filtering and human evaluation, we have as many test instances as described in this table.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrit dans ce tableau.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 216, "src_audio": "/acl6060/audio/en/216.wav", "src_ref": "Note that VALSE does not deliver any training data but only test data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que VALSE ne fournit pas de données de formation mais seulement des données de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 217, "src_audio": "/acl6060/audio/en/217.wav", "src_ref": "Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme il s'agit uniquement d'un indicateur de référence de test zero shot, il est conçu pour tirer parti des capacités existantes des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 218, "src_audio": "/acl6060/audio/en/218.wav", "src_ref": "Finetuning would only enable models to exploit artifacts or statistical biases in the data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement permettrait seulement aux modèles d'exploiter des artefacts ou des biais statistiques dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 219, "src_audio": "/acl6060/audio/en/219.wav", "src_ref": "And we all know that these models like to cheat and take shortcuts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 220, "src_audio": "/acl6060/audio/en/220.wav", "src_ref": "And as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et comme nous l'avons dit, cela nous intéresse d'évaluer les capacités des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 221, "src_audio": "/acl6060/audio/en/221.wav", "src_ref": "We experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec cinq modèles de langue et de vision sur VALSE, à savoir avec CLIP, LXMert, ViLBERT, ViLBERT douze en un, et VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 222, "src_audio": "/acl6060/audio/en/222.wav", "src_ref": "Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deux de nos indicateurs d'évaluation les plus importants sont la précision des modèles dans la classification de paires de phrases images en légendes et foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 223, "src_audio": "/acl6060/audio/en/223.wav", "src_ref": "Perhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Peut-être plus pertinent pour cette vidéo, nous présenterons notre indicateur plus permissif, la précision par paire, qui mesure si le score de sentence alignment d'image est plus élevé pour la bonne paire de texte image que pour sa paire ayant subi un foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 224, "src_audio": "/acl6060/audio/en/224.wav", "src_ref": "For more metrics and results on them, do check out our paper.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour plus d'indicateurs et de résultats, consultez notre article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 225, "src_audio": "/acl6060/audio/en/225.wav", "src_ref": "The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats avec une précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres indicateurs. La meilleure performance zero shot est obtenue par ViLBERT douze en un, suivi de ViLBERT, LXMert, CLIP, et enfin VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 226, "src_audio": "/acl6060/audio/en/226.wav", "src_ref": "It's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est remarquable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par ViLBERT douze en un, en soulignant que les modèles sont capables d'identifier des objets nommés et leur présence dans les images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 227, "src_audio": "/acl6060/audio/en/227.wav", "src_ref": "However, none of the remaining pieces can be reliably solved in our adversarial foiling settings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de foiling antagonistes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 228, "src_audio": "/acl6060/audio/en/228.wav", "src_ref": "We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous voyons à partir des instruments de pluralité et de comptage que les modèles de langue et de vision ont du mal à distinguer les références à des objets uniques par rapport à plusieurs, ou à les compter dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 229, "src_audio": "/acl6060/audio/en/229.wav", "src_ref": "The relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 230, "src_audio": "/acl6060/audio/en/230.wav", "src_ref": "They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ils ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité comme nous le voyons dans l'élément d'actions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 231, "src_audio": "/acl6060/audio/en/231.wav", "src_ref": "From the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À partir de l'élément de coréférence, nous découvrons que relever plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 232, "src_audio": "/acl6060/audio/en/232.wav", "src_ref": "As a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour vérifier la santé mentale, et parce que c'est une expérience intéressante, nous comparons également deux modèles à texte seulement, GPT un et GPT deux, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et ayant subi un foil, sans image ici, et en prévenant l'entrée avec la plus faible perplexité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 233, "src_audio": "/acl6060/audio/en/233.wav", "src_ref": "If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si la perplexité est plus élevée pour le foil, nous considérons cela comme une indication que la légende ayant subi un foil peut souffrir de biais de plausibilité ou d'autres biais linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 234, "src_audio": "/acl6060/audio/en/234.wav", "src_ref": "And it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il est intéressant de voir que dans certains cas, les modèles GPT à texte seulement ont capturé la plausibilité du monde mieux que les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 235, "src_audio": "/acl6060/audio/en/235.wav", "src_ref": "So to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour résumer, VALSE est un indicateur de référence qui utilise l'objectif des constructions linguistiques pour aider la communauté à améliorer les modèles de langue et de vision en testant durement leurs capacités de bases visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 236, "src_audio": "/acl6060/audio/en/236.wav", "src_ref": "Our experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que les modèles de langue et de vision identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais luttent pour ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 237, "src_audio": "/acl6060/audio/en/237.wav", "src_ref": "We would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers les bases de la langue avec des modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 238, "src_audio": "/acl6060/audio/en/238.wav", "src_ref": "And even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et plus encore, VALSE pourrait être utilisé comme une évaluation indirecte des données, car les modèles pourraient être évalués avant et après la formation ou le raffinement pour voir si des données aident les modèles à améliorer l'un des aspects testés par VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 239, "src_audio": "/acl6060/audio/en/239.wav", "src_ref": "If you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 240, "src_audio": "/acl6060/audio/en/240.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Kamezawa de l'Université de Tokyo.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 241, "src_audio": "/acl6060/audio/en/241.wav", "src_ref": "I'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter un article intitulé RNSum : des données à grande échelle pour la génération automatique de note de version via la synthèse des journaux de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 242, "src_audio": "/acl6060/audio/en/242.wav", "src_ref": "I'll be explaining in this order.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous expliquer dans cet ordre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 243, "src_audio": "/acl6060/audio/en/243.wav", "src_ref": "First, I will introduce automatic release note generation that we are working on in this research.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, je vais présenter la génération de note de version automatique sur laquelle nous travaillons dans cette recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 244, "src_audio": "/acl6060/audio/en/244.wav", "src_ref": "A release note is a technical document that summarizes the changes distributed with each release of a software product.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une note de version est un document technique qui résume les modifications distribuées à chaque version d'un produit logiciel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 245, "src_audio": "/acl6060/audio/en/245.wav", "src_ref": "The image shows a release note for version two point six point four of the vuejs library.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'image montre une note de version pour la version deux point six point quatre de la bibliothèque Vuejs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 246, "src_audio": "/acl6060/audio/en/246.wav", "src_ref": "Release notes play an important role in open source development but they're time consuming to prepare manually.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version jouent un rôle important dans le développement open source, mais elles mettent du temps à être préparées manuellement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 247, "src_audio": "/acl6060/audio/en/247.wav", "src_ref": "Therefore, it would be very useful to be able to automatically generate high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 248, "src_audio": "/acl6060/audio/en/248.wav", "src_ref": "I will defer to two previous researches on automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'en remettrai à deux recherches antérieures sur la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 249, "src_audio": "/acl6060/audio/en/249.wav", "src_ref": "The first is a system called ARENA released in twenty fourteen.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première est un système appelé ARENA sorti en deux-mille-quatorze.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 250, "src_audio": "/acl6060/audio/en/250.wav", "src_ref": "It takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il faut une approche basée sur des règles, par exemple, en utilisant l'extracteur de changement pour extraire toutes les différences, les changements de bibliothèque et les changements de document à partir des différences entre les versions, et enfin, en les combinant.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 251, "src_audio": "/acl6060/audio/en/251.wav", "src_ref": "The most notable feature of this system is the issue extractor in the upper right corner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 252, "src_audio": "/acl6060/audio/en/252.wav", "src_ref": "Which must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le système de suivi des problèmes, qui doit être laissé à Jira, ne peut être appliqué qu'aux projets qui utilisent Jira.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 253, "src_audio": "/acl6060/audio/en/253.wav", "src_ref": "In other words, it cannot be used for many projects on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En d'autres mots, il ne peut pas être utilisé pour de nombreux projets sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 254, "src_audio": "/acl6060/audio/en/254.wav", "src_ref": "The second is Glyph, recently announced in twenty twenty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde est Glyphe, récemment annoncée en deux-mille-vingt.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 255, "src_audio": "/acl6060/audio/en/255.wav", "src_ref": "It is available on the internet and can be installed via pip.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle est disponible sur internet et peut être installée via pip.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 256, "src_audio": "/acl6060/audio/en/256.wav", "src_ref": "This system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'une des cinq étiquettes telles que les fonctions ou corrections de bugs pour chaque message de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 257, "src_audio": "/acl6060/audio/en/257.wav", "src_ref": "This image is a sample usage that returns a corrective or bug fixes label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette image est un exemple d'utilisation qui renvoie une étiquette de correction ou de débogage.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 258, "src_audio": "/acl6060/audio/en/258.wav", "src_ref": "Glyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données de formation de Glyphe sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 259, "src_audio": "/acl6060/audio/en/259.wav", "src_ref": "The performance of the text classification model is not high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les performances du modèle de classification de texte ne sont pas élevées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 260, "src_audio": "/acl6060/audio/en/260.wav", "src_ref": "I present two related researches, but their problems are limited applicability and scarce data resources.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je présente deux recherches connexes, mais leurs problèmes sont l'applicabilité limitée et les ressources de données rares.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 261, "src_audio": "/acl6060/audio/en/261.wav", "src_ref": "Our paper solves these two problems and automatically generates high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 262, "src_audio": "/acl6060/audio/en/262.wav", "src_ref": "With a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec un problème d'applicabilité limitée, nous proposons une méthode de synthèse par classe de haute qualité en utilisant uniquement des messages de validation comme saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 263, "src_audio": "/acl6060/audio/en/263.wav", "src_ref": "This proposed method can be used for all English repositories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette méthode proposée peut être utilisée pour tous les référentiels anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 264, "src_audio": "/acl6060/audio/en/264.wav", "src_ref": "For the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour le deuxième problème de ressources de données rares, nous avons construit nos données RNSum composées d'environ quatre-vingt-deux mille éléments de données en collectant des données à partir de référentiels GitHub publics en utilisant l'API GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 265, "src_audio": "/acl6060/audio/en/265.wav", "src_ref": "Next, I'll describe our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais décrire nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 266, "src_audio": "/acl6060/audio/en/266.wav", "src_ref": "Here is an example of data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici un exemple de données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 267, "src_audio": "/acl6060/audio/en/267.wav", "src_ref": "The left side is a commit message and the right side is the release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le côté gauche est un message de validation et le côté droit représente les notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 268, "src_audio": "/acl6060/audio/en/268.wav", "src_ref": "Release notes are labeled as improvements or fixes, etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version sont étiquetées comme améliorations ou correctifs, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 269, "src_audio": "/acl6060/audio/en/269.wav", "src_ref": "We have set up a task that takes the commit messages as input and outputs a labeled release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons mis en place une tâche qui prend les messages de validation comme saisie et sorties des notes de version étiquetées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 270, "src_audio": "/acl6060/audio/en/270.wav", "src_ref": "This can be regarded as a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut être considéré comme une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 271, "src_audio": "/acl6060/audio/en/271.wav", "src_ref": "We have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons prédéfini quatre étiquettes : fonctions, améliorations, corrections de bugs, suppressions de dépréciations et modifications de rupture.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 272, "src_audio": "/acl6060/audio/en/272.wav", "src_ref": "These were set based on previous research and other factors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Celles-ci ont été établies sur la base de la recherche antérieure et d'autres facteurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 273, "src_audio": "/acl6060/audio/en/273.wav", "src_ref": "The release note on the bottom right is extracted from the release note on the bottom left.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La note de version en bas à droite est extraite de la note de version en bas à gauche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 274, "src_audio": "/acl6060/audio/en/274.wav", "src_ref": "At this time, it is necessary to detect the four labels that have been set up in advance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été mises en place à l'avance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 275, "src_audio": "/acl6060/audio/en/275.wav", "src_ref": "But the labels are not always consistent with each repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 276, "src_audio": "/acl6060/audio/en/276.wav", "src_ref": "For example, the improvements label includes improvements, enhancements, optimizations, and so on.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, l'étiquette d'améliorations inclut des améliorations, des perfectionnements, des optimisations, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 277, "src_audio": "/acl6060/audio/en/277.wav", "src_ref": "We prepared a vocabulary list of about thirty labels for each of these notational variations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons préparé une liste de vocabulaire d'une trentaine d'étiquettes pour chacune de ces variations de notation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 278, "src_audio": "/acl6060/audio/en/278.wav", "src_ref": "This is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de détecter la classe de note de version et de collecter le texte de la version qui suit en tant que phrase de note de version pour la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 279, "src_audio": "/acl6060/audio/en/279.wav", "src_ref": "Next is a commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il y a un message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 280, "src_audio": "/acl6060/audio/en/280.wav", "src_ref": "Commit messages are not tied to each release.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation ne sont pas liés à chaque version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 281, "src_audio": "/acl6060/audio/en/281.wav", "src_ref": "As shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme le montre l'image ci-dessous, si la version actuelle est la version deux point cinq à dix-neuf, nous devons identifier la version deux point cinq à dix-huit antérieure et obtenir un diff.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 282, "src_audio": "/acl6060/audio/en/282.wav", "src_ref": "This is a bit tedious and it is not enough to just get a list of releases and look at the before and after.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est un peu fastidieux et il ne suffit pas d'obtenir une liste de versions et de regarder l'avant et l'après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 283, "src_audio": "/acl6060/audio/en/283.wav", "src_ref": "We created a heuristic matching rule to get the previous and next versions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons créé une règle de correspondance heuristique pour obtenir les versions antérieures et suivantes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 284, "src_audio": "/acl6060/audio/en/284.wav", "src_ref": "Dataset analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Analyse des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 285, "src_audio": "/acl6060/audio/en/285.wav", "src_ref": "In the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En fin de compte, sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données ont été recueillis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 286, "src_audio": "/acl6060/audio/en/286.wav", "src_ref": "Also, the average number of release notes tokens is sixty three, which is quite high for a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le nombre moyen de gages de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 287, "src_audio": "/acl6060/audio/en/287.wav", "src_ref": "Also, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De même, le nombre de gages uniques est assez grand, s'élevant à huit mille huit cent trente mille.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 288, "src_audio": "/acl6060/audio/en/288.wav", "src_ref": "This is due to the large number of unique class or method names found in the repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela est dû au grand nombre de noms de méthode ou de classe unique trouvés dans le référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 289, "src_audio": "/acl6060/audio/en/289.wav", "src_ref": "Next, I will explain the proposed method.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais expliquer la méthode proposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 290, "src_audio": "/acl6060/audio/en/290.wav", "src_ref": "The classwise extractive then abstractive summarization model consists of two neural modules.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle de synthèse extractive puis abstractive par classe se compose de deux modules neuronaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 291, "src_audio": "/acl6060/audio/en/291.wav", "src_ref": "A classifier using BERT or CodeBERT and a generator using BART.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un classificateur utilisant BERT ou CodeBERT et un générateur utilisant BART.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 292, "src_audio": "/acl6060/audio/en/292.wav", "src_ref": "First, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, CEAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, qui utilisent des améliorations, des corrections de bogues, des dépréciations et d'autres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 293, "src_audio": "/acl6060/audio/en/293.wav", "src_ref": "The commit messages classified as other are discarded.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation classés comme autres sont supprimés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 294, "src_audio": "/acl6060/audio/en/294.wav", "src_ref": "Then CEAS applies the generator to the four labeled documents independently and generates release notes for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 295, "src_audio": "/acl6060/audio/en/295.wav", "src_ref": "In this task, the direct correspondences between commit messages and release notes are not known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 296, "src_audio": "/acl6060/audio/en/296.wav", "src_ref": "Therefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour former le classificateur, nous avons réaffecté les enquêtes à chaque message de validation de saisie en utilisant les dix premiers caractères de chaque message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 297, "src_audio": "/acl6060/audio/en/297.wav", "src_ref": "We modeled the classwise abstractive summarization approach by two different methods.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons modélisé l'approche de synthèse abstractive par classe à travers deux méthodes différentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 298, "src_audio": "/acl6060/audio/en/298.wav", "src_ref": "The first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le premier modèle, que nous appelons CAS unique, se compose d'un réseau de six à six unique et génère un seul texte de note de version donnant un enchaînement de messages de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 299, "src_audio": "/acl6060/audio/en/299.wav", "src_ref": "The output texts can be divided into classwise segments based on special class-specific endpoint symbols.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les textes de sortie peuvent être divisés en segments par classe sur la base de symboles de point d'extrémité particuliers, spécifiques à la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 300, "src_audio": "/acl6060/audio/en/300.wav", "src_ref": "The second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde méthode, que nous appelons CAS multiple, se compose de quatre réseaux seq2seq différents, dont chacun correspond à l'une des classes de notes de version fixes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 301, "src_audio": "/acl6060/audio/en/301.wav", "src_ref": "Okay, let me explain the experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ok, je vais vous expliquer les expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 302, "src_audio": "/acl6060/audio/en/302.wav", "src_ref": "Five methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cinq méthodes ont été comparées : CEAS, CAS unique, CAS multiple, regroupement et étude antérieure, Glyph.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 303, "src_audio": "/acl6060/audio/en/303.wav", "src_ref": "Regarding evaluation, in some cases, release notes are output in multiple sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 304, "src_audio": "/acl6060/audio/en/304.wav", "src_ref": "Since it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisqu'il est difficile de calculer le nombre de phrases telles qu'elles sont, elles sont combinées avec des espaces et traitées comme une seule phrase longue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 305, "src_audio": "/acl6060/audio/en/305.wav", "src_ref": "The BLEU is penalized when the system outputs a short sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le BLEU est pénalisé lorsque le système produit une phrase courte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 306, "src_audio": "/acl6060/audio/en/306.wav", "src_ref": "This penalty results in a lower BLEU value in the experiment results described next.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette pénalité se traduit par une valeur BLEU inférieure dans les résultats de l'expérience décrits ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 307, "src_audio": "/acl6060/audio/en/307.wav", "src_ref": "Finally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous calculons également la spécificité car ROUGE et BLEU ne peuvent pas être calculés si les notes de version sont vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 308, "src_audio": "/acl6060/audio/en/308.wav", "src_ref": "A higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une spécificité plus élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées être vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 309, "src_audio": "/acl6060/audio/en/309.wav", "src_ref": "Here are the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats sont indiqués ci-après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 310, "src_audio": "/acl6060/audio/en/310.wav", "src_ref": "Since the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné que les données contiennent des adresses électroniques, des valeurs hachées, etc., nous avons également évalué les données nettoyées, ce qui les exclut.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 311, "src_audio": "/acl6060/audio/en/311.wav", "src_ref": "CEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS et le CAS ont obtenu des scores de ROUGE-L supérieurs de plus de dix points par rapport aux bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 312, "src_audio": "/acl6060/audio/en/312.wav", "src_ref": "In particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les bases a atteint plus de vingt points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 313, "src_audio": "/acl6060/audio/en/313.wav", "src_ref": "These results indicate that CEAS and CAS are significantly affected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces résultats indiquent que le CEAS et le CAS sont considérablement touchés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 314, "src_audio": "/acl6060/audio/en/314.wav", "src_ref": "CEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS a obtenu un meilleur score ROUGE-L que le CAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour former le classificateur à l'aide de pseudo-étiquettes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 315, "src_audio": "/acl6060/audio/en/315.wav", "src_ref": "High coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une couverture élevée du CEAS peut être obtenue probablement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 316, "src_audio": "/acl6060/audio/en/316.wav", "src_ref": "CAS-Multi tended to yield higher ROUGE-L than CAS-Single.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CAS multiple tendait à produire plus de ROUGE-L que le CAS unique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 317, "src_audio": "/acl6060/audio/en/317.wav", "src_ref": "Suggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En suggérant qu'il est également efficace de développer indépendamment et différemment des modèles de abstractive summarization pour chaque classe de note de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 318, "src_audio": "/acl6060/audio/en/318.wav", "src_ref": "Here are an error analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici une analyse d'erreur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 319, "src_audio": "/acl6060/audio/en/319.wav", "src_ref": "CAS methods tend to output shorter sentences than human reference sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 320, "src_audio": "/acl6060/audio/en/320.wav", "src_ref": "In the figure on the right, the reference sentence has three or four sentences, while CAS has only one.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que le CAS n'en a qu'une.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 321, "src_audio": "/acl6060/audio/en/321.wav", "src_ref": "The reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La raison de la réticence de ce modèle est que dans les données de formation, seulement trente-trois pour cent des phrases sont présentes dans l'étiquette des fonctions et quarante pour cent dans l'étiquette des améliorations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 322, "src_audio": "/acl6060/audio/en/322.wav", "src_ref": "Furthermore, CAS methods cannot generate accurate release notes without additional information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les méthodes CAS ne peuvent pas générer des notes de version précises sans informations supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 323, "src_audio": "/acl6060/audio/en/323.wav", "src_ref": "The top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple en haut à droite est un exemple de message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la progression ou au problème correspondant(e).", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 324, "src_audio": "/acl6060/audio/en/324.wav", "src_ref": "The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple ci-dessous montre que les deux messages de validation dans la saisie sont liés et doivent être combinés en une phrase, mais cela n'est pas fait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 325, "src_audio": "/acl6060/audio/en/325.wav", "src_ref": "Finally, a conclusion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, passons à la conclusion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 326, "src_audio": "/acl6060/audio/en/326.wav", "src_ref": "We have built a new dataset for automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons construit de nouvelles données pour la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 327, "src_audio": "/acl6060/audio/en/327.wav", "src_ref": "We have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également formulé une tâche consistant à saisir des messages de validation et à les résumer afin qu'ils soient applicables à tous les projets écrits en anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 328, "src_audio": "/acl6060/audio/en/328.wav", "src_ref": "Our experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 329, "src_audio": "/acl6060/audio/en/329.wav", "src_ref": "Please check out our dataset on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Veuillez consulter nos données sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 330, "src_audio": "/acl6060/audio/en/330.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 331, "src_audio": "/acl6060/audio/en/331.wav", "src_ref": "Hello. My name is Asaf Harari.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour. Je m'appelle Asaf Harari.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 332, "src_audio": "/acl6060/audio/en/332.wav", "src_ref": "And I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et je vais vous présenter notre article : enrichissement de données tabulaires en quelques coups à l'aide d'architectures de transformateurs raffinées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 333, "src_audio": "/acl6060/audio/en/333.wav", "src_ref": "Data scientists analyze data and mainly focus on the manipulating the data's existing features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les scientifiques analysent les données et se concentrent principalement sur la manipulation des fonctions existantes des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 334, "src_audio": "/acl6060/audio/en/334.wav", "src_ref": "But sometimes, these features are limited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais parfois, ces fonctions sont limitées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 335, "src_audio": "/acl6060/audio/en/335.wav", "src_ref": "Feature generation using another data source may add substantial information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La génération de fonctions en utilisant une autre source de données peut ajouter des informations substantielles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 336, "src_audio": "/acl6060/audio/en/336.wav", "src_ref": "Our research goal is automatic tabular data enrichment using external sources' free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des textes libres de sources externes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 337, "src_audio": "/acl6060/audio/en/337.wav", "src_ref": "Assume we have a tabular dataset and a knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Supposons que nous ayons des données tabulaires et une base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 338, "src_audio": "/acl6060/audio/en/338.wav", "src_ref": "We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles fonctions du texte libre de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 339, "src_audio": "/acl6060/audio/en/339.wav", "src_ref": "Our framework FeSTE is exactly this automatic process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre cadre FeSTE est exactement ce processus automatique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 340, "src_audio": "/acl6060/audio/en/340.wav", "src_ref": "So let's see an example in a dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons donc un exemple dans des données introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 341, "src_audio": "/acl6060/audio/en/341.wav", "src_ref": "In this example, the dataset is university dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données sont des données universitaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 342, "src_audio": "/acl6060/audio/en/342.wav", "src_ref": "When its goal is to classify universities into low ranking universities and high-ranking universities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Quand l'objectif est de classer les universités en universités de bas rang et en universités de haut rang.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 343, "src_audio": "/acl6060/audio/en/343.wav", "src_ref": "As knowledge base, we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 344, "src_audio": "/acl6060/audio/en/344.wav", "src_ref": "The first phase of FeSTE is entity linking.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première phase de FeSTE est la liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 345, "src_audio": "/acl6060/audio/en/345.wav", "src_ref": "When each entity, in this example the university name, is linked to an entity within the knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 346, "src_audio": "/acl6060/audio/en/346.wav", "src_ref": "And and the text of the entities of the knowledge base is extracted and added to the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le texte des entités de la base de connaissances est extrait et ajouté aux données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 347, "src_audio": "/acl6060/audio/en/347.wav", "src_ref": "In this example, the text is the Wikipedia page's abstract.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, le texte est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 348, "src_audio": "/acl6060/audio/en/348.wav", "src_ref": "Now, we need to generate or extract features from the retrieved text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Maintenant, nous devons générer ou extraire des fonctions à partir du texte extrait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 349, "src_audio": "/acl6060/audio/en/349.wav", "src_ref": "So, we need to ah feature extraction phase ah which includes text analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc besoin de la phase d'extraction des fonctions qui comprend l'analyse de texte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 350, "src_audio": "/acl6060/audio/en/350.wav", "src_ref": "And this is the main novelty of this paper and I will deep dive into it in the next slides.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de la principale nouveauté de cet article et je vais mieux l'expliquer dans les prochaines diapositives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 351, "src_audio": "/acl6060/audio/en/351.wav", "src_ref": "After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Après la phase d'extraction des fonctions, il y a une phase de génération de fonctions lorsque nous utilisons les fonctions extraites pour générer un petit nombre de nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 352, "src_audio": "/acl6060/audio/en/352.wav", "src_ref": "First generate ah features in the number of classes of the original dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Générez d'abord les fonctions dans le nombre de classes des données d'origine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 353, "src_audio": "/acl6060/audio/en/353.wav", "src_ref": "In this example, the original dataset has two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données d'origine ont deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 354, "src_audio": "/acl6060/audio/en/354.wav", "src_ref": "So, FeSTE generates two new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, FeSTE génère deux nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 355, "src_audio": "/acl6060/audio/en/355.wav", "src_ref": "But if the dataset has five classes, FeSTE generates five new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si les données ont cinq classes, FeSTE génère cinq nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 356, "src_audio": "/acl6060/audio/en/356.wav", "src_ref": "Each feature represents the likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chaque fonction représente la probabilité pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 357, "src_audio": "/acl6060/audio/en/357.wav", "src_ref": "To analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour analyser le texte, nous utilisons les analyses de texte de pointe, qui sont des modèles de langue basés sur la conversion comme BERT, GPT, XLNet etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 358, "src_audio": "/acl6060/audio/en/358.wav", "src_ref": "It is but it is not likely that we can train language models using the input datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais il est peu probable que nous puissions former des modèles de langue en utilisant les données de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 359, "src_audio": "/acl6060/audio/en/359.wav", "src_ref": "So a naive approach will be ah target task finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une approche naïve sera le raffinement de la tâche cible.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 360, "src_audio": "/acl6060/audio/en/360.wav", "src_ref": "So, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la phase d'extraction des fonctions, nous pouvons télécharger des modèles de langue préformée et raffiner le modèle de langue sur les données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 361, "src_audio": "/acl6060/audio/en/361.wav", "src_ref": "In this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, raffiner le modèle de langue, classer le texte en classes, résumer en classes, basses ou hautes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 362, "src_audio": "/acl6060/audio/en/362.wav", "src_ref": "Receive the language model output, which is the likelihood for each class and use as new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Recevoir les résultats du modèle de langue, qui sont la probabilité pour chaque classe et les utiliser comme nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 363, "src_audio": "/acl6060/audio/en/363.wav", "src_ref": "The problem with this approach is datasets may have few distinct entities / texts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le problème avec cette approche est que les données peuvent avoir peu d'entités / de textes distinct(e)s.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 364, "src_audio": "/acl6060/audio/en/364.wav", "src_ref": "In our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre expérience, près de la moitié des données contiennent moins de quatre cents échantillons et les plus petites données contiennent trente-cinq échantillons dans un ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 365, "src_audio": "/acl6060/audio/en/365.wav", "src_ref": "So to finetune a language model over ah this dataset will be ineffective.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour raffiner un modèle de langue sur cela, les données seront inefficaces.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 366, "src_audio": "/acl6060/audio/en/366.wav", "src_ref": "But we can use prior knowledge about pre-analyzed datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons utiliser des connaissances préalables sur des données préanalysées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 367, "src_audio": "/acl6060/audio/en/367.wav", "src_ref": "Because FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisque nous appliquons FeSTE sur des données multiples, nous pouvons utiliser les données n moins un pour recueillir des informations sur les données n moins un, et utiliser ces informations lorsque nous analysons les nièmes données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 368, "src_audio": "/acl6060/audio/en/368.wav", "src_ref": "What we, what we suggest is to add, to add another finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce que nous suggérons, c'est d'ajouter une autre phase de raffinement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 369, "src_audio": "/acl6060/audio/en/369.wav", "src_ref": "A preliminary multitask finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une phase de raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 370, "src_audio": "/acl6060/audio/en/370.wav", "src_ref": "When you finetune the language model over the n minus one datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque vous raffinez le modèle de langue sur les données n moins un.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 371, "src_audio": "/acl6060/audio/en/371.wav", "src_ref": "And, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ensuite, nous exécutons une autre phase de raffinement qui est un raffinement de tâche cible, lorsque nous raffinons le modèle de langue sur les nièmes données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 372, "src_audio": "/acl6060/audio/en/372.wav", "src_ref": "The state-of-the-art in multitask ah multitask finetuning called MTDNN.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement multitâche de pointe appelé MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 373, "src_audio": "/acl6060/audio/en/373.wav", "src_ref": "In MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN maintient les têtes dans le nombre de tâches dans l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 374, "src_audio": "/acl6060/audio/en/374.wav", "src_ref": "So, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans cet exemple, il y a quatre tâches dans l'ensemble de formation. Le MTDNN maintient alors quatre têtes comme vous pouvez le voir sur l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 375, "src_audio": "/acl6060/audio/en/375.wav", "src_ref": "And it samples a random batch from ah from the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il échantillonne un lot aléatoire de l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 376, "src_audio": "/acl6060/audio/en/376.wav", "src_ref": "And if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si elles appartiennent à un lot aléatoire, par exemple, une seule tâche de classification de phrases, il exécute des chemins d'aller et retour à travers la première tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 377, "src_audio": "/acl6060/audio/en/377.wav", "src_ref": "And if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si le lot aléatoire appartient à la tâche de classement par paire, il exécute un chemin d'aller et retour dans la dernière tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 378, "src_audio": "/acl6060/audio/en/378.wav", "src_ref": "In our scenario, ah tabular datasets vary in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre scénario, les données tabulaires varient dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 379, "src_audio": "/acl6060/audio/en/379.wav", "src_ref": "So there are many tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il y a donc beaucoup de tâches.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 380, "src_audio": "/acl6060/audio/en/380.wav", "src_ref": "MTDNN maintained number of classes, heads, output layers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN a maintenu le nombre de classes, de têtes et de couches de sortie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 381, "src_audio": "/acl6060/audio/en/381.wav", "src_ref": "And the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en outre, le MTDNN doit initialiser de nouvelles têtes pour de nouvelles données avec une nouvelle tâche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 382, "src_audio": "/acl6060/audio/en/382.wav", "src_ref": "Our approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche, appelée raffinement de reformulation de tâche, est dans notre raffinement de reformulation de tâche d'approche. Au lieu de maintenir plusieurs têtes, nous reformulons chaque donnée dans une phrase par problème de classification, étant des tâches de deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 383, "src_audio": "/acl6060/audio/en/383.wav", "src_ref": "So let's see an example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Prenons donc un exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 384, "src_audio": "/acl6060/audio/en/384.wav", "src_ref": "Here is the our input dataset which consists of entities, features, text and classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici nos données de saisie qui se composent d'entités, de fonctions, de texte et de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 385, "src_audio": "/acl6060/audio/en/385.wav", "src_ref": "And, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous reformulons la tâche à partir d'une classification du texte en bas ou haut pour classer le texte, le résumé et la classe en vrai ou faux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 386, "src_audio": "/acl6060/audio/en/386.wav", "src_ref": "Or in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou en d'autres mots, nous avons formé le modèle de langue pour classer un résumé et une classe en résumé et classe, si le résumé appartient à la classe ou non.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 387, "src_audio": "/acl6060/audio/en/387.wav", "src_ref": "So the label vector in this case stays always ah which consists always with two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc dans ce cas, le vecteur d'étiquette consiste toujours en deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 388, "src_audio": "/acl6060/audio/en/388.wav", "src_ref": "And this is the ah algorithm for our fine, reformulated finetuning approach.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il s'agit de l'algorithme pour notre très bonne approche de raffinement reformulée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 389, "src_audio": "/acl6060/audio/en/389.wav", "src_ref": "So let's see the full framework.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons le cadre complet.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 390, "src_audio": "/acl6060/audio/en/390.wav", "src_ref": "Dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données sont introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 391, "src_audio": "/acl6060/audio/en/391.wav", "src_ref": "And then ah FeSTE executes entity linking phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis FeSTE exécute la phase de liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 392, "src_audio": "/acl6060/audio/en/392.wav", "src_ref": "It ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 393, "src_audio": "/acl6060/audio/en/393.wav", "src_ref": "Then it reformulated the task into a pairwise sentence classification task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il a reformulé la tâche en une tâche de classification des phrases par paire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 394, "src_audio": "/acl6060/audio/en/394.wav", "src_ref": "Applied the language model to the new task and the output likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il a appliqué le modèle de langue à la nouvelle tâche et la probabilité de sortie pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 395, "src_audio": "/acl6060/audio/en/395.wav", "src_ref": "And now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et maintenant, le modèle de langue est déjà raffiné sur des données n moins un en utilisant un raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 396, "src_audio": "/acl6060/audio/en/396.wav", "src_ref": "Then we use the output vector of the language model as a newly generated feature in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous utilisons le vecteur de sortie du modèle de langue comme une fonction nouvellement générée dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 397, "src_audio": "/acl6060/audio/en/397.wav", "src_ref": "To evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour évaluer notre cadre, nous utilisons dix-sept données de classification tabulaires qui varient en taille, fonctions, équilibre, domaine et performance initiale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 398, "src_audio": "/acl6060/audio/en/398.wav", "src_ref": "And as knowledge base we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 399, "src_audio": "/acl6060/audio/en/399.wav", "src_ref": "We design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous concevons notre expérience comme une évaluation où nous formons FeSTe sur seize données et l'appliquons à la dix-septième donnée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 400, "src_audio": "/acl6060/audio/en/400.wav", "src_ref": "We also, we also split each dataset into four folds and apply four folds cross validation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous divisons également chaque donnée en quatre plis et appliquons une validation croisée de quatre plis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 401, "src_audio": "/acl6060/audio/en/401.wav", "src_ref": "Then, we generate the new features and evaluate them using five evaluation classifiers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous générons les nouvelles fonctions et les évaluons en utilisant cinq classificateurs d'évaluation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 402, "src_audio": "/acl6060/audio/en/402.wav", "src_ref": "We use in our experiments base BERT base architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons dans nos expériences l'architecture de base des Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 403, "src_audio": "/acl6060/audio/en/403.wav", "src_ref": "Here are the results for our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici les résultats de nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 404, "src_audio": "/acl6060/audio/en/404.wav", "src_ref": "You can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez voir que nous comparons notre cadre au raffinement des données cibles, au raffinement de la tâche cible et à un raffinement préliminaire MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 405, "src_audio": "/acl6060/audio/en/405.wav", "src_ref": "And our reformulated finetuning achieves the best result, the best performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et notre raffinement reformulé atteint le meilleur résultat et la meilleure performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 406, "src_audio": "/acl6060/audio/en/406.wav", "src_ref": "While MTDNN achieved two percent improvement over the target dataset finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que le MTDNN a obtenu une amélioration de deux pour cent par rapport au raffinement des données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 407, "src_audio": "/acl6060/audio/en/407.wav", "src_ref": "Our approach achieved six percent improvement.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche a obtenu une amélioration de six pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 408, "src_audio": "/acl6060/audio/en/408.wav", "src_ref": "When we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque nous regardons les petites données, nous pouvons voir que la performance du MTDNN diminue et l'amélioration de la phase de raffinement multitâche préliminaire diminue aussi à un virgule cinq pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 409, "src_audio": "/acl6060/audio/en/409.wav", "src_ref": "But our performance increased to eleven percent compared to the target task finetuning alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais notre performance a augmenté à onze pour cent comparé au raffinement de tâche cible seul.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 410, "src_audio": "/acl6060/audio/en/410.wav", "src_ref": "For summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour résumer, FeSTE permet un enrichissement en quelques coups à partir de trente-cinq échantillons dans nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 411, "src_audio": "/acl6060/audio/en/411.wav", "src_ref": "It uses one architecture for all tasks and datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il utilise une architecture pour toutes les tâches et données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 412, "src_audio": "/acl6060/audio/en/412.wav", "src_ref": "And it keeps the head of ah of the model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il garde la tête du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 413, "src_audio": "/acl6060/audio/en/413.wav", "src_ref": "But it adds reformulation phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cela ajoute une phase de reformulation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 414, "src_audio": "/acl6060/audio/en/414.wav", "src_ref": "It augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il augmente l'ensemble de formation et a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'introduire dans le modèle de langue et l'utiliser dans le problème de classification par paire de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 415, "src_audio": "/acl6060/audio/en/415.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 416, "src_audio": "/acl6060/audio/en/416.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.\nI'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.\nFirst, I'd like to talk about our motivation for reasoning.\nSo here we show an examples where multi-step reasoning is helpful.\nSo this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.\nSo on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.\nBut if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.\nSo it is good to have interpretable multi-step reasoning as output.\nAnd we also think math word problem is a straightforward application to evaluate such reasoning abilities.\nSo, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.\nSo in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.\nSo, certain assumptions ah also apply as in previous work.\nWe assume the precision of quantities are known.\nAnd we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.\nFurthermore, complicated operators can be actually decomposed into these basic operators.\nSo, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.\nSo, traditional sequence to sequence model convert the expression to a specific sequence for generation.\nAnd it is pretty easy to implement and it can generalize to many different complicated problem.\nBut the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.\nBut actually this direction is still quite popular because of um the transformer model.\nSo, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.\nSo here we keep generating the operators until we reach the leaves, which are the quantities.\nSo here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.\nAnd the second thing is that it also contains some repetitive computations.\nSo here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.\nSo, in our proposed approach we want to solve those problems in a step by step and interpretable manners.\nSo for example, here in the second step, ah we can obtain these divisors which is twenty seven.\nAnd we can also refer back to the original questions to find the relevant contents.\nAnd in these steps we obtain the divisors.\nSo, ah and then at this third step we actually get the quotient.\nAlright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.\nSo, here we actually generate the whole expression directly rather than generating a single operators or quantities.\nSo this makes the process more accurate.\nSo, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.\nSo, the expression is represented by e i j o p.\nWhere we perform operator from q_i to q_j, and such expression is actually directed.\nSo, we also have subtraction with words here to represent the opposite direction.\nThis is quite similar to relation extraction.\nSo in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.\nWe add it to the next state to become a new quantity.\nSo, these slides actually visualize the evolution of the state where we keep adding expression to the current state.\nSo in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.\nSo, once we get the quantity representations, we can start to do inference.\nHere we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.\nFirst we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.\nAnd then finally we obtain the expression representation q_1 divided by q_2.\nBut in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.\nSo, here all the possible expression is equals to three times the number of operators.\nSo the nice thing here is that we can easily add constraints to control this search this search space.\nFor example, if this expression is not allowed, we can simply remove this expression in our search space.\nSo in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.\nSo this quantity come from the previous calculated expression.\nSo finally we can obtain this final expression q_3 times q_4.\nAnd we can also see the number of all the possible ah expression is different from the previous step.\nSo, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.\nSo the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.\nAnd here we also use this tau to represent when we should terminate this generation process.\nAnd here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.\nAnd it also allows us to impose certain constraints from prior from prior knowledge.\nSo we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.\nAnd here we briefly show the results compared with the previous best approaches.\nSo our best performing variant is Roberta-DeductiveReasoner.\nAnd in fact we do not use beam search, in contrast all previous approaches are using beam search.\nAll right. So, the best approaches are often tree based model.\nSo, overall our reasoner is able to significantl significantly outperform this tree based model.\nBut we can see the absolute numbers on MathQA or SVAMP are not really high.\nSo we further investigate the results on SVAMP.\nAnd this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.\nSo, in our prediction we find some of the intermediate values are actually negatives.\nFor example, um, in these questions we are asking how many apples does Jake have?\nBut we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.\nSo, our model makes some prediction like this which is producing negative values.\nAnd we observe these two expressions actually have similar scores.\nSo, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.\nSo um we further find such constraint actually improves quite a lot for some models.\nFor example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.\nSo better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.\nAnd we also try to analyze the difficulty behind these behind all these datasets.\nWe assume the number of unused quantities can be regarded as irrelevant information here.\nSo ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.\nAnd here we also show the overall performance.\nFor those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.\nBut with those samples that with unused quantity is actually way worse than the, worse than the overall performance.\nFor MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.\nSo, finally we want to show the interpretability through a question perturbation example.\nSo here our model actually makes a wrong prediction at the first step.\nSo, we can actually correlate this expression with the sentence here. Alright.\nSo, we think this sentence might be misleading the model to an incorrect predictions.\nSo here planting another thirty five makes the model makes the model think it should be an addition operator.\nSo we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.\nSo, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.\nSo, this study shows how the interpretable predictions help us understand the model behavior.\nSo to conclude our work, so first our model is actually pretty efficient.\nAnd we are able to provide interpretable solving procedure.\nAnd we can easily incorporate some prior knowledge as constraint which can help improve the performance.\nAnd the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.\nWe also have certain limitations.\nAh, if we have a large number of operators or constants, the memory consumption could be pretty high.\nAnd the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.\nSo this is the end of the talk, and questions are welcomed. Thank you.", "tgt_ref": "Bonjour tout le monde. Aujourd'hui, je vais vous présenter notre travail de recherche « Apprendre à raisonner par déduction » : résolution de problèmes de mots mathématiques comme extraction de relation complexe.\nJe m'appelle Allan du laboratoire d'intelligence artificielle ByteDance, et ceci est un travail en commun avec Jierui Li de l'Université du Texas à Austin et Wei Lu du SUTD.\nTout d'abord, j'aimerais parler de notre motivation pour le raisonnement.\nDonc ici, nous montrons un exemple où le raisonnement en plusieurs étapes est utile.\nCe chiffre est donc tiré de l'article PaLM où ils incitent à résoudre le problème de réseau dans le scénario d'apprentissage de quelques prises de vue.\nDonc, sur le côté gauche, nous pouvons voir que si nous fournissons quelques exemples avec juste des questions et des réponses, nous pourrions ne pas être en mesure d'obtenir les bonnes réponses.\nMais si nous fournissons plus de description du raisonnement, le modèle est capable de prévenir la description du raisonnement, mais aussi d'effectuer ici une prévention correcte.\nIl est alors bon d'avoir comme résultat un raisonnement à plusieurs étapes interprétable.\nEt nous pensons également que les problèmes de mots mathématiques sont une application directe pour évaluer de telles capacités de raisonnement.\nDonc ici, dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques.\nAinsi, dans nos données, nous recevons également l‘expression mathématique qui conduit à cette réponse particulière.\nDonc, certaines hypothèses s'appliquent également comme dans les travaux antérieurs.\nNous supposons que la précision des quantités est connue.\nEt nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle.\nEn outre, les opérateurs compliqués peuvent effectivement être décomposés en ces opérateurs de base.\nAinsi, le travail antérieur dans la résolution de problèmes de mots mathématiques peut en effet être classé dans un modèle séquence à séquence et séquence à arbre.\nLe modèle séquence à séquence traditionnel convertit donc l'expression en une séquence spécifique pour la génération.\nIl est assez facile à mettre en œuvre et peut être généralisé à de nombreux problèmes compliqués différents.\nMais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle structuré et son manque d'interopérabilité pour la prévention.\nMais en réalité, cette direction est encore très populaire en raison du modèle de conversion.\nDonc, dans les modèles à base d'arbre, nous structurons en réalité ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres.\nDonc ici, nous continuons à produire les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités.\nDonc ici, la bonne chose est qu'en fait, cela nous donne cette structure d'arbre binaire. Mais en réalité, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur et ensuite, à la fin, nous générons les quantités.\nEt la deuxième chose est que cela contient également des calculs répétitifs.\nDonc ici, si nous regardons cette expression, huit fois trois plus trois est en fait généré deux fois, mais en réalité, nous devrions réutiliser les résultats.\nAinsi, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable.\nPar exemple, ici dans la deuxième étape, nous pouvons obtenir ces diviseurs qui sont vingt-sept.\nEt nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent.\nEt dans ces étapes, nous obtenons les diviseurs.\nPuis, à cette troisième étape, nous obtenons alors le quotient.\nTrès bien. Et après ces trois étapes, nous pouvons alors réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, et enfin, nous pouvons obtenir les dividendes.\nDonc ici, nous générons en réalité l'expression entière directement plutôt que de produire un seul opérateur ou une seule quantité.\nCela rend le processus plus précis.\nAinsi, dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et incluant également une certaine constante comme notre état initial.\nAinsi, l'expression est représentée par e i j o p.\nOù nous effectuons l'opérateur de q_i à q_j, et cette expression est en fait dirigée.\nDonc, nous avons également ici la soustraction avec des mots pour représenter la direction opposée.\nC'est tout à fait similaire à l'extraction de relation.\nDonc, dans un système déductif formel, à un pas de temps t, on applique l'opérateur entre la paire q_i et q_j, puis on obtient cette nouvelle expression.\nNous l'ajoutons à l'état suivant pour devenir une nouvelle quantité.\nAinsi, ces diapositives visualisent en réalité l'évolution de l'état où nous continuons à ajouter de l'expression à l'état actuel.\nDonc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langue préformée qui peut être BERT ou Roberta, puis nous encodons la phrase et obtenons ensuite ces représentations de quantité.\nDonc, une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire l'inférence.\nNous montrons ici un exemple de q_1 pour obtenir la représentation pour q_2 divisée par q_2 multipliée par q_3.\nTout d'abord, nous obtenons la représentation de paire, qui n'est essentiellement que l'enchaînement entre q_1 et q_2, puis nous appliquons un réseau prédictif qui est paramétré par l'opérateur.\nEt enfin, nous obtenons la représentation de l'expression q_1 divisée par q_2.\nMais en réalité, dans la pratique, au stade de l'inférence, nous pourrions également obtenir une expression incorrecte.\nDonc ici, toute l'expression possible est égale à trois fois le nombre d'opérateurs.\nAlors, ce qui est bien ici, c'est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche.\nPar exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.\nDonc, dans la deuxième étape, nous faisons la même chose, mais la seule différence est que nous avons une quantité de plus.\nCette quantité provient donc de l'expression calculée antérieure.\nFinalement, nous pouvons obtenir cette expression finale q_3 multipliée par q_4.\nEt nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape antérieure.\nAinsi, une telle différence rend difficile l'application de la beam search car la distribution de probabilité entre ces deux étapes est déséquilibrée.\nLa procédure de formation est donc similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps.\nEt ici, nous utilisons également ce tau pour représenter le moment où nous devrions mettre fin à ce processus de génération.\nEt ici, l'espace est différent de séquence à séquence car il est différent à chaque pas de temps, alors que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire.\nEt cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.\nNous menons donc des expériences sur les données de problèmes de mots mathématiques, MAWPS, Math23K, MathQA et SVAMP couramment utilisées.\nEt ici, nous montrons brièvement les résultats comparés aux meilleures approches antérieures.\nDonc, notre variante la plus performante est Roberta-DeuctiveReasoner.\nEt en réalité, nous n'utilisons pas la beam search ; au contraire, toutes les approches antérieures utilisent la beam search.\nTrès bien. Ainsi, les meilleures approches sont souvent un modèle à base d'arbre.\nDonc, dans l'ensemble, notre raisonneur est capable de dépasser significativement ce modèle à base d'arbre.\nMais nous pouvons voir que les nombres absolus sur MathQA ou SVAMP ne sont pas vraiment élevés.\nNous étudions donc plus en détail les résultats sur SVAMP.\nEt ces données sont difficiles parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle TAL traitement automatique du langage naturel comme ajouter des informations non pertinentes et des quantités supplémentaires.\nDonc, dans notre prévention, nous trouvons que certaines des valeurs intermédiaires sont en réalité négatives.\nPar exemple, dans ces questions, nous demandons « combien de pommes a Jake ? »\nMais nous avons des informations supplémentaires comme « dix-sept photos de moins », et « Steven en a huit », ce qui est totalement hors de propos.\nAinsi, notre modèle effectue une prévention comme celle-ci qui produit des valeurs négatives.\nEt nous constatons que ces deux expressions ont en réalité des scores similaires.\nNous pouvons alors limiter cet espace de recherche en supprimant les résultats négatifs afin que nous puissions rendre la réponse correcte.\nNous trouvons donc que cette contrainte améliore en réalité beaucoup pour certains modèles.\nPar exemple, pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous améliorons sept points, puis pour le modèle de base Roberta, nous avons amélioré deux points.\nLe meilleur modèle de langue a donc de meilleures capacités de compréhension de la langue de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs.\nEt nous essayons également d'analyser la difficulté derrière toutes ces données.\nNous supposons que le nombre de quantités inutilisées peut être considéré ici comme une information non pertinente.\nDonc ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et les données SVAMP ont la plus grande partie.\nEt ici, nous montrons également la performance globale.\nPour ces échantillons sans quantités inutilisées, la performance est en réalité supérieure à la performance globale.\nMais avec ces échantillons ayant une quantité inutilisée, c'est en fait bien pire que la performance globale.\nPour MAWPS, nous n'avons pas vraiment trop de cas de test, alors j'ignore simplement cette partie.\nFinalement, nous voulons montrer l'interopérabilité à travers un exemple de perturbation de question.\nDonc ici, notre modèle effectue en réalité une prévention erronée à la première étape.\nAinsi, nous pouvons effectivement corréler cette expression avec la phrase ici. Très bien.\nNous pensons donc que cette phrase pourrait induire le modèle en erreur avec des préventions incorrectes.\nDonc ici, en planter trente-cinq autres fait penser au modèle qu'il devrait être un opérateur d'addition.\nNous essayons alors de réviser la phrase pour que cela soit quelque chose comme « le nombre de poiriers est inférieur de trente-cinq à celui des pommiers ».\nNous faisons en sorte de transmettre une sémantique plus précise afin que le modèle soit capable de rendre la prévention correcte.\nAinsi, cette étude montre comment les préventions interprétables nous aident à comprendre le comportement du modèle.\nDonc, pour conclure notre travail, notre modèle est en réalité assez efficace.\nEt nous sommes en mesure de fournir une procédure de résolution interprétable.\nEt nous pouvons facilement incorporer des connaissances antérieures en tant que contraintes qui peuvent aider à améliorer la performance.\nEt la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution des problèmes de réseau, mais aussi aux autres tâches qui impliquent un raisonnement à plusieurs étapes.\nNous avons également certaines limites.\nSi nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée.\nEt la deuxième chose est que, comme mentionné, puisque la distribution de probabilité est déséquilibrée entre les différentes étapes de temps, il est donc également assez difficile d'appliquer la stratégie de beam search.\nNous arrivons donc à la fin de la discussion, et vos questions sont les bienvenues. Merci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 417, "src_audio": "/acl6060/audio/en/417.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.\nI will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.\nLegal issues are an integral part of many people's lives.\nBut the majority of citizens have little to know knowledge about their rights and fundamental legal processes.\nAs a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.\nAll work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.\nSuch a system could provide a free professional legal help service for unskilled humans.\nBefore diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.\nGiven a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?\nA model is required to retrieve all relevant statutory articles from a large body of legislation.\nThis information retrieval task comes with its own set of challenges.\nFirst, it deals with two types of language.\nCommon natural language for the questions and complex legal language for the statutes.\nThis difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.\nBesides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.\nInstead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.\nLastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.\nHere, there are long documents that may be up to six thousand words.\nThe recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.\nBut statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.\nIn this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.\nOur Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.\nThese questions cover a wide range of topics from family, housing, money, to work and social security.\nEach of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.\nLet's now talk about how we collected this dataset.\nFirst, we started by compiling a large corpus of legal articles.\nWe considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.\nThen we gathered legal questions with references to relevant statutes.\nTo do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.\nWe were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.\nWe collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.\nLastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.\nThe remaining references were matched and converted to the corresponding article ids from our corpus.\nWe eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.\nIn addition, each question comes with the main category and a concatenation of subcategories.\nAnd each articles comes with a concatenation of the subsequence heading in the structure of the law.\nThis extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.\nLet's look at some characteristic of our dataset.\nThe questions are between five and forty four words long with a median of fourteen words.\nThe articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.\nThe lengthiest one being up to five thousand seven hundred and ninety words.\nAs previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.\nWhile the remaining fifteen percent concern either social security, foreigners or work.\nThe article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.\nHere's the total number of articles collected from each of these Belgian codes.\nOut of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.\nAnd around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.\nMeanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.\nWhich can be explained by the fact that those codes focused less on individuals and their concerns.\nOverall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.\nUsing all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.\nGiven a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.\nWe experiment with the standard TF-IDF and BM25 ranking functions.\nThe main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.\nTo overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.\nWe use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.\nThese embeddings typically result from a pooling operation on the output of a word embedding model.\nFirst, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.\nWe experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.\nAdditionally, we train our own CamemBERT based model ah bi-encoders on our dataset.\nNote that for training, we experiment with the two flavors of the bi-encoder architecture.\nSiamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.\nWe experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.\nHere are the result of our baseline on the test sets.\nWith the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.\nOverall, the finetuned bi-encoder significantly outperforms all the other baselines.\nThe two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.\nAlthough BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.\nRegarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.\nFurthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.\nAlthough promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.\nLet's conclude by discussing two limitations of our dataset.\nFirst, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.\nDuring the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.\nThis information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.\nSecond, we should note that not all legal questions can be answered with statutes alone.\nFor instance, the question, can I evict my tenants if they make too much noise?\nMight not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.\nInstead, the landlord should probably rely more on case law and find precedents similar to their current situation.\nFor example, the tenants makes two parties a week until two AM.\nHence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.\nWe hope that our work sparks interest in developing practical and reliable statutory article retrieval models.\nThat can help improve access to justice for all.\nYou can check out our paper, dataset and code at the following links. Thank you.", "tgt_ref": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht.\nJe vais vous présenter mon travail en commun avec Jerry, qui porte sur de nouvelles données pour l'extraction des articles de droit.\nLes questions juridiques font partie intégrante de la vie de nombreuses personnes.\nMais la majorité des citoyens ont peu de connaissances sur leurs droits et leurs processus juridiques fondamentaux.\nEn conséquence, de nombreux citoyens vulnérables qui n'ont pas les moyens de se payer l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités.\nTous les travaux visent à combler le fossé entre les personnes et la loi en développant un système d'extraction efficace pour les articles de droit.\nUn tel système pourrait fournir un service d'aide juridique professionnel gratuit aux personnes non qualifiées.\nAvant de plonger dans la principale contribution de ce travail, nous allons d'abord décrire le problème d'extraction des articles de droit.\nCompte tenu d'une simple question sur une question juridique telle que : « qu'est-ce que je risque si je viole la confidentialité professionnelle ? »\nUn modèle est nécessaire pour extraire tous les articles de droit pertinents d'un grand corpus législatif.\nCette tâche d'extraction des informations comporte son propre ensemble de défis.\nTout d'abord, elle traite de deux types de langue.\nLa langue naturelle commune pour les questions et la langue juridique complexe pour les lois.\nCette différence dans les distributions de langue rend plus difficile pour un système d'extraire les candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des lois.\nEn outre, le droit écrit n'est pas une pile d'articles indépendants qui peuvent être traités comme une source complète d'informations par eux-mêmes, contrairement aux nouvelles ou aux recettes, par exemple.\nAu lieu de cela, il s'agit d'un ensemble structuré de dispositions juridiques qui n'ont un sens entier que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les domaines et sous-domaines auxquels elles appartiennent et leur place dans la structure de la loi.\nEnfin, les articles de droit ne sont pas de petits paragraphes constituant généralement l'unité d'extraction typique dans la plupart des travaux d'extraction.\nIci, il y a de longs documents qui peuvent aller jusqu'à six mille mots.\nLes recent advances en matière de TAL traitement automatique du langage naturel ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prévention du jugement juridique ou l'examen des contrats de contacts automatisés.\nMais l'extraction de l'article de droit est restée essentiellement inchangée en raison du manque de grandes données étiquetées de haute qualité.\nDans ce travail, nous présentons de nouvelles données centrées sur le citoyen natif français pour étudier si l'extraction des modèles peut se rapprocher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche d'extraction de l'article de droit.\nNos données d'extraction de l'article de droit belge BSARD se composent de plus de mille cent questions juridiques posées par des citoyens belges.\nCes questions couvrent un wide range de sujets allant de la famille au logement, à l'argent, au travail et à la sécurité sociale.\nChacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques de codes de droit belge.\nParlons maintenant de la façon dont nous avons collecté ces données.\nTout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques.\nNous avons examiné trente-deux codes belges accessibles au public et extrait tous les articles ainsi que les titres de section correspondants.\nEnsuite, nous avons rassemblé les questions juridiques avec les références aux lois pertinentes.\nPour ce faire, nous nous associons au cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur une question juridique personnelle.\nNous avons eu la chance d'avoir accès à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges.\nNous avons recueilli des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux lois pertinentes.\nEnfin, nous avons fait passer les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avons considérés.\nLes références restantes ont été appariées et converties aux identifiants d'article correspondants de notre corpus.\nNous nous sommes finalement retrouvés avec mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de droit.\nDe plus, chaque question est accompagnée de la catégorie principale et d'un enchaînement de sous-catégories.\nEt chaque article comporte un enchaînement de la rubrique de sous-séquence dans la structure de la loi.\nCes informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour des recherches futures sur l'extraction d'informations juridiques ou la classification de textes juridiques.\nJetons un coup d'œil à certaines caractéristiques de nos données.\nLes questions comportent entre cinq et quarante-quatre mots avec une moyenne de quatorze mots.\nLes articles sont beaucoup plus longs avec une longueur moyenne de soixante-dix-sept mots, avec cent quarante-deux d'entre eux dépassant les mille mots.\nLe plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots.\nComme mentionné précédemment, les questions couvrent un wide range de sujets, dont environ quatre-vingt-cinq pour cent concernent la famille, le logement, l'argent ou la justice.\nAlors que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail.\nL'article est également très diversifié car il provient de trente-deux codes belges différents qui couvrent un grand nombre de sujets juridiques.\nVoici le nombre total d'articles collectés à partir de chacun de ces codes belges.\nSur les vingt-deux mille six cent trente-trois articles, seuls mille six cent douze sont désignés comme pertinents à au moins une question dans les données.\nEt environ quatre-vingts pour cent de ces articles cités proviennent du code civil, des codes judiciaires, des codes d'enquête criminelle ou des codes pénaux.\nPendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question.\nCe qui peut s'expliquer par le fait que ces codes se concentraient moins sur les individus et leurs préoccupations.\nDans l'ensemble, le nombre moyen de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.\nEn utilisant toutes les données, nous avons comparé plusieurs approches d'extraction, y compris l'architecture lexicale et dense.\nÉtant donné une requête et un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de requête des poids de chacun de ces termes dans cet article.\nNous expérimentons avec les fonctions de classement TF-IDF et BM25 standard.\nLe principal problème de ces approches est qu'elles ne peuvent extraire que les articles contenant des mots-clés présents dans la requête.\nPour surmonter cette limitation, nous expérimentons une architecture neuronale qui peut capturer les relations sémantiques entre les requêtes et l'article.\nNous utilisons un modèle bi-encodeur qui cartographie les requêtes et les articles en représentations vecteurs denses et calculons un score de pertinence entre une paire d'articles de requête par la similitude de leurs intégrations.\nCes intégrations résultent typiquement d'une opération de pooling sur la sortie d'un modèle d'intégration de mots.\nTout d'abord, nous étudions l'efficacité des bi-encodeurs siamois dans une configuration d'évaluation « zero shot », ce qui signifie que des modèles d'intégration de mots préformés sont appliqués immédiatement sans aucun raffinement supplémentaire.\nNous expérimentons avec l'encodeur de texte indépendant du contexte, à savoir word2vec et fastText, et des modèles d'intégration dépendants du contexte, à savoir Roberta et plus précisément CamemBERT qui est un modèle Roberta français.\nDe plus, nous formons nos propres bi-encodeurs de modèles basés sur CamemBERT sur nos données.\nNotez que pour la formation, nous expérimentons avec les deux modèles de l'architecture bi-encodeur.\nLe siamois, qui utilise un modèle d'intégration de mots unique cartographiant la requête et l'article ensemble dans un vector space dense partagé, et le modèle à deux tours, qui utilise deux modèles d'intégration de mots indépendants encodant la requête et l'article séparément dans différents espaces d'intégration.\nNous expérimentons la mise en commun moyenne, maximale et CLS, ainsi que le produit et cosinus pour calculer les similitudes.\nVoici le résultat de notre base sur les ensembles de test.\nAvec les méthodes lexicales ci-dessus, les bi-encodeurs siamois ont évalué dans une configuration zero shot au milieu, et les bi-encodeurs raffinés ci-dessous.\nDans l'ensemble, le bi-encodeur raffiné dépasse de manière significative toutes les autres bases.\nLe modèle à deux tours s'améliore par rapport à ses variantes siamoises lors du rappel à cent, mais fonctionne de la même manière sur les autres indicateurs.\nBien que BM25 ait sous-performé le bi-encodeur formé de manière significative, ses performances indiquent qu'il s'agit toujours d'une base solide pour l'extraction spécifique au domaine.\nEn ce qui concerne l'évaluation zero shot du bi-encodeur siamois, nous constatons que l'utilisation directe des intégrations d'un modèle CamemBERT préformé sans optimiser pour la tâche d'extraction d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats antérieurs.\nEn outre, nous observons que le bi-encodeur basé sur word2vec a dépassé de manière significative les modèles basés sur fastText et les Représentations d'encodeurs bidirectionnels à partir de transformateurs, suggérant que peut-être les intégrations de niveau mot préformées sont plus appropriées pour la tâche que les intégrations de niveau caractère ou sous-mot lorsqu'elles sont utilisées immédiatement.\nBien que prometteurs, ces résultats suggèrent de nombreuses possibilités d'amélioration comparé à un expert juridique qualifié qui peut éventuellement extraire tous les articles pertinents à n'importe quelle question, et ainsi obtenir des scores parfaits.\nConcluons en discutant de deux limites de nos données.\nPremièrement, le corpus d'article est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants.\nAu cours de la construction des données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions ne se retrouvent qu'avec une fraction du nombre initial d'articles pertinents.\nCette information implique donc que la réponse contenue dans les autres articles pertinents pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.\nDeuxièmement, il convient de noter qu'on ne peut pas répondre à toutes les questions juridiques uniquement par des lois.\nPar exemple, la question « puis-je expulser mes locataires s'ils font trop de bruit ? »\nElle peut ne pas avoir de réponse détaillée dans le droit écrit qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée.\nAu lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des antécédents similaires à sa situation actuelle.\nPar exemple, les locataires font la fête deux fois par semaine jusqu'à deux heures du matin.\nPar conséquent, certaines questions sont mieux adaptées que d'autres à la tâche d'extraction de l'article de droit, et le domaine des moins adaptées reste à déterminer.\nNous espérons que nos travaux susciteront l'intérêt pour l'élaboration de modèles d'extraction d'article de droit pratiques et fiables.\nCela peut aider à améliorer l'accès à la justice pour tous.\nVous pouvez consulter notre article, nos données et notre code aux liens suivants. Merci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 418, "src_audio": "/acl6060/audio/en/418.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.\nWhy did we do the trouble in setting up this benchmark?\nWell, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.\nEach one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.\nSo we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.\nBut do we know what the models have actually learned?\nWhat is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?\nAnd the low score for this one?\nDo vision and language models focus on the right thing?\nOr do they focus on biases as shown by previous work?\nTo shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.\nWe target existence, plurality, counting, spatial relations, actions and entity coreference.\nBut how do we test whether the vision and language models have captured this phenomena?\nBy foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.\nFoiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.\nAnd we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.\nFor example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.\nCounting and coreference also are pieces that have more than one instrument.\nAnd we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.\nThis is not easy to do because a foiled caption may be less likely than the original caption.\nFor example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.\nTherefore, to obtain valid foils, we must take action.\nFirst, we make use of strong language models to propose foils.\nSecond, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.\nTo test this automatically, we apply natural language inference with the following rationale.\nWe consider an image to be the premise and its caption its entailed hypothesis.\nIn addition, we consider the caption to be the premise, and the foil is its hypothesis.\nIf an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.\nIf an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.\nBut this procedure is not perfect, it is just an indicator for valid foils.\nTherefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.\nSo, after filtering and human evaluation, we have as many test instances as described in this table.\nNote that VALSE does not deliver any training data but only test data.\nSince it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.\nFinetuning would only enable models to exploit artifacts or statistical biases in the data.\nAnd we all know that these models like to cheat and take shortcuts.\nAnd as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.\nWe experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.\nTwo of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.\nPerhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.\nFor more metrics and results on them, do check out our paper.\nThe results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.\nIt's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.\nHowever, none of the remaining pieces can be reliably solved in our adversarial foiling settings.\nWe see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.\nThe relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.\nThey also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.\nFrom the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.\nAs a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.\nIf the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.\nAnd it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.\nSo to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.\nOur experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.\nWe would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.\nAnd even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.\nIf you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "tgt_ref": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE ; un indice de référence indépendant de la tâche destiné à tester les modèles de langue et de vision avec des phénomènes linguistiques spécifiques.\nPourquoi avons-nous pris la peine de mettre en place cet indice de référence ?\nEh bien, au cours des dernières années, nous avons vu une explosion de la vision basée sur le transformateur et des modèles de langue préformés sur de grandes quantités de paires de textes d'images.\nChacun de ces modèles pousse les tâches de pointe en matière de vision et de langue telles que la réponse aux questions visuelles, le raisonnement de bon sens visuel, l'extraction d'images et les bases de phrases.\nNous avons donc reçu un message : les précisions sur ces tâches et les indices de référence spécifiques augmentent régulièrement.\nMais savons-nous ce que les modèles ont réellement appris ?\nQu'est-ce qu'une vision et un transformateur de langue ont compris lors de l'attribution d'un score élevé pour que cette image et cette phrase correspondent ?\nEt le score bas pour celle-ci ?\nEst-ce que les modèles de langue et de vision se concentrent sur la bonne chose ?\nOu se concentrent-ils sur les biais comme le montre le travail antérieur ?\nPour éclairer davantage cet aspect, nous proposons une direction plus indépendante des tâches et introduisons VALSE qui teste la sensibilité des modèles de langue et de vision à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistiques et visuelles.\nNous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités.\nMais comment tester si les modèles de langue et de vision ont capturé ce phénomène ?\nEn pratiquant le foiling sur une méthode précédemment appliquée pour les modèles de langue et de vision seulement pour des phrases nominales de Ravi Shekhar et de ses collaborateurs, et en comptant par nous dans les travaux antérieurs.\nEffectuer un foiling signifie essentiellement que nous prenons la légende d'une image et produisons un foil en modifiant la légende de sorte qu'elle ne décrive plus l'image.\nEt nous apportons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous aurions trouvé plus d'une façon intéressante de créer des instances de foil.\nPar exemple, dans le cas de l'élément d'actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié avec une action différente, et un dans lequel les acteurs sont échangés.\nLe comptage et la coréférence sont également des éléments qui ont plusieurs instruments.\nEt nous créons ces foils en nous assurant qu'ils ne décrivent pas l'image et que ce sont des phrases grammaticales et autrement valides.\nCe n'est pas facile à faire car une légende ayant subi un foil peut être moins probable que la légende originale.\nPar exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que les plantes coupent un homme qu'un homme coupe les plantes, et la grande vision et les modèles de langue pourraient capter cela.\nPar conséquent, pour obtenir des foils valides, nous devons agir.\nTout d'abord, nous utilisons des modèles de langue forts pour proposer des foils.\nDeuxièmement, nous utilisons l'inférence de la langue naturelle ou la NLI courte pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image.\nPour tester cela automatiquement, nous appliquons l'inférence de la langue naturelle avec la justification suivante.\nNous considérons qu'une image est la prémisse et sa légende son hypothèse implicite.\nEn outre, nous considérons la légende comme la prémisse, et le foil est son hypothèse.\nSi un modèle NLI prévient que le foil est en contradiction ou neutre par rapport à la légende, nous considérons cela comme un indicateur d'un foil valide.\nSi une NLI prévient le foil à engendrer par la légende, cela ne peut pas être un bon foil, car par transitivité, cela donnera une description véridique de l'image et nous filtrons ces foils.\nMais cette procédure n'est pas parfaite, il s'agit simplement d'un indicateur pour les foils valides.\nPar conséquent, comme troisième mesure pour produire des foils valides, nous utilisons des annotateurs humains pour valider les données utilisées dans VALSE.\nDonc, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrit dans ce tableau.\nNotez que VALSE ne fournit pas de données de formation mais seulement des données de test.\nComme il s'agit uniquement d'un indicateur de référence de test zero shot, il est conçu pour tirer parti des capacités existantes des modèles de langue et de vision après la préformation.\nLe raffinement permettrait seulement aux modèles d'exploiter des artefacts ou des biais statistiques dans les données.\nEt nous savons tous que ces modèles aiment tricher et prendre des raccourcis.\nEt comme nous l'avons dit, cela nous intéresse d'évaluer les capacités des modèles de langue et de vision après la préformation.\nNous expérimentons avec cinq modèles de langue et de vision sur VALSE, à savoir avec CLIP, LXMert, ViLBERT, ViLBERT douze en un, et VisualBERT.\nDeux de nos indicateurs d'évaluation les plus importants sont la précision des modèles dans la classification de paires de phrases images en légendes et foils.\nPeut-être plus pertinent pour cette vidéo, nous présenterons notre indicateur plus permissif, la précision par paire, qui mesure si le score de sentence alignment d'image est plus élevé pour la bonne paire de texte image que pour sa paire ayant subi un foil.\nPour plus d'indicateurs et de résultats, consultez notre article.\nLes résultats avec une précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres indicateurs. La meilleure performance zero shot est obtenue par ViLBERT douze en un, suivi de ViLBERT, LXMert, CLIP, et enfin VisualBERT.\nIl est remarquable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par ViLBERT douze en un, en soulignant que les modèles sont capables d'identifier des objets nommés et leur présence dans les images.\nCependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de foiling antagonistes.\nNous voyons à partir des instruments de pluralité et de comptage que les modèles de langue et de vision ont du mal à distinguer les références à des objets uniques par rapport à plusieurs, ou à les compter dans une image.\nL'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image.\nIls ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité comme nous le voyons dans l'élément d'actions.\nÀ partir de l'élément de coréférence, nous découvrons que relever plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de langue et de vision.\nPour vérifier la santé mentale, et parce que c'est une expérience intéressante, nous comparons également deux modèles à texte seulement, GPT un et GPT deux, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et ayant subi un foil, sans image ici, et en prévenant l'entrée avec la plus faible perplexité.\nSi la perplexité est plus élevée pour le foil, nous considérons cela comme une indication que la légende ayant subi un foil peut souffrir de biais de plausibilité ou d'autres biais linguistiques.\nEt il est intéressant de voir que dans certains cas, les modèles GPT à texte seulement ont capturé la plausibilité du monde mieux que les modèles de langue et de vision.\nDonc, pour résumer, VALSE est un indicateur de référence qui utilise l'objectif des constructions linguistiques pour aider la communauté à améliorer les modèles de langue et de vision en testant durement leurs capacités de bases visuelles.\nNos expériences montrent que les modèles de langue et de vision identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais luttent pour ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques.\nNous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers les bases de la langue avec des modèles de langue et de vision.\nEt plus encore, VALSE pourrait être utilisé comme une évaluation indirecte des données, car les modèles pourraient être évalués avant et après la formation ou le raffinement pour voir si des données aident les modèles à améliorer l'un des aspects testés par VALSE.\nSi vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 419, "src_audio": "/acl6060/audio/en/419.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.\nI'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.\nI'll be explaining in this order.\nFirst, I will introduce automatic release note generation that we are working on in this research.\nA release note is a technical document that summarizes the changes distributed with each release of a software product.\nThe image shows a release note for version two point six point four of the vuejs library.\nRelease notes play an important role in open source development but they're time consuming to prepare manually.\nTherefore, it would be very useful to be able to automatically generate high quality release notes.\nI will defer to two previous researches on automatic release note generation.\nThe first is a system called ARENA released in twenty fourteen.\nIt takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.\nThe most notable feature of this system is the issue extractor in the upper right corner.\nWhich must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.\nIn other words, it cannot be used for many projects on GitHub.\nThe second is Glyph, recently announced in twenty twenty.\nIt is available on the internet and can be installed via pip.\nThis system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.\nThis image is a sample usage that returns a corrective or bug fixes label.\nGlyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.\nThe performance of the text classification model is not high.\nI present two related researches, but their problems are limited applicability and scarce data resources.\nOur paper solves these two problems and automatically generates high quality release notes.\nWith a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.\nThis proposed method can be used for all English repositories.\nFor the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.\nNext, I'll describe our dataset.\nHere is an example of data.\nThe left side is a commit message and the right side is the release notes.\nRelease notes are labeled as improvements or fixes, etc.\nWe have set up a task that takes the commit messages as input and outputs a labeled release notes.\nThis can be regarded as a summarization task.\nWe have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.\nThese were set based on previous research and other factors.\nThe release note on the bottom right is extracted from the release note on the bottom left.\nAt this time, it is necessary to detect the four labels that have been set up in advance.\nBut the labels are not always consistent with each repository.\nFor example, the improvements label includes improvements, enhancements, optimizations, and so on.\nWe prepared a vocabulary list of about thirty labels for each of these notational variations.\nThis is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.\nNext is a commit message.\nCommit messages are not tied to each release.\nAs shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.\nThis is a bit tedious and it is not enough to just get a list of releases and look at the before and after.\nWe created a heuristic matching rule to get the previous and next versions.\nDataset analysis.\nIn the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.\nAlso, the average number of release notes tokens is sixty three, which is quite high for a summarization task.\nAlso, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.\nThis is due to the large number of unique class or method names found in the repository.\nNext, I will explain the proposed method.\nThe classwise extractive then abstractive summarization model consists of two neural modules.\nA classifier using BERT or CodeBERT and a generator using BART.\nFirst, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.\nThe commit messages classified as other are discarded.\nThen CEAS applies the generator to the four labeled documents independently and generates release notes for each class.\nIn this task, the direct correspondences between commit messages and release notes are not known.\nTherefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.\nWe modeled the classwise abstractive summarization approach by two different methods.\nThe first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.\nThe output texts can be divided into classwise segments based on special class-specific endpoint symbols.\nThe second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.\nOkay, let me explain the experiments.\nFive methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.\nRegarding evaluation, in some cases, release notes are output in multiple sentences.\nSince it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.\nThe BLEU is penalized when the system outputs a short sentence.\nThis penalty results in a lower BLEU value in the experiment results described next.\nFinally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.\nA higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.\nHere are the results.\nSince the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.\nCEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.\nIn particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.\nThese results indicate that CEAS and CAS are significantly affected.\nCEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.\nHigh coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.\nCAS-Multi tended to yield higher ROUGE-L than CAS-Single.\nSuggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.\nHere are an error analysis.\nCAS methods tend to output shorter sentences than human reference sentences.\nIn the figure on the right, the reference sentence has three or four sentences, while CAS has only one.\nThe reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.\nFurthermore, CAS methods cannot generate accurate release notes without additional information.\nThe top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.\nThe example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.\nFinally, a conclusion.\nWe have built a new dataset for automatic release note generation.\nWe have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.\nOur experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.\nPlease check out our dataset on GitHub.\nThank you.", "tgt_ref": "Bonjour, je m'appelle Kamezawa de l'Université de Tokyo.\nJe vais vous présenter un article intitulé RNSum : des données à grande échelle pour la génération automatique de note de version via la synthèse des journaux de validation.\nJe vais vous expliquer dans cet ordre.\nTout d'abord, je vais présenter la génération de note de version automatique sur laquelle nous travaillons dans cette recherche.\nUne note de version est un document technique qui résume les modifications distribuées à chaque version d'un produit logiciel.\nL'image montre une note de version pour la version deux point six point quatre de la bibliothèque Vuejs.\nLes notes de version jouent un rôle important dans le développement open source, mais elles mettent du temps à être préparées manuellement.\nPar conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.\nJe m'en remettrai à deux recherches antérieures sur la génération automatique de notes de version.\nLa première est un système appelé ARENA sorti en deux-mille-quatorze.\nIl faut une approche basée sur des règles, par exemple, en utilisant l'extracteur de changement pour extraire toutes les différences, les changements de bibliothèque et les changements de document à partir des différences entre les versions, et enfin, en les combinant.\nLa caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit.\nLe système de suivi des problèmes, qui doit être laissé à Jira, ne peut être appliqué qu'aux projets qui utilisent Jira.\nEn d'autres mots, il ne peut pas être utilisé pour de nombreux projets sur GitHub.\nLa seconde est Glyphe, récemment annoncée en deux-mille-vingt.\nElle est disponible sur internet et peut être installée via pip.\nCe système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'une des cinq étiquettes telles que les fonctions ou corrections de bugs pour chaque message de validation de saisie.\nCette image est un exemple d'utilisation qui renvoie une étiquette de correction ou de débogage.\nLes données de formation de Glyphe sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous.\nLes performances du modèle de classification de texte ne sont pas élevées.\nJe présente deux recherches connexes, mais leurs problèmes sont l'applicabilité limitée et les ressources de données rares.\nNotre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité.\nAvec un problème d'applicabilité limitée, nous proposons une méthode de synthèse par classe de haute qualité en utilisant uniquement des messages de validation comme saisie.\nCette méthode proposée peut être utilisée pour tous les référentiels anglais.\nPour le deuxième problème de ressources de données rares, nous avons construit nos données RNSum composées d'environ quatre-vingt-deux mille éléments de données en collectant des données à partir de référentiels GitHub publics en utilisant l'API GitHub.\nEnsuite, je vais décrire nos données.\nVoici un exemple de données.\nLe côté gauche est un message de validation et le côté droit représente les notes de version.\nLes notes de version sont étiquetées comme améliorations ou correctifs, etc.\nNous avons mis en place une tâche qui prend les messages de validation comme saisie et sorties des notes de version étiquetées.\nCela peut être considéré comme une tâche de synthèse.\nNous avons prédéfini quatre étiquettes : fonctions, améliorations, corrections de bugs, suppressions de dépréciations et modifications de rupture.\nCelles-ci ont été établies sur la base de la recherche antérieure et d'autres facteurs.\nLa note de version en bas à droite est extraite de la note de version en bas à gauche.\nÀ ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été mises en place à l'avance.\nMais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel.\nPar exemple, l'étiquette d'améliorations inclut des améliorations, des perfectionnements, des optimisations, etc.\nNous avons préparé une liste de vocabulaire d'une trentaine d'étiquettes pour chacune de ces variations de notation.\nIl s'agit de détecter la classe de note de version et de collecter le texte de la version qui suit en tant que phrase de note de version pour la classe.\nEnsuite, il y a un message de validation.\nLes messages de validation ne sont pas liés à chaque version.\nComme le montre l'image ci-dessous, si la version actuelle est la version deux point cinq à dix-neuf, nous devons identifier la version deux point cinq à dix-huit antérieure et obtenir un diff.\nC'est un peu fastidieux et il ne suffit pas d'obtenir une liste de versions et de regarder l'avant et l'après.\nNous avons créé une règle de correspondance heuristique pour obtenir les versions antérieures et suivantes.\nAnalyse des données.\nEn fin de compte, sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données ont été recueillis.\nEn outre, le nombre moyen de gages de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse.\nDe même, le nombre de gages uniques est assez grand, s'élevant à huit mille huit cent trente mille.\nCela est dû au grand nombre de noms de méthode ou de classe unique trouvés dans le référentiel.\nEnsuite, je vais expliquer la méthode proposée.\nLe modèle de synthèse extractive puis abstractive par classe se compose de deux modules neuronaux.\nUn classificateur utilisant BERT ou CodeBERT et un générateur utilisant BART.\nTout d'abord, CEAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, qui utilisent des améliorations, des corrections de bogues, des dépréciations et d'autres.\nLes messages de validation classés comme autres sont supprimés.\nEnsuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.\nDans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues.\nPar conséquent, pour former le classificateur, nous avons réaffecté les enquêtes à chaque message de validation de saisie en utilisant les dix premiers caractères de chaque message de validation.\nNous avons modélisé l'approche de synthèse abstractive par classe à travers deux méthodes différentes.\nLe premier modèle, que nous appelons CAS unique, se compose d'un réseau de six à six unique et génère un seul texte de note de version donnant un enchaînement de messages de validation de saisie.\nLes textes de sortie peuvent être divisés en segments par classe sur la base de symboles de point d'extrémité particuliers, spécifiques à la classe.\nLa seconde méthode, que nous appelons CAS multiple, se compose de quatre réseaux seq2seq différents, dont chacun correspond à l'une des classes de notes de version fixes.\nOk, je vais vous expliquer les expériences.\nCinq méthodes ont été comparées : CEAS, CAS unique, CAS multiple, regroupement et étude antérieure, Glyph.\nEn ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases.\nPuisqu'il est difficile de calculer le nombre de phrases telles qu'elles sont, elles sont combinées avec des espaces et traitées comme une seule phrase longue.\nLe BLEU est pénalisé lorsque le système produit une phrase courte.\nCette pénalité se traduit par une valeur BLEU inférieure dans les résultats de l'expérience décrits ci-dessous.\nEnfin, nous calculons également la spécificité car ROUGE et BLEU ne peuvent pas être calculés si les notes de version sont vides.\nUne spécificité plus élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées être vides.\nLes résultats sont indiqués ci-après.\nÉtant donné que les données contiennent des adresses électroniques, des valeurs hachées, etc., nous avons également évalué les données nettoyées, ce qui les exclut.\nLe CEAS et le CAS ont obtenu des scores de ROUGE-L supérieurs de plus de dix points par rapport aux bases.\nEn particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les bases a atteint plus de vingt points.\nCes résultats indiquent que le CEAS et le CAS sont considérablement touchés.\nLe CEAS a obtenu un meilleur score ROUGE-L que le CAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour former le classificateur à l'aide de pseudo-étiquettes.\nUne couverture élevée du CEAS peut être obtenue probablement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.\nLe CAS multiple tendait à produire plus de ROUGE-L que le CAS unique.\nEn suggérant qu'il est également efficace de développer indépendamment et différemment des modèles de abstractive summarization pour chaque classe de note de version.\nVoici une analyse d'erreur.\nLes méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines.\nDans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que le CAS n'en a qu'une.\nLa raison de la réticence de ce modèle est que dans les données de formation, seulement trente-trois pour cent des phrases sont présentes dans l'étiquette des fonctions et quarante pour cent dans l'étiquette des améliorations.\nEn outre, les méthodes CAS ne peuvent pas générer des notes de version précises sans informations supplémentaires.\nL'exemple en haut à droite est un exemple de message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la progression ou au problème correspondant(e).\nL'exemple ci-dessous montre que les deux messages de validation dans la saisie sont liés et doivent être combinés en une phrase, mais cela n'est pas fait.\nEnfin, passons à la conclusion.\nNous avons construit de nouvelles données pour la génération automatique de notes de version.\nNous avons également formulé une tâche consistant à saisir des messages de validation et à les résumer afin qu'ils soient applicables à tous les projets écrits en anglais.\nNos expériences montrent que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les bases.\nVeuillez consulter nos données sur GitHub.\nMerci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 420, "src_audio": "/acl6060/audio/en/420.wav", "src_ref": "Hello. My name is Asaf Harari.\nAnd I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.\nData scientists analyze data and mainly focus on the manipulating the data's existing features.\nBut sometimes, these features are limited.\nFeature generation using another data source may add substantial information.\nOur research goal is automatic tabular data enrichment using external sources' free text.\nAssume we have a tabular dataset and a knowledge base.\nWe need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.\nOur framework FeSTE is exactly this automatic process.\nSo let's see an example in a dataset fed into FeSTE.\nIn this example, the dataset is university dataset.\nWhen its goal is to classify universities into low ranking universities and high-ranking universities.\nAs knowledge base, we use Wikipedia.\nThe first phase of FeSTE is entity linking.\nWhen each entity, in this example the university name, is linked to an entity within the knowledge base.\nAnd and the text of the entities of the knowledge base is extracted and added to the dataset.\nIn this example, the text is the Wikipedia page's abstract.\nNow, we need to generate or extract features from the retrieved text.\nSo, we need to ah feature extraction phase ah which includes text analysis.\nAnd this is the main novelty of this paper and I will deep dive into it in the next slides.\nAfter the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.\nFirst generate ah features in the number of classes of the original dataset.\nIn this example, the original dataset has two classes.\nSo, FeSTE generates two new features.\nBut if the dataset has five classes, FeSTE generates five new features.\nEach feature represents the likelihood for each class.\nTo analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.\nIt is but it is not likely that we can train language models using the input datasets.\nSo a naive approach will be ah target task finetuning.\nSo, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.\nIn this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.\nReceive the language model output, which is the likelihood for each class and use as new features.\nThe problem with this approach is datasets may have few distinct entities / texts.\nIn our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.\nSo to finetune a language model over ah this dataset will be ineffective.\nBut we can use prior knowledge about pre-analyzed datasets.\nBecause FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.\nWhat we, what we suggest is to add, to add another finetuning phase.\nA preliminary multitask finetuning phase.\nWhen you finetune the language model over the n minus one datasets.\nAnd, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.\nThe state-of-the-art in multitask ah multitask finetuning called MTDNN.\nIn MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.\nSo, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.\nAnd it samples a random batch from ah from the training set.\nAnd if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.\nAnd if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.\nIn our scenario, ah tabular datasets vary in the number of classes.\nSo there are many tasks.\nMTDNN maintained number of classes, heads, output layers.\nAnd the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.\nOur approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.\nSo let's see an example.\nHere is the our input dataset which consists of entities, features, text and classes.\nAnd, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.\nOr in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.\nSo the label vector in this case stays always ah which consists always with two classes.\nAnd this is the ah algorithm for our fine, reformulated finetuning approach.\nSo let's see the full framework.\nDataset fed into FeSTE.\nAnd then ah FeSTE executes entity linking phase.\nIt ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.\nThen it reformulated the task into a pairwise sentence classification task.\nApplied the language model to the new task and the output likelihood for each class.\nAnd now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.\nThen we use the output vector of the language model as a newly generated feature in the number of classes.\nTo evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.\nAnd as knowledge base we use Wikipedia.\nWe design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.\nWe also, we also split each dataset into four folds and apply four folds cross validation.\nThen, we generate the new features and evaluate them using five evaluation classifiers.\nWe use in our experiments base BERT base architecture.\nHere are the results for our experiments.\nYou can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.\nAnd our reformulated finetuning achieves the best result, the best performance.\nWhile MTDNN achieved two percent improvement over the target dataset finetuning.\nOur approach achieved six percent improvement.\nWhen we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.\nBut our performance increased to eleven percent compared to the target task finetuning alone.\nFor summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.\nIt uses one architecture for all tasks and datasets.\nAnd it keeps the head of ah of the model.\nBut it adds reformulation phase.\nIt augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.\nThank you.", "tgt_ref": "Bonjour. Je m'appelle Asaf Harari.\nEt je vais vous présenter notre article : enrichissement de données tabulaires en quelques coups à l'aide d'architectures de transformateurs raffinées.\nLes scientifiques analysent les données et se concentrent principalement sur la manipulation des fonctions existantes des données.\nMais parfois, ces fonctions sont limitées.\nLa génération de fonctions en utilisant une autre source de données peut ajouter des informations substantielles.\nNotre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des textes libres de sources externes.\nSupposons que nous ayons des données tabulaires et une base de connaissances.\nNous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles fonctions du texte libre de la base de connaissances.\nNotre cadre FeSTE est exactement ce processus automatique.\nVoyons donc un exemple dans des données introduites dans FeSTE.\nDans cet exemple, les données sont des données universitaires.\nQuand l'objectif est de classer les universités en universités de bas rang et en universités de haut rang.\nEn tant que base de connaissances, nous utilisons Wikipédia.\nLa première phase de FeSTE est la liaison d'entités.\nLorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances.\nEt le texte des entités de la base de connaissances est extrait et ajouté aux données.\nDans cet exemple, le texte est le résumé de la page Wikipédia.\nMaintenant, nous devons générer ou extraire des fonctions à partir du texte extrait.\nNous avons donc besoin de la phase d'extraction des fonctions qui comprend l'analyse de texte.\nIl s'agit de la principale nouveauté de cet article et je vais mieux l'expliquer dans les prochaines diapositives.\nAprès la phase d'extraction des fonctions, il y a une phase de génération de fonctions lorsque nous utilisons les fonctions extraites pour générer un petit nombre de nouvelles fonctions.\nGénérez d'abord les fonctions dans le nombre de classes des données d'origine.\nDans cet exemple, les données d'origine ont deux classes.\nAinsi, FeSTE génère deux nouvelles fonctions.\nMais si les données ont cinq classes, FeSTE génère cinq nouvelles fonctions.\nChaque fonction représente la probabilité pour chaque classe.\nPour analyser le texte, nous utilisons les analyses de texte de pointe, qui sont des modèles de langue basés sur la conversion comme BERT, GPT, XLNet etc.\nMais il est peu probable que nous puissions former des modèles de langue en utilisant les données de saisie.\nAinsi, une approche naïve sera le raffinement de la tâche cible.\nDans la phase d'extraction des fonctions, nous pouvons télécharger des modèles de langue préformée et raffiner le modèle de langue sur les données cibles.\nDans cet exemple, raffiner le modèle de langue, classer le texte en classes, résumer en classes, basses ou hautes.\nRecevoir les résultats du modèle de langue, qui sont la probabilité pour chaque classe et les utiliser comme nouvelles fonctions.\nLe problème avec cette approche est que les données peuvent avoir peu d'entités / de textes distinct(e)s.\nDans notre expérience, près de la moitié des données contiennent moins de quatre cents échantillons et les plus petites données contiennent trente-cinq échantillons dans un ensemble de formation.\nDonc, pour raffiner un modèle de langue sur cela, les données seront inefficaces.\nMais nous pouvons utiliser des connaissances préalables sur des données préanalysées.\nPuisque nous appliquons FeSTE sur des données multiples, nous pouvons utiliser les données n moins un pour recueillir des informations sur les données n moins un, et utiliser ces informations lorsque nous analysons les nièmes données.\nCe que nous suggérons, c'est d'ajouter une autre phase de raffinement.\nUne phase de raffinement multitâche préliminaire.\nLorsque vous raffinez le modèle de langue sur les données n moins un.\nEt ensuite, nous exécutons une autre phase de raffinement qui est un raffinement de tâche cible, lorsque nous raffinons le modèle de langue sur les nièmes données cibles.\nLe raffinement multitâche de pointe appelé MTDNN.\nLe MTDNN maintient les têtes dans le nombre de tâches dans l'ensemble de formation.\nDonc, dans cet exemple, il y a quatre tâches dans l'ensemble de formation. Le MTDNN maintient alors quatre têtes comme vous pouvez le voir sur l'image.\nEt il échantillonne un lot aléatoire de l'ensemble de formation.\nEt si elles appartiennent à un lot aléatoire, par exemple, une seule tâche de classification de phrases, il exécute des chemins d'aller et retour à travers la première tête.\nEt si le lot aléatoire appartient à la tâche de classement par paire, il exécute un chemin d'aller et retour dans la dernière tête.\nDans notre scénario, les données tabulaires varient dans le nombre de classes.\nIl y a donc beaucoup de tâches.\nLe MTDNN a maintenu le nombre de classes, de têtes et de couches de sortie.\nEt en outre, le MTDNN doit initialiser de nouvelles têtes pour de nouvelles données avec une nouvelle tâche.\nNotre approche, appelée raffinement de reformulation de tâche, est dans notre raffinement de reformulation de tâche d'approche. Au lieu de maintenir plusieurs têtes, nous reformulons chaque donnée dans une phrase par problème de classification, étant des tâches de deux classes.\nPrenons donc un exemple.\nVoici nos données de saisie qui se composent d'entités, de fonctions, de texte et de classes.\nEt nous reformulons la tâche à partir d'une classification du texte en bas ou haut pour classer le texte, le résumé et la classe en vrai ou faux.\nOu en d'autres mots, nous avons formé le modèle de langue pour classer un résumé et une classe en résumé et classe, si le résumé appartient à la classe ou non.\nDonc dans ce cas, le vecteur d'étiquette consiste toujours en deux classes.\nEt il s'agit de l'algorithme pour notre très bonne approche de raffinement reformulée.\nVoyons le cadre complet.\nLes données sont introduites dans FeSTE.\nPuis FeSTE exécute la phase de liaison d'entités.\nIl extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia.\nEnsuite, il a reformulé la tâche en une tâche de classification des phrases par paire.\nIl a appliqué le modèle de langue à la nouvelle tâche et la probabilité de sortie pour chaque classe.\nEt maintenant, le modèle de langue est déjà raffiné sur des données n moins un en utilisant un raffinement multitâche préliminaire.\nEnsuite, nous utilisons le vecteur de sortie du modèle de langue comme une fonction nouvellement générée dans le nombre de classes.\nPour évaluer notre cadre, nous utilisons dix-sept données de classification tabulaires qui varient en taille, fonctions, équilibre, domaine et performance initiale.\nEt en tant que base de connaissances, nous utilisons Wikipédia.\nNous concevons notre expérience comme une évaluation où nous formons FeSTe sur seize données et l'appliquons à la dix-septième donnée.\nNous divisons également chaque donnée en quatre plis et appliquons une validation croisée de quatre plis.\nEnsuite, nous générons les nouvelles fonctions et les évaluons en utilisant cinq classificateurs d'évaluation.\nNous utilisons dans nos expériences l'architecture de base des Représentations d'encodeurs bidirectionnels à partir de transformateurs.\nVoici les résultats de nos expériences.\nVous pouvez voir que nous comparons notre cadre au raffinement des données cibles, au raffinement de la tâche cible et à un raffinement préliminaire MTDNN.\nEt notre raffinement reformulé atteint le meilleur résultat et la meilleure performance.\nAlors que le MTDNN a obtenu une amélioration de deux pour cent par rapport au raffinement des données cibles.\nNotre approche a obtenu une amélioration de six pour cent.\nLorsque nous regardons les petites données, nous pouvons voir que la performance du MTDNN diminue et l'amélioration de la phase de raffinement multitâche préliminaire diminue aussi à un virgule cinq pour cent.\nMais notre performance a augmenté à onze pour cent comparé au raffinement de tâche cible seul.\nPour résumer, FeSTE permet un enrichissement en quelques coups à partir de trente-cinq échantillons dans nos expériences.\nIl utilise une architecture pour toutes les tâches et données.\nEt il garde la tête du modèle.\nMais cela ajoute une phase de reformulation.\nIl augmente l'ensemble de formation et a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'introduire dans le modèle de langue et l'utiliser dans le problème de classification par paire de phrases.\nMerci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 0, "src_audio": "/acl6060/audio/en/0.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour tout le monde. Aujourd'hui, je vais vous présenter notre travail de recherche « Apprendre à raisonner par déduction » : résolution de problèmes de mots mathématiques comme extraction de relation complexe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 1, "src_audio": "/acl6060/audio/en/1.wav", "src_ref": "I'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'appelle Allan du laboratoire d'intelligence artificielle ByteDance, et ceci est un travail en commun avec Jierui Li de l'Université du Texas à Austin et Wei Lu du SUTD.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 2, "src_audio": "/acl6060/audio/en/2.wav", "src_ref": "First, I'd like to talk about our motivation for reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, j'aimerais parler de notre motivation pour le raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 3, "src_audio": "/acl6060/audio/en/3.wav", "src_ref": "So here we show an examples where multi-step reasoning is helpful.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous montrons un exemple où le raisonnement en plusieurs étapes est utile.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 4, "src_audio": "/acl6060/audio/en/4.wav", "src_ref": "So this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce chiffre est donc tiré de l'article PaLM où ils incitent à résoudre le problème de réseau dans le scénario d'apprentissage de quelques prises de vue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 5, "src_audio": "/acl6060/audio/en/5.wav", "src_ref": "So on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, sur le côté gauche, nous pouvons voir que si nous fournissons quelques exemples avec juste des questions et des réponses, nous pourrions ne pas être en mesure d'obtenir les bonnes réponses.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 6, "src_audio": "/acl6060/audio/en/6.wav", "src_ref": "But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si nous fournissons plus de description du raisonnement, le modèle est capable de prévenir la description du raisonnement, mais aussi d'effectuer ici une prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 7, "src_audio": "/acl6060/audio/en/7.wav", "src_ref": "So it is good to have interpretable multi-step reasoning as output.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est alors bon d'avoir comme résultat un raisonnement à plusieurs étapes interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 8, "src_audio": "/acl6060/audio/en/8.wav", "src_ref": "And we also think math word problem is a straightforward application to evaluate such reasoning abilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pensons également que les problèmes de mots mathématiques sont une application directe pour évaluer de telles capacités de raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 9, "src_audio": "/acl6060/audio/en/9.wav", "src_ref": "So, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 10, "src_audio": "/acl6060/audio/en/10.wav", "src_ref": "So in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans nos données, nous recevons également l‘expression mathématique qui conduit à cette réponse particulière.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 11, "src_audio": "/acl6060/audio/en/11.wav", "src_ref": "So, certain assumptions ah also apply as in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, certaines hypothèses s'appliquent également comme dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 12, "src_audio": "/acl6060/audio/en/12.wav", "src_ref": "We assume the precision of quantities are known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que la précision des quantités est connue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 13, "src_audio": "/acl6060/audio/en/13.wav", "src_ref": "And we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 14, "src_audio": "/acl6060/audio/en/14.wav", "src_ref": "Furthermore, complicated operators can be actually decomposed into these basic operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les opérateurs compliqués peuvent effectivement être décomposés en ces opérateurs de base.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 15, "src_audio": "/acl6060/audio/en/15.wav", "src_ref": "So, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, le travail antérieur dans la résolution de problèmes de mots mathématiques peut en effet être classé dans un modèle séquence à séquence et séquence à arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 16, "src_audio": "/acl6060/audio/en/16.wav", "src_ref": "So, traditional sequence to sequence model convert the expression to a specific sequence for generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle séquence à séquence traditionnel convertit donc l'expression en une séquence spécifique pour la génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 17, "src_audio": "/acl6060/audio/en/17.wav", "src_ref": "And it is pretty easy to implement and it can generalize to many different complicated problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est assez facile à mettre en œuvre et peut être généralisé à de nombreux problèmes compliqués différents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 18, "src_audio": "/acl6060/audio/en/18.wav", "src_ref": "But the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle structuré et son manque d'interopérabilité pour la prévention.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 19, "src_audio": "/acl6060/audio/en/19.wav", "src_ref": "But actually this direction is still quite popular because of um the transformer model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, cette direction est encore très populaire en raison du modèle de conversion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 20, "src_audio": "/acl6060/audio/en/20.wav", "src_ref": "So, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans les modèles à base d'arbre, nous structurons en réalité ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 21, "src_audio": "/acl6060/audio/en/21.wav", "src_ref": "So here we keep generating the operators until we reach the leaves, which are the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous continuons à produire les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 22, "src_audio": "/acl6060/audio/en/22.wav", "src_ref": "So here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, la bonne chose est qu'en fait, cela nous donne cette structure d'arbre binaire. Mais en réalité, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur et ensuite, à la fin, nous générons les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 23, "src_audio": "/acl6060/audio/en/23.wav", "src_ref": "And the second thing is that it also contains some repetitive computations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que cela contient également des calculs répétitifs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 24, "src_audio": "/acl6060/audio/en/24.wav", "src_ref": "So here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, si nous regardons cette expression, huit fois trois plus trois est en fait généré deux fois, mais en réalité, nous devrions réutiliser les résultats.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 25, "src_audio": "/acl6060/audio/en/25.wav", "src_ref": "So, in our proposed approach we want to solve those problems in a step by step and interpretable manners.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 26, "src_audio": "/acl6060/audio/en/26.wav", "src_ref": "So for example, here in the second step, ah we can obtain these divisors which is twenty seven.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, ici dans la deuxième étape, nous pouvons obtenir ces diviseurs qui sont vingt-sept.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 27, "src_audio": "/acl6060/audio/en/27.wav", "src_ref": "And we can also refer back to the original questions to find the relevant contents.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 28, "src_audio": "/acl6060/audio/en/28.wav", "src_ref": "And in these steps we obtain the divisors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et dans ces étapes, nous obtenons les diviseurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 29, "src_audio": "/acl6060/audio/en/29.wav", "src_ref": "So, ah and then at this third step we actually get the quotient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis, à cette troisième étape, nous obtenons alors le quotient.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 30, "src_audio": "/acl6060/audio/en/30.wav", "src_ref": "Alright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Et après ces trois étapes, nous pouvons alors réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, et enfin, nous pouvons obtenir les dividendes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 31, "src_audio": "/acl6060/audio/en/31.wav", "src_ref": "So, here we actually generate the whole expression directly rather than generating a single operators or quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous générons en réalité l'expression entière directement plutôt que de produire un seul opérateur ou une seule quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 32, "src_audio": "/acl6060/audio/en/32.wav", "src_ref": "So this makes the process more accurate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela rend le processus plus précis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 33, "src_audio": "/acl6060/audio/en/33.wav", "src_ref": "So, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et incluant également une certaine constante comme notre état initial.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 34, "src_audio": "/acl6060/audio/en/34.wav", "src_ref": "So, the expression is represented by e i j o p.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, l'expression est représentée par e i j o p.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 35, "src_audio": "/acl6060/audio/en/35.wav", "src_ref": "Where we perform operator from q_i to q_j, and such expression is actually directed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Où nous effectuons l'opérateur de q_i à q_j, et cette expression est en fait dirigée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 36, "src_audio": "/acl6060/audio/en/36.wav", "src_ref": "So, we also have subtraction with words here to represent the opposite direction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, nous avons également ici la soustraction avec des mots pour représenter la direction opposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 37, "src_audio": "/acl6060/audio/en/37.wav", "src_ref": "This is quite similar to relation extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est tout à fait similaire à l'extraction de relation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 38, "src_audio": "/acl6060/audio/en/38.wav", "src_ref": "So in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans un système déductif formel, à un pas de temps t, on applique l'opérateur entre la paire q_i et q_j, puis on obtient cette nouvelle expression.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 39, "src_audio": "/acl6060/audio/en/39.wav", "src_ref": "We add it to the next state to become a new quantity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous l'ajoutons à l'état suivant pour devenir une nouvelle quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 40, "src_audio": "/acl6060/audio/en/40.wav", "src_ref": "So, these slides actually visualize the evolution of the state where we keep adding expression to the current state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, ces diapositives visualisent en réalité l'évolution de l'état où nous continuons à ajouter de l'expression à l'état actuel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 41, "src_audio": "/acl6060/audio/en/41.wav", "src_ref": "So in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langue préformée qui peut être BERT ou Roberta, puis nous encodons la phrase et obtenons ensuite ces représentations de quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 42, "src_audio": "/acl6060/audio/en/42.wav", "src_ref": "So, once we get the quantity representations, we can start to do inference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire l'inférence.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 43, "src_audio": "/acl6060/audio/en/43.wav", "src_ref": "Here we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous montrons ici un exemple de q_1 pour obtenir la représentation pour q_2 divisée par q_2 multipliée par q_3.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 44, "src_audio": "/acl6060/audio/en/44.wav", "src_ref": "First we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous obtenons la représentation de paire, qui n'est essentiellement que l'enchaînement entre q_1 et q_2, puis nous appliquons un réseau prédictif qui est paramétré par l'opérateur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 45, "src_audio": "/acl6060/audio/en/45.wav", "src_ref": "And then finally we obtain the expression representation q_1 divided by q_2.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et enfin, nous obtenons la représentation de l'expression q_1 divisée par q_2.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 46, "src_audio": "/acl6060/audio/en/46.wav", "src_ref": "But in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, dans la pratique, au stade de l'inférence, nous pourrions également obtenir une expression incorrecte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 47, "src_audio": "/acl6060/audio/en/47.wav", "src_ref": "So, here all the possible expression is equals to three times the number of operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, toute l'expression possible est égale à trois fois le nombre d'opérateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 48, "src_audio": "/acl6060/audio/en/48.wav", "src_ref": "So the nice thing here is that we can easily add constraints to control this search this search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors, ce qui est bien ici, c'est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 49, "src_audio": "/acl6060/audio/en/49.wav", "src_ref": "For example, if this expression is not allowed, we can simply remove this expression in our search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 50, "src_audio": "/acl6060/audio/en/50.wav", "src_ref": "So in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans la deuxième étape, nous faisons la même chose, mais la seule différence est que nous avons une quantité de plus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 51, "src_audio": "/acl6060/audio/en/51.wav", "src_ref": "So this quantity come from the previous calculated expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette quantité provient donc de l'expression calculée antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 52, "src_audio": "/acl6060/audio/en/52.wav", "src_ref": "So finally we can obtain this final expression q_3 times q_4.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous pouvons obtenir cette expression finale q_3 multipliée par q_4.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 53, "src_audio": "/acl6060/audio/en/53.wav", "src_ref": "And we can also see the number of all the possible ah expression is different from the previous step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 54, "src_audio": "/acl6060/audio/en/54.wav", "src_ref": "So, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une telle différence rend difficile l'application de la beam search car la distribution de probabilité entre ces deux étapes est déséquilibrée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 55, "src_audio": "/acl6060/audio/en/55.wav", "src_ref": "So the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La procédure de formation est donc similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 56, "src_audio": "/acl6060/audio/en/56.wav", "src_ref": "And here we also use this tau to represent when we should terminate this generation process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous utilisons également ce tau pour représenter le moment où nous devrions mettre fin à ce processus de génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 57, "src_audio": "/acl6060/audio/en/57.wav", "src_ref": "And here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, l'espace est différent de séquence à séquence car il est différent à chaque pas de temps, alors que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 58, "src_audio": "/acl6060/audio/en/58.wav", "src_ref": "And it also allows us to impose certain constraints from prior from prior knowledge.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 59, "src_audio": "/acl6060/audio/en/59.wav", "src_ref": "So we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous menons donc des expériences sur les données de problèmes de mots mathématiques, MAWPS, Math23K, MathQA et SVAMP couramment utilisées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 60, "src_audio": "/acl6060/audio/en/60.wav", "src_ref": "And here we briefly show the results compared with the previous best approaches.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons brièvement les résultats comparés aux meilleures approches antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 61, "src_audio": "/acl6060/audio/en/61.wav", "src_ref": "So our best performing variant is Roberta-DeductiveReasoner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, notre variante la plus performante est Roberta-DeuctiveReasoner.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 62, "src_audio": "/acl6060/audio/en/62.wav", "src_ref": "And in fact we do not use beam search, in contrast all previous approaches are using beam search.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en réalité, nous n'utilisons pas la beam search ; au contraire, toutes les approches antérieures utilisent la beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 63, "src_audio": "/acl6060/audio/en/63.wav", "src_ref": "All right. So, the best approaches are often tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Ainsi, les meilleures approches sont souvent un modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 64, "src_audio": "/acl6060/audio/en/64.wav", "src_ref": "So, overall our reasoner is able to significantl significantly outperform this tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans l'ensemble, notre raisonneur est capable de dépasser significativement ce modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 65, "src_audio": "/acl6060/audio/en/65.wav", "src_ref": "But we can see the absolute numbers on MathQA or SVAMP are not really high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons voir que les nombres absolus sur MathQA ou SVAMP ne sont pas vraiment élevés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 66, "src_audio": "/acl6060/audio/en/66.wav", "src_ref": "So we further investigate the results on SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous étudions donc plus en détail les résultats sur SVAMP.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 67, "src_audio": "/acl6060/audio/en/67.wav", "src_ref": "And this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ces données sont difficiles parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle TAL traitement automatique du langage naturel comme ajouter des informations non pertinentes et des quantités supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 68, "src_audio": "/acl6060/audio/en/68.wav", "src_ref": "So, in our prediction we find some of the intermediate values are actually negatives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans notre prévention, nous trouvons que certaines des valeurs intermédiaires sont en réalité négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 69, "src_audio": "/acl6060/audio/en/69.wav", "src_ref": "For example, um, in these questions we are asking how many apples does Jake have?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans ces questions, nous demandons « combien de pommes a Jake ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 70, "src_audio": "/acl6060/audio/en/70.wav", "src_ref": "But we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous avons des informations supplémentaires comme « dix-sept photos de moins », et « Steven en a huit », ce qui est totalement hors de propos.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 71, "src_audio": "/acl6060/audio/en/71.wav", "src_ref": "So, our model makes some prediction like this which is producing negative values.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, notre modèle effectue une prévention comme celle-ci qui produit des valeurs négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 72, "src_audio": "/acl6060/audio/en/72.wav", "src_ref": "And we observe these two expressions actually have similar scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous constatons que ces deux expressions ont en réalité des scores similaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 73, "src_audio": "/acl6060/audio/en/73.wav", "src_ref": "So, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pouvons alors limiter cet espace de recherche en supprimant les résultats négatifs afin que nous puissions rendre la réponse correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 74, "src_audio": "/acl6060/audio/en/74.wav", "src_ref": "So um we further find such constraint actually improves quite a lot for some models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous trouvons donc que cette contrainte améliore en réalité beaucoup pour certains modèles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 75, "src_audio": "/acl6060/audio/en/75.wav", "src_ref": "For example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous améliorons sept points, puis pour le modèle de base Roberta, nous avons amélioré deux points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 76, "src_audio": "/acl6060/audio/en/76.wav", "src_ref": "So better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le meilleur modèle de langue a donc de meilleures capacités de compréhension de la langue de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 77, "src_audio": "/acl6060/audio/en/77.wav", "src_ref": "And we also try to analyze the difficulty behind these behind all these datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous essayons également d'analyser la difficulté derrière toutes ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 78, "src_audio": "/acl6060/audio/en/78.wav", "src_ref": "We assume the number of unused quantities can be regarded as irrelevant information here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que le nombre de quantités inutilisées peut être considéré ici comme une information non pertinente.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 79, "src_audio": "/acl6060/audio/en/79.wav", "src_ref": "So ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et les données SVAMP ont la plus grande partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 80, "src_audio": "/acl6060/audio/en/80.wav", "src_ref": "And here we also show the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons également la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 81, "src_audio": "/acl6060/audio/en/81.wav", "src_ref": "For those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ces échantillons sans quantités inutilisées, la performance est en réalité supérieure à la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 82, "src_audio": "/acl6060/audio/en/82.wav", "src_ref": "But with those samples that with unused quantity is actually way worse than the, worse than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais avec ces échantillons ayant une quantité inutilisée, c'est en fait bien pire que la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 83, "src_audio": "/acl6060/audio/en/83.wav", "src_ref": "For MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour MAWPS, nous n'avons pas vraiment trop de cas de test, alors j'ignore simplement cette partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 84, "src_audio": "/acl6060/audio/en/84.wav", "src_ref": "So, finally we want to show the interpretability through a question perturbation example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous voulons montrer l'interopérabilité à travers un exemple de perturbation de question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 85, "src_audio": "/acl6060/audio/en/85.wav", "src_ref": "So here our model actually makes a wrong prediction at the first step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, notre modèle effectue en réalité une prévention erronée à la première étape.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 86, "src_audio": "/acl6060/audio/en/86.wav", "src_ref": "So, we can actually correlate this expression with the sentence here. Alright.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, nous pouvons effectivement corréler cette expression avec la phrase ici. Très bien.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 87, "src_audio": "/acl6060/audio/en/87.wav", "src_ref": "So, we think this sentence might be misleading the model to an incorrect predictions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pensons donc que cette phrase pourrait induire le modèle en erreur avec des préventions incorrectes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 88, "src_audio": "/acl6060/audio/en/88.wav", "src_ref": "So here planting another thirty five makes the model makes the model think it should be an addition operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, en planter trente-cinq autres fait penser au modèle qu'il devrait être un opérateur d'addition.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 89, "src_audio": "/acl6060/audio/en/89.wav", "src_ref": "So we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous essayons alors de réviser la phrase pour que cela soit quelque chose comme « le nombre de poiriers est inférieur de trente-cinq à celui des pommiers ».", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 90, "src_audio": "/acl6060/audio/en/90.wav", "src_ref": "So, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous faisons en sorte de transmettre une sémantique plus précise afin que le modèle soit capable de rendre la prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 91, "src_audio": "/acl6060/audio/en/91.wav", "src_ref": "So, this study shows how the interpretable predictions help us understand the model behavior.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, cette étude montre comment les préventions interprétables nous aident à comprendre le comportement du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 92, "src_audio": "/acl6060/audio/en/92.wav", "src_ref": "So to conclude our work, so first our model is actually pretty efficient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour conclure notre travail, notre modèle est en réalité assez efficace.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 93, "src_audio": "/acl6060/audio/en/93.wav", "src_ref": "And we are able to provide interpretable solving procedure.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous sommes en mesure de fournir une procédure de résolution interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 94, "src_audio": "/acl6060/audio/en/94.wav", "src_ref": "And we can easily incorporate some prior knowledge as constraint which can help improve the performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons facilement incorporer des connaissances antérieures en tant que contraintes qui peuvent aider à améliorer la performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 95, "src_audio": "/acl6060/audio/en/95.wav", "src_ref": "And the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution des problèmes de réseau, mais aussi aux autres tâches qui impliquent un raisonnement à plusieurs étapes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 96, "src_audio": "/acl6060/audio/en/96.wav", "src_ref": "We also have certain limitations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également certaines limites.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 97, "src_audio": "/acl6060/audio/en/97.wav", "src_ref": "Ah, if we have a large number of operators or constants, the memory consumption could be pretty high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 98, "src_audio": "/acl6060/audio/en/98.wav", "src_ref": "And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que, comme mentionné, puisque la distribution de probabilité est déséquilibrée entre les différentes étapes de temps, il est donc également assez difficile d'appliquer la stratégie de beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 99, "src_audio": "/acl6060/audio/en/99.wav", "src_ref": "So this is the end of the talk, and questions are welcomed. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous arrivons donc à la fin de la discussion, et vos questions sont les bienvenues. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 100, "src_audio": "/acl6060/audio/en/100.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 101, "src_audio": "/acl6060/audio/en/101.wav", "src_ref": "I will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter mon travail en commun avec Jerry, qui porte sur de nouvelles données pour l'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 102, "src_audio": "/acl6060/audio/en/102.wav", "src_ref": "Legal issues are an integral part of many people's lives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions juridiques font partie intégrante de la vie de nombreuses personnes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 103, "src_audio": "/acl6060/audio/en/103.wav", "src_ref": "But the majority of citizens have little to know knowledge about their rights and fundamental legal processes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais la majorité des citoyens ont peu de connaissances sur leurs droits et leurs processus juridiques fondamentaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 104, "src_audio": "/acl6060/audio/en/104.wav", "src_ref": "As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En conséquence, de nombreux citoyens vulnérables qui n'ont pas les moyens de se payer l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 105, "src_audio": "/acl6060/audio/en/105.wav", "src_ref": "All work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tous les travaux visent à combler le fossé entre les personnes et la loi en développant un système d'extraction efficace pour les articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 106, "src_audio": "/acl6060/audio/en/106.wav", "src_ref": "Such a system could provide a free professional legal help service for unskilled humans.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un tel système pourrait fournir un service d'aide juridique professionnel gratuit aux personnes non qualifiées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 107, "src_audio": "/acl6060/audio/en/107.wav", "src_ref": "Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avant de plonger dans la principale contribution de ce travail, nous allons d'abord décrire le problème d'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 108, "src_audio": "/acl6060/audio/en/108.wav", "src_ref": "Given a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Compte tenu d'une simple question sur une question juridique telle que : « qu'est-ce que je risque si je viole la confidentialité professionnelle ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 109, "src_audio": "/acl6060/audio/en/109.wav", "src_ref": "A model is required to retrieve all relevant statutory articles from a large body of legislation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un modèle est nécessaire pour extraire tous les articles de droit pertinents d'un grand corpus législatif.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 110, "src_audio": "/acl6060/audio/en/110.wav", "src_ref": "This information retrieval task comes with its own set of challenges.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette tâche d'extraction des informations comporte son propre ensemble de défis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 111, "src_audio": "/acl6060/audio/en/111.wav", "src_ref": "First, it deals with two types of language.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, elle traite de deux types de langue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 112, "src_audio": "/acl6060/audio/en/112.wav", "src_ref": "Common natural language for the questions and complex legal language for the statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La langue naturelle commune pour les questions et la langue juridique complexe pour les lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 113, "src_audio": "/acl6060/audio/en/113.wav", "src_ref": "This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette différence dans les distributions de langue rend plus difficile pour un système d'extraire les candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 114, "src_audio": "/acl6060/audio/en/114.wav", "src_ref": "Besides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le droit écrit n'est pas une pile d'articles indépendants qui peuvent être traités comme une source complète d'informations par eux-mêmes, contrairement aux nouvelles ou aux recettes, par exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 115, "src_audio": "/acl6060/audio/en/115.wav", "src_ref": "Instead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, il s'agit d'un ensemble structuré de dispositions juridiques qui n'ont un sens entier que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les domaines et sous-domaines auxquels elles appartiennent et leur place dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 116, "src_audio": "/acl6060/audio/en/116.wav", "src_ref": "Lastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, les articles de droit ne sont pas de petits paragraphes constituant généralement l'unité d'extraction typique dans la plupart des travaux d'extraction.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 117, "src_audio": "/acl6060/audio/en/117.wav", "src_ref": "Here, there are long documents that may be up to six thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ici, il y a de longs documents qui peuvent aller jusqu'à six mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 118, "src_audio": "/acl6060/audio/en/118.wav", "src_ref": "The recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les recent advances en matière de TAL traitement automatique du langage naturel ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prévention du jugement juridique ou l'examen des contrats de contacts automatisés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 119, "src_audio": "/acl6060/audio/en/119.wav", "src_ref": "But statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais l'extraction de l'article de droit est restée essentiellement inchangée en raison du manque de grandes données étiquetées de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 120, "src_audio": "/acl6060/audio/en/120.wav", "src_ref": "In this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans ce travail, nous présentons de nouvelles données centrées sur le citoyen natif français pour étudier si l'extraction des modèles peut se rapprocher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche d'extraction de l'article de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 121, "src_audio": "/acl6060/audio/en/121.wav", "src_ref": "Our Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos données d'extraction de l'article de droit belge BSARD se composent de plus de mille cent questions juridiques posées par des citoyens belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 122, "src_audio": "/acl6060/audio/en/122.wav", "src_ref": "These questions cover a wide range of topics from family, housing, money, to work and social security.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces questions couvrent un wide range de sujets allant de la famille au logement, à l'argent, au travail et à la sécurité sociale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 123, "src_audio": "/acl6060/audio/en/123.wav", "src_ref": "Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques de codes de droit belge.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 124, "src_audio": "/acl6060/audio/en/124.wav", "src_ref": "Let's now talk about how we collected this dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Parlons maintenant de la façon dont nous avons collecté ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 125, "src_audio": "/acl6060/audio/en/125.wav", "src_ref": "First, we started by compiling a large corpus of legal articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 126, "src_audio": "/acl6060/audio/en/126.wav", "src_ref": "We considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons examiné trente-deux codes belges accessibles au public et extrait tous les articles ainsi que les titres de section correspondants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 127, "src_audio": "/acl6060/audio/en/127.wav", "src_ref": "Then we gathered legal questions with references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous avons rassemblé les questions juridiques avec les références aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 128, "src_audio": "/acl6060/audio/en/128.wav", "src_ref": "To do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ce faire, nous nous associons au cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur une question juridique personnelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 129, "src_audio": "/acl6060/audio/en/129.wav", "src_ref": "We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons eu la chance d'avoir accès à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 130, "src_audio": "/acl6060/audio/en/130.wav", "src_ref": "We collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons recueilli des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 131, "src_audio": "/acl6060/audio/en/131.wav", "src_ref": "Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous avons fait passer les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avons considérés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 132, "src_audio": "/acl6060/audio/en/132.wav", "src_ref": "The remaining references were matched and converted to the corresponding article ids from our corpus.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les références restantes ont été appariées et converties aux identifiants d'article correspondants de notre corpus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 133, "src_audio": "/acl6060/audio/en/133.wav", "src_ref": "We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous nous sommes finalement retrouvés avec mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 134, "src_audio": "/acl6060/audio/en/134.wav", "src_ref": "In addition, each question comes with the main category and a concatenation of subcategories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, chaque question est accompagnée de la catégorie principale et d'un enchaînement de sous-catégories.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 135, "src_audio": "/acl6060/audio/en/135.wav", "src_ref": "And each articles comes with a concatenation of the subsequence heading in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et chaque article comporte un enchaînement de la rubrique de sous-séquence dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 136, "src_audio": "/acl6060/audio/en/136.wav", "src_ref": "This extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour des recherches futures sur l'extraction d'informations juridiques ou la classification de textes juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 137, "src_audio": "/acl6060/audio/en/137.wav", "src_ref": "Let's look at some characteristic of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jetons un coup d'œil à certaines caractéristiques de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 138, "src_audio": "/acl6060/audio/en/138.wav", "src_ref": "The questions are between five and forty four words long with a median of fourteen words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions comportent entre cinq et quarante-quatre mots avec une moyenne de quatorze mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 139, "src_audio": "/acl6060/audio/en/139.wav", "src_ref": "The articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les articles sont beaucoup plus longs avec une longueur moyenne de soixante-dix-sept mots, avec cent quarante-deux d'entre eux dépassant les mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 140, "src_audio": "/acl6060/audio/en/140.wav", "src_ref": "The lengthiest one being up to five thousand seven hundred and ninety words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 141, "src_audio": "/acl6060/audio/en/141.wav", "src_ref": "As previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme mentionné précédemment, les questions couvrent un wide range de sujets, dont environ quatre-vingt-cinq pour cent concernent la famille, le logement, l'argent ou la justice.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 142, "src_audio": "/acl6060/audio/en/142.wav", "src_ref": "While the remaining fifteen percent concern either social security, foreigners or work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 143, "src_audio": "/acl6060/audio/en/143.wav", "src_ref": "The article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'article est également très diversifié car il provient de trente-deux codes belges différents qui couvrent un grand nombre de sujets juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 144, "src_audio": "/acl6060/audio/en/144.wav", "src_ref": "Here's the total number of articles collected from each of these Belgian codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le nombre total d'articles collectés à partir de chacun de ces codes belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 145, "src_audio": "/acl6060/audio/en/145.wav", "src_ref": "Out of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sur les vingt-deux mille six cent trente-trois articles, seuls mille six cent douze sont désignés comme pertinents à au moins une question dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 146, "src_audio": "/acl6060/audio/en/146.wav", "src_ref": "And around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et environ quatre-vingts pour cent de ces articles cités proviennent du code civil, des codes judiciaires, des codes d'enquête criminelle ou des codes pénaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 147, "src_audio": "/acl6060/audio/en/147.wav", "src_ref": "Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 148, "src_audio": "/acl6060/audio/en/148.wav", "src_ref": "Which can be explained by the fact that those codes focused less on individuals and their concerns.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce qui peut s'expliquer par le fait que ces codes se concentraient moins sur les individus et leurs préoccupations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 149, "src_audio": "/acl6060/audio/en/149.wav", "src_ref": "Overall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le nombre moyen de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 150, "src_audio": "/acl6060/audio/en/150.wav", "src_ref": "Using all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En utilisant toutes les données, nous avons comparé plusieurs approches d'extraction, y compris l'architecture lexicale et dense.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 151, "src_audio": "/acl6060/audio/en/151.wav", "src_ref": "Given a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné une requête et un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de requête des poids de chacun de ces termes dans cet article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 152, "src_audio": "/acl6060/audio/en/152.wav", "src_ref": "We experiment with the standard TF-IDF and BM25 ranking functions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec les fonctions de classement TF-IDF et BM25 standard.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 153, "src_audio": "/acl6060/audio/en/153.wav", "src_ref": "The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le principal problème de ces approches est qu'elles ne peuvent extraire que les articles contenant des mots-clés présents dans la requête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 154, "src_audio": "/acl6060/audio/en/154.wav", "src_ref": "To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour surmonter cette limitation, nous expérimentons une architecture neuronale qui peut capturer les relations sémantiques entre les requêtes et l'article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 155, "src_audio": "/acl6060/audio/en/155.wav", "src_ref": "We use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons un modèle bi-encodeur qui cartographie les requêtes et les articles en représentations vecteurs denses et calculons un score de pertinence entre une paire d'articles de requête par la similitude de leurs intégrations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 156, "src_audio": "/acl6060/audio/en/156.wav", "src_ref": "These embeddings typically result from a pooling operation on the output of a word embedding model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces intégrations résultent typiquement d'une opération de pooling sur la sortie d'un modèle d'intégration de mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 157, "src_audio": "/acl6060/audio/en/157.wav", "src_ref": "First, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous étudions l'efficacité des bi-encodeurs siamois dans une configuration d'évaluation « zero shot », ce qui signifie que des modèles d'intégration de mots préformés sont appliqués immédiatement sans aucun raffinement supplémentaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 158, "src_audio": "/acl6060/audio/en/158.wav", "src_ref": "We experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec l'encodeur de texte indépendant du contexte, à savoir word2vec et fastText, et des modèles d'intégration dépendants du contexte, à savoir Roberta et plus précisément CamemBERT qui est un modèle Roberta français.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 159, "src_audio": "/acl6060/audio/en/159.wav", "src_ref": "Additionally, we train our own CamemBERT based model ah bi-encoders on our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, nous formons nos propres bi-encodeurs de modèles basés sur CamemBERT sur nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 160, "src_audio": "/acl6060/audio/en/160.wav", "src_ref": "Note that for training, we experiment with the two flavors of the bi-encoder architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que pour la formation, nous expérimentons avec les deux modèles de l'architecture bi-encodeur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 161, "src_audio": "/acl6060/audio/en/161.wav", "src_ref": "Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le siamois, qui utilise un modèle d'intégration de mots unique cartographiant la requête et l'article ensemble dans un vector space dense partagé, et le modèle à deux tours, qui utilise deux modèles d'intégration de mots indépendants encodant la requête et l'article séparément dans différents espaces d'intégration.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 162, "src_audio": "/acl6060/audio/en/162.wav", "src_ref": "We experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons la mise en commun moyenne, maximale et CLS, ainsi que le produit et cosinus pour calculer les similitudes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 163, "src_audio": "/acl6060/audio/en/163.wav", "src_ref": "Here are the result of our baseline on the test sets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le résultat de notre base sur les ensembles de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 164, "src_audio": "/acl6060/audio/en/164.wav", "src_ref": "With the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec les méthodes lexicales ci-dessus, les bi-encodeurs siamois ont évalué dans une configuration zero shot au milieu, et les bi-encodeurs raffinés ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 165, "src_audio": "/acl6060/audio/en/165.wav", "src_ref": "Overall, the finetuned bi-encoder significantly outperforms all the other baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le bi-encodeur raffiné dépasse de manière significative toutes les autres bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 166, "src_audio": "/acl6060/audio/en/166.wav", "src_ref": "The two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle à deux tours s'améliore par rapport à ses variantes siamoises lors du rappel à cent, mais fonctionne de la même manière sur les autres indicateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 167, "src_audio": "/acl6060/audio/en/167.wav", "src_ref": "Although BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que BM25 ait sous-performé le bi-encodeur formé de manière significative, ses performances indiquent qu'il s'agit toujours d'une base solide pour l'extraction spécifique au domaine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 168, "src_audio": "/acl6060/audio/en/168.wav", "src_ref": "Regarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation zero shot du bi-encodeur siamois, nous constatons que l'utilisation directe des intégrations d'un modèle CamemBERT préformé sans optimiser pour la tâche d'extraction d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 169, "src_audio": "/acl6060/audio/en/169.wav", "src_ref": "Furthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous observons que le bi-encodeur basé sur word2vec a dépassé de manière significative les modèles basés sur fastText et les Représentations d'encodeurs bidirectionnels à partir de transformateurs, suggérant que peut-être les intégrations de niveau mot préformées sont plus appropriées pour la tâche que les intégrations de niveau caractère ou sous-mot lorsqu'elles sont utilisées immédiatement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 170, "src_audio": "/acl6060/audio/en/170.wav", "src_ref": "Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que prometteurs, ces résultats suggèrent de nombreuses possibilités d'amélioration comparé à un expert juridique qualifié qui peut éventuellement extraire tous les articles pertinents à n'importe quelle question, et ainsi obtenir des scores parfaits.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 171, "src_audio": "/acl6060/audio/en/171.wav", "src_ref": "Let's conclude by discussing two limitations of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Concluons en discutant de deux limites de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 172, "src_audio": "/acl6060/audio/en/172.wav", "src_ref": "First, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Premièrement, le corpus d'article est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 173, "src_audio": "/acl6060/audio/en/173.wav", "src_ref": "During the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au cours de la construction des données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions ne se retrouvent qu'avec une fraction du nombre initial d'articles pertinents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 174, "src_audio": "/acl6060/audio/en/174.wav", "src_ref": "This information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette information implique donc que la réponse contenue dans les autres articles pertinents pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 175, "src_audio": "/acl6060/audio/en/175.wav", "src_ref": "Second, we should note that not all legal questions can be answered with statutes alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, il convient de noter qu'on ne peut pas répondre à toutes les questions juridiques uniquement par des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 176, "src_audio": "/acl6060/audio/en/176.wav", "src_ref": "For instance, the question, can I evict my tenants if they make too much noise?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, la question « puis-je expulser mes locataires s'ils font trop de bruit ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 177, "src_audio": "/acl6060/audio/en/177.wav", "src_ref": "Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle peut ne pas avoir de réponse détaillée dans le droit écrit qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 178, "src_audio": "/acl6060/audio/en/178.wav", "src_ref": "Instead, the landlord should probably rely more on case law and find precedents similar to their current situation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des antécédents similaires à sa situation actuelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 179, "src_audio": "/acl6060/audio/en/179.wav", "src_ref": "For example, the tenants makes two parties a week until two AM.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, les locataires font la fête deux fois par semaine jusqu'à deux heures du matin.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 180, "src_audio": "/acl6060/audio/en/180.wav", "src_ref": "Hence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche d'extraction de l'article de droit, et le domaine des moins adaptées reste à déterminer.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 181, "src_audio": "/acl6060/audio/en/181.wav", "src_ref": "We hope that our work sparks interest in developing practical and reliable statutory article retrieval models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous espérons que nos travaux susciteront l'intérêt pour l'élaboration de modèles d'extraction d'article de droit pratiques et fiables.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 182, "src_audio": "/acl6060/audio/en/182.wav", "src_ref": "That can help improve access to justice for all.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut aider à améliorer l'accès à la justice pour tous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 183, "src_audio": "/acl6060/audio/en/183.wav", "src_ref": "You can check out our paper, dataset and code at the following links. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez consulter notre article, nos données et notre code aux liens suivants. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 184, "src_audio": "/acl6060/audio/en/184.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE ; un indice de référence indépendant de la tâche destiné à tester les modèles de langue et de vision avec des phénomènes linguistiques spécifiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 185, "src_audio": "/acl6060/audio/en/185.wav", "src_ref": "Why did we do the trouble in setting up this benchmark?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pourquoi avons-nous pris la peine de mettre en place cet indice de référence ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 186, "src_audio": "/acl6060/audio/en/186.wav", "src_ref": "Well, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eh bien, au cours des dernières années, nous avons vu une explosion de la vision basée sur le transformateur et des modèles de langue préformés sur de grandes quantités de paires de textes d'images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 187, "src_audio": "/acl6060/audio/en/187.wav", "src_ref": "Each one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacun de ces modèles pousse les tâches de pointe en matière de vision et de langue telles que la réponse aux questions visuelles, le raisonnement de bon sens visuel, l'extraction d'images et les bases de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 188, "src_audio": "/acl6060/audio/en/188.wav", "src_ref": "So we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc reçu un message : les précisions sur ces tâches et les indices de référence spécifiques augmentent régulièrement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 189, "src_audio": "/acl6060/audio/en/189.wav", "src_ref": "But do we know what the models have actually learned?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais savons-nous ce que les modèles ont réellement appris ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 190, "src_audio": "/acl6060/audio/en/190.wav", "src_ref": "What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Qu'est-ce qu'une vision et un transformateur de langue ont compris lors de l'attribution d'un score élevé pour que cette image et cette phrase correspondent ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 191, "src_audio": "/acl6060/audio/en/191.wav", "src_ref": "And the low score for this one?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le score bas pour celle-ci ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 192, "src_audio": "/acl6060/audio/en/192.wav", "src_ref": "Do vision and language models focus on the right thing?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Est-ce que les modèles de langue et de vision se concentrent sur la bonne chose ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 193, "src_audio": "/acl6060/audio/en/193.wav", "src_ref": "Or do they focus on biases as shown by previous work?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou se concentrent-ils sur les biais comme le montre le travail antérieur ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 194, "src_audio": "/acl6060/audio/en/194.wav", "src_ref": "To shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour éclairer davantage cet aspect, nous proposons une direction plus indépendante des tâches et introduisons VALSE qui teste la sensibilité des modèles de langue et de vision à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistiques et visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 195, "src_audio": "/acl6060/audio/en/195.wav", "src_ref": "We target existence, plurality, counting, spatial relations, actions and entity coreference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 196, "src_audio": "/acl6060/audio/en/196.wav", "src_ref": "But how do we test whether the vision and language models have captured this phenomena?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais comment tester si les modèles de langue et de vision ont capturé ce phénomène ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 197, "src_audio": "/acl6060/audio/en/197.wav", "src_ref": "By foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En pratiquant le foiling sur une méthode précédemment appliquée pour les modèles de langue et de vision seulement pour des phrases nominales de Ravi Shekhar et de ses collaborateurs, et en comptant par nous dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 198, "src_audio": "/acl6060/audio/en/198.wav", "src_ref": "Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Effectuer un foiling signifie essentiellement que nous prenons la légende d'une image et produisons un foil en modifiant la légende de sorte qu'elle ne décrive plus l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 199, "src_audio": "/acl6060/audio/en/199.wav", "src_ref": "And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous apportons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous aurions trouvé plus d'une façon intéressante de créer des instances de foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 200, "src_audio": "/acl6060/audio/en/200.wav", "src_ref": "For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans le cas de l'élément d'actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié avec une action différente, et un dans lequel les acteurs sont échangés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 201, "src_audio": "/acl6060/audio/en/201.wav", "src_ref": "Counting and coreference also are pieces that have more than one instrument.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le comptage et la coréférence sont également des éléments qui ont plusieurs instruments.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 202, "src_audio": "/acl6060/audio/en/202.wav", "src_ref": "And we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous créons ces foils en nous assurant qu'ils ne décrivent pas l'image et que ce sont des phrases grammaticales et autrement valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 203, "src_audio": "/acl6060/audio/en/203.wav", "src_ref": "This is not easy to do because a foiled caption may be less likely than the original caption.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce n'est pas facile à faire car une légende ayant subi un foil peut être moins probable que la légende originale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 204, "src_audio": "/acl6060/audio/en/204.wav", "src_ref": "For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que les plantes coupent un homme qu'un homme coupe les plantes, et la grande vision et les modèles de langue pourraient capter cela.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 205, "src_audio": "/acl6060/audio/en/205.wav", "src_ref": "Therefore, to obtain valid foils, we must take action.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour obtenir des foils valides, nous devons agir.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 206, "src_audio": "/acl6060/audio/en/206.wav", "src_ref": "First, we make use of strong language models to propose foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous utilisons des modèles de langue forts pour proposer des foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 207, "src_audio": "/acl6060/audio/en/207.wav", "src_ref": "Second, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, nous utilisons l'inférence de la langue naturelle ou la NLI courte pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 208, "src_audio": "/acl6060/audio/en/208.wav", "src_ref": "To test this automatically, we apply natural language inference with the following rationale.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour tester cela automatiquement, nous appliquons l'inférence de la langue naturelle avec la justification suivante.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 209, "src_audio": "/acl6060/audio/en/209.wav", "src_ref": "We consider an image to be the premise and its caption its entailed hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous considérons qu'une image est la prémisse et sa légende son hypothèse implicite.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 210, "src_audio": "/acl6060/audio/en/210.wav", "src_ref": "In addition, we consider the caption to be the premise, and the foil is its hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous considérons la légende comme la prémisse, et le foil est son hypothèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 211, "src_audio": "/acl6060/audio/en/211.wav", "src_ref": "If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si un modèle NLI prévient que le foil est en contradiction ou neutre par rapport à la légende, nous considérons cela comme un indicateur d'un foil valide.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 212, "src_audio": "/acl6060/audio/en/212.wav", "src_ref": "If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si une NLI prévient le foil à engendrer par la légende, cela ne peut pas être un bon foil, car par transitivité, cela donnera une description véridique de l'image et nous filtrons ces foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 213, "src_audio": "/acl6060/audio/en/213.wav", "src_ref": "But this procedure is not perfect, it is just an indicator for valid foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cette procédure n'est pas parfaite, il s'agit simplement d'un indicateur pour les foils valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 214, "src_audio": "/acl6060/audio/en/214.wav", "src_ref": "Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, comme troisième mesure pour produire des foils valides, nous utilisons des annotateurs humains pour valider les données utilisées dans VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 215, "src_audio": "/acl6060/audio/en/215.wav", "src_ref": "So, after filtering and human evaluation, we have as many test instances as described in this table.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrit dans ce tableau.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 216, "src_audio": "/acl6060/audio/en/216.wav", "src_ref": "Note that VALSE does not deliver any training data but only test data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que VALSE ne fournit pas de données de formation mais seulement des données de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 217, "src_audio": "/acl6060/audio/en/217.wav", "src_ref": "Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme il s'agit uniquement d'un indicateur de référence de test zero shot, il est conçu pour tirer parti des capacités existantes des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 218, "src_audio": "/acl6060/audio/en/218.wav", "src_ref": "Finetuning would only enable models to exploit artifacts or statistical biases in the data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement permettrait seulement aux modèles d'exploiter des artefacts ou des biais statistiques dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 219, "src_audio": "/acl6060/audio/en/219.wav", "src_ref": "And we all know that these models like to cheat and take shortcuts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 220, "src_audio": "/acl6060/audio/en/220.wav", "src_ref": "And as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et comme nous l'avons dit, cela nous intéresse d'évaluer les capacités des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 221, "src_audio": "/acl6060/audio/en/221.wav", "src_ref": "We experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec cinq modèles de langue et de vision sur VALSE, à savoir avec CLIP, LXMert, ViLBERT, ViLBERT douze en un, et VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 222, "src_audio": "/acl6060/audio/en/222.wav", "src_ref": "Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deux de nos indicateurs d'évaluation les plus importants sont la précision des modèles dans la classification de paires de phrases images en légendes et foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 223, "src_audio": "/acl6060/audio/en/223.wav", "src_ref": "Perhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Peut-être plus pertinent pour cette vidéo, nous présenterons notre indicateur plus permissif, la précision par paire, qui mesure si le score de sentence alignment d'image est plus élevé pour la bonne paire de texte image que pour sa paire ayant subi un foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 224, "src_audio": "/acl6060/audio/en/224.wav", "src_ref": "For more metrics and results on them, do check out our paper.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour plus d'indicateurs et de résultats, consultez notre article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 225, "src_audio": "/acl6060/audio/en/225.wav", "src_ref": "The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats avec une précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres indicateurs. La meilleure performance zero shot est obtenue par ViLBERT douze en un, suivi de ViLBERT, LXMert, CLIP, et enfin VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 226, "src_audio": "/acl6060/audio/en/226.wav", "src_ref": "It's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est remarquable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par ViLBERT douze en un, en soulignant que les modèles sont capables d'identifier des objets nommés et leur présence dans les images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 227, "src_audio": "/acl6060/audio/en/227.wav", "src_ref": "However, none of the remaining pieces can be reliably solved in our adversarial foiling settings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de foiling antagonistes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 228, "src_audio": "/acl6060/audio/en/228.wav", "src_ref": "We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous voyons à partir des instruments de pluralité et de comptage que les modèles de langue et de vision ont du mal à distinguer les références à des objets uniques par rapport à plusieurs, ou à les compter dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 229, "src_audio": "/acl6060/audio/en/229.wav", "src_ref": "The relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 230, "src_audio": "/acl6060/audio/en/230.wav", "src_ref": "They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ils ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité comme nous le voyons dans l'élément d'actions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 231, "src_audio": "/acl6060/audio/en/231.wav", "src_ref": "From the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À partir de l'élément de coréférence, nous découvrons que relever plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 232, "src_audio": "/acl6060/audio/en/232.wav", "src_ref": "As a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour vérifier la santé mentale, et parce que c'est une expérience intéressante, nous comparons également deux modèles à texte seulement, GPT un et GPT deux, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et ayant subi un foil, sans image ici, et en prévenant l'entrée avec la plus faible perplexité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 233, "src_audio": "/acl6060/audio/en/233.wav", "src_ref": "If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si la perplexité est plus élevée pour le foil, nous considérons cela comme une indication que la légende ayant subi un foil peut souffrir de biais de plausibilité ou d'autres biais linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 234, "src_audio": "/acl6060/audio/en/234.wav", "src_ref": "And it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il est intéressant de voir que dans certains cas, les modèles GPT à texte seulement ont capturé la plausibilité du monde mieux que les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 235, "src_audio": "/acl6060/audio/en/235.wav", "src_ref": "So to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour résumer, VALSE est un indicateur de référence qui utilise l'objectif des constructions linguistiques pour aider la communauté à améliorer les modèles de langue et de vision en testant durement leurs capacités de bases visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 236, "src_audio": "/acl6060/audio/en/236.wav", "src_ref": "Our experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que les modèles de langue et de vision identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais luttent pour ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 237, "src_audio": "/acl6060/audio/en/237.wav", "src_ref": "We would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers les bases de la langue avec des modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 238, "src_audio": "/acl6060/audio/en/238.wav", "src_ref": "And even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et plus encore, VALSE pourrait être utilisé comme une évaluation indirecte des données, car les modèles pourraient être évalués avant et après la formation ou le raffinement pour voir si des données aident les modèles à améliorer l'un des aspects testés par VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 239, "src_audio": "/acl6060/audio/en/239.wav", "src_ref": "If you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 240, "src_audio": "/acl6060/audio/en/240.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Kamezawa de l'Université de Tokyo.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 241, "src_audio": "/acl6060/audio/en/241.wav", "src_ref": "I'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter un article intitulé RNSum : des données à grande échelle pour la génération automatique de note de version via la synthèse des journaux de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 242, "src_audio": "/acl6060/audio/en/242.wav", "src_ref": "I'll be explaining in this order.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous expliquer dans cet ordre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 243, "src_audio": "/acl6060/audio/en/243.wav", "src_ref": "First, I will introduce automatic release note generation that we are working on in this research.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, je vais présenter la génération de note de version automatique sur laquelle nous travaillons dans cette recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 244, "src_audio": "/acl6060/audio/en/244.wav", "src_ref": "A release note is a technical document that summarizes the changes distributed with each release of a software product.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une note de version est un document technique qui résume les modifications distribuées à chaque version d'un produit logiciel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 245, "src_audio": "/acl6060/audio/en/245.wav", "src_ref": "The image shows a release note for version two point six point four of the vuejs library.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'image montre une note de version pour la version deux point six point quatre de la bibliothèque Vuejs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 246, "src_audio": "/acl6060/audio/en/246.wav", "src_ref": "Release notes play an important role in open source development but they're time consuming to prepare manually.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version jouent un rôle important dans le développement open source, mais elles mettent du temps à être préparées manuellement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 247, "src_audio": "/acl6060/audio/en/247.wav", "src_ref": "Therefore, it would be very useful to be able to automatically generate high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 248, "src_audio": "/acl6060/audio/en/248.wav", "src_ref": "I will defer to two previous researches on automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'en remettrai à deux recherches antérieures sur la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 249, "src_audio": "/acl6060/audio/en/249.wav", "src_ref": "The first is a system called ARENA released in twenty fourteen.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première est un système appelé ARENA sorti en deux-mille-quatorze.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 250, "src_audio": "/acl6060/audio/en/250.wav", "src_ref": "It takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il faut une approche basée sur des règles, par exemple, en utilisant l'extracteur de changement pour extraire toutes les différences, les changements de bibliothèque et les changements de document à partir des différences entre les versions, et enfin, en les combinant.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 251, "src_audio": "/acl6060/audio/en/251.wav", "src_ref": "The most notable feature of this system is the issue extractor in the upper right corner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 252, "src_audio": "/acl6060/audio/en/252.wav", "src_ref": "Which must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le système de suivi des problèmes, qui doit être laissé à Jira, ne peut être appliqué qu'aux projets qui utilisent Jira.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 253, "src_audio": "/acl6060/audio/en/253.wav", "src_ref": "In other words, it cannot be used for many projects on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En d'autres mots, il ne peut pas être utilisé pour de nombreux projets sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 254, "src_audio": "/acl6060/audio/en/254.wav", "src_ref": "The second is Glyph, recently announced in twenty twenty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde est Glyphe, récemment annoncée en deux-mille-vingt.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 255, "src_audio": "/acl6060/audio/en/255.wav", "src_ref": "It is available on the internet and can be installed via pip.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle est disponible sur internet et peut être installée via pip.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 256, "src_audio": "/acl6060/audio/en/256.wav", "src_ref": "This system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'une des cinq étiquettes telles que les fonctions ou corrections de bugs pour chaque message de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 257, "src_audio": "/acl6060/audio/en/257.wav", "src_ref": "This image is a sample usage that returns a corrective or bug fixes label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette image est un exemple d'utilisation qui renvoie une étiquette de correction ou de débogage.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 258, "src_audio": "/acl6060/audio/en/258.wav", "src_ref": "Glyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données de formation de Glyphe sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 259, "src_audio": "/acl6060/audio/en/259.wav", "src_ref": "The performance of the text classification model is not high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les performances du modèle de classification de texte ne sont pas élevées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 260, "src_audio": "/acl6060/audio/en/260.wav", "src_ref": "I present two related researches, but their problems are limited applicability and scarce data resources.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je présente deux recherches connexes, mais leurs problèmes sont l'applicabilité limitée et les ressources de données rares.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 261, "src_audio": "/acl6060/audio/en/261.wav", "src_ref": "Our paper solves these two problems and automatically generates high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 262, "src_audio": "/acl6060/audio/en/262.wav", "src_ref": "With a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec un problème d'applicabilité limitée, nous proposons une méthode de synthèse par classe de haute qualité en utilisant uniquement des messages de validation comme saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 263, "src_audio": "/acl6060/audio/en/263.wav", "src_ref": "This proposed method can be used for all English repositories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette méthode proposée peut être utilisée pour tous les référentiels anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 264, "src_audio": "/acl6060/audio/en/264.wav", "src_ref": "For the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour le deuxième problème de ressources de données rares, nous avons construit nos données RNSum composées d'environ quatre-vingt-deux mille éléments de données en collectant des données à partir de référentiels GitHub publics en utilisant l'API GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 265, "src_audio": "/acl6060/audio/en/265.wav", "src_ref": "Next, I'll describe our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais décrire nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 266, "src_audio": "/acl6060/audio/en/266.wav", "src_ref": "Here is an example of data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici un exemple de données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 267, "src_audio": "/acl6060/audio/en/267.wav", "src_ref": "The left side is a commit message and the right side is the release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le côté gauche est un message de validation et le côté droit représente les notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 268, "src_audio": "/acl6060/audio/en/268.wav", "src_ref": "Release notes are labeled as improvements or fixes, etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version sont étiquetées comme améliorations ou correctifs, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 269, "src_audio": "/acl6060/audio/en/269.wav", "src_ref": "We have set up a task that takes the commit messages as input and outputs a labeled release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons mis en place une tâche qui prend les messages de validation comme saisie et sorties des notes de version étiquetées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 270, "src_audio": "/acl6060/audio/en/270.wav", "src_ref": "This can be regarded as a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut être considéré comme une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 271, "src_audio": "/acl6060/audio/en/271.wav", "src_ref": "We have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons prédéfini quatre étiquettes : fonctions, améliorations, corrections de bugs, suppressions de dépréciations et modifications de rupture.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 272, "src_audio": "/acl6060/audio/en/272.wav", "src_ref": "These were set based on previous research and other factors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Celles-ci ont été établies sur la base de la recherche antérieure et d'autres facteurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 273, "src_audio": "/acl6060/audio/en/273.wav", "src_ref": "The release note on the bottom right is extracted from the release note on the bottom left.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La note de version en bas à droite est extraite de la note de version en bas à gauche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 274, "src_audio": "/acl6060/audio/en/274.wav", "src_ref": "At this time, it is necessary to detect the four labels that have been set up in advance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été mises en place à l'avance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 275, "src_audio": "/acl6060/audio/en/275.wav", "src_ref": "But the labels are not always consistent with each repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 276, "src_audio": "/acl6060/audio/en/276.wav", "src_ref": "For example, the improvements label includes improvements, enhancements, optimizations, and so on.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, l'étiquette d'améliorations inclut des améliorations, des perfectionnements, des optimisations, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 277, "src_audio": "/acl6060/audio/en/277.wav", "src_ref": "We prepared a vocabulary list of about thirty labels for each of these notational variations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons préparé une liste de vocabulaire d'une trentaine d'étiquettes pour chacune de ces variations de notation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 278, "src_audio": "/acl6060/audio/en/278.wav", "src_ref": "This is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de détecter la classe de note de version et de collecter le texte de la version qui suit en tant que phrase de note de version pour la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 279, "src_audio": "/acl6060/audio/en/279.wav", "src_ref": "Next is a commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il y a un message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 280, "src_audio": "/acl6060/audio/en/280.wav", "src_ref": "Commit messages are not tied to each release.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation ne sont pas liés à chaque version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 281, "src_audio": "/acl6060/audio/en/281.wav", "src_ref": "As shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme le montre l'image ci-dessous, si la version actuelle est la version deux point cinq à dix-neuf, nous devons identifier la version deux point cinq à dix-huit antérieure et obtenir un diff.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 282, "src_audio": "/acl6060/audio/en/282.wav", "src_ref": "This is a bit tedious and it is not enough to just get a list of releases and look at the before and after.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est un peu fastidieux et il ne suffit pas d'obtenir une liste de versions et de regarder l'avant et l'après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 283, "src_audio": "/acl6060/audio/en/283.wav", "src_ref": "We created a heuristic matching rule to get the previous and next versions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons créé une règle de correspondance heuristique pour obtenir les versions antérieures et suivantes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 284, "src_audio": "/acl6060/audio/en/284.wav", "src_ref": "Dataset analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Analyse des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 285, "src_audio": "/acl6060/audio/en/285.wav", "src_ref": "In the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En fin de compte, sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données ont été recueillis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 286, "src_audio": "/acl6060/audio/en/286.wav", "src_ref": "Also, the average number of release notes tokens is sixty three, which is quite high for a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le nombre moyen de gages de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 287, "src_audio": "/acl6060/audio/en/287.wav", "src_ref": "Also, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De même, le nombre de gages uniques est assez grand, s'élevant à huit mille huit cent trente mille.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 288, "src_audio": "/acl6060/audio/en/288.wav", "src_ref": "This is due to the large number of unique class or method names found in the repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela est dû au grand nombre de noms de méthode ou de classe unique trouvés dans le référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 289, "src_audio": "/acl6060/audio/en/289.wav", "src_ref": "Next, I will explain the proposed method.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais expliquer la méthode proposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 290, "src_audio": "/acl6060/audio/en/290.wav", "src_ref": "The classwise extractive then abstractive summarization model consists of two neural modules.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle de synthèse extractive puis abstractive par classe se compose de deux modules neuronaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 291, "src_audio": "/acl6060/audio/en/291.wav", "src_ref": "A classifier using BERT or CodeBERT and a generator using BART.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un classificateur utilisant BERT ou CodeBERT et un générateur utilisant BART.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 292, "src_audio": "/acl6060/audio/en/292.wav", "src_ref": "First, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, CEAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, qui utilisent des améliorations, des corrections de bogues, des dépréciations et d'autres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 293, "src_audio": "/acl6060/audio/en/293.wav", "src_ref": "The commit messages classified as other are discarded.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation classés comme autres sont supprimés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 294, "src_audio": "/acl6060/audio/en/294.wav", "src_ref": "Then CEAS applies the generator to the four labeled documents independently and generates release notes for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 295, "src_audio": "/acl6060/audio/en/295.wav", "src_ref": "In this task, the direct correspondences between commit messages and release notes are not known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 296, "src_audio": "/acl6060/audio/en/296.wav", "src_ref": "Therefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour former le classificateur, nous avons réaffecté les enquêtes à chaque message de validation de saisie en utilisant les dix premiers caractères de chaque message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 297, "src_audio": "/acl6060/audio/en/297.wav", "src_ref": "We modeled the classwise abstractive summarization approach by two different methods.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons modélisé l'approche de synthèse abstractive par classe à travers deux méthodes différentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 298, "src_audio": "/acl6060/audio/en/298.wav", "src_ref": "The first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le premier modèle, que nous appelons CAS unique, se compose d'un réseau de six à six unique et génère un seul texte de note de version donnant un enchaînement de messages de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 299, "src_audio": "/acl6060/audio/en/299.wav", "src_ref": "The output texts can be divided into classwise segments based on special class-specific endpoint symbols.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les textes de sortie peuvent être divisés en segments par classe sur la base de symboles de point d'extrémité particuliers, spécifiques à la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 300, "src_audio": "/acl6060/audio/en/300.wav", "src_ref": "The second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde méthode, que nous appelons CAS multiple, se compose de quatre réseaux seq2seq différents, dont chacun correspond à l'une des classes de notes de version fixes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 301, "src_audio": "/acl6060/audio/en/301.wav", "src_ref": "Okay, let me explain the experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ok, je vais vous expliquer les expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 302, "src_audio": "/acl6060/audio/en/302.wav", "src_ref": "Five methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cinq méthodes ont été comparées : CEAS, CAS unique, CAS multiple, regroupement et étude antérieure, Glyph.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 303, "src_audio": "/acl6060/audio/en/303.wav", "src_ref": "Regarding evaluation, in some cases, release notes are output in multiple sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 304, "src_audio": "/acl6060/audio/en/304.wav", "src_ref": "Since it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisqu'il est difficile de calculer le nombre de phrases telles qu'elles sont, elles sont combinées avec des espaces et traitées comme une seule phrase longue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 305, "src_audio": "/acl6060/audio/en/305.wav", "src_ref": "The BLEU is penalized when the system outputs a short sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le BLEU est pénalisé lorsque le système produit une phrase courte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 306, "src_audio": "/acl6060/audio/en/306.wav", "src_ref": "This penalty results in a lower BLEU value in the experiment results described next.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette pénalité se traduit par une valeur BLEU inférieure dans les résultats de l'expérience décrits ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 307, "src_audio": "/acl6060/audio/en/307.wav", "src_ref": "Finally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous calculons également la spécificité car ROUGE et BLEU ne peuvent pas être calculés si les notes de version sont vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 308, "src_audio": "/acl6060/audio/en/308.wav", "src_ref": "A higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une spécificité plus élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées être vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 309, "src_audio": "/acl6060/audio/en/309.wav", "src_ref": "Here are the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats sont indiqués ci-après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 310, "src_audio": "/acl6060/audio/en/310.wav", "src_ref": "Since the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné que les données contiennent des adresses électroniques, des valeurs hachées, etc., nous avons également évalué les données nettoyées, ce qui les exclut.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 311, "src_audio": "/acl6060/audio/en/311.wav", "src_ref": "CEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS et le CAS ont obtenu des scores de ROUGE-L supérieurs de plus de dix points par rapport aux bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 312, "src_audio": "/acl6060/audio/en/312.wav", "src_ref": "In particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les bases a atteint plus de vingt points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 313, "src_audio": "/acl6060/audio/en/313.wav", "src_ref": "These results indicate that CEAS and CAS are significantly affected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces résultats indiquent que le CEAS et le CAS sont considérablement touchés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 314, "src_audio": "/acl6060/audio/en/314.wav", "src_ref": "CEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS a obtenu un meilleur score ROUGE-L que le CAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour former le classificateur à l'aide de pseudo-étiquettes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 315, "src_audio": "/acl6060/audio/en/315.wav", "src_ref": "High coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une couverture élevée du CEAS peut être obtenue probablement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 316, "src_audio": "/acl6060/audio/en/316.wav", "src_ref": "CAS-Multi tended to yield higher ROUGE-L than CAS-Single.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CAS multiple tendait à produire plus de ROUGE-L que le CAS unique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 317, "src_audio": "/acl6060/audio/en/317.wav", "src_ref": "Suggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En suggérant qu'il est également efficace de développer indépendamment et différemment des modèles de abstractive summarization pour chaque classe de note de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 318, "src_audio": "/acl6060/audio/en/318.wav", "src_ref": "Here are an error analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici une analyse d'erreur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 319, "src_audio": "/acl6060/audio/en/319.wav", "src_ref": "CAS methods tend to output shorter sentences than human reference sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 320, "src_audio": "/acl6060/audio/en/320.wav", "src_ref": "In the figure on the right, the reference sentence has three or four sentences, while CAS has only one.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que le CAS n'en a qu'une.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 321, "src_audio": "/acl6060/audio/en/321.wav", "src_ref": "The reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La raison de la réticence de ce modèle est que dans les données de formation, seulement trente-trois pour cent des phrases sont présentes dans l'étiquette des fonctions et quarante pour cent dans l'étiquette des améliorations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 322, "src_audio": "/acl6060/audio/en/322.wav", "src_ref": "Furthermore, CAS methods cannot generate accurate release notes without additional information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les méthodes CAS ne peuvent pas générer des notes de version précises sans informations supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 323, "src_audio": "/acl6060/audio/en/323.wav", "src_ref": "The top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple en haut à droite est un exemple de message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la progression ou au problème correspondant(e).", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 324, "src_audio": "/acl6060/audio/en/324.wav", "src_ref": "The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple ci-dessous montre que les deux messages de validation dans la saisie sont liés et doivent être combinés en une phrase, mais cela n'est pas fait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 325, "src_audio": "/acl6060/audio/en/325.wav", "src_ref": "Finally, a conclusion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, passons à la conclusion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 326, "src_audio": "/acl6060/audio/en/326.wav", "src_ref": "We have built a new dataset for automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons construit de nouvelles données pour la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 327, "src_audio": "/acl6060/audio/en/327.wav", "src_ref": "We have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également formulé une tâche consistant à saisir des messages de validation et à les résumer afin qu'ils soient applicables à tous les projets écrits en anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 328, "src_audio": "/acl6060/audio/en/328.wav", "src_ref": "Our experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 329, "src_audio": "/acl6060/audio/en/329.wav", "src_ref": "Please check out our dataset on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Veuillez consulter nos données sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 330, "src_audio": "/acl6060/audio/en/330.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 331, "src_audio": "/acl6060/audio/en/331.wav", "src_ref": "Hello. My name is Asaf Harari.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour. Je m'appelle Asaf Harari.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 332, "src_audio": "/acl6060/audio/en/332.wav", "src_ref": "And I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et je vais vous présenter notre article : enrichissement de données tabulaires en quelques coups à l'aide d'architectures de transformateurs raffinées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 333, "src_audio": "/acl6060/audio/en/333.wav", "src_ref": "Data scientists analyze data and mainly focus on the manipulating the data's existing features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les scientifiques analysent les données et se concentrent principalement sur la manipulation des fonctions existantes des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 334, "src_audio": "/acl6060/audio/en/334.wav", "src_ref": "But sometimes, these features are limited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais parfois, ces fonctions sont limitées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 335, "src_audio": "/acl6060/audio/en/335.wav", "src_ref": "Feature generation using another data source may add substantial information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La génération de fonctions en utilisant une autre source de données peut ajouter des informations substantielles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 336, "src_audio": "/acl6060/audio/en/336.wav", "src_ref": "Our research goal is automatic tabular data enrichment using external sources' free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des textes libres de sources externes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 337, "src_audio": "/acl6060/audio/en/337.wav", "src_ref": "Assume we have a tabular dataset and a knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Supposons que nous ayons des données tabulaires et une base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 338, "src_audio": "/acl6060/audio/en/338.wav", "src_ref": "We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles fonctions du texte libre de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 339, "src_audio": "/acl6060/audio/en/339.wav", "src_ref": "Our framework FeSTE is exactly this automatic process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre cadre FeSTE est exactement ce processus automatique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 340, "src_audio": "/acl6060/audio/en/340.wav", "src_ref": "So let's see an example in a dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons donc un exemple dans des données introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 341, "src_audio": "/acl6060/audio/en/341.wav", "src_ref": "In this example, the dataset is university dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données sont des données universitaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 342, "src_audio": "/acl6060/audio/en/342.wav", "src_ref": "When its goal is to classify universities into low ranking universities and high-ranking universities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Quand l'objectif est de classer les universités en universités de bas rang et en universités de haut rang.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 343, "src_audio": "/acl6060/audio/en/343.wav", "src_ref": "As knowledge base, we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 344, "src_audio": "/acl6060/audio/en/344.wav", "src_ref": "The first phase of FeSTE is entity linking.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première phase de FeSTE est la liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 345, "src_audio": "/acl6060/audio/en/345.wav", "src_ref": "When each entity, in this example the university name, is linked to an entity within the knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 346, "src_audio": "/acl6060/audio/en/346.wav", "src_ref": "And and the text of the entities of the knowledge base is extracted and added to the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le texte des entités de la base de connaissances est extrait et ajouté aux données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 347, "src_audio": "/acl6060/audio/en/347.wav", "src_ref": "In this example, the text is the Wikipedia page's abstract.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, le texte est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 348, "src_audio": "/acl6060/audio/en/348.wav", "src_ref": "Now, we need to generate or extract features from the retrieved text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Maintenant, nous devons générer ou extraire des fonctions à partir du texte extrait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 349, "src_audio": "/acl6060/audio/en/349.wav", "src_ref": "So, we need to ah feature extraction phase ah which includes text analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc besoin de la phase d'extraction des fonctions qui comprend l'analyse de texte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 350, "src_audio": "/acl6060/audio/en/350.wav", "src_ref": "And this is the main novelty of this paper and I will deep dive into it in the next slides.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de la principale nouveauté de cet article et je vais mieux l'expliquer dans les prochaines diapositives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 351, "src_audio": "/acl6060/audio/en/351.wav", "src_ref": "After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Après la phase d'extraction des fonctions, il y a une phase de génération de fonctions lorsque nous utilisons les fonctions extraites pour générer un petit nombre de nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 352, "src_audio": "/acl6060/audio/en/352.wav", "src_ref": "First generate ah features in the number of classes of the original dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Générez d'abord les fonctions dans le nombre de classes des données d'origine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 353, "src_audio": "/acl6060/audio/en/353.wav", "src_ref": "In this example, the original dataset has two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données d'origine ont deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 354, "src_audio": "/acl6060/audio/en/354.wav", "src_ref": "So, FeSTE generates two new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, FeSTE génère deux nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 355, "src_audio": "/acl6060/audio/en/355.wav", "src_ref": "But if the dataset has five classes, FeSTE generates five new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si les données ont cinq classes, FeSTE génère cinq nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 356, "src_audio": "/acl6060/audio/en/356.wav", "src_ref": "Each feature represents the likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chaque fonction représente la probabilité pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 357, "src_audio": "/acl6060/audio/en/357.wav", "src_ref": "To analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour analyser le texte, nous utilisons les analyses de texte de pointe, qui sont des modèles de langue basés sur la conversion comme BERT, GPT, XLNet etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 358, "src_audio": "/acl6060/audio/en/358.wav", "src_ref": "It is but it is not likely that we can train language models using the input datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais il est peu probable que nous puissions former des modèles de langue en utilisant les données de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 359, "src_audio": "/acl6060/audio/en/359.wav", "src_ref": "So a naive approach will be ah target task finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une approche naïve sera le raffinement de la tâche cible.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 360, "src_audio": "/acl6060/audio/en/360.wav", "src_ref": "So, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la phase d'extraction des fonctions, nous pouvons télécharger des modèles de langue préformée et raffiner le modèle de langue sur les données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 361, "src_audio": "/acl6060/audio/en/361.wav", "src_ref": "In this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, raffiner le modèle de langue, classer le texte en classes, résumer en classes, basses ou hautes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 362, "src_audio": "/acl6060/audio/en/362.wav", "src_ref": "Receive the language model output, which is the likelihood for each class and use as new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Recevoir les résultats du modèle de langue, qui sont la probabilité pour chaque classe et les utiliser comme nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 363, "src_audio": "/acl6060/audio/en/363.wav", "src_ref": "The problem with this approach is datasets may have few distinct entities / texts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le problème avec cette approche est que les données peuvent avoir peu d'entités / de textes distinct(e)s.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 364, "src_audio": "/acl6060/audio/en/364.wav", "src_ref": "In our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre expérience, près de la moitié des données contiennent moins de quatre cents échantillons et les plus petites données contiennent trente-cinq échantillons dans un ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 365, "src_audio": "/acl6060/audio/en/365.wav", "src_ref": "So to finetune a language model over ah this dataset will be ineffective.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour raffiner un modèle de langue sur cela, les données seront inefficaces.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 366, "src_audio": "/acl6060/audio/en/366.wav", "src_ref": "But we can use prior knowledge about pre-analyzed datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons utiliser des connaissances préalables sur des données préanalysées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 367, "src_audio": "/acl6060/audio/en/367.wav", "src_ref": "Because FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisque nous appliquons FeSTE sur des données multiples, nous pouvons utiliser les données n moins un pour recueillir des informations sur les données n moins un, et utiliser ces informations lorsque nous analysons les nièmes données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 368, "src_audio": "/acl6060/audio/en/368.wav", "src_ref": "What we, what we suggest is to add, to add another finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce que nous suggérons, c'est d'ajouter une autre phase de raffinement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 369, "src_audio": "/acl6060/audio/en/369.wav", "src_ref": "A preliminary multitask finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une phase de raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 370, "src_audio": "/acl6060/audio/en/370.wav", "src_ref": "When you finetune the language model over the n minus one datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque vous raffinez le modèle de langue sur les données n moins un.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 371, "src_audio": "/acl6060/audio/en/371.wav", "src_ref": "And, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ensuite, nous exécutons une autre phase de raffinement qui est un raffinement de tâche cible, lorsque nous raffinons le modèle de langue sur les nièmes données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 372, "src_audio": "/acl6060/audio/en/372.wav", "src_ref": "The state-of-the-art in multitask ah multitask finetuning called MTDNN.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement multitâche de pointe appelé MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 373, "src_audio": "/acl6060/audio/en/373.wav", "src_ref": "In MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN maintient les têtes dans le nombre de tâches dans l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 374, "src_audio": "/acl6060/audio/en/374.wav", "src_ref": "So, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans cet exemple, il y a quatre tâches dans l'ensemble de formation. Le MTDNN maintient alors quatre têtes comme vous pouvez le voir sur l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 375, "src_audio": "/acl6060/audio/en/375.wav", "src_ref": "And it samples a random batch from ah from the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il échantillonne un lot aléatoire de l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 376, "src_audio": "/acl6060/audio/en/376.wav", "src_ref": "And if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si elles appartiennent à un lot aléatoire, par exemple, une seule tâche de classification de phrases, il exécute des chemins d'aller et retour à travers la première tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 377, "src_audio": "/acl6060/audio/en/377.wav", "src_ref": "And if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si le lot aléatoire appartient à la tâche de classement par paire, il exécute un chemin d'aller et retour dans la dernière tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 378, "src_audio": "/acl6060/audio/en/378.wav", "src_ref": "In our scenario, ah tabular datasets vary in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre scénario, les données tabulaires varient dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 379, "src_audio": "/acl6060/audio/en/379.wav", "src_ref": "So there are many tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il y a donc beaucoup de tâches.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 380, "src_audio": "/acl6060/audio/en/380.wav", "src_ref": "MTDNN maintained number of classes, heads, output layers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN a maintenu le nombre de classes, de têtes et de couches de sortie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 381, "src_audio": "/acl6060/audio/en/381.wav", "src_ref": "And the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en outre, le MTDNN doit initialiser de nouvelles têtes pour de nouvelles données avec une nouvelle tâche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 382, "src_audio": "/acl6060/audio/en/382.wav", "src_ref": "Our approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche, appelée raffinement de reformulation de tâche, est dans notre raffinement de reformulation de tâche d'approche. Au lieu de maintenir plusieurs têtes, nous reformulons chaque donnée dans une phrase par problème de classification, étant des tâches de deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 383, "src_audio": "/acl6060/audio/en/383.wav", "src_ref": "So let's see an example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Prenons donc un exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 384, "src_audio": "/acl6060/audio/en/384.wav", "src_ref": "Here is the our input dataset which consists of entities, features, text and classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici nos données de saisie qui se composent d'entités, de fonctions, de texte et de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 385, "src_audio": "/acl6060/audio/en/385.wav", "src_ref": "And, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous reformulons la tâche à partir d'une classification du texte en bas ou haut pour classer le texte, le résumé et la classe en vrai ou faux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 386, "src_audio": "/acl6060/audio/en/386.wav", "src_ref": "Or in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou en d'autres mots, nous avons formé le modèle de langue pour classer un résumé et une classe en résumé et classe, si le résumé appartient à la classe ou non.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 387, "src_audio": "/acl6060/audio/en/387.wav", "src_ref": "So the label vector in this case stays always ah which consists always with two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc dans ce cas, le vecteur d'étiquette consiste toujours en deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 388, "src_audio": "/acl6060/audio/en/388.wav", "src_ref": "And this is the ah algorithm for our fine, reformulated finetuning approach.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il s'agit de l'algorithme pour notre très bonne approche de raffinement reformulée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 389, "src_audio": "/acl6060/audio/en/389.wav", "src_ref": "So let's see the full framework.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons le cadre complet.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 390, "src_audio": "/acl6060/audio/en/390.wav", "src_ref": "Dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données sont introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 391, "src_audio": "/acl6060/audio/en/391.wav", "src_ref": "And then ah FeSTE executes entity linking phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis FeSTE exécute la phase de liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 392, "src_audio": "/acl6060/audio/en/392.wav", "src_ref": "It ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 393, "src_audio": "/acl6060/audio/en/393.wav", "src_ref": "Then it reformulated the task into a pairwise sentence classification task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il a reformulé la tâche en une tâche de classification des phrases par paire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 394, "src_audio": "/acl6060/audio/en/394.wav", "src_ref": "Applied the language model to the new task and the output likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il a appliqué le modèle de langue à la nouvelle tâche et la probabilité de sortie pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 395, "src_audio": "/acl6060/audio/en/395.wav", "src_ref": "And now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et maintenant, le modèle de langue est déjà raffiné sur des données n moins un en utilisant un raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 396, "src_audio": "/acl6060/audio/en/396.wav", "src_ref": "Then we use the output vector of the language model as a newly generated feature in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous utilisons le vecteur de sortie du modèle de langue comme une fonction nouvellement générée dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 397, "src_audio": "/acl6060/audio/en/397.wav", "src_ref": "To evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour évaluer notre cadre, nous utilisons dix-sept données de classification tabulaires qui varient en taille, fonctions, équilibre, domaine et performance initiale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 398, "src_audio": "/acl6060/audio/en/398.wav", "src_ref": "And as knowledge base we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 399, "src_audio": "/acl6060/audio/en/399.wav", "src_ref": "We design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous concevons notre expérience comme une évaluation où nous formons FeSTe sur seize données et l'appliquons à la dix-septième donnée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 400, "src_audio": "/acl6060/audio/en/400.wav", "src_ref": "We also, we also split each dataset into four folds and apply four folds cross validation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous divisons également chaque donnée en quatre plis et appliquons une validation croisée de quatre plis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 401, "src_audio": "/acl6060/audio/en/401.wav", "src_ref": "Then, we generate the new features and evaluate them using five evaluation classifiers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous générons les nouvelles fonctions et les évaluons en utilisant cinq classificateurs d'évaluation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 402, "src_audio": "/acl6060/audio/en/402.wav", "src_ref": "We use in our experiments base BERT base architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons dans nos expériences l'architecture de base des Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 403, "src_audio": "/acl6060/audio/en/403.wav", "src_ref": "Here are the results for our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici les résultats de nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 404, "src_audio": "/acl6060/audio/en/404.wav", "src_ref": "You can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez voir que nous comparons notre cadre au raffinement des données cibles, au raffinement de la tâche cible et à un raffinement préliminaire MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 405, "src_audio": "/acl6060/audio/en/405.wav", "src_ref": "And our reformulated finetuning achieves the best result, the best performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et notre raffinement reformulé atteint le meilleur résultat et la meilleure performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 406, "src_audio": "/acl6060/audio/en/406.wav", "src_ref": "While MTDNN achieved two percent improvement over the target dataset finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que le MTDNN a obtenu une amélioration de deux pour cent par rapport au raffinement des données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 407, "src_audio": "/acl6060/audio/en/407.wav", "src_ref": "Our approach achieved six percent improvement.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche a obtenu une amélioration de six pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 408, "src_audio": "/acl6060/audio/en/408.wav", "src_ref": "When we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque nous regardons les petites données, nous pouvons voir que la performance du MTDNN diminue et l'amélioration de la phase de raffinement multitâche préliminaire diminue aussi à un virgule cinq pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 409, "src_audio": "/acl6060/audio/en/409.wav", "src_ref": "But our performance increased to eleven percent compared to the target task finetuning alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais notre performance a augmenté à onze pour cent comparé au raffinement de tâche cible seul.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 410, "src_audio": "/acl6060/audio/en/410.wav", "src_ref": "For summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour résumer, FeSTE permet un enrichissement en quelques coups à partir de trente-cinq échantillons dans nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 411, "src_audio": "/acl6060/audio/en/411.wav", "src_ref": "It uses one architecture for all tasks and datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il utilise une architecture pour toutes les tâches et données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 412, "src_audio": "/acl6060/audio/en/412.wav", "src_ref": "And it keeps the head of ah of the model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il garde la tête du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 413, "src_audio": "/acl6060/audio/en/413.wav", "src_ref": "But it adds reformulation phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cela ajoute une phase de reformulation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 414, "src_audio": "/acl6060/audio/en/414.wav", "src_ref": "It augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il augmente l'ensemble de formation et a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'introduire dans le modèle de langue et l'utiliser dans le problème de classification par paire de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 415, "src_audio": "/acl6060/audio/en/415.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 416, "src_audio": "/acl6060/audio/en/416.wav", "src_ref": "Hello. My name is Asaf Harari.\nAnd I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.\nData scientists analyze data and mainly focus on the manipulating the data's existing features.\nBut sometimes, these features are limited.\nFeature generation using another data source may add substantial information.\nOur research goal is automatic tabular data enrichment using external sources' free text.\nAssume we have a tabular dataset and a knowledge base.\nWe need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.\nOur framework FeSTE is exactly this automatic process.\nSo let's see an example in a dataset fed into FeSTE.\nIn this example, the dataset is university dataset.\nWhen its goal is to classify universities into low ranking universities and high-ranking universities.\nAs knowledge base, we use Wikipedia.\nThe first phase of FeSTE is entity linking.\nWhen each entity, in this example the university name, is linked to an entity within the knowledge base.\nAnd and the text of the entities of the knowledge base is extracted and added to the dataset.\nIn this example, the text is the Wikipedia page's abstract.\nNow, we need to generate or extract features from the retrieved text.\nSo, we need to ah feature extraction phase ah which includes text analysis.\nAnd this is the main novelty of this paper and I will deep dive into it in the next slides.\nAfter the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.\nFirst generate ah features in the number of classes of the original dataset.\nIn this example, the original dataset has two classes.\nSo, FeSTE generates two new features.\nBut if the dataset has five classes, FeSTE generates five new features.\nEach feature represents the likelihood for each class.\nTo analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.\nIt is but it is not likely that we can train language models using the input datasets.\nSo a naive approach will be ah target task finetuning.\nSo, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.\nIn this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.\nReceive the language model output, which is the likelihood for each class and use as new features.\nThe problem with this approach is datasets may have few distinct entities / texts.\nIn our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.\nSo to finetune a language model over ah this dataset will be ineffective.\nBut we can use prior knowledge about pre-analyzed datasets.\nBecause FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.\nWhat we, what we suggest is to add, to add another finetuning phase.\nA preliminary multitask finetuning phase.\nWhen you finetune the language model over the n minus one datasets.\nAnd, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.\nThe state-of-the-art in multitask ah multitask finetuning called MTDNN.\nIn MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.\nSo, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.\nAnd it samples a random batch from ah from the training set.\nAnd if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.\nAnd if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.\nIn our scenario, ah tabular datasets vary in the number of classes.\nSo there are many tasks.\nMTDNN maintained number of classes, heads, output layers.\nAnd the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.\nOur approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.\nSo let's see an example.\nHere is the our input dataset which consists of entities, features, text and classes.\nAnd, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.\nOr in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.\nSo the label vector in this case stays always ah which consists always with two classes.\nAnd this is the ah algorithm for our fine, reformulated finetuning approach.\nSo let's see the full framework.\nDataset fed into FeSTE.\nAnd then ah FeSTE executes entity linking phase.\nIt ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.\nThen it reformulated the task into a pairwise sentence classification task.\nApplied the language model to the new task and the output likelihood for each class.\nAnd now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.\nThen we use the output vector of the language model as a newly generated feature in the number of classes.\nTo evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.\nAnd as knowledge base we use Wikipedia.\nWe design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.\nWe also, we also split each dataset into four folds and apply four folds cross validation.\nThen, we generate the new features and evaluate them using five evaluation classifiers.\nWe use in our experiments base BERT base architecture.\nHere are the results for our experiments.\nYou can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.\nAnd our reformulated finetuning achieves the best result, the best performance.\nWhile MTDNN achieved two percent improvement over the target dataset finetuning.\nOur approach achieved six percent improvement.\nWhen we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.\nBut our performance increased to eleven percent compared to the target task finetuning alone.\nFor summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.\nIt uses one architecture for all tasks and datasets.\nAnd it keeps the head of ah of the model.\nBut it adds reformulation phase.\nIt augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.\nThank you.", "tgt_ref": "Bonjour. Je m'appelle Asaf Harari.\nEt je vais vous présenter notre article : enrichissement de données tabulaires en quelques coups à l'aide d'architectures de transformateurs raffinées.\nLes scientifiques analysent les données et se concentrent principalement sur la manipulation des fonctions existantes des données.\nMais parfois, ces fonctions sont limitées.\nLa génération de fonctions en utilisant une autre source de données peut ajouter des informations substantielles.\nNotre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des textes libres de sources externes.\nSupposons que nous ayons des données tabulaires et une base de connaissances.\nNous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles fonctions du texte libre de la base de connaissances.\nNotre cadre FeSTE est exactement ce processus automatique.\nVoyons donc un exemple dans des données introduites dans FeSTE.\nDans cet exemple, les données sont des données universitaires.\nQuand l'objectif est de classer les universités en universités de bas rang et en universités de haut rang.\nEn tant que base de connaissances, nous utilisons Wikipédia.\nLa première phase de FeSTE est la liaison d'entités.\nLorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances.\nEt le texte des entités de la base de connaissances est extrait et ajouté aux données.\nDans cet exemple, le texte est le résumé de la page Wikipédia.\nMaintenant, nous devons générer ou extraire des fonctions à partir du texte extrait.\nNous avons donc besoin de la phase d'extraction des fonctions qui comprend l'analyse de texte.\nIl s'agit de la principale nouveauté de cet article et je vais mieux l'expliquer dans les prochaines diapositives.\nAprès la phase d'extraction des fonctions, il y a une phase de génération de fonctions lorsque nous utilisons les fonctions extraites pour générer un petit nombre de nouvelles fonctions.\nGénérez d'abord les fonctions dans le nombre de classes des données d'origine.\nDans cet exemple, les données d'origine ont deux classes.\nAinsi, FeSTE génère deux nouvelles fonctions.\nMais si les données ont cinq classes, FeSTE génère cinq nouvelles fonctions.\nChaque fonction représente la probabilité pour chaque classe.\nPour analyser le texte, nous utilisons les analyses de texte de pointe, qui sont des modèles de langue basés sur la conversion comme BERT, GPT, XLNet etc.\nMais il est peu probable que nous puissions former des modèles de langue en utilisant les données de saisie.\nAinsi, une approche naïve sera le raffinement de la tâche cible.\nDans la phase d'extraction des fonctions, nous pouvons télécharger des modèles de langue préformée et raffiner le modèle de langue sur les données cibles.\nDans cet exemple, raffiner le modèle de langue, classer le texte en classes, résumer en classes, basses ou hautes.\nRecevoir les résultats du modèle de langue, qui sont la probabilité pour chaque classe et les utiliser comme nouvelles fonctions.\nLe problème avec cette approche est que les données peuvent avoir peu d'entités / de textes distinct(e)s.\nDans notre expérience, près de la moitié des données contiennent moins de quatre cents échantillons et les plus petites données contiennent trente-cinq échantillons dans un ensemble de formation.\nDonc, pour raffiner un modèle de langue sur cela, les données seront inefficaces.\nMais nous pouvons utiliser des connaissances préalables sur des données préanalysées.\nPuisque nous appliquons FeSTE sur des données multiples, nous pouvons utiliser les données n moins un pour recueillir des informations sur les données n moins un, et utiliser ces informations lorsque nous analysons les nièmes données.\nCe que nous suggérons, c'est d'ajouter une autre phase de raffinement.\nUne phase de raffinement multitâche préliminaire.\nLorsque vous raffinez le modèle de langue sur les données n moins un.\nEt ensuite, nous exécutons une autre phase de raffinement qui est un raffinement de tâche cible, lorsque nous raffinons le modèle de langue sur les nièmes données cibles.\nLe raffinement multitâche de pointe appelé MTDNN.\nLe MTDNN maintient les têtes dans le nombre de tâches dans l'ensemble de formation.\nDonc, dans cet exemple, il y a quatre tâches dans l'ensemble de formation. Le MTDNN maintient alors quatre têtes comme vous pouvez le voir sur l'image.\nEt il échantillonne un lot aléatoire de l'ensemble de formation.\nEt si elles appartiennent à un lot aléatoire, par exemple, une seule tâche de classification de phrases, il exécute des chemins d'aller et retour à travers la première tête.\nEt si le lot aléatoire appartient à la tâche de classement par paire, il exécute un chemin d'aller et retour dans la dernière tête.\nDans notre scénario, les données tabulaires varient dans le nombre de classes.\nIl y a donc beaucoup de tâches.\nLe MTDNN a maintenu le nombre de classes, de têtes et de couches de sortie.\nEt en outre, le MTDNN doit initialiser de nouvelles têtes pour de nouvelles données avec une nouvelle tâche.\nNotre approche, appelée raffinement de reformulation de tâche, est dans notre raffinement de reformulation de tâche d'approche. Au lieu de maintenir plusieurs têtes, nous reformulons chaque donnée dans une phrase par problème de classification, étant des tâches de deux classes.\nPrenons donc un exemple.\nVoici nos données de saisie qui se composent d'entités, de fonctions, de texte et de classes.\nEt nous reformulons la tâche à partir d'une classification du texte en bas ou haut pour classer le texte, le résumé et la classe en vrai ou faux.\nOu en d'autres mots, nous avons formé le modèle de langue pour classer un résumé et une classe en résumé et classe, si le résumé appartient à la classe ou non.\nDonc dans ce cas, le vecteur d'étiquette consiste toujours en deux classes.\nEt il s'agit de l'algorithme pour notre très bonne approche de raffinement reformulée.\nVoyons le cadre complet.\nLes données sont introduites dans FeSTE.\nPuis FeSTE exécute la phase de liaison d'entités.\nIl extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia.\nEnsuite, il a reformulé la tâche en une tâche de classification des phrases par paire.\nIl a appliqué le modèle de langue à la nouvelle tâche et la probabilité de sortie pour chaque classe.\nEt maintenant, le modèle de langue est déjà raffiné sur des données n moins un en utilisant un raffinement multitâche préliminaire.\nEnsuite, nous utilisons le vecteur de sortie du modèle de langue comme une fonction nouvellement générée dans le nombre de classes.\nPour évaluer notre cadre, nous utilisons dix-sept données de classification tabulaires qui varient en taille, fonctions, équilibre, domaine et performance initiale.\nEt en tant que base de connaissances, nous utilisons Wikipédia.\nNous concevons notre expérience comme une évaluation où nous formons FeSTe sur seize données et l'appliquons à la dix-septième donnée.\nNous divisons également chaque donnée en quatre plis et appliquons une validation croisée de quatre plis.\nEnsuite, nous générons les nouvelles fonctions et les évaluons en utilisant cinq classificateurs d'évaluation.\nNous utilisons dans nos expériences l'architecture de base des Représentations d'encodeurs bidirectionnels à partir de transformateurs.\nVoici les résultats de nos expériences.\nVous pouvez voir que nous comparons notre cadre au raffinement des données cibles, au raffinement de la tâche cible et à un raffinement préliminaire MTDNN.\nEt notre raffinement reformulé atteint le meilleur résultat et la meilleure performance.\nAlors que le MTDNN a obtenu une amélioration de deux pour cent par rapport au raffinement des données cibles.\nNotre approche a obtenu une amélioration de six pour cent.\nLorsque nous regardons les petites données, nous pouvons voir que la performance du MTDNN diminue et l'amélioration de la phase de raffinement multitâche préliminaire diminue aussi à un virgule cinq pour cent.\nMais notre performance a augmenté à onze pour cent comparé au raffinement de tâche cible seul.\nPour résumer, FeSTE permet un enrichissement en quelques coups à partir de trente-cinq échantillons dans nos expériences.\nIl utilise une architecture pour toutes les tâches et données.\nEt il garde la tête du modèle.\nMais cela ajoute une phase de reformulation.\nIl augmente l'ensemble de formation et a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'introduire dans le modèle de langue et l'utiliser dans le problème de classification par paire de phrases.\nMerci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 417, "src_audio": "/acl6060/audio/en/417.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.\nI'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.\nFirst, I'd like to talk about our motivation for reasoning.\nSo here we show an examples where multi-step reasoning is helpful.\nSo this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.\nSo on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.\nBut if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.\nSo it is good to have interpretable multi-step reasoning as output.\nAnd we also think math word problem is a straightforward application to evaluate such reasoning abilities.\nSo, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.\nSo in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.\nSo, certain assumptions ah also apply as in previous work.\nWe assume the precision of quantities are known.\nAnd we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.\nFurthermore, complicated operators can be actually decomposed into these basic operators.\nSo, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.\nSo, traditional sequence to sequence model convert the expression to a specific sequence for generation.\nAnd it is pretty easy to implement and it can generalize to many different complicated problem.\nBut the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.\nBut actually this direction is still quite popular because of um the transformer model.\nSo, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.\nSo here we keep generating the operators until we reach the leaves, which are the quantities.\nSo here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.\nAnd the second thing is that it also contains some repetitive computations.\nSo here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.\nSo, in our proposed approach we want to solve those problems in a step by step and interpretable manners.\nSo for example, here in the second step, ah we can obtain these divisors which is twenty seven.\nAnd we can also refer back to the original questions to find the relevant contents.\nAnd in these steps we obtain the divisors.\nSo, ah and then at this third step we actually get the quotient.\nAlright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.\nSo, here we actually generate the whole expression directly rather than generating a single operators or quantities.\nSo this makes the process more accurate.\nSo, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.\nSo, the expression is represented by e i j o p.\nWhere we perform operator from q_i to q_j, and such expression is actually directed.\nSo, we also have subtraction with words here to represent the opposite direction.\nThis is quite similar to relation extraction.\nSo in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.\nWe add it to the next state to become a new quantity.\nSo, these slides actually visualize the evolution of the state where we keep adding expression to the current state.\nSo in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.\nSo, once we get the quantity representations, we can start to do inference.\nHere we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.\nFirst we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.\nAnd then finally we obtain the expression representation q_1 divided by q_2.\nBut in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.\nSo, here all the possible expression is equals to three times the number of operators.\nSo the nice thing here is that we can easily add constraints to control this search this search space.\nFor example, if this expression is not allowed, we can simply remove this expression in our search space.\nSo in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.\nSo this quantity come from the previous calculated expression.\nSo finally we can obtain this final expression q_3 times q_4.\nAnd we can also see the number of all the possible ah expression is different from the previous step.\nSo, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.\nSo the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.\nAnd here we also use this tau to represent when we should terminate this generation process.\nAnd here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.\nAnd it also allows us to impose certain constraints from prior from prior knowledge.\nSo we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.\nAnd here we briefly show the results compared with the previous best approaches.\nSo our best performing variant is Roberta-DeductiveReasoner.\nAnd in fact we do not use beam search, in contrast all previous approaches are using beam search.\nAll right. So, the best approaches are often tree based model.\nSo, overall our reasoner is able to significantl significantly outperform this tree based model.\nBut we can see the absolute numbers on MathQA or SVAMP are not really high.\nSo we further investigate the results on SVAMP.\nAnd this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.\nSo, in our prediction we find some of the intermediate values are actually negatives.\nFor example, um, in these questions we are asking how many apples does Jake have?\nBut we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.\nSo, our model makes some prediction like this which is producing negative values.\nAnd we observe these two expressions actually have similar scores.\nSo, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.\nSo um we further find such constraint actually improves quite a lot for some models.\nFor example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.\nSo better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.\nAnd we also try to analyze the difficulty behind these behind all these datasets.\nWe assume the number of unused quantities can be regarded as irrelevant information here.\nSo ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.\nAnd here we also show the overall performance.\nFor those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.\nBut with those samples that with unused quantity is actually way worse than the, worse than the overall performance.\nFor MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.\nSo, finally we want to show the interpretability through a question perturbation example.\nSo here our model actually makes a wrong prediction at the first step.\nSo, we can actually correlate this expression with the sentence here. Alright.\nSo, we think this sentence might be misleading the model to an incorrect predictions.\nSo here planting another thirty five makes the model makes the model think it should be an addition operator.\nSo we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.\nSo, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.\nSo, this study shows how the interpretable predictions help us understand the model behavior.\nSo to conclude our work, so first our model is actually pretty efficient.\nAnd we are able to provide interpretable solving procedure.\nAnd we can easily incorporate some prior knowledge as constraint which can help improve the performance.\nAnd the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.\nWe also have certain limitations.\nAh, if we have a large number of operators or constants, the memory consumption could be pretty high.\nAnd the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.\nSo this is the end of the talk, and questions are welcomed. Thank you.", "tgt_ref": "Bonjour tout le monde. Aujourd'hui, je vais vous présenter notre travail de recherche « Apprendre à raisonner par déduction » : résolution de problèmes de mots mathématiques comme extraction de relation complexe.\nJe m'appelle Allan du laboratoire d'intelligence artificielle ByteDance, et ceci est un travail en commun avec Jierui Li de l'Université du Texas à Austin et Wei Lu du SUTD.\nTout d'abord, j'aimerais parler de notre motivation pour le raisonnement.\nDonc ici, nous montrons un exemple où le raisonnement en plusieurs étapes est utile.\nCe chiffre est donc tiré de l'article PaLM où ils incitent à résoudre le problème de réseau dans le scénario d'apprentissage de quelques prises de vue.\nDonc, sur le côté gauche, nous pouvons voir que si nous fournissons quelques exemples avec juste des questions et des réponses, nous pourrions ne pas être en mesure d'obtenir les bonnes réponses.\nMais si nous fournissons plus de description du raisonnement, le modèle est capable de prévenir la description du raisonnement, mais aussi d'effectuer ici une prévention correcte.\nIl est alors bon d'avoir comme résultat un raisonnement à plusieurs étapes interprétable.\nEt nous pensons également que les problèmes de mots mathématiques sont une application directe pour évaluer de telles capacités de raisonnement.\nDonc ici, dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques.\nAinsi, dans nos données, nous recevons également l‘expression mathématique qui conduit à cette réponse particulière.\nDonc, certaines hypothèses s'appliquent également comme dans les travaux antérieurs.\nNous supposons que la précision des quantités est connue.\nEt nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle.\nEn outre, les opérateurs compliqués peuvent effectivement être décomposés en ces opérateurs de base.\nAinsi, le travail antérieur dans la résolution de problèmes de mots mathématiques peut en effet être classé dans un modèle séquence à séquence et séquence à arbre.\nLe modèle séquence à séquence traditionnel convertit donc l'expression en une séquence spécifique pour la génération.\nIl est assez facile à mettre en œuvre et peut être généralisé à de nombreux problèmes compliqués différents.\nMais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle structuré et son manque d'interopérabilité pour la prévention.\nMais en réalité, cette direction est encore très populaire en raison du modèle de conversion.\nDonc, dans les modèles à base d'arbre, nous structurons en réalité ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres.\nDonc ici, nous continuons à produire les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités.\nDonc ici, la bonne chose est qu'en fait, cela nous donne cette structure d'arbre binaire. Mais en réalité, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur et ensuite, à la fin, nous générons les quantités.\nEt la deuxième chose est que cela contient également des calculs répétitifs.\nDonc ici, si nous regardons cette expression, huit fois trois plus trois est en fait généré deux fois, mais en réalité, nous devrions réutiliser les résultats.\nAinsi, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable.\nPar exemple, ici dans la deuxième étape, nous pouvons obtenir ces diviseurs qui sont vingt-sept.\nEt nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent.\nEt dans ces étapes, nous obtenons les diviseurs.\nPuis, à cette troisième étape, nous obtenons alors le quotient.\nTrès bien. Et après ces trois étapes, nous pouvons alors réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, et enfin, nous pouvons obtenir les dividendes.\nDonc ici, nous générons en réalité l'expression entière directement plutôt que de produire un seul opérateur ou une seule quantité.\nCela rend le processus plus précis.\nAinsi, dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et incluant également une certaine constante comme notre état initial.\nAinsi, l'expression est représentée par e i j o p.\nOù nous effectuons l'opérateur de q_i à q_j, et cette expression est en fait dirigée.\nDonc, nous avons également ici la soustraction avec des mots pour représenter la direction opposée.\nC'est tout à fait similaire à l'extraction de relation.\nDonc, dans un système déductif formel, à un pas de temps t, on applique l'opérateur entre la paire q_i et q_j, puis on obtient cette nouvelle expression.\nNous l'ajoutons à l'état suivant pour devenir une nouvelle quantité.\nAinsi, ces diapositives visualisent en réalité l'évolution de l'état où nous continuons à ajouter de l'expression à l'état actuel.\nDonc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langue préformée qui peut être BERT ou Roberta, puis nous encodons la phrase et obtenons ensuite ces représentations de quantité.\nDonc, une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire l'inférence.\nNous montrons ici un exemple de q_1 pour obtenir la représentation pour q_2 divisée par q_2 multipliée par q_3.\nTout d'abord, nous obtenons la représentation de paire, qui n'est essentiellement que l'enchaînement entre q_1 et q_2, puis nous appliquons un réseau prédictif qui est paramétré par l'opérateur.\nEt enfin, nous obtenons la représentation de l'expression q_1 divisée par q_2.\nMais en réalité, dans la pratique, au stade de l'inférence, nous pourrions également obtenir une expression incorrecte.\nDonc ici, toute l'expression possible est égale à trois fois le nombre d'opérateurs.\nAlors, ce qui est bien ici, c'est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche.\nPar exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.\nDonc, dans la deuxième étape, nous faisons la même chose, mais la seule différence est que nous avons une quantité de plus.\nCette quantité provient donc de l'expression calculée antérieure.\nFinalement, nous pouvons obtenir cette expression finale q_3 multipliée par q_4.\nEt nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape antérieure.\nAinsi, une telle différence rend difficile l'application de la beam search car la distribution de probabilité entre ces deux étapes est déséquilibrée.\nLa procédure de formation est donc similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps.\nEt ici, nous utilisons également ce tau pour représenter le moment où nous devrions mettre fin à ce processus de génération.\nEt ici, l'espace est différent de séquence à séquence car il est différent à chaque pas de temps, alors que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire.\nEt cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.\nNous menons donc des expériences sur les données de problèmes de mots mathématiques, MAWPS, Math23K, MathQA et SVAMP couramment utilisées.\nEt ici, nous montrons brièvement les résultats comparés aux meilleures approches antérieures.\nDonc, notre variante la plus performante est Roberta-DeuctiveReasoner.\nEt en réalité, nous n'utilisons pas la beam search ; au contraire, toutes les approches antérieures utilisent la beam search.\nTrès bien. Ainsi, les meilleures approches sont souvent un modèle à base d'arbre.\nDonc, dans l'ensemble, notre raisonneur est capable de dépasser significativement ce modèle à base d'arbre.\nMais nous pouvons voir que les nombres absolus sur MathQA ou SVAMP ne sont pas vraiment élevés.\nNous étudions donc plus en détail les résultats sur SVAMP.\nEt ces données sont difficiles parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle TAL traitement automatique du langage naturel comme ajouter des informations non pertinentes et des quantités supplémentaires.\nDonc, dans notre prévention, nous trouvons que certaines des valeurs intermédiaires sont en réalité négatives.\nPar exemple, dans ces questions, nous demandons « combien de pommes a Jake ? »\nMais nous avons des informations supplémentaires comme « dix-sept photos de moins », et « Steven en a huit », ce qui est totalement hors de propos.\nAinsi, notre modèle effectue une prévention comme celle-ci qui produit des valeurs négatives.\nEt nous constatons que ces deux expressions ont en réalité des scores similaires.\nNous pouvons alors limiter cet espace de recherche en supprimant les résultats négatifs afin que nous puissions rendre la réponse correcte.\nNous trouvons donc que cette contrainte améliore en réalité beaucoup pour certains modèles.\nPar exemple, pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous améliorons sept points, puis pour le modèle de base Roberta, nous avons amélioré deux points.\nLe meilleur modèle de langue a donc de meilleures capacités de compréhension de la langue de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs.\nEt nous essayons également d'analyser la difficulté derrière toutes ces données.\nNous supposons que le nombre de quantités inutilisées peut être considéré ici comme une information non pertinente.\nDonc ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et les données SVAMP ont la plus grande partie.\nEt ici, nous montrons également la performance globale.\nPour ces échantillons sans quantités inutilisées, la performance est en réalité supérieure à la performance globale.\nMais avec ces échantillons ayant une quantité inutilisée, c'est en fait bien pire que la performance globale.\nPour MAWPS, nous n'avons pas vraiment trop de cas de test, alors j'ignore simplement cette partie.\nFinalement, nous voulons montrer l'interopérabilité à travers un exemple de perturbation de question.\nDonc ici, notre modèle effectue en réalité une prévention erronée à la première étape.\nAinsi, nous pouvons effectivement corréler cette expression avec la phrase ici. Très bien.\nNous pensons donc que cette phrase pourrait induire le modèle en erreur avec des préventions incorrectes.\nDonc ici, en planter trente-cinq autres fait penser au modèle qu'il devrait être un opérateur d'addition.\nNous essayons alors de réviser la phrase pour que cela soit quelque chose comme « le nombre de poiriers est inférieur de trente-cinq à celui des pommiers ».\nNous faisons en sorte de transmettre une sémantique plus précise afin que le modèle soit capable de rendre la prévention correcte.\nAinsi, cette étude montre comment les préventions interprétables nous aident à comprendre le comportement du modèle.\nDonc, pour conclure notre travail, notre modèle est en réalité assez efficace.\nEt nous sommes en mesure de fournir une procédure de résolution interprétable.\nEt nous pouvons facilement incorporer des connaissances antérieures en tant que contraintes qui peuvent aider à améliorer la performance.\nEt la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution des problèmes de réseau, mais aussi aux autres tâches qui impliquent un raisonnement à plusieurs étapes.\nNous avons également certaines limites.\nSi nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée.\nEt la deuxième chose est que, comme mentionné, puisque la distribution de probabilité est déséquilibrée entre les différentes étapes de temps, il est donc également assez difficile d'appliquer la stratégie de beam search.\nNous arrivons donc à la fin de la discussion, et vos questions sont les bienvenues. Merci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 418, "src_audio": "/acl6060/audio/en/418.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.\nI will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.\nLegal issues are an integral part of many people's lives.\nBut the majority of citizens have little to know knowledge about their rights and fundamental legal processes.\nAs a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.\nAll work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.\nSuch a system could provide a free professional legal help service for unskilled humans.\nBefore diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.\nGiven a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?\nA model is required to retrieve all relevant statutory articles from a large body of legislation.\nThis information retrieval task comes with its own set of challenges.\nFirst, it deals with two types of language.\nCommon natural language for the questions and complex legal language for the statutes.\nThis difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.\nBesides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.\nInstead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.\nLastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.\nHere, there are long documents that may be up to six thousand words.\nThe recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.\nBut statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.\nIn this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.\nOur Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.\nThese questions cover a wide range of topics from family, housing, money, to work and social security.\nEach of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.\nLet's now talk about how we collected this dataset.\nFirst, we started by compiling a large corpus of legal articles.\nWe considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.\nThen we gathered legal questions with references to relevant statutes.\nTo do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.\nWe were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.\nWe collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.\nLastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.\nThe remaining references were matched and converted to the corresponding article ids from our corpus.\nWe eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.\nIn addition, each question comes with the main category and a concatenation of subcategories.\nAnd each articles comes with a concatenation of the subsequence heading in the structure of the law.\nThis extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.\nLet's look at some characteristic of our dataset.\nThe questions are between five and forty four words long with a median of fourteen words.\nThe articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.\nThe lengthiest one being up to five thousand seven hundred and ninety words.\nAs previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.\nWhile the remaining fifteen percent concern either social security, foreigners or work.\nThe article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.\nHere's the total number of articles collected from each of these Belgian codes.\nOut of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.\nAnd around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.\nMeanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.\nWhich can be explained by the fact that those codes focused less on individuals and their concerns.\nOverall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.\nUsing all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.\nGiven a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.\nWe experiment with the standard TF-IDF and BM25 ranking functions.\nThe main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.\nTo overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.\nWe use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.\nThese embeddings typically result from a pooling operation on the output of a word embedding model.\nFirst, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.\nWe experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.\nAdditionally, we train our own CamemBERT based model ah bi-encoders on our dataset.\nNote that for training, we experiment with the two flavors of the bi-encoder architecture.\nSiamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.\nWe experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.\nHere are the result of our baseline on the test sets.\nWith the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.\nOverall, the finetuned bi-encoder significantly outperforms all the other baselines.\nThe two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.\nAlthough BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.\nRegarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.\nFurthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.\nAlthough promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.\nLet's conclude by discussing two limitations of our dataset.\nFirst, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.\nDuring the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.\nThis information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.\nSecond, we should note that not all legal questions can be answered with statutes alone.\nFor instance, the question, can I evict my tenants if they make too much noise?\nMight not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.\nInstead, the landlord should probably rely more on case law and find precedents similar to their current situation.\nFor example, the tenants makes two parties a week until two AM.\nHence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.\nWe hope that our work sparks interest in developing practical and reliable statutory article retrieval models.\nThat can help improve access to justice for all.\nYou can check out our paper, dataset and code at the following links. Thank you.", "tgt_ref": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht.\nJe vais vous présenter mon travail en commun avec Jerry, qui porte sur de nouvelles données pour l'extraction des articles de droit.\nLes questions juridiques font partie intégrante de la vie de nombreuses personnes.\nMais la majorité des citoyens ont peu de connaissances sur leurs droits et leurs processus juridiques fondamentaux.\nEn conséquence, de nombreux citoyens vulnérables qui n'ont pas les moyens de se payer l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités.\nTous les travaux visent à combler le fossé entre les personnes et la loi en développant un système d'extraction efficace pour les articles de droit.\nUn tel système pourrait fournir un service d'aide juridique professionnel gratuit aux personnes non qualifiées.\nAvant de plonger dans la principale contribution de ce travail, nous allons d'abord décrire le problème d'extraction des articles de droit.\nCompte tenu d'une simple question sur une question juridique telle que : « qu'est-ce que je risque si je viole la confidentialité professionnelle ? »\nUn modèle est nécessaire pour extraire tous les articles de droit pertinents d'un grand corpus législatif.\nCette tâche d'extraction des informations comporte son propre ensemble de défis.\nTout d'abord, elle traite de deux types de langue.\nLa langue naturelle commune pour les questions et la langue juridique complexe pour les lois.\nCette différence dans les distributions de langue rend plus difficile pour un système d'extraire les candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des lois.\nEn outre, le droit écrit n'est pas une pile d'articles indépendants qui peuvent être traités comme une source complète d'informations par eux-mêmes, contrairement aux nouvelles ou aux recettes, par exemple.\nAu lieu de cela, il s'agit d'un ensemble structuré de dispositions juridiques qui n'ont un sens entier que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les domaines et sous-domaines auxquels elles appartiennent et leur place dans la structure de la loi.\nEnfin, les articles de droit ne sont pas de petits paragraphes constituant généralement l'unité d'extraction typique dans la plupart des travaux d'extraction.\nIci, il y a de longs documents qui peuvent aller jusqu'à six mille mots.\nLes recent advances en matière de TAL traitement automatique du langage naturel ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prévention du jugement juridique ou l'examen des contrats de contacts automatisés.\nMais l'extraction de l'article de droit est restée essentiellement inchangée en raison du manque de grandes données étiquetées de haute qualité.\nDans ce travail, nous présentons de nouvelles données centrées sur le citoyen natif français pour étudier si l'extraction des modèles peut se rapprocher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche d'extraction de l'article de droit.\nNos données d'extraction de l'article de droit belge BSARD se composent de plus de mille cent questions juridiques posées par des citoyens belges.\nCes questions couvrent un wide range de sujets allant de la famille au logement, à l'argent, au travail et à la sécurité sociale.\nChacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques de codes de droit belge.\nParlons maintenant de la façon dont nous avons collecté ces données.\nTout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques.\nNous avons examiné trente-deux codes belges accessibles au public et extrait tous les articles ainsi que les titres de section correspondants.\nEnsuite, nous avons rassemblé les questions juridiques avec les références aux lois pertinentes.\nPour ce faire, nous nous associons au cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur une question juridique personnelle.\nNous avons eu la chance d'avoir accès à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges.\nNous avons recueilli des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux lois pertinentes.\nEnfin, nous avons fait passer les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avons considérés.\nLes références restantes ont été appariées et converties aux identifiants d'article correspondants de notre corpus.\nNous nous sommes finalement retrouvés avec mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de droit.\nDe plus, chaque question est accompagnée de la catégorie principale et d'un enchaînement de sous-catégories.\nEt chaque article comporte un enchaînement de la rubrique de sous-séquence dans la structure de la loi.\nCes informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour des recherches futures sur l'extraction d'informations juridiques ou la classification de textes juridiques.\nJetons un coup d'œil à certaines caractéristiques de nos données.\nLes questions comportent entre cinq et quarante-quatre mots avec une moyenne de quatorze mots.\nLes articles sont beaucoup plus longs avec une longueur moyenne de soixante-dix-sept mots, avec cent quarante-deux d'entre eux dépassant les mille mots.\nLe plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots.\nComme mentionné précédemment, les questions couvrent un wide range de sujets, dont environ quatre-vingt-cinq pour cent concernent la famille, le logement, l'argent ou la justice.\nAlors que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail.\nL'article est également très diversifié car il provient de trente-deux codes belges différents qui couvrent un grand nombre de sujets juridiques.\nVoici le nombre total d'articles collectés à partir de chacun de ces codes belges.\nSur les vingt-deux mille six cent trente-trois articles, seuls mille six cent douze sont désignés comme pertinents à au moins une question dans les données.\nEt environ quatre-vingts pour cent de ces articles cités proviennent du code civil, des codes judiciaires, des codes d'enquête criminelle ou des codes pénaux.\nPendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question.\nCe qui peut s'expliquer par le fait que ces codes se concentraient moins sur les individus et leurs préoccupations.\nDans l'ensemble, le nombre moyen de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.\nEn utilisant toutes les données, nous avons comparé plusieurs approches d'extraction, y compris l'architecture lexicale et dense.\nÉtant donné une requête et un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de requête des poids de chacun de ces termes dans cet article.\nNous expérimentons avec les fonctions de classement TF-IDF et BM25 standard.\nLe principal problème de ces approches est qu'elles ne peuvent extraire que les articles contenant des mots-clés présents dans la requête.\nPour surmonter cette limitation, nous expérimentons une architecture neuronale qui peut capturer les relations sémantiques entre les requêtes et l'article.\nNous utilisons un modèle bi-encodeur qui cartographie les requêtes et les articles en représentations vecteurs denses et calculons un score de pertinence entre une paire d'articles de requête par la similitude de leurs intégrations.\nCes intégrations résultent typiquement d'une opération de pooling sur la sortie d'un modèle d'intégration de mots.\nTout d'abord, nous étudions l'efficacité des bi-encodeurs siamois dans une configuration d'évaluation « zero shot », ce qui signifie que des modèles d'intégration de mots préformés sont appliqués immédiatement sans aucun raffinement supplémentaire.\nNous expérimentons avec l'encodeur de texte indépendant du contexte, à savoir word2vec et fastText, et des modèles d'intégration dépendants du contexte, à savoir Roberta et plus précisément CamemBERT qui est un modèle Roberta français.\nDe plus, nous formons nos propres bi-encodeurs de modèles basés sur CamemBERT sur nos données.\nNotez que pour la formation, nous expérimentons avec les deux modèles de l'architecture bi-encodeur.\nLe siamois, qui utilise un modèle d'intégration de mots unique cartographiant la requête et l'article ensemble dans un vector space dense partagé, et le modèle à deux tours, qui utilise deux modèles d'intégration de mots indépendants encodant la requête et l'article séparément dans différents espaces d'intégration.\nNous expérimentons la mise en commun moyenne, maximale et CLS, ainsi que le produit et cosinus pour calculer les similitudes.\nVoici le résultat de notre base sur les ensembles de test.\nAvec les méthodes lexicales ci-dessus, les bi-encodeurs siamois ont évalué dans une configuration zero shot au milieu, et les bi-encodeurs raffinés ci-dessous.\nDans l'ensemble, le bi-encodeur raffiné dépasse de manière significative toutes les autres bases.\nLe modèle à deux tours s'améliore par rapport à ses variantes siamoises lors du rappel à cent, mais fonctionne de la même manière sur les autres indicateurs.\nBien que BM25 ait sous-performé le bi-encodeur formé de manière significative, ses performances indiquent qu'il s'agit toujours d'une base solide pour l'extraction spécifique au domaine.\nEn ce qui concerne l'évaluation zero shot du bi-encodeur siamois, nous constatons que l'utilisation directe des intégrations d'un modèle CamemBERT préformé sans optimiser pour la tâche d'extraction d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats antérieurs.\nEn outre, nous observons que le bi-encodeur basé sur word2vec a dépassé de manière significative les modèles basés sur fastText et les Représentations d'encodeurs bidirectionnels à partir de transformateurs, suggérant que peut-être les intégrations de niveau mot préformées sont plus appropriées pour la tâche que les intégrations de niveau caractère ou sous-mot lorsqu'elles sont utilisées immédiatement.\nBien que prometteurs, ces résultats suggèrent de nombreuses possibilités d'amélioration comparé à un expert juridique qualifié qui peut éventuellement extraire tous les articles pertinents à n'importe quelle question, et ainsi obtenir des scores parfaits.\nConcluons en discutant de deux limites de nos données.\nPremièrement, le corpus d'article est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants.\nAu cours de la construction des données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions ne se retrouvent qu'avec une fraction du nombre initial d'articles pertinents.\nCette information implique donc que la réponse contenue dans les autres articles pertinents pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.\nDeuxièmement, il convient de noter qu'on ne peut pas répondre à toutes les questions juridiques uniquement par des lois.\nPar exemple, la question « puis-je expulser mes locataires s'ils font trop de bruit ? »\nElle peut ne pas avoir de réponse détaillée dans le droit écrit qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée.\nAu lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des antécédents similaires à sa situation actuelle.\nPar exemple, les locataires font la fête deux fois par semaine jusqu'à deux heures du matin.\nPar conséquent, certaines questions sont mieux adaptées que d'autres à la tâche d'extraction de l'article de droit, et le domaine des moins adaptées reste à déterminer.\nNous espérons que nos travaux susciteront l'intérêt pour l'élaboration de modèles d'extraction d'article de droit pratiques et fiables.\nCela peut aider à améliorer l'accès à la justice pour tous.\nVous pouvez consulter notre article, nos données et notre code aux liens suivants. Merci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 419, "src_audio": "/acl6060/audio/en/419.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.\nWhy did we do the trouble in setting up this benchmark?\nWell, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.\nEach one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.\nSo we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.\nBut do we know what the models have actually learned?\nWhat is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?\nAnd the low score for this one?\nDo vision and language models focus on the right thing?\nOr do they focus on biases as shown by previous work?\nTo shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.\nWe target existence, plurality, counting, spatial relations, actions and entity coreference.\nBut how do we test whether the vision and language models have captured this phenomena?\nBy foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.\nFoiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.\nAnd we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.\nFor example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.\nCounting and coreference also are pieces that have more than one instrument.\nAnd we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.\nThis is not easy to do because a foiled caption may be less likely than the original caption.\nFor example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.\nTherefore, to obtain valid foils, we must take action.\nFirst, we make use of strong language models to propose foils.\nSecond, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.\nTo test this automatically, we apply natural language inference with the following rationale.\nWe consider an image to be the premise and its caption its entailed hypothesis.\nIn addition, we consider the caption to be the premise, and the foil is its hypothesis.\nIf an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.\nIf an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.\nBut this procedure is not perfect, it is just an indicator for valid foils.\nTherefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.\nSo, after filtering and human evaluation, we have as many test instances as described in this table.\nNote that VALSE does not deliver any training data but only test data.\nSince it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.\nFinetuning would only enable models to exploit artifacts or statistical biases in the data.\nAnd we all know that these models like to cheat and take shortcuts.\nAnd as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.\nWe experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.\nTwo of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.\nPerhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.\nFor more metrics and results on them, do check out our paper.\nThe results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.\nIt's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.\nHowever, none of the remaining pieces can be reliably solved in our adversarial foiling settings.\nWe see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.\nThe relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.\nThey also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.\nFrom the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.\nAs a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.\nIf the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.\nAnd it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.\nSo to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.\nOur experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.\nWe would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.\nAnd even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.\nIf you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "tgt_ref": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE ; un indice de référence indépendant de la tâche destiné à tester les modèles de langue et de vision avec des phénomènes linguistiques spécifiques.\nPourquoi avons-nous pris la peine de mettre en place cet indice de référence ?\nEh bien, au cours des dernières années, nous avons vu une explosion de la vision basée sur le transformateur et des modèles de langue préformés sur de grandes quantités de paires de textes d'images.\nChacun de ces modèles pousse les tâches de pointe en matière de vision et de langue telles que la réponse aux questions visuelles, le raisonnement de bon sens visuel, l'extraction d'images et les bases de phrases.\nNous avons donc reçu un message : les précisions sur ces tâches et les indices de référence spécifiques augmentent régulièrement.\nMais savons-nous ce que les modèles ont réellement appris ?\nQu'est-ce qu'une vision et un transformateur de langue ont compris lors de l'attribution d'un score élevé pour que cette image et cette phrase correspondent ?\nEt le score bas pour celle-ci ?\nEst-ce que les modèles de langue et de vision se concentrent sur la bonne chose ?\nOu se concentrent-ils sur les biais comme le montre le travail antérieur ?\nPour éclairer davantage cet aspect, nous proposons une direction plus indépendante des tâches et introduisons VALSE qui teste la sensibilité des modèles de langue et de vision à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistiques et visuelles.\nNous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités.\nMais comment tester si les modèles de langue et de vision ont capturé ce phénomène ?\nEn pratiquant le foiling sur une méthode précédemment appliquée pour les modèles de langue et de vision seulement pour des phrases nominales de Ravi Shekhar et de ses collaborateurs, et en comptant par nous dans les travaux antérieurs.\nEffectuer un foiling signifie essentiellement que nous prenons la légende d'une image et produisons un foil en modifiant la légende de sorte qu'elle ne décrive plus l'image.\nEt nous apportons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous aurions trouvé plus d'une façon intéressante de créer des instances de foil.\nPar exemple, dans le cas de l'élément d'actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié avec une action différente, et un dans lequel les acteurs sont échangés.\nLe comptage et la coréférence sont également des éléments qui ont plusieurs instruments.\nEt nous créons ces foils en nous assurant qu'ils ne décrivent pas l'image et que ce sont des phrases grammaticales et autrement valides.\nCe n'est pas facile à faire car une légende ayant subi un foil peut être moins probable que la légende originale.\nPar exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que les plantes coupent un homme qu'un homme coupe les plantes, et la grande vision et les modèles de langue pourraient capter cela.\nPar conséquent, pour obtenir des foils valides, nous devons agir.\nTout d'abord, nous utilisons des modèles de langue forts pour proposer des foils.\nDeuxièmement, nous utilisons l'inférence de la langue naturelle ou la NLI courte pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image.\nPour tester cela automatiquement, nous appliquons l'inférence de la langue naturelle avec la justification suivante.\nNous considérons qu'une image est la prémisse et sa légende son hypothèse implicite.\nEn outre, nous considérons la légende comme la prémisse, et le foil est son hypothèse.\nSi un modèle NLI prévient que le foil est en contradiction ou neutre par rapport à la légende, nous considérons cela comme un indicateur d'un foil valide.\nSi une NLI prévient le foil à engendrer par la légende, cela ne peut pas être un bon foil, car par transitivité, cela donnera une description véridique de l'image et nous filtrons ces foils.\nMais cette procédure n'est pas parfaite, il s'agit simplement d'un indicateur pour les foils valides.\nPar conséquent, comme troisième mesure pour produire des foils valides, nous utilisons des annotateurs humains pour valider les données utilisées dans VALSE.\nDonc, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrit dans ce tableau.\nNotez que VALSE ne fournit pas de données de formation mais seulement des données de test.\nComme il s'agit uniquement d'un indicateur de référence de test zero shot, il est conçu pour tirer parti des capacités existantes des modèles de langue et de vision après la préformation.\nLe raffinement permettrait seulement aux modèles d'exploiter des artefacts ou des biais statistiques dans les données.\nEt nous savons tous que ces modèles aiment tricher et prendre des raccourcis.\nEt comme nous l'avons dit, cela nous intéresse d'évaluer les capacités des modèles de langue et de vision après la préformation.\nNous expérimentons avec cinq modèles de langue et de vision sur VALSE, à savoir avec CLIP, LXMert, ViLBERT, ViLBERT douze en un, et VisualBERT.\nDeux de nos indicateurs d'évaluation les plus importants sont la précision des modèles dans la classification de paires de phrases images en légendes et foils.\nPeut-être plus pertinent pour cette vidéo, nous présenterons notre indicateur plus permissif, la précision par paire, qui mesure si le score de sentence alignment d'image est plus élevé pour la bonne paire de texte image que pour sa paire ayant subi un foil.\nPour plus d'indicateurs et de résultats, consultez notre article.\nLes résultats avec une précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres indicateurs. La meilleure performance zero shot est obtenue par ViLBERT douze en un, suivi de ViLBERT, LXMert, CLIP, et enfin VisualBERT.\nIl est remarquable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par ViLBERT douze en un, en soulignant que les modèles sont capables d'identifier des objets nommés et leur présence dans les images.\nCependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de foiling antagonistes.\nNous voyons à partir des instruments de pluralité et de comptage que les modèles de langue et de vision ont du mal à distinguer les références à des objets uniques par rapport à plusieurs, ou à les compter dans une image.\nL'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image.\nIls ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité comme nous le voyons dans l'élément d'actions.\nÀ partir de l'élément de coréférence, nous découvrons que relever plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de langue et de vision.\nPour vérifier la santé mentale, et parce que c'est une expérience intéressante, nous comparons également deux modèles à texte seulement, GPT un et GPT deux, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et ayant subi un foil, sans image ici, et en prévenant l'entrée avec la plus faible perplexité.\nSi la perplexité est plus élevée pour le foil, nous considérons cela comme une indication que la légende ayant subi un foil peut souffrir de biais de plausibilité ou d'autres biais linguistiques.\nEt il est intéressant de voir que dans certains cas, les modèles GPT à texte seulement ont capturé la plausibilité du monde mieux que les modèles de langue et de vision.\nDonc, pour résumer, VALSE est un indicateur de référence qui utilise l'objectif des constructions linguistiques pour aider la communauté à améliorer les modèles de langue et de vision en testant durement leurs capacités de bases visuelles.\nNos expériences montrent que les modèles de langue et de vision identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais luttent pour ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques.\nNous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers les bases de la langue avec des modèles de langue et de vision.\nEt plus encore, VALSE pourrait être utilisé comme une évaluation indirecte des données, car les modèles pourraient être évalués avant et après la formation ou le raffinement pour voir si des données aident les modèles à améliorer l'un des aspects testés par VALSE.\nSi vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 420, "src_audio": "/acl6060/audio/en/420.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.\nI'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.\nI'll be explaining in this order.\nFirst, I will introduce automatic release note generation that we are working on in this research.\nA release note is a technical document that summarizes the changes distributed with each release of a software product.\nThe image shows a release note for version two point six point four of the vuejs library.\nRelease notes play an important role in open source development but they're time consuming to prepare manually.\nTherefore, it would be very useful to be able to automatically generate high quality release notes.\nI will defer to two previous researches on automatic release note generation.\nThe first is a system called ARENA released in twenty fourteen.\nIt takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.\nThe most notable feature of this system is the issue extractor in the upper right corner.\nWhich must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.\nIn other words, it cannot be used for many projects on GitHub.\nThe second is Glyph, recently announced in twenty twenty.\nIt is available on the internet and can be installed via pip.\nThis system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.\nThis image is a sample usage that returns a corrective or bug fixes label.\nGlyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.\nThe performance of the text classification model is not high.\nI present two related researches, but their problems are limited applicability and scarce data resources.\nOur paper solves these two problems and automatically generates high quality release notes.\nWith a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.\nThis proposed method can be used for all English repositories.\nFor the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.\nNext, I'll describe our dataset.\nHere is an example of data.\nThe left side is a commit message and the right side is the release notes.\nRelease notes are labeled as improvements or fixes, etc.\nWe have set up a task that takes the commit messages as input and outputs a labeled release notes.\nThis can be regarded as a summarization task.\nWe have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.\nThese were set based on previous research and other factors.\nThe release note on the bottom right is extracted from the release note on the bottom left.\nAt this time, it is necessary to detect the four labels that have been set up in advance.\nBut the labels are not always consistent with each repository.\nFor example, the improvements label includes improvements, enhancements, optimizations, and so on.\nWe prepared a vocabulary list of about thirty labels for each of these notational variations.\nThis is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.\nNext is a commit message.\nCommit messages are not tied to each release.\nAs shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.\nThis is a bit tedious and it is not enough to just get a list of releases and look at the before and after.\nWe created a heuristic matching rule to get the previous and next versions.\nDataset analysis.\nIn the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.\nAlso, the average number of release notes tokens is sixty three, which is quite high for a summarization task.\nAlso, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.\nThis is due to the large number of unique class or method names found in the repository.\nNext, I will explain the proposed method.\nThe classwise extractive then abstractive summarization model consists of two neural modules.\nA classifier using BERT or CodeBERT and a generator using BART.\nFirst, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.\nThe commit messages classified as other are discarded.\nThen CEAS applies the generator to the four labeled documents independently and generates release notes for each class.\nIn this task, the direct correspondences between commit messages and release notes are not known.\nTherefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.\nWe modeled the classwise abstractive summarization approach by two different methods.\nThe first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.\nThe output texts can be divided into classwise segments based on special class-specific endpoint symbols.\nThe second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.\nOkay, let me explain the experiments.\nFive methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.\nRegarding evaluation, in some cases, release notes are output in multiple sentences.\nSince it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.\nThe BLEU is penalized when the system outputs a short sentence.\nThis penalty results in a lower BLEU value in the experiment results described next.\nFinally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.\nA higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.\nHere are the results.\nSince the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.\nCEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.\nIn particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.\nThese results indicate that CEAS and CAS are significantly affected.\nCEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.\nHigh coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.\nCAS-Multi tended to yield higher ROUGE-L than CAS-Single.\nSuggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.\nHere are an error analysis.\nCAS methods tend to output shorter sentences than human reference sentences.\nIn the figure on the right, the reference sentence has three or four sentences, while CAS has only one.\nThe reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.\nFurthermore, CAS methods cannot generate accurate release notes without additional information.\nThe top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.\nThe example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.\nFinally, a conclusion.\nWe have built a new dataset for automatic release note generation.\nWe have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.\nOur experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.\nPlease check out our dataset on GitHub.\nThank you.", "tgt_ref": "Bonjour, je m'appelle Kamezawa de l'Université de Tokyo.\nJe vais vous présenter un article intitulé RNSum : des données à grande échelle pour la génération automatique de note de version via la synthèse des journaux de validation.\nJe vais vous expliquer dans cet ordre.\nTout d'abord, je vais présenter la génération de note de version automatique sur laquelle nous travaillons dans cette recherche.\nUne note de version est un document technique qui résume les modifications distribuées à chaque version d'un produit logiciel.\nL'image montre une note de version pour la version deux point six point quatre de la bibliothèque Vuejs.\nLes notes de version jouent un rôle important dans le développement open source, mais elles mettent du temps à être préparées manuellement.\nPar conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.\nJe m'en remettrai à deux recherches antérieures sur la génération automatique de notes de version.\nLa première est un système appelé ARENA sorti en deux-mille-quatorze.\nIl faut une approche basée sur des règles, par exemple, en utilisant l'extracteur de changement pour extraire toutes les différences, les changements de bibliothèque et les changements de document à partir des différences entre les versions, et enfin, en les combinant.\nLa caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit.\nLe système de suivi des problèmes, qui doit être laissé à Jira, ne peut être appliqué qu'aux projets qui utilisent Jira.\nEn d'autres mots, il ne peut pas être utilisé pour de nombreux projets sur GitHub.\nLa seconde est Glyphe, récemment annoncée en deux-mille-vingt.\nElle est disponible sur internet et peut être installée via pip.\nCe système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'une des cinq étiquettes telles que les fonctions ou corrections de bugs pour chaque message de validation de saisie.\nCette image est un exemple d'utilisation qui renvoie une étiquette de correction ou de débogage.\nLes données de formation de Glyphe sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous.\nLes performances du modèle de classification de texte ne sont pas élevées.\nJe présente deux recherches connexes, mais leurs problèmes sont l'applicabilité limitée et les ressources de données rares.\nNotre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité.\nAvec un problème d'applicabilité limitée, nous proposons une méthode de synthèse par classe de haute qualité en utilisant uniquement des messages de validation comme saisie.\nCette méthode proposée peut être utilisée pour tous les référentiels anglais.\nPour le deuxième problème de ressources de données rares, nous avons construit nos données RNSum composées d'environ quatre-vingt-deux mille éléments de données en collectant des données à partir de référentiels GitHub publics en utilisant l'API GitHub.\nEnsuite, je vais décrire nos données.\nVoici un exemple de données.\nLe côté gauche est un message de validation et le côté droit représente les notes de version.\nLes notes de version sont étiquetées comme améliorations ou correctifs, etc.\nNous avons mis en place une tâche qui prend les messages de validation comme saisie et sorties des notes de version étiquetées.\nCela peut être considéré comme une tâche de synthèse.\nNous avons prédéfini quatre étiquettes : fonctions, améliorations, corrections de bugs, suppressions de dépréciations et modifications de rupture.\nCelles-ci ont été établies sur la base de la recherche antérieure et d'autres facteurs.\nLa note de version en bas à droite est extraite de la note de version en bas à gauche.\nÀ ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été mises en place à l'avance.\nMais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel.\nPar exemple, l'étiquette d'améliorations inclut des améliorations, des perfectionnements, des optimisations, etc.\nNous avons préparé une liste de vocabulaire d'une trentaine d'étiquettes pour chacune de ces variations de notation.\nIl s'agit de détecter la classe de note de version et de collecter le texte de la version qui suit en tant que phrase de note de version pour la classe.\nEnsuite, il y a un message de validation.\nLes messages de validation ne sont pas liés à chaque version.\nComme le montre l'image ci-dessous, si la version actuelle est la version deux point cinq à dix-neuf, nous devons identifier la version deux point cinq à dix-huit antérieure et obtenir un diff.\nC'est un peu fastidieux et il ne suffit pas d'obtenir une liste de versions et de regarder l'avant et l'après.\nNous avons créé une règle de correspondance heuristique pour obtenir les versions antérieures et suivantes.\nAnalyse des données.\nEn fin de compte, sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données ont été recueillis.\nEn outre, le nombre moyen de gages de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse.\nDe même, le nombre de gages uniques est assez grand, s'élevant à huit mille huit cent trente mille.\nCela est dû au grand nombre de noms de méthode ou de classe unique trouvés dans le référentiel.\nEnsuite, je vais expliquer la méthode proposée.\nLe modèle de synthèse extractive puis abstractive par classe se compose de deux modules neuronaux.\nUn classificateur utilisant BERT ou CodeBERT et un générateur utilisant BART.\nTout d'abord, CEAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, qui utilisent des améliorations, des corrections de bogues, des dépréciations et d'autres.\nLes messages de validation classés comme autres sont supprimés.\nEnsuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.\nDans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues.\nPar conséquent, pour former le classificateur, nous avons réaffecté les enquêtes à chaque message de validation de saisie en utilisant les dix premiers caractères de chaque message de validation.\nNous avons modélisé l'approche de synthèse abstractive par classe à travers deux méthodes différentes.\nLe premier modèle, que nous appelons CAS unique, se compose d'un réseau de six à six unique et génère un seul texte de note de version donnant un enchaînement de messages de validation de saisie.\nLes textes de sortie peuvent être divisés en segments par classe sur la base de symboles de point d'extrémité particuliers, spécifiques à la classe.\nLa seconde méthode, que nous appelons CAS multiple, se compose de quatre réseaux seq2seq différents, dont chacun correspond à l'une des classes de notes de version fixes.\nOk, je vais vous expliquer les expériences.\nCinq méthodes ont été comparées : CEAS, CAS unique, CAS multiple, regroupement et étude antérieure, Glyph.\nEn ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases.\nPuisqu'il est difficile de calculer le nombre de phrases telles qu'elles sont, elles sont combinées avec des espaces et traitées comme une seule phrase longue.\nLe BLEU est pénalisé lorsque le système produit une phrase courte.\nCette pénalité se traduit par une valeur BLEU inférieure dans les résultats de l'expérience décrits ci-dessous.\nEnfin, nous calculons également la spécificité car ROUGE et BLEU ne peuvent pas être calculés si les notes de version sont vides.\nUne spécificité plus élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées être vides.\nLes résultats sont indiqués ci-après.\nÉtant donné que les données contiennent des adresses électroniques, des valeurs hachées, etc., nous avons également évalué les données nettoyées, ce qui les exclut.\nLe CEAS et le CAS ont obtenu des scores de ROUGE-L supérieurs de plus de dix points par rapport aux bases.\nEn particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les bases a atteint plus de vingt points.\nCes résultats indiquent que le CEAS et le CAS sont considérablement touchés.\nLe CEAS a obtenu un meilleur score ROUGE-L que le CAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour former le classificateur à l'aide de pseudo-étiquettes.\nUne couverture élevée du CEAS peut être obtenue probablement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.\nLe CAS multiple tendait à produire plus de ROUGE-L que le CAS unique.\nEn suggérant qu'il est également efficace de développer indépendamment et différemment des modèles de abstractive summarization pour chaque classe de note de version.\nVoici une analyse d'erreur.\nLes méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines.\nDans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que le CAS n'en a qu'une.\nLa raison de la réticence de ce modèle est que dans les données de formation, seulement trente-trois pour cent des phrases sont présentes dans l'étiquette des fonctions et quarante pour cent dans l'étiquette des améliorations.\nEn outre, les méthodes CAS ne peuvent pas générer des notes de version précises sans informations supplémentaires.\nL'exemple en haut à droite est un exemple de message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la progression ou au problème correspondant(e).\nL'exemple ci-dessous montre que les deux messages de validation dans la saisie sont liés et doivent être combinés en une phrase, mais cela n'est pas fait.\nEnfin, passons à la conclusion.\nNous avons construit de nouvelles données pour la génération automatique de notes de version.\nNous avons également formulé une tâche consistant à saisir des messages de validation et à les résumer afin qu'ils soient applicables à tous les projets écrits en anglais.\nNos expériences montrent que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les bases.\nVeuillez consulter nos données sur GitHub.\nMerci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 0, "src_audio": "/acl6060/audio/en/0.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour tout le monde. Aujourd'hui, je vais vous présenter notre travail de recherche « Apprendre à raisonner par déduction » : résolution de problèmes de mots mathématiques comme extraction de relation complexe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 1, "src_audio": "/acl6060/audio/en/1.wav", "src_ref": "I'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'appelle Allan du laboratoire d'intelligence artificielle ByteDance, et ceci est un travail en commun avec Jierui Li de l'Université du Texas à Austin et Wei Lu du SUTD.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 2, "src_audio": "/acl6060/audio/en/2.wav", "src_ref": "First, I'd like to talk about our motivation for reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, j'aimerais parler de notre motivation pour le raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 3, "src_audio": "/acl6060/audio/en/3.wav", "src_ref": "So here we show an examples where multi-step reasoning is helpful.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous montrons un exemple où le raisonnement en plusieurs étapes est utile.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 4, "src_audio": "/acl6060/audio/en/4.wav", "src_ref": "So this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce chiffre est donc tiré de l'article PaLM où ils incitent à résoudre le problème de réseau dans le scénario d'apprentissage de quelques prises de vue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 5, "src_audio": "/acl6060/audio/en/5.wav", "src_ref": "So on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, sur le côté gauche, nous pouvons voir que si nous fournissons quelques exemples avec juste des questions et des réponses, nous pourrions ne pas être en mesure d'obtenir les bonnes réponses.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 6, "src_audio": "/acl6060/audio/en/6.wav", "src_ref": "But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si nous fournissons plus de description du raisonnement, le modèle est capable de prévenir la description du raisonnement, mais aussi d'effectuer ici une prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 7, "src_audio": "/acl6060/audio/en/7.wav", "src_ref": "So it is good to have interpretable multi-step reasoning as output.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est alors bon d'avoir comme résultat un raisonnement à plusieurs étapes interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 8, "src_audio": "/acl6060/audio/en/8.wav", "src_ref": "And we also think math word problem is a straightforward application to evaluate such reasoning abilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pensons également que les problèmes de mots mathématiques sont une application directe pour évaluer de telles capacités de raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 9, "src_audio": "/acl6060/audio/en/9.wav", "src_ref": "So, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 10, "src_audio": "/acl6060/audio/en/10.wav", "src_ref": "So in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans nos données, nous recevons également l‘expression mathématique qui conduit à cette réponse particulière.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 11, "src_audio": "/acl6060/audio/en/11.wav", "src_ref": "So, certain assumptions ah also apply as in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, certaines hypothèses s'appliquent également comme dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 12, "src_audio": "/acl6060/audio/en/12.wav", "src_ref": "We assume the precision of quantities are known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que la précision des quantités est connue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 13, "src_audio": "/acl6060/audio/en/13.wav", "src_ref": "And we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 14, "src_audio": "/acl6060/audio/en/14.wav", "src_ref": "Furthermore, complicated operators can be actually decomposed into these basic operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les opérateurs compliqués peuvent effectivement être décomposés en ces opérateurs de base.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 15, "src_audio": "/acl6060/audio/en/15.wav", "src_ref": "So, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, le travail antérieur dans la résolution de problèmes de mots mathématiques peut en effet être classé dans un modèle séquence à séquence et séquence à arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 16, "src_audio": "/acl6060/audio/en/16.wav", "src_ref": "So, traditional sequence to sequence model convert the expression to a specific sequence for generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle séquence à séquence traditionnel convertit donc l'expression en une séquence spécifique pour la génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 17, "src_audio": "/acl6060/audio/en/17.wav", "src_ref": "And it is pretty easy to implement and it can generalize to many different complicated problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est assez facile à mettre en œuvre et peut être généralisé à de nombreux problèmes compliqués différents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 18, "src_audio": "/acl6060/audio/en/18.wav", "src_ref": "But the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle structuré et son manque d'interopérabilité pour la prévention.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 19, "src_audio": "/acl6060/audio/en/19.wav", "src_ref": "But actually this direction is still quite popular because of um the transformer model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, cette direction est encore très populaire en raison du modèle de conversion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 20, "src_audio": "/acl6060/audio/en/20.wav", "src_ref": "So, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans les modèles à base d'arbre, nous structurons en réalité ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 21, "src_audio": "/acl6060/audio/en/21.wav", "src_ref": "So here we keep generating the operators until we reach the leaves, which are the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous continuons à produire les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 22, "src_audio": "/acl6060/audio/en/22.wav", "src_ref": "So here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, la bonne chose est qu'en fait, cela nous donne cette structure d'arbre binaire. Mais en réalité, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur et ensuite, à la fin, nous générons les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 23, "src_audio": "/acl6060/audio/en/23.wav", "src_ref": "And the second thing is that it also contains some repetitive computations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que cela contient également des calculs répétitifs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 24, "src_audio": "/acl6060/audio/en/24.wav", "src_ref": "So here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, si nous regardons cette expression, huit fois trois plus trois est en fait généré deux fois, mais en réalité, nous devrions réutiliser les résultats.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 25, "src_audio": "/acl6060/audio/en/25.wav", "src_ref": "So, in our proposed approach we want to solve those problems in a step by step and interpretable manners.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 26, "src_audio": "/acl6060/audio/en/26.wav", "src_ref": "So for example, here in the second step, ah we can obtain these divisors which is twenty seven.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, ici dans la deuxième étape, nous pouvons obtenir ces diviseurs qui sont vingt-sept.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 27, "src_audio": "/acl6060/audio/en/27.wav", "src_ref": "And we can also refer back to the original questions to find the relevant contents.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 28, "src_audio": "/acl6060/audio/en/28.wav", "src_ref": "And in these steps we obtain the divisors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et dans ces étapes, nous obtenons les diviseurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 29, "src_audio": "/acl6060/audio/en/29.wav", "src_ref": "So, ah and then at this third step we actually get the quotient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis, à cette troisième étape, nous obtenons alors le quotient.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 30, "src_audio": "/acl6060/audio/en/30.wav", "src_ref": "Alright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Et après ces trois étapes, nous pouvons alors réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, et enfin, nous pouvons obtenir les dividendes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 31, "src_audio": "/acl6060/audio/en/31.wav", "src_ref": "So, here we actually generate the whole expression directly rather than generating a single operators or quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous générons en réalité l'expression entière directement plutôt que de produire un seul opérateur ou une seule quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 32, "src_audio": "/acl6060/audio/en/32.wav", "src_ref": "So this makes the process more accurate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela rend le processus plus précis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 33, "src_audio": "/acl6060/audio/en/33.wav", "src_ref": "So, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et incluant également une certaine constante comme notre état initial.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 34, "src_audio": "/acl6060/audio/en/34.wav", "src_ref": "So, the expression is represented by e i j o p.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, l'expression est représentée par e i j o p.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 35, "src_audio": "/acl6060/audio/en/35.wav", "src_ref": "Where we perform operator from q_i to q_j, and such expression is actually directed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Où nous effectuons l'opérateur de q_i à q_j, et cette expression est en fait dirigée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 36, "src_audio": "/acl6060/audio/en/36.wav", "src_ref": "So, we also have subtraction with words here to represent the opposite direction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, nous avons également ici la soustraction avec des mots pour représenter la direction opposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 37, "src_audio": "/acl6060/audio/en/37.wav", "src_ref": "This is quite similar to relation extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est tout à fait similaire à l'extraction de relation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 38, "src_audio": "/acl6060/audio/en/38.wav", "src_ref": "So in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans un système déductif formel, à un pas de temps t, on applique l'opérateur entre la paire q_i et q_j, puis on obtient cette nouvelle expression.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 39, "src_audio": "/acl6060/audio/en/39.wav", "src_ref": "We add it to the next state to become a new quantity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous l'ajoutons à l'état suivant pour devenir une nouvelle quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 40, "src_audio": "/acl6060/audio/en/40.wav", "src_ref": "So, these slides actually visualize the evolution of the state where we keep adding expression to the current state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, ces diapositives visualisent en réalité l'évolution de l'état où nous continuons à ajouter de l'expression à l'état actuel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 41, "src_audio": "/acl6060/audio/en/41.wav", "src_ref": "So in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langue préformée qui peut être BERT ou Roberta, puis nous encodons la phrase et obtenons ensuite ces représentations de quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 42, "src_audio": "/acl6060/audio/en/42.wav", "src_ref": "So, once we get the quantity representations, we can start to do inference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire l'inférence.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 43, "src_audio": "/acl6060/audio/en/43.wav", "src_ref": "Here we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous montrons ici un exemple de q_1 pour obtenir la représentation pour q_2 divisée par q_2 multipliée par q_3.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 44, "src_audio": "/acl6060/audio/en/44.wav", "src_ref": "First we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous obtenons la représentation de paire, qui n'est essentiellement que l'enchaînement entre q_1 et q_2, puis nous appliquons un réseau prédictif qui est paramétré par l'opérateur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 45, "src_audio": "/acl6060/audio/en/45.wav", "src_ref": "And then finally we obtain the expression representation q_1 divided by q_2.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et enfin, nous obtenons la représentation de l'expression q_1 divisée par q_2.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 46, "src_audio": "/acl6060/audio/en/46.wav", "src_ref": "But in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, dans la pratique, au stade de l'inférence, nous pourrions également obtenir une expression incorrecte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 47, "src_audio": "/acl6060/audio/en/47.wav", "src_ref": "So, here all the possible expression is equals to three times the number of operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, toute l'expression possible est égale à trois fois le nombre d'opérateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 48, "src_audio": "/acl6060/audio/en/48.wav", "src_ref": "So the nice thing here is that we can easily add constraints to control this search this search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors, ce qui est bien ici, c'est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 49, "src_audio": "/acl6060/audio/en/49.wav", "src_ref": "For example, if this expression is not allowed, we can simply remove this expression in our search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 50, "src_audio": "/acl6060/audio/en/50.wav", "src_ref": "So in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans la deuxième étape, nous faisons la même chose, mais la seule différence est que nous avons une quantité de plus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 51, "src_audio": "/acl6060/audio/en/51.wav", "src_ref": "So this quantity come from the previous calculated expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette quantité provient donc de l'expression calculée antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 52, "src_audio": "/acl6060/audio/en/52.wav", "src_ref": "So finally we can obtain this final expression q_3 times q_4.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous pouvons obtenir cette expression finale q_3 multipliée par q_4.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 53, "src_audio": "/acl6060/audio/en/53.wav", "src_ref": "And we can also see the number of all the possible ah expression is different from the previous step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 54, "src_audio": "/acl6060/audio/en/54.wav", "src_ref": "So, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une telle différence rend difficile l'application de la beam search car la distribution de probabilité entre ces deux étapes est déséquilibrée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 55, "src_audio": "/acl6060/audio/en/55.wav", "src_ref": "So the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La procédure de formation est donc similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 56, "src_audio": "/acl6060/audio/en/56.wav", "src_ref": "And here we also use this tau to represent when we should terminate this generation process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous utilisons également ce tau pour représenter le moment où nous devrions mettre fin à ce processus de génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 57, "src_audio": "/acl6060/audio/en/57.wav", "src_ref": "And here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, l'espace est différent de séquence à séquence car il est différent à chaque pas de temps, alors que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 58, "src_audio": "/acl6060/audio/en/58.wav", "src_ref": "And it also allows us to impose certain constraints from prior from prior knowledge.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 59, "src_audio": "/acl6060/audio/en/59.wav", "src_ref": "So we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous menons donc des expériences sur les données de problèmes de mots mathématiques, MAWPS, Math23K, MathQA et SVAMP couramment utilisées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 60, "src_audio": "/acl6060/audio/en/60.wav", "src_ref": "And here we briefly show the results compared with the previous best approaches.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons brièvement les résultats comparés aux meilleures approches antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 61, "src_audio": "/acl6060/audio/en/61.wav", "src_ref": "So our best performing variant is Roberta-DeductiveReasoner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, notre variante la plus performante est Roberta-DeuctiveReasoner.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 62, "src_audio": "/acl6060/audio/en/62.wav", "src_ref": "And in fact we do not use beam search, in contrast all previous approaches are using beam search.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en réalité, nous n'utilisons pas la beam search ; au contraire, toutes les approches antérieures utilisent la beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 63, "src_audio": "/acl6060/audio/en/63.wav", "src_ref": "All right. So, the best approaches are often tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Ainsi, les meilleures approches sont souvent un modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 64, "src_audio": "/acl6060/audio/en/64.wav", "src_ref": "So, overall our reasoner is able to significantl significantly outperform this tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans l'ensemble, notre raisonneur est capable de dépasser significativement ce modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 65, "src_audio": "/acl6060/audio/en/65.wav", "src_ref": "But we can see the absolute numbers on MathQA or SVAMP are not really high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons voir que les nombres absolus sur MathQA ou SVAMP ne sont pas vraiment élevés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 66, "src_audio": "/acl6060/audio/en/66.wav", "src_ref": "So we further investigate the results on SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous étudions donc plus en détail les résultats sur SVAMP.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 67, "src_audio": "/acl6060/audio/en/67.wav", "src_ref": "And this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ces données sont difficiles parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle TAL traitement automatique du langage naturel comme ajouter des informations non pertinentes et des quantités supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 68, "src_audio": "/acl6060/audio/en/68.wav", "src_ref": "So, in our prediction we find some of the intermediate values are actually negatives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans notre prévention, nous trouvons que certaines des valeurs intermédiaires sont en réalité négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 69, "src_audio": "/acl6060/audio/en/69.wav", "src_ref": "For example, um, in these questions we are asking how many apples does Jake have?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans ces questions, nous demandons « combien de pommes a Jake ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 70, "src_audio": "/acl6060/audio/en/70.wav", "src_ref": "But we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous avons des informations supplémentaires comme « dix-sept photos de moins », et « Steven en a huit », ce qui est totalement hors de propos.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 71, "src_audio": "/acl6060/audio/en/71.wav", "src_ref": "So, our model makes some prediction like this which is producing negative values.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, notre modèle effectue une prévention comme celle-ci qui produit des valeurs négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 72, "src_audio": "/acl6060/audio/en/72.wav", "src_ref": "And we observe these two expressions actually have similar scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous constatons que ces deux expressions ont en réalité des scores similaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 73, "src_audio": "/acl6060/audio/en/73.wav", "src_ref": "So, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pouvons alors limiter cet espace de recherche en supprimant les résultats négatifs afin que nous puissions rendre la réponse correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 74, "src_audio": "/acl6060/audio/en/74.wav", "src_ref": "So um we further find such constraint actually improves quite a lot for some models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous trouvons donc que cette contrainte améliore en réalité beaucoup pour certains modèles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 75, "src_audio": "/acl6060/audio/en/75.wav", "src_ref": "For example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous améliorons sept points, puis pour le modèle de base Roberta, nous avons amélioré deux points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 76, "src_audio": "/acl6060/audio/en/76.wav", "src_ref": "So better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le meilleur modèle de langue a donc de meilleures capacités de compréhension de la langue de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 77, "src_audio": "/acl6060/audio/en/77.wav", "src_ref": "And we also try to analyze the difficulty behind these behind all these datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous essayons également d'analyser la difficulté derrière toutes ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 78, "src_audio": "/acl6060/audio/en/78.wav", "src_ref": "We assume the number of unused quantities can be regarded as irrelevant information here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que le nombre de quantités inutilisées peut être considéré ici comme une information non pertinente.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 79, "src_audio": "/acl6060/audio/en/79.wav", "src_ref": "So ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et les données SVAMP ont la plus grande partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 80, "src_audio": "/acl6060/audio/en/80.wav", "src_ref": "And here we also show the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons également la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 81, "src_audio": "/acl6060/audio/en/81.wav", "src_ref": "For those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ces échantillons sans quantités inutilisées, la performance est en réalité supérieure à la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 82, "src_audio": "/acl6060/audio/en/82.wav", "src_ref": "But with those samples that with unused quantity is actually way worse than the, worse than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais avec ces échantillons ayant une quantité inutilisée, c'est en fait bien pire que la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 83, "src_audio": "/acl6060/audio/en/83.wav", "src_ref": "For MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour MAWPS, nous n'avons pas vraiment trop de cas de test, alors j'ignore simplement cette partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 84, "src_audio": "/acl6060/audio/en/84.wav", "src_ref": "So, finally we want to show the interpretability through a question perturbation example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous voulons montrer l'interopérabilité à travers un exemple de perturbation de question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 85, "src_audio": "/acl6060/audio/en/85.wav", "src_ref": "So here our model actually makes a wrong prediction at the first step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, notre modèle effectue en réalité une prévention erronée à la première étape.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 86, "src_audio": "/acl6060/audio/en/86.wav", "src_ref": "So, we can actually correlate this expression with the sentence here. Alright.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, nous pouvons effectivement corréler cette expression avec la phrase ici. Très bien.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 87, "src_audio": "/acl6060/audio/en/87.wav", "src_ref": "So, we think this sentence might be misleading the model to an incorrect predictions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pensons donc que cette phrase pourrait induire le modèle en erreur avec des préventions incorrectes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 88, "src_audio": "/acl6060/audio/en/88.wav", "src_ref": "So here planting another thirty five makes the model makes the model think it should be an addition operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, en planter trente-cinq autres fait penser au modèle qu'il devrait être un opérateur d'addition.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 89, "src_audio": "/acl6060/audio/en/89.wav", "src_ref": "So we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous essayons alors de réviser la phrase pour que cela soit quelque chose comme « le nombre de poiriers est inférieur de trente-cinq à celui des pommiers ».", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 90, "src_audio": "/acl6060/audio/en/90.wav", "src_ref": "So, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous faisons en sorte de transmettre une sémantique plus précise afin que le modèle soit capable de rendre la prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 91, "src_audio": "/acl6060/audio/en/91.wav", "src_ref": "So, this study shows how the interpretable predictions help us understand the model behavior.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, cette étude montre comment les préventions interprétables nous aident à comprendre le comportement du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 92, "src_audio": "/acl6060/audio/en/92.wav", "src_ref": "So to conclude our work, so first our model is actually pretty efficient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour conclure notre travail, notre modèle est en réalité assez efficace.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 93, "src_audio": "/acl6060/audio/en/93.wav", "src_ref": "And we are able to provide interpretable solving procedure.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous sommes en mesure de fournir une procédure de résolution interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 94, "src_audio": "/acl6060/audio/en/94.wav", "src_ref": "And we can easily incorporate some prior knowledge as constraint which can help improve the performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons facilement incorporer des connaissances antérieures en tant que contraintes qui peuvent aider à améliorer la performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 95, "src_audio": "/acl6060/audio/en/95.wav", "src_ref": "And the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution des problèmes de réseau, mais aussi aux autres tâches qui impliquent un raisonnement à plusieurs étapes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 96, "src_audio": "/acl6060/audio/en/96.wav", "src_ref": "We also have certain limitations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également certaines limites.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 97, "src_audio": "/acl6060/audio/en/97.wav", "src_ref": "Ah, if we have a large number of operators or constants, the memory consumption could be pretty high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 98, "src_audio": "/acl6060/audio/en/98.wav", "src_ref": "And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que, comme mentionné, puisque la distribution de probabilité est déséquilibrée entre les différentes étapes de temps, il est donc également assez difficile d'appliquer la stratégie de beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 99, "src_audio": "/acl6060/audio/en/99.wav", "src_ref": "So this is the end of the talk, and questions are welcomed. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous arrivons donc à la fin de la discussion, et vos questions sont les bienvenues. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 100, "src_audio": "/acl6060/audio/en/100.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 101, "src_audio": "/acl6060/audio/en/101.wav", "src_ref": "I will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter mon travail en commun avec Jerry, qui porte sur de nouvelles données pour l'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 102, "src_audio": "/acl6060/audio/en/102.wav", "src_ref": "Legal issues are an integral part of many people's lives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions juridiques font partie intégrante de la vie de nombreuses personnes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 103, "src_audio": "/acl6060/audio/en/103.wav", "src_ref": "But the majority of citizens have little to know knowledge about their rights and fundamental legal processes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais la majorité des citoyens ont peu de connaissances sur leurs droits et leurs processus juridiques fondamentaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 104, "src_audio": "/acl6060/audio/en/104.wav", "src_ref": "As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En conséquence, de nombreux citoyens vulnérables qui n'ont pas les moyens de se payer l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 105, "src_audio": "/acl6060/audio/en/105.wav", "src_ref": "All work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tous les travaux visent à combler le fossé entre les personnes et la loi en développant un système d'extraction efficace pour les articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 106, "src_audio": "/acl6060/audio/en/106.wav", "src_ref": "Such a system could provide a free professional legal help service for unskilled humans.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un tel système pourrait fournir un service d'aide juridique professionnel gratuit aux personnes non qualifiées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 107, "src_audio": "/acl6060/audio/en/107.wav", "src_ref": "Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avant de plonger dans la principale contribution de ce travail, nous allons d'abord décrire le problème d'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 108, "src_audio": "/acl6060/audio/en/108.wav", "src_ref": "Given a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Compte tenu d'une simple question sur une question juridique telle que : « qu'est-ce que je risque si je viole la confidentialité professionnelle ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 109, "src_audio": "/acl6060/audio/en/109.wav", "src_ref": "A model is required to retrieve all relevant statutory articles from a large body of legislation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un modèle est nécessaire pour extraire tous les articles de droit pertinents d'un grand corpus législatif.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 110, "src_audio": "/acl6060/audio/en/110.wav", "src_ref": "This information retrieval task comes with its own set of challenges.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette tâche d'extraction des informations comporte son propre ensemble de défis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 111, "src_audio": "/acl6060/audio/en/111.wav", "src_ref": "First, it deals with two types of language.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, elle traite de deux types de langue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 112, "src_audio": "/acl6060/audio/en/112.wav", "src_ref": "Common natural language for the questions and complex legal language for the statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La langue naturelle commune pour les questions et la langue juridique complexe pour les lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 113, "src_audio": "/acl6060/audio/en/113.wav", "src_ref": "This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette différence dans les distributions de langue rend plus difficile pour un système d'extraire les candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 114, "src_audio": "/acl6060/audio/en/114.wav", "src_ref": "Besides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le droit écrit n'est pas une pile d'articles indépendants qui peuvent être traités comme une source complète d'informations par eux-mêmes, contrairement aux nouvelles ou aux recettes, par exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 115, "src_audio": "/acl6060/audio/en/115.wav", "src_ref": "Instead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, il s'agit d'un ensemble structuré de dispositions juridiques qui n'ont un sens entier que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les domaines et sous-domaines auxquels elles appartiennent et leur place dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 116, "src_audio": "/acl6060/audio/en/116.wav", "src_ref": "Lastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, les articles de droit ne sont pas de petits paragraphes constituant généralement l'unité d'extraction typique dans la plupart des travaux d'extraction.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 117, "src_audio": "/acl6060/audio/en/117.wav", "src_ref": "Here, there are long documents that may be up to six thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ici, il y a de longs documents qui peuvent aller jusqu'à six mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 118, "src_audio": "/acl6060/audio/en/118.wav", "src_ref": "The recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les recent advances en matière de TAL traitement automatique du langage naturel ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prévention du jugement juridique ou l'examen des contrats de contacts automatisés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 119, "src_audio": "/acl6060/audio/en/119.wav", "src_ref": "But statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais l'extraction de l'article de droit est restée essentiellement inchangée en raison du manque de grandes données étiquetées de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 120, "src_audio": "/acl6060/audio/en/120.wav", "src_ref": "In this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans ce travail, nous présentons de nouvelles données centrées sur le citoyen natif français pour étudier si l'extraction des modèles peut se rapprocher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche d'extraction de l'article de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 121, "src_audio": "/acl6060/audio/en/121.wav", "src_ref": "Our Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos données d'extraction de l'article de droit belge BSARD se composent de plus de mille cent questions juridiques posées par des citoyens belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 122, "src_audio": "/acl6060/audio/en/122.wav", "src_ref": "These questions cover a wide range of topics from family, housing, money, to work and social security.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces questions couvrent un wide range de sujets allant de la famille au logement, à l'argent, au travail et à la sécurité sociale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 123, "src_audio": "/acl6060/audio/en/123.wav", "src_ref": "Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques de codes de droit belge.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 124, "src_audio": "/acl6060/audio/en/124.wav", "src_ref": "Let's now talk about how we collected this dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Parlons maintenant de la façon dont nous avons collecté ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 125, "src_audio": "/acl6060/audio/en/125.wav", "src_ref": "First, we started by compiling a large corpus of legal articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 126, "src_audio": "/acl6060/audio/en/126.wav", "src_ref": "We considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons examiné trente-deux codes belges accessibles au public et extrait tous les articles ainsi que les titres de section correspondants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 127, "src_audio": "/acl6060/audio/en/127.wav", "src_ref": "Then we gathered legal questions with references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous avons rassemblé les questions juridiques avec les références aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 128, "src_audio": "/acl6060/audio/en/128.wav", "src_ref": "To do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ce faire, nous nous associons au cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur une question juridique personnelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 129, "src_audio": "/acl6060/audio/en/129.wav", "src_ref": "We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons eu la chance d'avoir accès à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 130, "src_audio": "/acl6060/audio/en/130.wav", "src_ref": "We collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons recueilli des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 131, "src_audio": "/acl6060/audio/en/131.wav", "src_ref": "Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous avons fait passer les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avons considérés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 132, "src_audio": "/acl6060/audio/en/132.wav", "src_ref": "The remaining references were matched and converted to the corresponding article ids from our corpus.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les références restantes ont été appariées et converties aux identifiants d'article correspondants de notre corpus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 133, "src_audio": "/acl6060/audio/en/133.wav", "src_ref": "We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous nous sommes finalement retrouvés avec mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 134, "src_audio": "/acl6060/audio/en/134.wav", "src_ref": "In addition, each question comes with the main category and a concatenation of subcategories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, chaque question est accompagnée de la catégorie principale et d'un enchaînement de sous-catégories.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 135, "src_audio": "/acl6060/audio/en/135.wav", "src_ref": "And each articles comes with a concatenation of the subsequence heading in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et chaque article comporte un enchaînement de la rubrique de sous-séquence dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 136, "src_audio": "/acl6060/audio/en/136.wav", "src_ref": "This extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour des recherches futures sur l'extraction d'informations juridiques ou la classification de textes juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 137, "src_audio": "/acl6060/audio/en/137.wav", "src_ref": "Let's look at some characteristic of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jetons un coup d'œil à certaines caractéristiques de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 138, "src_audio": "/acl6060/audio/en/138.wav", "src_ref": "The questions are between five and forty four words long with a median of fourteen words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions comportent entre cinq et quarante-quatre mots avec une moyenne de quatorze mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 139, "src_audio": "/acl6060/audio/en/139.wav", "src_ref": "The articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les articles sont beaucoup plus longs avec une longueur moyenne de soixante-dix-sept mots, avec cent quarante-deux d'entre eux dépassant les mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 140, "src_audio": "/acl6060/audio/en/140.wav", "src_ref": "The lengthiest one being up to five thousand seven hundred and ninety words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 141, "src_audio": "/acl6060/audio/en/141.wav", "src_ref": "As previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme mentionné précédemment, les questions couvrent un wide range de sujets, dont environ quatre-vingt-cinq pour cent concernent la famille, le logement, l'argent ou la justice.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 142, "src_audio": "/acl6060/audio/en/142.wav", "src_ref": "While the remaining fifteen percent concern either social security, foreigners or work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 143, "src_audio": "/acl6060/audio/en/143.wav", "src_ref": "The article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'article est également très diversifié car il provient de trente-deux codes belges différents qui couvrent un grand nombre de sujets juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 144, "src_audio": "/acl6060/audio/en/144.wav", "src_ref": "Here's the total number of articles collected from each of these Belgian codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le nombre total d'articles collectés à partir de chacun de ces codes belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 145, "src_audio": "/acl6060/audio/en/145.wav", "src_ref": "Out of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sur les vingt-deux mille six cent trente-trois articles, seuls mille six cent douze sont désignés comme pertinents à au moins une question dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 146, "src_audio": "/acl6060/audio/en/146.wav", "src_ref": "And around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et environ quatre-vingts pour cent de ces articles cités proviennent du code civil, des codes judiciaires, des codes d'enquête criminelle ou des codes pénaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 147, "src_audio": "/acl6060/audio/en/147.wav", "src_ref": "Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 148, "src_audio": "/acl6060/audio/en/148.wav", "src_ref": "Which can be explained by the fact that those codes focused less on individuals and their concerns.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce qui peut s'expliquer par le fait que ces codes se concentraient moins sur les individus et leurs préoccupations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 149, "src_audio": "/acl6060/audio/en/149.wav", "src_ref": "Overall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le nombre moyen de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 150, "src_audio": "/acl6060/audio/en/150.wav", "src_ref": "Using all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En utilisant toutes les données, nous avons comparé plusieurs approches d'extraction, y compris l'architecture lexicale et dense.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 151, "src_audio": "/acl6060/audio/en/151.wav", "src_ref": "Given a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné une requête et un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de requête des poids de chacun de ces termes dans cet article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 152, "src_audio": "/acl6060/audio/en/152.wav", "src_ref": "We experiment with the standard TF-IDF and BM25 ranking functions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec les fonctions de classement TF-IDF et BM25 standard.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 153, "src_audio": "/acl6060/audio/en/153.wav", "src_ref": "The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le principal problème de ces approches est qu'elles ne peuvent extraire que les articles contenant des mots-clés présents dans la requête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 154, "src_audio": "/acl6060/audio/en/154.wav", "src_ref": "To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour surmonter cette limitation, nous expérimentons une architecture neuronale qui peut capturer les relations sémantiques entre les requêtes et l'article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 155, "src_audio": "/acl6060/audio/en/155.wav", "src_ref": "We use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons un modèle bi-encodeur qui cartographie les requêtes et les articles en représentations vecteurs denses et calculons un score de pertinence entre une paire d'articles de requête par la similitude de leurs intégrations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 156, "src_audio": "/acl6060/audio/en/156.wav", "src_ref": "These embeddings typically result from a pooling operation on the output of a word embedding model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces intégrations résultent typiquement d'une opération de pooling sur la sortie d'un modèle d'intégration de mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 157, "src_audio": "/acl6060/audio/en/157.wav", "src_ref": "First, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous étudions l'efficacité des bi-encodeurs siamois dans une configuration d'évaluation « zero shot », ce qui signifie que des modèles d'intégration de mots préformés sont appliqués immédiatement sans aucun raffinement supplémentaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 158, "src_audio": "/acl6060/audio/en/158.wav", "src_ref": "We experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec l'encodeur de texte indépendant du contexte, à savoir word2vec et fastText, et des modèles d'intégration dépendants du contexte, à savoir Roberta et plus précisément CamemBERT qui est un modèle Roberta français.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 159, "src_audio": "/acl6060/audio/en/159.wav", "src_ref": "Additionally, we train our own CamemBERT based model ah bi-encoders on our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, nous formons nos propres bi-encodeurs de modèles basés sur CamemBERT sur nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 160, "src_audio": "/acl6060/audio/en/160.wav", "src_ref": "Note that for training, we experiment with the two flavors of the bi-encoder architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que pour la formation, nous expérimentons avec les deux modèles de l'architecture bi-encodeur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 161, "src_audio": "/acl6060/audio/en/161.wav", "src_ref": "Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le siamois, qui utilise un modèle d'intégration de mots unique cartographiant la requête et l'article ensemble dans un vector space dense partagé, et le modèle à deux tours, qui utilise deux modèles d'intégration de mots indépendants encodant la requête et l'article séparément dans différents espaces d'intégration.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 162, "src_audio": "/acl6060/audio/en/162.wav", "src_ref": "We experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons la mise en commun moyenne, maximale et CLS, ainsi que le produit et cosinus pour calculer les similitudes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 163, "src_audio": "/acl6060/audio/en/163.wav", "src_ref": "Here are the result of our baseline on the test sets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le résultat de notre base sur les ensembles de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 164, "src_audio": "/acl6060/audio/en/164.wav", "src_ref": "With the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec les méthodes lexicales ci-dessus, les bi-encodeurs siamois ont évalué dans une configuration zero shot au milieu, et les bi-encodeurs raffinés ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 165, "src_audio": "/acl6060/audio/en/165.wav", "src_ref": "Overall, the finetuned bi-encoder significantly outperforms all the other baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le bi-encodeur raffiné dépasse de manière significative toutes les autres bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 166, "src_audio": "/acl6060/audio/en/166.wav", "src_ref": "The two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle à deux tours s'améliore par rapport à ses variantes siamoises lors du rappel à cent, mais fonctionne de la même manière sur les autres indicateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 167, "src_audio": "/acl6060/audio/en/167.wav", "src_ref": "Although BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que BM25 ait sous-performé le bi-encodeur formé de manière significative, ses performances indiquent qu'il s'agit toujours d'une base solide pour l'extraction spécifique au domaine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 168, "src_audio": "/acl6060/audio/en/168.wav", "src_ref": "Regarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation zero shot du bi-encodeur siamois, nous constatons que l'utilisation directe des intégrations d'un modèle CamemBERT préformé sans optimiser pour la tâche d'extraction d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 169, "src_audio": "/acl6060/audio/en/169.wav", "src_ref": "Furthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous observons que le bi-encodeur basé sur word2vec a dépassé de manière significative les modèles basés sur fastText et les Représentations d'encodeurs bidirectionnels à partir de transformateurs, suggérant que peut-être les intégrations de niveau mot préformées sont plus appropriées pour la tâche que les intégrations de niveau caractère ou sous-mot lorsqu'elles sont utilisées immédiatement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 170, "src_audio": "/acl6060/audio/en/170.wav", "src_ref": "Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que prometteurs, ces résultats suggèrent de nombreuses possibilités d'amélioration comparé à un expert juridique qualifié qui peut éventuellement extraire tous les articles pertinents à n'importe quelle question, et ainsi obtenir des scores parfaits.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 171, "src_audio": "/acl6060/audio/en/171.wav", "src_ref": "Let's conclude by discussing two limitations of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Concluons en discutant de deux limites de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 172, "src_audio": "/acl6060/audio/en/172.wav", "src_ref": "First, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Premièrement, le corpus d'article est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 173, "src_audio": "/acl6060/audio/en/173.wav", "src_ref": "During the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au cours de la construction des données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions ne se retrouvent qu'avec une fraction du nombre initial d'articles pertinents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 174, "src_audio": "/acl6060/audio/en/174.wav", "src_ref": "This information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette information implique donc que la réponse contenue dans les autres articles pertinents pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 175, "src_audio": "/acl6060/audio/en/175.wav", "src_ref": "Second, we should note that not all legal questions can be answered with statutes alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, il convient de noter qu'on ne peut pas répondre à toutes les questions juridiques uniquement par des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 176, "src_audio": "/acl6060/audio/en/176.wav", "src_ref": "For instance, the question, can I evict my tenants if they make too much noise?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, la question « puis-je expulser mes locataires s'ils font trop de bruit ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 177, "src_audio": "/acl6060/audio/en/177.wav", "src_ref": "Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle peut ne pas avoir de réponse détaillée dans le droit écrit qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 178, "src_audio": "/acl6060/audio/en/178.wav", "src_ref": "Instead, the landlord should probably rely more on case law and find precedents similar to their current situation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des antécédents similaires à sa situation actuelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 179, "src_audio": "/acl6060/audio/en/179.wav", "src_ref": "For example, the tenants makes two parties a week until two AM.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, les locataires font la fête deux fois par semaine jusqu'à deux heures du matin.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 180, "src_audio": "/acl6060/audio/en/180.wav", "src_ref": "Hence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche d'extraction de l'article de droit, et le domaine des moins adaptées reste à déterminer.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 181, "src_audio": "/acl6060/audio/en/181.wav", "src_ref": "We hope that our work sparks interest in developing practical and reliable statutory article retrieval models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous espérons que nos travaux susciteront l'intérêt pour l'élaboration de modèles d'extraction d'article de droit pratiques et fiables.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 182, "src_audio": "/acl6060/audio/en/182.wav", "src_ref": "That can help improve access to justice for all.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut aider à améliorer l'accès à la justice pour tous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 183, "src_audio": "/acl6060/audio/en/183.wav", "src_ref": "You can check out our paper, dataset and code at the following links. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez consulter notre article, nos données et notre code aux liens suivants. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 184, "src_audio": "/acl6060/audio/en/184.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE ; un indice de référence indépendant de la tâche destiné à tester les modèles de langue et de vision avec des phénomènes linguistiques spécifiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 185, "src_audio": "/acl6060/audio/en/185.wav", "src_ref": "Why did we do the trouble in setting up this benchmark?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pourquoi avons-nous pris la peine de mettre en place cet indice de référence ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 186, "src_audio": "/acl6060/audio/en/186.wav", "src_ref": "Well, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eh bien, au cours des dernières années, nous avons vu une explosion de la vision basée sur le transformateur et des modèles de langue préformés sur de grandes quantités de paires de textes d'images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 187, "src_audio": "/acl6060/audio/en/187.wav", "src_ref": "Each one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacun de ces modèles pousse les tâches de pointe en matière de vision et de langue telles que la réponse aux questions visuelles, le raisonnement de bon sens visuel, l'extraction d'images et les bases de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 188, "src_audio": "/acl6060/audio/en/188.wav", "src_ref": "So we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc reçu un message : les précisions sur ces tâches et les indices de référence spécifiques augmentent régulièrement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 189, "src_audio": "/acl6060/audio/en/189.wav", "src_ref": "But do we know what the models have actually learned?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais savons-nous ce que les modèles ont réellement appris ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 190, "src_audio": "/acl6060/audio/en/190.wav", "src_ref": "What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Qu'est-ce qu'une vision et un transformateur de langue ont compris lors de l'attribution d'un score élevé pour que cette image et cette phrase correspondent ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 191, "src_audio": "/acl6060/audio/en/191.wav", "src_ref": "And the low score for this one?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le score bas pour celle-ci ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 192, "src_audio": "/acl6060/audio/en/192.wav", "src_ref": "Do vision and language models focus on the right thing?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Est-ce que les modèles de langue et de vision se concentrent sur la bonne chose ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 193, "src_audio": "/acl6060/audio/en/193.wav", "src_ref": "Or do they focus on biases as shown by previous work?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou se concentrent-ils sur les biais comme le montre le travail antérieur ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 194, "src_audio": "/acl6060/audio/en/194.wav", "src_ref": "To shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour éclairer davantage cet aspect, nous proposons une direction plus indépendante des tâches et introduisons VALSE qui teste la sensibilité des modèles de langue et de vision à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistiques et visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 195, "src_audio": "/acl6060/audio/en/195.wav", "src_ref": "We target existence, plurality, counting, spatial relations, actions and entity coreference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 196, "src_audio": "/acl6060/audio/en/196.wav", "src_ref": "But how do we test whether the vision and language models have captured this phenomena?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais comment tester si les modèles de langue et de vision ont capturé ce phénomène ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 197, "src_audio": "/acl6060/audio/en/197.wav", "src_ref": "By foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En pratiquant le foiling sur une méthode précédemment appliquée pour les modèles de langue et de vision seulement pour des phrases nominales de Ravi Shekhar et de ses collaborateurs, et en comptant par nous dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 198, "src_audio": "/acl6060/audio/en/198.wav", "src_ref": "Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Effectuer un foiling signifie essentiellement que nous prenons la légende d'une image et produisons un foil en modifiant la légende de sorte qu'elle ne décrive plus l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 199, "src_audio": "/acl6060/audio/en/199.wav", "src_ref": "And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous apportons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous aurions trouvé plus d'une façon intéressante de créer des instances de foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 200, "src_audio": "/acl6060/audio/en/200.wav", "src_ref": "For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans le cas de l'élément d'actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié avec une action différente, et un dans lequel les acteurs sont échangés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 201, "src_audio": "/acl6060/audio/en/201.wav", "src_ref": "Counting and coreference also are pieces that have more than one instrument.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le comptage et la coréférence sont également des éléments qui ont plusieurs instruments.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 202, "src_audio": "/acl6060/audio/en/202.wav", "src_ref": "And we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous créons ces foils en nous assurant qu'ils ne décrivent pas l'image et que ce sont des phrases grammaticales et autrement valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 203, "src_audio": "/acl6060/audio/en/203.wav", "src_ref": "This is not easy to do because a foiled caption may be less likely than the original caption.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce n'est pas facile à faire car une légende ayant subi un foil peut être moins probable que la légende originale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 204, "src_audio": "/acl6060/audio/en/204.wav", "src_ref": "For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que les plantes coupent un homme qu'un homme coupe les plantes, et la grande vision et les modèles de langue pourraient capter cela.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 205, "src_audio": "/acl6060/audio/en/205.wav", "src_ref": "Therefore, to obtain valid foils, we must take action.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour obtenir des foils valides, nous devons agir.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 206, "src_audio": "/acl6060/audio/en/206.wav", "src_ref": "First, we make use of strong language models to propose foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous utilisons des modèles de langue forts pour proposer des foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 207, "src_audio": "/acl6060/audio/en/207.wav", "src_ref": "Second, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, nous utilisons l'inférence de la langue naturelle ou la NLI courte pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 208, "src_audio": "/acl6060/audio/en/208.wav", "src_ref": "To test this automatically, we apply natural language inference with the following rationale.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour tester cela automatiquement, nous appliquons l'inférence de la langue naturelle avec la justification suivante.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 209, "src_audio": "/acl6060/audio/en/209.wav", "src_ref": "We consider an image to be the premise and its caption its entailed hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous considérons qu'une image est la prémisse et sa légende son hypothèse implicite.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 210, "src_audio": "/acl6060/audio/en/210.wav", "src_ref": "In addition, we consider the caption to be the premise, and the foil is its hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous considérons la légende comme la prémisse, et le foil est son hypothèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 211, "src_audio": "/acl6060/audio/en/211.wav", "src_ref": "If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si un modèle NLI prévient que le foil est en contradiction ou neutre par rapport à la légende, nous considérons cela comme un indicateur d'un foil valide.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 212, "src_audio": "/acl6060/audio/en/212.wav", "src_ref": "If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si une NLI prévient le foil à engendrer par la légende, cela ne peut pas être un bon foil, car par transitivité, cela donnera une description véridique de l'image et nous filtrons ces foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 213, "src_audio": "/acl6060/audio/en/213.wav", "src_ref": "But this procedure is not perfect, it is just an indicator for valid foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cette procédure n'est pas parfaite, il s'agit simplement d'un indicateur pour les foils valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 214, "src_audio": "/acl6060/audio/en/214.wav", "src_ref": "Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, comme troisième mesure pour produire des foils valides, nous utilisons des annotateurs humains pour valider les données utilisées dans VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 215, "src_audio": "/acl6060/audio/en/215.wav", "src_ref": "So, after filtering and human evaluation, we have as many test instances as described in this table.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrit dans ce tableau.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 216, "src_audio": "/acl6060/audio/en/216.wav", "src_ref": "Note that VALSE does not deliver any training data but only test data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que VALSE ne fournit pas de données de formation mais seulement des données de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 217, "src_audio": "/acl6060/audio/en/217.wav", "src_ref": "Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme il s'agit uniquement d'un indicateur de référence de test zero shot, il est conçu pour tirer parti des capacités existantes des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 218, "src_audio": "/acl6060/audio/en/218.wav", "src_ref": "Finetuning would only enable models to exploit artifacts or statistical biases in the data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement permettrait seulement aux modèles d'exploiter des artefacts ou des biais statistiques dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 219, "src_audio": "/acl6060/audio/en/219.wav", "src_ref": "And we all know that these models like to cheat and take shortcuts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 220, "src_audio": "/acl6060/audio/en/220.wav", "src_ref": "And as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et comme nous l'avons dit, cela nous intéresse d'évaluer les capacités des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 221, "src_audio": "/acl6060/audio/en/221.wav", "src_ref": "We experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec cinq modèles de langue et de vision sur VALSE, à savoir avec CLIP, LXMert, ViLBERT, ViLBERT douze en un, et VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 222, "src_audio": "/acl6060/audio/en/222.wav", "src_ref": "Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deux de nos indicateurs d'évaluation les plus importants sont la précision des modèles dans la classification de paires de phrases images en légendes et foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 223, "src_audio": "/acl6060/audio/en/223.wav", "src_ref": "Perhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Peut-être plus pertinent pour cette vidéo, nous présenterons notre indicateur plus permissif, la précision par paire, qui mesure si le score de sentence alignment d'image est plus élevé pour la bonne paire de texte image que pour sa paire ayant subi un foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 224, "src_audio": "/acl6060/audio/en/224.wav", "src_ref": "For more metrics and results on them, do check out our paper.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour plus d'indicateurs et de résultats, consultez notre article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 225, "src_audio": "/acl6060/audio/en/225.wav", "src_ref": "The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats avec une précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres indicateurs. La meilleure performance zero shot est obtenue par ViLBERT douze en un, suivi de ViLBERT, LXMert, CLIP, et enfin VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 226, "src_audio": "/acl6060/audio/en/226.wav", "src_ref": "It's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est remarquable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par ViLBERT douze en un, en soulignant que les modèles sont capables d'identifier des objets nommés et leur présence dans les images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 227, "src_audio": "/acl6060/audio/en/227.wav", "src_ref": "However, none of the remaining pieces can be reliably solved in our adversarial foiling settings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de foiling antagonistes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 228, "src_audio": "/acl6060/audio/en/228.wav", "src_ref": "We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous voyons à partir des instruments de pluralité et de comptage que les modèles de langue et de vision ont du mal à distinguer les références à des objets uniques par rapport à plusieurs, ou à les compter dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 229, "src_audio": "/acl6060/audio/en/229.wav", "src_ref": "The relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 230, "src_audio": "/acl6060/audio/en/230.wav", "src_ref": "They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ils ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité comme nous le voyons dans l'élément d'actions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 231, "src_audio": "/acl6060/audio/en/231.wav", "src_ref": "From the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À partir de l'élément de coréférence, nous découvrons que relever plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 232, "src_audio": "/acl6060/audio/en/232.wav", "src_ref": "As a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour vérifier la santé mentale, et parce que c'est une expérience intéressante, nous comparons également deux modèles à texte seulement, GPT un et GPT deux, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et ayant subi un foil, sans image ici, et en prévenant l'entrée avec la plus faible perplexité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 233, "src_audio": "/acl6060/audio/en/233.wav", "src_ref": "If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si la perplexité est plus élevée pour le foil, nous considérons cela comme une indication que la légende ayant subi un foil peut souffrir de biais de plausibilité ou d'autres biais linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 234, "src_audio": "/acl6060/audio/en/234.wav", "src_ref": "And it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il est intéressant de voir que dans certains cas, les modèles GPT à texte seulement ont capturé la plausibilité du monde mieux que les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 235, "src_audio": "/acl6060/audio/en/235.wav", "src_ref": "So to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour résumer, VALSE est un indicateur de référence qui utilise l'objectif des constructions linguistiques pour aider la communauté à améliorer les modèles de langue et de vision en testant durement leurs capacités de bases visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 236, "src_audio": "/acl6060/audio/en/236.wav", "src_ref": "Our experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que les modèles de langue et de vision identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais luttent pour ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 237, "src_audio": "/acl6060/audio/en/237.wav", "src_ref": "We would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers les bases de la langue avec des modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 238, "src_audio": "/acl6060/audio/en/238.wav", "src_ref": "And even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et plus encore, VALSE pourrait être utilisé comme une évaluation indirecte des données, car les modèles pourraient être évalués avant et après la formation ou le raffinement pour voir si des données aident les modèles à améliorer l'un des aspects testés par VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 239, "src_audio": "/acl6060/audio/en/239.wav", "src_ref": "If you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 240, "src_audio": "/acl6060/audio/en/240.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Kamezawa de l'Université de Tokyo.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 241, "src_audio": "/acl6060/audio/en/241.wav", "src_ref": "I'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter un article intitulé RNSum : des données à grande échelle pour la génération automatique de note de version via la synthèse des journaux de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 242, "src_audio": "/acl6060/audio/en/242.wav", "src_ref": "I'll be explaining in this order.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous expliquer dans cet ordre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 243, "src_audio": "/acl6060/audio/en/243.wav", "src_ref": "First, I will introduce automatic release note generation that we are working on in this research.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, je vais présenter la génération de note de version automatique sur laquelle nous travaillons dans cette recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 244, "src_audio": "/acl6060/audio/en/244.wav", "src_ref": "A release note is a technical document that summarizes the changes distributed with each release of a software product.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une note de version est un document technique qui résume les modifications distribuées à chaque version d'un produit logiciel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 245, "src_audio": "/acl6060/audio/en/245.wav", "src_ref": "The image shows a release note for version two point six point four of the vuejs library.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'image montre une note de version pour la version deux point six point quatre de la bibliothèque Vuejs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 246, "src_audio": "/acl6060/audio/en/246.wav", "src_ref": "Release notes play an important role in open source development but they're time consuming to prepare manually.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version jouent un rôle important dans le développement open source, mais elles mettent du temps à être préparées manuellement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 247, "src_audio": "/acl6060/audio/en/247.wav", "src_ref": "Therefore, it would be very useful to be able to automatically generate high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 248, "src_audio": "/acl6060/audio/en/248.wav", "src_ref": "I will defer to two previous researches on automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'en remettrai à deux recherches antérieures sur la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 249, "src_audio": "/acl6060/audio/en/249.wav", "src_ref": "The first is a system called ARENA released in twenty fourteen.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première est un système appelé ARENA sorti en deux-mille-quatorze.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 250, "src_audio": "/acl6060/audio/en/250.wav", "src_ref": "It takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il faut une approche basée sur des règles, par exemple, en utilisant l'extracteur de changement pour extraire toutes les différences, les changements de bibliothèque et les changements de document à partir des différences entre les versions, et enfin, en les combinant.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 251, "src_audio": "/acl6060/audio/en/251.wav", "src_ref": "The most notable feature of this system is the issue extractor in the upper right corner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 252, "src_audio": "/acl6060/audio/en/252.wav", "src_ref": "Which must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le système de suivi des problèmes, qui doit être laissé à Jira, ne peut être appliqué qu'aux projets qui utilisent Jira.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 253, "src_audio": "/acl6060/audio/en/253.wav", "src_ref": "In other words, it cannot be used for many projects on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En d'autres mots, il ne peut pas être utilisé pour de nombreux projets sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 254, "src_audio": "/acl6060/audio/en/254.wav", "src_ref": "The second is Glyph, recently announced in twenty twenty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde est Glyphe, récemment annoncée en deux-mille-vingt.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 255, "src_audio": "/acl6060/audio/en/255.wav", "src_ref": "It is available on the internet and can be installed via pip.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle est disponible sur internet et peut être installée via pip.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 256, "src_audio": "/acl6060/audio/en/256.wav", "src_ref": "This system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'une des cinq étiquettes telles que les fonctions ou corrections de bugs pour chaque message de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 257, "src_audio": "/acl6060/audio/en/257.wav", "src_ref": "This image is a sample usage that returns a corrective or bug fixes label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette image est un exemple d'utilisation qui renvoie une étiquette de correction ou de débogage.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 258, "src_audio": "/acl6060/audio/en/258.wav", "src_ref": "Glyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données de formation de Glyphe sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 259, "src_audio": "/acl6060/audio/en/259.wav", "src_ref": "The performance of the text classification model is not high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les performances du modèle de classification de texte ne sont pas élevées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 260, "src_audio": "/acl6060/audio/en/260.wav", "src_ref": "I present two related researches, but their problems are limited applicability and scarce data resources.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je présente deux recherches connexes, mais leurs problèmes sont l'applicabilité limitée et les ressources de données rares.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 261, "src_audio": "/acl6060/audio/en/261.wav", "src_ref": "Our paper solves these two problems and automatically generates high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 262, "src_audio": "/acl6060/audio/en/262.wav", "src_ref": "With a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec un problème d'applicabilité limitée, nous proposons une méthode de synthèse par classe de haute qualité en utilisant uniquement des messages de validation comme saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 263, "src_audio": "/acl6060/audio/en/263.wav", "src_ref": "This proposed method can be used for all English repositories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette méthode proposée peut être utilisée pour tous les référentiels anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 264, "src_audio": "/acl6060/audio/en/264.wav", "src_ref": "For the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour le deuxième problème de ressources de données rares, nous avons construit nos données RNSum composées d'environ quatre-vingt-deux mille éléments de données en collectant des données à partir de référentiels GitHub publics en utilisant l'API GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 265, "src_audio": "/acl6060/audio/en/265.wav", "src_ref": "Next, I'll describe our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais décrire nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 266, "src_audio": "/acl6060/audio/en/266.wav", "src_ref": "Here is an example of data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici un exemple de données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 267, "src_audio": "/acl6060/audio/en/267.wav", "src_ref": "The left side is a commit message and the right side is the release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le côté gauche est un message de validation et le côté droit représente les notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 268, "src_audio": "/acl6060/audio/en/268.wav", "src_ref": "Release notes are labeled as improvements or fixes, etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version sont étiquetées comme améliorations ou correctifs, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 269, "src_audio": "/acl6060/audio/en/269.wav", "src_ref": "We have set up a task that takes the commit messages as input and outputs a labeled release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons mis en place une tâche qui prend les messages de validation comme saisie et sorties des notes de version étiquetées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 270, "src_audio": "/acl6060/audio/en/270.wav", "src_ref": "This can be regarded as a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut être considéré comme une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 271, "src_audio": "/acl6060/audio/en/271.wav", "src_ref": "We have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons prédéfini quatre étiquettes : fonctions, améliorations, corrections de bugs, suppressions de dépréciations et modifications de rupture.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 272, "src_audio": "/acl6060/audio/en/272.wav", "src_ref": "These were set based on previous research and other factors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Celles-ci ont été établies sur la base de la recherche antérieure et d'autres facteurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 273, "src_audio": "/acl6060/audio/en/273.wav", "src_ref": "The release note on the bottom right is extracted from the release note on the bottom left.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La note de version en bas à droite est extraite de la note de version en bas à gauche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 274, "src_audio": "/acl6060/audio/en/274.wav", "src_ref": "At this time, it is necessary to detect the four labels that have been set up in advance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été mises en place à l'avance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 275, "src_audio": "/acl6060/audio/en/275.wav", "src_ref": "But the labels are not always consistent with each repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 276, "src_audio": "/acl6060/audio/en/276.wav", "src_ref": "For example, the improvements label includes improvements, enhancements, optimizations, and so on.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, l'étiquette d'améliorations inclut des améliorations, des perfectionnements, des optimisations, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 277, "src_audio": "/acl6060/audio/en/277.wav", "src_ref": "We prepared a vocabulary list of about thirty labels for each of these notational variations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons préparé une liste de vocabulaire d'une trentaine d'étiquettes pour chacune de ces variations de notation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 278, "src_audio": "/acl6060/audio/en/278.wav", "src_ref": "This is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de détecter la classe de note de version et de collecter le texte de la version qui suit en tant que phrase de note de version pour la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 279, "src_audio": "/acl6060/audio/en/279.wav", "src_ref": "Next is a commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il y a un message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 280, "src_audio": "/acl6060/audio/en/280.wav", "src_ref": "Commit messages are not tied to each release.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation ne sont pas liés à chaque version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 281, "src_audio": "/acl6060/audio/en/281.wav", "src_ref": "As shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme le montre l'image ci-dessous, si la version actuelle est la version deux point cinq à dix-neuf, nous devons identifier la version deux point cinq à dix-huit antérieure et obtenir un diff.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 282, "src_audio": "/acl6060/audio/en/282.wav", "src_ref": "This is a bit tedious and it is not enough to just get a list of releases and look at the before and after.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est un peu fastidieux et il ne suffit pas d'obtenir une liste de versions et de regarder l'avant et l'après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 283, "src_audio": "/acl6060/audio/en/283.wav", "src_ref": "We created a heuristic matching rule to get the previous and next versions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons créé une règle de correspondance heuristique pour obtenir les versions antérieures et suivantes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 284, "src_audio": "/acl6060/audio/en/284.wav", "src_ref": "Dataset analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Analyse des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 285, "src_audio": "/acl6060/audio/en/285.wav", "src_ref": "In the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En fin de compte, sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données ont été recueillis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 286, "src_audio": "/acl6060/audio/en/286.wav", "src_ref": "Also, the average number of release notes tokens is sixty three, which is quite high for a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le nombre moyen de gages de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 287, "src_audio": "/acl6060/audio/en/287.wav", "src_ref": "Also, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De même, le nombre de gages uniques est assez grand, s'élevant à huit mille huit cent trente mille.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 288, "src_audio": "/acl6060/audio/en/288.wav", "src_ref": "This is due to the large number of unique class or method names found in the repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela est dû au grand nombre de noms de méthode ou de classe unique trouvés dans le référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 289, "src_audio": "/acl6060/audio/en/289.wav", "src_ref": "Next, I will explain the proposed method.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais expliquer la méthode proposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 290, "src_audio": "/acl6060/audio/en/290.wav", "src_ref": "The classwise extractive then abstractive summarization model consists of two neural modules.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle de synthèse extractive puis abstractive par classe se compose de deux modules neuronaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 291, "src_audio": "/acl6060/audio/en/291.wav", "src_ref": "A classifier using BERT or CodeBERT and a generator using BART.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un classificateur utilisant BERT ou CodeBERT et un générateur utilisant BART.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 292, "src_audio": "/acl6060/audio/en/292.wav", "src_ref": "First, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, CEAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, qui utilisent des améliorations, des corrections de bogues, des dépréciations et d'autres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 293, "src_audio": "/acl6060/audio/en/293.wav", "src_ref": "The commit messages classified as other are discarded.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation classés comme autres sont supprimés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 294, "src_audio": "/acl6060/audio/en/294.wav", "src_ref": "Then CEAS applies the generator to the four labeled documents independently and generates release notes for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 295, "src_audio": "/acl6060/audio/en/295.wav", "src_ref": "In this task, the direct correspondences between commit messages and release notes are not known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 296, "src_audio": "/acl6060/audio/en/296.wav", "src_ref": "Therefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour former le classificateur, nous avons réaffecté les enquêtes à chaque message de validation de saisie en utilisant les dix premiers caractères de chaque message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 297, "src_audio": "/acl6060/audio/en/297.wav", "src_ref": "We modeled the classwise abstractive summarization approach by two different methods.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons modélisé l'approche de synthèse abstractive par classe à travers deux méthodes différentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 298, "src_audio": "/acl6060/audio/en/298.wav", "src_ref": "The first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le premier modèle, que nous appelons CAS unique, se compose d'un réseau de six à six unique et génère un seul texte de note de version donnant un enchaînement de messages de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 299, "src_audio": "/acl6060/audio/en/299.wav", "src_ref": "The output texts can be divided into classwise segments based on special class-specific endpoint symbols.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les textes de sortie peuvent être divisés en segments par classe sur la base de symboles de point d'extrémité particuliers, spécifiques à la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 300, "src_audio": "/acl6060/audio/en/300.wav", "src_ref": "The second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde méthode, que nous appelons CAS multiple, se compose de quatre réseaux seq2seq différents, dont chacun correspond à l'une des classes de notes de version fixes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 301, "src_audio": "/acl6060/audio/en/301.wav", "src_ref": "Okay, let me explain the experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ok, je vais vous expliquer les expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 302, "src_audio": "/acl6060/audio/en/302.wav", "src_ref": "Five methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cinq méthodes ont été comparées : CEAS, CAS unique, CAS multiple, regroupement et étude antérieure, Glyph.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 303, "src_audio": "/acl6060/audio/en/303.wav", "src_ref": "Regarding evaluation, in some cases, release notes are output in multiple sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 304, "src_audio": "/acl6060/audio/en/304.wav", "src_ref": "Since it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisqu'il est difficile de calculer le nombre de phrases telles qu'elles sont, elles sont combinées avec des espaces et traitées comme une seule phrase longue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 305, "src_audio": "/acl6060/audio/en/305.wav", "src_ref": "The BLEU is penalized when the system outputs a short sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le BLEU est pénalisé lorsque le système produit une phrase courte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 306, "src_audio": "/acl6060/audio/en/306.wav", "src_ref": "This penalty results in a lower BLEU value in the experiment results described next.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette pénalité se traduit par une valeur BLEU inférieure dans les résultats de l'expérience décrits ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 307, "src_audio": "/acl6060/audio/en/307.wav", "src_ref": "Finally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous calculons également la spécificité car ROUGE et BLEU ne peuvent pas être calculés si les notes de version sont vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 308, "src_audio": "/acl6060/audio/en/308.wav", "src_ref": "A higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une spécificité plus élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées être vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 309, "src_audio": "/acl6060/audio/en/309.wav", "src_ref": "Here are the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats sont indiqués ci-après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 310, "src_audio": "/acl6060/audio/en/310.wav", "src_ref": "Since the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné que les données contiennent des adresses électroniques, des valeurs hachées, etc., nous avons également évalué les données nettoyées, ce qui les exclut.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 311, "src_audio": "/acl6060/audio/en/311.wav", "src_ref": "CEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS et le CAS ont obtenu des scores de ROUGE-L supérieurs de plus de dix points par rapport aux bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 312, "src_audio": "/acl6060/audio/en/312.wav", "src_ref": "In particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les bases a atteint plus de vingt points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 313, "src_audio": "/acl6060/audio/en/313.wav", "src_ref": "These results indicate that CEAS and CAS are significantly affected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces résultats indiquent que le CEAS et le CAS sont considérablement touchés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 314, "src_audio": "/acl6060/audio/en/314.wav", "src_ref": "CEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS a obtenu un meilleur score ROUGE-L que le CAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour former le classificateur à l'aide de pseudo-étiquettes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 315, "src_audio": "/acl6060/audio/en/315.wav", "src_ref": "High coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une couverture élevée du CEAS peut être obtenue probablement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 316, "src_audio": "/acl6060/audio/en/316.wav", "src_ref": "CAS-Multi tended to yield higher ROUGE-L than CAS-Single.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CAS multiple tendait à produire plus de ROUGE-L que le CAS unique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 317, "src_audio": "/acl6060/audio/en/317.wav", "src_ref": "Suggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En suggérant qu'il est également efficace de développer indépendamment et différemment des modèles de abstractive summarization pour chaque classe de note de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 318, "src_audio": "/acl6060/audio/en/318.wav", "src_ref": "Here are an error analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici une analyse d'erreur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 319, "src_audio": "/acl6060/audio/en/319.wav", "src_ref": "CAS methods tend to output shorter sentences than human reference sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 320, "src_audio": "/acl6060/audio/en/320.wav", "src_ref": "In the figure on the right, the reference sentence has three or four sentences, while CAS has only one.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que le CAS n'en a qu'une.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 321, "src_audio": "/acl6060/audio/en/321.wav", "src_ref": "The reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La raison de la réticence de ce modèle est que dans les données de formation, seulement trente-trois pour cent des phrases sont présentes dans l'étiquette des fonctions et quarante pour cent dans l'étiquette des améliorations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 322, "src_audio": "/acl6060/audio/en/322.wav", "src_ref": "Furthermore, CAS methods cannot generate accurate release notes without additional information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les méthodes CAS ne peuvent pas générer des notes de version précises sans informations supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 323, "src_audio": "/acl6060/audio/en/323.wav", "src_ref": "The top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple en haut à droite est un exemple de message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la progression ou au problème correspondant(e).", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 324, "src_audio": "/acl6060/audio/en/324.wav", "src_ref": "The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple ci-dessous montre que les deux messages de validation dans la saisie sont liés et doivent être combinés en une phrase, mais cela n'est pas fait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 325, "src_audio": "/acl6060/audio/en/325.wav", "src_ref": "Finally, a conclusion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, passons à la conclusion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 326, "src_audio": "/acl6060/audio/en/326.wav", "src_ref": "We have built a new dataset for automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons construit de nouvelles données pour la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 327, "src_audio": "/acl6060/audio/en/327.wav", "src_ref": "We have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également formulé une tâche consistant à saisir des messages de validation et à les résumer afin qu'ils soient applicables à tous les projets écrits en anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 328, "src_audio": "/acl6060/audio/en/328.wav", "src_ref": "Our experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 329, "src_audio": "/acl6060/audio/en/329.wav", "src_ref": "Please check out our dataset on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Veuillez consulter nos données sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 330, "src_audio": "/acl6060/audio/en/330.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 331, "src_audio": "/acl6060/audio/en/331.wav", "src_ref": "Hello. My name is Asaf Harari.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour. Je m'appelle Asaf Harari.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 332, "src_audio": "/acl6060/audio/en/332.wav", "src_ref": "And I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et je vais vous présenter notre article : enrichissement de données tabulaires en quelques coups à l'aide d'architectures de transformateurs raffinées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 333, "src_audio": "/acl6060/audio/en/333.wav", "src_ref": "Data scientists analyze data and mainly focus on the manipulating the data's existing features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les scientifiques analysent les données et se concentrent principalement sur la manipulation des fonctions existantes des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 334, "src_audio": "/acl6060/audio/en/334.wav", "src_ref": "But sometimes, these features are limited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais parfois, ces fonctions sont limitées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 335, "src_audio": "/acl6060/audio/en/335.wav", "src_ref": "Feature generation using another data source may add substantial information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La génération de fonctions en utilisant une autre source de données peut ajouter des informations substantielles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 336, "src_audio": "/acl6060/audio/en/336.wav", "src_ref": "Our research goal is automatic tabular data enrichment using external sources' free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des textes libres de sources externes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 337, "src_audio": "/acl6060/audio/en/337.wav", "src_ref": "Assume we have a tabular dataset and a knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Supposons que nous ayons des données tabulaires et une base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 338, "src_audio": "/acl6060/audio/en/338.wav", "src_ref": "We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles fonctions du texte libre de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 339, "src_audio": "/acl6060/audio/en/339.wav", "src_ref": "Our framework FeSTE is exactly this automatic process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre cadre FeSTE est exactement ce processus automatique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 340, "src_audio": "/acl6060/audio/en/340.wav", "src_ref": "So let's see an example in a dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons donc un exemple dans des données introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 341, "src_audio": "/acl6060/audio/en/341.wav", "src_ref": "In this example, the dataset is university dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données sont des données universitaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 342, "src_audio": "/acl6060/audio/en/342.wav", "src_ref": "When its goal is to classify universities into low ranking universities and high-ranking universities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Quand l'objectif est de classer les universités en universités de bas rang et en universités de haut rang.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 343, "src_audio": "/acl6060/audio/en/343.wav", "src_ref": "As knowledge base, we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 344, "src_audio": "/acl6060/audio/en/344.wav", "src_ref": "The first phase of FeSTE is entity linking.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première phase de FeSTE est la liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 345, "src_audio": "/acl6060/audio/en/345.wav", "src_ref": "When each entity, in this example the university name, is linked to an entity within the knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 346, "src_audio": "/acl6060/audio/en/346.wav", "src_ref": "And and the text of the entities of the knowledge base is extracted and added to the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le texte des entités de la base de connaissances est extrait et ajouté aux données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 347, "src_audio": "/acl6060/audio/en/347.wav", "src_ref": "In this example, the text is the Wikipedia page's abstract.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, le texte est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 348, "src_audio": "/acl6060/audio/en/348.wav", "src_ref": "Now, we need to generate or extract features from the retrieved text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Maintenant, nous devons générer ou extraire des fonctions à partir du texte extrait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 349, "src_audio": "/acl6060/audio/en/349.wav", "src_ref": "So, we need to ah feature extraction phase ah which includes text analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc besoin de la phase d'extraction des fonctions qui comprend l'analyse de texte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 350, "src_audio": "/acl6060/audio/en/350.wav", "src_ref": "And this is the main novelty of this paper and I will deep dive into it in the next slides.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de la principale nouveauté de cet article et je vais mieux l'expliquer dans les prochaines diapositives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 351, "src_audio": "/acl6060/audio/en/351.wav", "src_ref": "After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Après la phase d'extraction des fonctions, il y a une phase de génération de fonctions lorsque nous utilisons les fonctions extraites pour générer un petit nombre de nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 352, "src_audio": "/acl6060/audio/en/352.wav", "src_ref": "First generate ah features in the number of classes of the original dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Générez d'abord les fonctions dans le nombre de classes des données d'origine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 353, "src_audio": "/acl6060/audio/en/353.wav", "src_ref": "In this example, the original dataset has two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données d'origine ont deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 354, "src_audio": "/acl6060/audio/en/354.wav", "src_ref": "So, FeSTE generates two new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, FeSTE génère deux nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 355, "src_audio": "/acl6060/audio/en/355.wav", "src_ref": "But if the dataset has five classes, FeSTE generates five new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si les données ont cinq classes, FeSTE génère cinq nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 356, "src_audio": "/acl6060/audio/en/356.wav", "src_ref": "Each feature represents the likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chaque fonction représente la probabilité pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 357, "src_audio": "/acl6060/audio/en/357.wav", "src_ref": "To analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour analyser le texte, nous utilisons les analyses de texte de pointe, qui sont des modèles de langue basés sur la conversion comme BERT, GPT, XLNet etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 358, "src_audio": "/acl6060/audio/en/358.wav", "src_ref": "It is but it is not likely that we can train language models using the input datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais il est peu probable que nous puissions former des modèles de langue en utilisant les données de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 359, "src_audio": "/acl6060/audio/en/359.wav", "src_ref": "So a naive approach will be ah target task finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une approche naïve sera le raffinement de la tâche cible.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 360, "src_audio": "/acl6060/audio/en/360.wav", "src_ref": "So, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la phase d'extraction des fonctions, nous pouvons télécharger des modèles de langue préformée et raffiner le modèle de langue sur les données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 361, "src_audio": "/acl6060/audio/en/361.wav", "src_ref": "In this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, raffiner le modèle de langue, classer le texte en classes, résumer en classes, basses ou hautes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 362, "src_audio": "/acl6060/audio/en/362.wav", "src_ref": "Receive the language model output, which is the likelihood for each class and use as new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Recevoir les résultats du modèle de langue, qui sont la probabilité pour chaque classe et les utiliser comme nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 363, "src_audio": "/acl6060/audio/en/363.wav", "src_ref": "The problem with this approach is datasets may have few distinct entities / texts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le problème avec cette approche est que les données peuvent avoir peu d'entités / de textes distinct(e)s.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 364, "src_audio": "/acl6060/audio/en/364.wav", "src_ref": "In our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre expérience, près de la moitié des données contiennent moins de quatre cents échantillons et les plus petites données contiennent trente-cinq échantillons dans un ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 365, "src_audio": "/acl6060/audio/en/365.wav", "src_ref": "So to finetune a language model over ah this dataset will be ineffective.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour raffiner un modèle de langue sur cela, les données seront inefficaces.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 366, "src_audio": "/acl6060/audio/en/366.wav", "src_ref": "But we can use prior knowledge about pre-analyzed datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons utiliser des connaissances préalables sur des données préanalysées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 367, "src_audio": "/acl6060/audio/en/367.wav", "src_ref": "Because FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisque nous appliquons FeSTE sur des données multiples, nous pouvons utiliser les données n moins un pour recueillir des informations sur les données n moins un, et utiliser ces informations lorsque nous analysons les nièmes données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 368, "src_audio": "/acl6060/audio/en/368.wav", "src_ref": "What we, what we suggest is to add, to add another finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce que nous suggérons, c'est d'ajouter une autre phase de raffinement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 369, "src_audio": "/acl6060/audio/en/369.wav", "src_ref": "A preliminary multitask finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une phase de raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 370, "src_audio": "/acl6060/audio/en/370.wav", "src_ref": "When you finetune the language model over the n minus one datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque vous raffinez le modèle de langue sur les données n moins un.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 371, "src_audio": "/acl6060/audio/en/371.wav", "src_ref": "And, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ensuite, nous exécutons une autre phase de raffinement qui est un raffinement de tâche cible, lorsque nous raffinons le modèle de langue sur les nièmes données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 372, "src_audio": "/acl6060/audio/en/372.wav", "src_ref": "The state-of-the-art in multitask ah multitask finetuning called MTDNN.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement multitâche de pointe appelé MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 373, "src_audio": "/acl6060/audio/en/373.wav", "src_ref": "In MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN maintient les têtes dans le nombre de tâches dans l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 374, "src_audio": "/acl6060/audio/en/374.wav", "src_ref": "So, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans cet exemple, il y a quatre tâches dans l'ensemble de formation. Le MTDNN maintient alors quatre têtes comme vous pouvez le voir sur l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 375, "src_audio": "/acl6060/audio/en/375.wav", "src_ref": "And it samples a random batch from ah from the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il échantillonne un lot aléatoire de l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 376, "src_audio": "/acl6060/audio/en/376.wav", "src_ref": "And if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si elles appartiennent à un lot aléatoire, par exemple, une seule tâche de classification de phrases, il exécute des chemins d'aller et retour à travers la première tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 377, "src_audio": "/acl6060/audio/en/377.wav", "src_ref": "And if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si le lot aléatoire appartient à la tâche de classement par paire, il exécute un chemin d'aller et retour dans la dernière tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 378, "src_audio": "/acl6060/audio/en/378.wav", "src_ref": "In our scenario, ah tabular datasets vary in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre scénario, les données tabulaires varient dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 379, "src_audio": "/acl6060/audio/en/379.wav", "src_ref": "So there are many tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il y a donc beaucoup de tâches.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 380, "src_audio": "/acl6060/audio/en/380.wav", "src_ref": "MTDNN maintained number of classes, heads, output layers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN a maintenu le nombre de classes, de têtes et de couches de sortie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 381, "src_audio": "/acl6060/audio/en/381.wav", "src_ref": "And the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en outre, le MTDNN doit initialiser de nouvelles têtes pour de nouvelles données avec une nouvelle tâche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 382, "src_audio": "/acl6060/audio/en/382.wav", "src_ref": "Our approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche, appelée raffinement de reformulation de tâche, est dans notre raffinement de reformulation de tâche d'approche. Au lieu de maintenir plusieurs têtes, nous reformulons chaque donnée dans une phrase par problème de classification, étant des tâches de deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 383, "src_audio": "/acl6060/audio/en/383.wav", "src_ref": "So let's see an example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Prenons donc un exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 384, "src_audio": "/acl6060/audio/en/384.wav", "src_ref": "Here is the our input dataset which consists of entities, features, text and classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici nos données de saisie qui se composent d'entités, de fonctions, de texte et de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 385, "src_audio": "/acl6060/audio/en/385.wav", "src_ref": "And, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous reformulons la tâche à partir d'une classification du texte en bas ou haut pour classer le texte, le résumé et la classe en vrai ou faux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 386, "src_audio": "/acl6060/audio/en/386.wav", "src_ref": "Or in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou en d'autres mots, nous avons formé le modèle de langue pour classer un résumé et une classe en résumé et classe, si le résumé appartient à la classe ou non.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 387, "src_audio": "/acl6060/audio/en/387.wav", "src_ref": "So the label vector in this case stays always ah which consists always with two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc dans ce cas, le vecteur d'étiquette consiste toujours en deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 388, "src_audio": "/acl6060/audio/en/388.wav", "src_ref": "And this is the ah algorithm for our fine, reformulated finetuning approach.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il s'agit de l'algorithme pour notre très bonne approche de raffinement reformulée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 389, "src_audio": "/acl6060/audio/en/389.wav", "src_ref": "So let's see the full framework.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons le cadre complet.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 390, "src_audio": "/acl6060/audio/en/390.wav", "src_ref": "Dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données sont introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 391, "src_audio": "/acl6060/audio/en/391.wav", "src_ref": "And then ah FeSTE executes entity linking phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis FeSTE exécute la phase de liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 392, "src_audio": "/acl6060/audio/en/392.wav", "src_ref": "It ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 393, "src_audio": "/acl6060/audio/en/393.wav", "src_ref": "Then it reformulated the task into a pairwise sentence classification task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il a reformulé la tâche en une tâche de classification des phrases par paire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 394, "src_audio": "/acl6060/audio/en/394.wav", "src_ref": "Applied the language model to the new task and the output likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il a appliqué le modèle de langue à la nouvelle tâche et la probabilité de sortie pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 395, "src_audio": "/acl6060/audio/en/395.wav", "src_ref": "And now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et maintenant, le modèle de langue est déjà raffiné sur des données n moins un en utilisant un raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 396, "src_audio": "/acl6060/audio/en/396.wav", "src_ref": "Then we use the output vector of the language model as a newly generated feature in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous utilisons le vecteur de sortie du modèle de langue comme une fonction nouvellement générée dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 397, "src_audio": "/acl6060/audio/en/397.wav", "src_ref": "To evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour évaluer notre cadre, nous utilisons dix-sept données de classification tabulaires qui varient en taille, fonctions, équilibre, domaine et performance initiale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 398, "src_audio": "/acl6060/audio/en/398.wav", "src_ref": "And as knowledge base we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 399, "src_audio": "/acl6060/audio/en/399.wav", "src_ref": "We design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous concevons notre expérience comme une évaluation où nous formons FeSTe sur seize données et l'appliquons à la dix-septième donnée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 400, "src_audio": "/acl6060/audio/en/400.wav", "src_ref": "We also, we also split each dataset into four folds and apply four folds cross validation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous divisons également chaque donnée en quatre plis et appliquons une validation croisée de quatre plis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 401, "src_audio": "/acl6060/audio/en/401.wav", "src_ref": "Then, we generate the new features and evaluate them using five evaluation classifiers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous générons les nouvelles fonctions et les évaluons en utilisant cinq classificateurs d'évaluation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 402, "src_audio": "/acl6060/audio/en/402.wav", "src_ref": "We use in our experiments base BERT base architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons dans nos expériences l'architecture de base des Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 403, "src_audio": "/acl6060/audio/en/403.wav", "src_ref": "Here are the results for our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici les résultats de nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 404, "src_audio": "/acl6060/audio/en/404.wav", "src_ref": "You can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez voir que nous comparons notre cadre au raffinement des données cibles, au raffinement de la tâche cible et à un raffinement préliminaire MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 405, "src_audio": "/acl6060/audio/en/405.wav", "src_ref": "And our reformulated finetuning achieves the best result, the best performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et notre raffinement reformulé atteint le meilleur résultat et la meilleure performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 406, "src_audio": "/acl6060/audio/en/406.wav", "src_ref": "While MTDNN achieved two percent improvement over the target dataset finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que le MTDNN a obtenu une amélioration de deux pour cent par rapport au raffinement des données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 407, "src_audio": "/acl6060/audio/en/407.wav", "src_ref": "Our approach achieved six percent improvement.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche a obtenu une amélioration de six pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 408, "src_audio": "/acl6060/audio/en/408.wav", "src_ref": "When we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque nous regardons les petites données, nous pouvons voir que la performance du MTDNN diminue et l'amélioration de la phase de raffinement multitâche préliminaire diminue aussi à un virgule cinq pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 409, "src_audio": "/acl6060/audio/en/409.wav", "src_ref": "But our performance increased to eleven percent compared to the target task finetuning alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais notre performance a augmenté à onze pour cent comparé au raffinement de tâche cible seul.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 410, "src_audio": "/acl6060/audio/en/410.wav", "src_ref": "For summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour résumer, FeSTE permet un enrichissement en quelques coups à partir de trente-cinq échantillons dans nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 411, "src_audio": "/acl6060/audio/en/411.wav", "src_ref": "It uses one architecture for all tasks and datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il utilise une architecture pour toutes les tâches et données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 412, "src_audio": "/acl6060/audio/en/412.wav", "src_ref": "And it keeps the head of ah of the model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il garde la tête du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 413, "src_audio": "/acl6060/audio/en/413.wav", "src_ref": "But it adds reformulation phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cela ajoute une phase de reformulation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 414, "src_audio": "/acl6060/audio/en/414.wav", "src_ref": "It augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il augmente l'ensemble de formation et a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'introduire dans le modèle de langue et l'utiliser dans le problème de classification par paire de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 415, "src_audio": "/acl6060/audio/en/415.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 416, "src_audio": "/acl6060/audio/en/416.wav", "src_ref": "Hello. My name is Asaf Harari.\nAnd I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.\nData scientists analyze data and mainly focus on the manipulating the data's existing features.\nBut sometimes, these features are limited.\nFeature generation using another data source may add substantial information.\nOur research goal is automatic tabular data enrichment using external sources' free text.\nAssume we have a tabular dataset and a knowledge base.\nWe need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.\nOur framework FeSTE is exactly this automatic process.\nSo let's see an example in a dataset fed into FeSTE.\nIn this example, the dataset is university dataset.\nWhen its goal is to classify universities into low ranking universities and high-ranking universities.\nAs knowledge base, we use Wikipedia.\nThe first phase of FeSTE is entity linking.\nWhen each entity, in this example the university name, is linked to an entity within the knowledge base.\nAnd and the text of the entities of the knowledge base is extracted and added to the dataset.\nIn this example, the text is the Wikipedia page's abstract.\nNow, we need to generate or extract features from the retrieved text.\nSo, we need to ah feature extraction phase ah which includes text analysis.\nAnd this is the main novelty of this paper and I will deep dive into it in the next slides.\nAfter the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.\nFirst generate ah features in the number of classes of the original dataset.\nIn this example, the original dataset has two classes.\nSo, FeSTE generates two new features.\nBut if the dataset has five classes, FeSTE generates five new features.\nEach feature represents the likelihood for each class.\nTo analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.\nIt is but it is not likely that we can train language models using the input datasets.\nSo a naive approach will be ah target task finetuning.\nSo, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.\nIn this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.\nReceive the language model output, which is the likelihood for each class and use as new features.\nThe problem with this approach is datasets may have few distinct entities / texts.\nIn our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.\nSo to finetune a language model over ah this dataset will be ineffective.\nBut we can use prior knowledge about pre-analyzed datasets.\nBecause FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.\nWhat we, what we suggest is to add, to add another finetuning phase.\nA preliminary multitask finetuning phase.\nWhen you finetune the language model over the n minus one datasets.\nAnd, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.\nThe state-of-the-art in multitask ah multitask finetuning called MTDNN.\nIn MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.\nSo, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.\nAnd it samples a random batch from ah from the training set.\nAnd if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.\nAnd if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.\nIn our scenario, ah tabular datasets vary in the number of classes.\nSo there are many tasks.\nMTDNN maintained number of classes, heads, output layers.\nAnd the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.\nOur approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.\nSo let's see an example.\nHere is the our input dataset which consists of entities, features, text and classes.\nAnd, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.\nOr in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.\nSo the label vector in this case stays always ah which consists always with two classes.\nAnd this is the ah algorithm for our fine, reformulated finetuning approach.\nSo let's see the full framework.\nDataset fed into FeSTE.\nAnd then ah FeSTE executes entity linking phase.\nIt ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.\nThen it reformulated the task into a pairwise sentence classification task.\nApplied the language model to the new task and the output likelihood for each class.\nAnd now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.\nThen we use the output vector of the language model as a newly generated feature in the number of classes.\nTo evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.\nAnd as knowledge base we use Wikipedia.\nWe design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.\nWe also, we also split each dataset into four folds and apply four folds cross validation.\nThen, we generate the new features and evaluate them using five evaluation classifiers.\nWe use in our experiments base BERT base architecture.\nHere are the results for our experiments.\nYou can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.\nAnd our reformulated finetuning achieves the best result, the best performance.\nWhile MTDNN achieved two percent improvement over the target dataset finetuning.\nOur approach achieved six percent improvement.\nWhen we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.\nBut our performance increased to eleven percent compared to the target task finetuning alone.\nFor summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.\nIt uses one architecture for all tasks and datasets.\nAnd it keeps the head of ah of the model.\nBut it adds reformulation phase.\nIt augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.\nThank you.", "tgt_ref": "Bonjour. Je m'appelle Asaf Harari.\nEt je vais vous présenter notre article : enrichissement de données tabulaires en quelques coups à l'aide d'architectures de transformateurs raffinées.\nLes scientifiques analysent les données et se concentrent principalement sur la manipulation des fonctions existantes des données.\nMais parfois, ces fonctions sont limitées.\nLa génération de fonctions en utilisant une autre source de données peut ajouter des informations substantielles.\nNotre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des textes libres de sources externes.\nSupposons que nous ayons des données tabulaires et une base de connaissances.\nNous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles fonctions du texte libre de la base de connaissances.\nNotre cadre FeSTE est exactement ce processus automatique.\nVoyons donc un exemple dans des données introduites dans FeSTE.\nDans cet exemple, les données sont des données universitaires.\nQuand l'objectif est de classer les universités en universités de bas rang et en universités de haut rang.\nEn tant que base de connaissances, nous utilisons Wikipédia.\nLa première phase de FeSTE est la liaison d'entités.\nLorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances.\nEt le texte des entités de la base de connaissances est extrait et ajouté aux données.\nDans cet exemple, le texte est le résumé de la page Wikipédia.\nMaintenant, nous devons générer ou extraire des fonctions à partir du texte extrait.\nNous avons donc besoin de la phase d'extraction des fonctions qui comprend l'analyse de texte.\nIl s'agit de la principale nouveauté de cet article et je vais mieux l'expliquer dans les prochaines diapositives.\nAprès la phase d'extraction des fonctions, il y a une phase de génération de fonctions lorsque nous utilisons les fonctions extraites pour générer un petit nombre de nouvelles fonctions.\nGénérez d'abord les fonctions dans le nombre de classes des données d'origine.\nDans cet exemple, les données d'origine ont deux classes.\nAinsi, FeSTE génère deux nouvelles fonctions.\nMais si les données ont cinq classes, FeSTE génère cinq nouvelles fonctions.\nChaque fonction représente la probabilité pour chaque classe.\nPour analyser le texte, nous utilisons les analyses de texte de pointe, qui sont des modèles de langue basés sur la conversion comme BERT, GPT, XLNet etc.\nMais il est peu probable que nous puissions former des modèles de langue en utilisant les données de saisie.\nAinsi, une approche naïve sera le raffinement de la tâche cible.\nDans la phase d'extraction des fonctions, nous pouvons télécharger des modèles de langue préformée et raffiner le modèle de langue sur les données cibles.\nDans cet exemple, raffiner le modèle de langue, classer le texte en classes, résumer en classes, basses ou hautes.\nRecevoir les résultats du modèle de langue, qui sont la probabilité pour chaque classe et les utiliser comme nouvelles fonctions.\nLe problème avec cette approche est que les données peuvent avoir peu d'entités / de textes distinct(e)s.\nDans notre expérience, près de la moitié des données contiennent moins de quatre cents échantillons et les plus petites données contiennent trente-cinq échantillons dans un ensemble de formation.\nDonc, pour raffiner un modèle de langue sur cela, les données seront inefficaces.\nMais nous pouvons utiliser des connaissances préalables sur des données préanalysées.\nPuisque nous appliquons FeSTE sur des données multiples, nous pouvons utiliser les données n moins un pour recueillir des informations sur les données n moins un, et utiliser ces informations lorsque nous analysons les nièmes données.\nCe que nous suggérons, c'est d'ajouter une autre phase de raffinement.\nUne phase de raffinement multitâche préliminaire.\nLorsque vous raffinez le modèle de langue sur les données n moins un.\nEt ensuite, nous exécutons une autre phase de raffinement qui est un raffinement de tâche cible, lorsque nous raffinons le modèle de langue sur les nièmes données cibles.\nLe raffinement multitâche de pointe appelé MTDNN.\nLe MTDNN maintient les têtes dans le nombre de tâches dans l'ensemble de formation.\nDonc, dans cet exemple, il y a quatre tâches dans l'ensemble de formation. Le MTDNN maintient alors quatre têtes comme vous pouvez le voir sur l'image.\nEt il échantillonne un lot aléatoire de l'ensemble de formation.\nEt si elles appartiennent à un lot aléatoire, par exemple, une seule tâche de classification de phrases, il exécute des chemins d'aller et retour à travers la première tête.\nEt si le lot aléatoire appartient à la tâche de classement par paire, il exécute un chemin d'aller et retour dans la dernière tête.\nDans notre scénario, les données tabulaires varient dans le nombre de classes.\nIl y a donc beaucoup de tâches.\nLe MTDNN a maintenu le nombre de classes, de têtes et de couches de sortie.\nEt en outre, le MTDNN doit initialiser de nouvelles têtes pour de nouvelles données avec une nouvelle tâche.\nNotre approche, appelée raffinement de reformulation de tâche, est dans notre raffinement de reformulation de tâche d'approche. Au lieu de maintenir plusieurs têtes, nous reformulons chaque donnée dans une phrase par problème de classification, étant des tâches de deux classes.\nPrenons donc un exemple.\nVoici nos données de saisie qui se composent d'entités, de fonctions, de texte et de classes.\nEt nous reformulons la tâche à partir d'une classification du texte en bas ou haut pour classer le texte, le résumé et la classe en vrai ou faux.\nOu en d'autres mots, nous avons formé le modèle de langue pour classer un résumé et une classe en résumé et classe, si le résumé appartient à la classe ou non.\nDonc dans ce cas, le vecteur d'étiquette consiste toujours en deux classes.\nEt il s'agit de l'algorithme pour notre très bonne approche de raffinement reformulée.\nVoyons le cadre complet.\nLes données sont introduites dans FeSTE.\nPuis FeSTE exécute la phase de liaison d'entités.\nIl extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia.\nEnsuite, il a reformulé la tâche en une tâche de classification des phrases par paire.\nIl a appliqué le modèle de langue à la nouvelle tâche et la probabilité de sortie pour chaque classe.\nEt maintenant, le modèle de langue est déjà raffiné sur des données n moins un en utilisant un raffinement multitâche préliminaire.\nEnsuite, nous utilisons le vecteur de sortie du modèle de langue comme une fonction nouvellement générée dans le nombre de classes.\nPour évaluer notre cadre, nous utilisons dix-sept données de classification tabulaires qui varient en taille, fonctions, équilibre, domaine et performance initiale.\nEt en tant que base de connaissances, nous utilisons Wikipédia.\nNous concevons notre expérience comme une évaluation où nous formons FeSTe sur seize données et l'appliquons à la dix-septième donnée.\nNous divisons également chaque donnée en quatre plis et appliquons une validation croisée de quatre plis.\nEnsuite, nous générons les nouvelles fonctions et les évaluons en utilisant cinq classificateurs d'évaluation.\nNous utilisons dans nos expériences l'architecture de base des Représentations d'encodeurs bidirectionnels à partir de transformateurs.\nVoici les résultats de nos expériences.\nVous pouvez voir que nous comparons notre cadre au raffinement des données cibles, au raffinement de la tâche cible et à un raffinement préliminaire MTDNN.\nEt notre raffinement reformulé atteint le meilleur résultat et la meilleure performance.\nAlors que le MTDNN a obtenu une amélioration de deux pour cent par rapport au raffinement des données cibles.\nNotre approche a obtenu une amélioration de six pour cent.\nLorsque nous regardons les petites données, nous pouvons voir que la performance du MTDNN diminue et l'amélioration de la phase de raffinement multitâche préliminaire diminue aussi à un virgule cinq pour cent.\nMais notre performance a augmenté à onze pour cent comparé au raffinement de tâche cible seul.\nPour résumer, FeSTE permet un enrichissement en quelques coups à partir de trente-cinq échantillons dans nos expériences.\nIl utilise une architecture pour toutes les tâches et données.\nEt il garde la tête du modèle.\nMais cela ajoute une phase de reformulation.\nIl augmente l'ensemble de formation et a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'introduire dans le modèle de langue et l'utiliser dans le problème de classification par paire de phrases.\nMerci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 417, "src_audio": "/acl6060/audio/en/417.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.\nI'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.\nFirst, I'd like to talk about our motivation for reasoning.\nSo here we show an examples where multi-step reasoning is helpful.\nSo this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.\nSo on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.\nBut if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.\nSo it is good to have interpretable multi-step reasoning as output.\nAnd we also think math word problem is a straightforward application to evaluate such reasoning abilities.\nSo, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.\nSo in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.\nSo, certain assumptions ah also apply as in previous work.\nWe assume the precision of quantities are known.\nAnd we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.\nFurthermore, complicated operators can be actually decomposed into these basic operators.\nSo, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.\nSo, traditional sequence to sequence model convert the expression to a specific sequence for generation.\nAnd it is pretty easy to implement and it can generalize to many different complicated problem.\nBut the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.\nBut actually this direction is still quite popular because of um the transformer model.\nSo, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.\nSo here we keep generating the operators until we reach the leaves, which are the quantities.\nSo here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.\nAnd the second thing is that it also contains some repetitive computations.\nSo here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.\nSo, in our proposed approach we want to solve those problems in a step by step and interpretable manners.\nSo for example, here in the second step, ah we can obtain these divisors which is twenty seven.\nAnd we can also refer back to the original questions to find the relevant contents.\nAnd in these steps we obtain the divisors.\nSo, ah and then at this third step we actually get the quotient.\nAlright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.\nSo, here we actually generate the whole expression directly rather than generating a single operators or quantities.\nSo this makes the process more accurate.\nSo, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.\nSo, the expression is represented by e i j o p.\nWhere we perform operator from q_i to q_j, and such expression is actually directed.\nSo, we also have subtraction with words here to represent the opposite direction.\nThis is quite similar to relation extraction.\nSo in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.\nWe add it to the next state to become a new quantity.\nSo, these slides actually visualize the evolution of the state where we keep adding expression to the current state.\nSo in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.\nSo, once we get the quantity representations, we can start to do inference.\nHere we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.\nFirst we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.\nAnd then finally we obtain the expression representation q_1 divided by q_2.\nBut in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.\nSo, here all the possible expression is equals to three times the number of operators.\nSo the nice thing here is that we can easily add constraints to control this search this search space.\nFor example, if this expression is not allowed, we can simply remove this expression in our search space.\nSo in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.\nSo this quantity come from the previous calculated expression.\nSo finally we can obtain this final expression q_3 times q_4.\nAnd we can also see the number of all the possible ah expression is different from the previous step.\nSo, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.\nSo the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.\nAnd here we also use this tau to represent when we should terminate this generation process.\nAnd here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.\nAnd it also allows us to impose certain constraints from prior from prior knowledge.\nSo we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.\nAnd here we briefly show the results compared with the previous best approaches.\nSo our best performing variant is Roberta-DeductiveReasoner.\nAnd in fact we do not use beam search, in contrast all previous approaches are using beam search.\nAll right. So, the best approaches are often tree based model.\nSo, overall our reasoner is able to significantl significantly outperform this tree based model.\nBut we can see the absolute numbers on MathQA or SVAMP are not really high.\nSo we further investigate the results on SVAMP.\nAnd this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.\nSo, in our prediction we find some of the intermediate values are actually negatives.\nFor example, um, in these questions we are asking how many apples does Jake have?\nBut we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.\nSo, our model makes some prediction like this which is producing negative values.\nAnd we observe these two expressions actually have similar scores.\nSo, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.\nSo um we further find such constraint actually improves quite a lot for some models.\nFor example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.\nSo better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.\nAnd we also try to analyze the difficulty behind these behind all these datasets.\nWe assume the number of unused quantities can be regarded as irrelevant information here.\nSo ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.\nAnd here we also show the overall performance.\nFor those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.\nBut with those samples that with unused quantity is actually way worse than the, worse than the overall performance.\nFor MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.\nSo, finally we want to show the interpretability through a question perturbation example.\nSo here our model actually makes a wrong prediction at the first step.\nSo, we can actually correlate this expression with the sentence here. Alright.\nSo, we think this sentence might be misleading the model to an incorrect predictions.\nSo here planting another thirty five makes the model makes the model think it should be an addition operator.\nSo we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.\nSo, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.\nSo, this study shows how the interpretable predictions help us understand the model behavior.\nSo to conclude our work, so first our model is actually pretty efficient.\nAnd we are able to provide interpretable solving procedure.\nAnd we can easily incorporate some prior knowledge as constraint which can help improve the performance.\nAnd the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.\nWe also have certain limitations.\nAh, if we have a large number of operators or constants, the memory consumption could be pretty high.\nAnd the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.\nSo this is the end of the talk, and questions are welcomed. Thank you.", "tgt_ref": "Bonjour tout le monde. Aujourd'hui, je vais vous présenter notre travail de recherche « Apprendre à raisonner par déduction » : résolution de problèmes de mots mathématiques comme extraction de relation complexe.\nJe m'appelle Allan du laboratoire d'intelligence artificielle ByteDance, et ceci est un travail en commun avec Jierui Li de l'Université du Texas à Austin et Wei Lu du SUTD.\nTout d'abord, j'aimerais parler de notre motivation pour le raisonnement.\nDonc ici, nous montrons un exemple où le raisonnement en plusieurs étapes est utile.\nCe chiffre est donc tiré de l'article PaLM où ils incitent à résoudre le problème de réseau dans le scénario d'apprentissage de quelques prises de vue.\nDonc, sur le côté gauche, nous pouvons voir que si nous fournissons quelques exemples avec juste des questions et des réponses, nous pourrions ne pas être en mesure d'obtenir les bonnes réponses.\nMais si nous fournissons plus de description du raisonnement, le modèle est capable de prévenir la description du raisonnement, mais aussi d'effectuer ici une prévention correcte.\nIl est alors bon d'avoir comme résultat un raisonnement à plusieurs étapes interprétable.\nEt nous pensons également que les problèmes de mots mathématiques sont une application directe pour évaluer de telles capacités de raisonnement.\nDonc ici, dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques.\nAinsi, dans nos données, nous recevons également l‘expression mathématique qui conduit à cette réponse particulière.\nDonc, certaines hypothèses s'appliquent également comme dans les travaux antérieurs.\nNous supposons que la précision des quantités est connue.\nEt nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle.\nEn outre, les opérateurs compliqués peuvent effectivement être décomposés en ces opérateurs de base.\nAinsi, le travail antérieur dans la résolution de problèmes de mots mathématiques peut en effet être classé dans un modèle séquence à séquence et séquence à arbre.\nLe modèle séquence à séquence traditionnel convertit donc l'expression en une séquence spécifique pour la génération.\nIl est assez facile à mettre en œuvre et peut être généralisé à de nombreux problèmes compliqués différents.\nMais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle structuré et son manque d'interopérabilité pour la prévention.\nMais en réalité, cette direction est encore très populaire en raison du modèle de conversion.\nDonc, dans les modèles à base d'arbre, nous structurons en réalité ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres.\nDonc ici, nous continuons à produire les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités.\nDonc ici, la bonne chose est qu'en fait, cela nous donne cette structure d'arbre binaire. Mais en réalité, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur et ensuite, à la fin, nous générons les quantités.\nEt la deuxième chose est que cela contient également des calculs répétitifs.\nDonc ici, si nous regardons cette expression, huit fois trois plus trois est en fait généré deux fois, mais en réalité, nous devrions réutiliser les résultats.\nAinsi, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable.\nPar exemple, ici dans la deuxième étape, nous pouvons obtenir ces diviseurs qui sont vingt-sept.\nEt nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent.\nEt dans ces étapes, nous obtenons les diviseurs.\nPuis, à cette troisième étape, nous obtenons alors le quotient.\nTrès bien. Et après ces trois étapes, nous pouvons alors réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, et enfin, nous pouvons obtenir les dividendes.\nDonc ici, nous générons en réalité l'expression entière directement plutôt que de produire un seul opérateur ou une seule quantité.\nCela rend le processus plus précis.\nAinsi, dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et incluant également une certaine constante comme notre état initial.\nAinsi, l'expression est représentée par e i j o p.\nOù nous effectuons l'opérateur de q_i à q_j, et cette expression est en fait dirigée.\nDonc, nous avons également ici la soustraction avec des mots pour représenter la direction opposée.\nC'est tout à fait similaire à l'extraction de relation.\nDonc, dans un système déductif formel, à un pas de temps t, on applique l'opérateur entre la paire q_i et q_j, puis on obtient cette nouvelle expression.\nNous l'ajoutons à l'état suivant pour devenir une nouvelle quantité.\nAinsi, ces diapositives visualisent en réalité l'évolution de l'état où nous continuons à ajouter de l'expression à l'état actuel.\nDonc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langue préformée qui peut être BERT ou Roberta, puis nous encodons la phrase et obtenons ensuite ces représentations de quantité.\nDonc, une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire l'inférence.\nNous montrons ici un exemple de q_1 pour obtenir la représentation pour q_2 divisée par q_2 multipliée par q_3.\nTout d'abord, nous obtenons la représentation de paire, qui n'est essentiellement que l'enchaînement entre q_1 et q_2, puis nous appliquons un réseau prédictif qui est paramétré par l'opérateur.\nEt enfin, nous obtenons la représentation de l'expression q_1 divisée par q_2.\nMais en réalité, dans la pratique, au stade de l'inférence, nous pourrions également obtenir une expression incorrecte.\nDonc ici, toute l'expression possible est égale à trois fois le nombre d'opérateurs.\nAlors, ce qui est bien ici, c'est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche.\nPar exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.\nDonc, dans la deuxième étape, nous faisons la même chose, mais la seule différence est que nous avons une quantité de plus.\nCette quantité provient donc de l'expression calculée antérieure.\nFinalement, nous pouvons obtenir cette expression finale q_3 multipliée par q_4.\nEt nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape antérieure.\nAinsi, une telle différence rend difficile l'application de la beam search car la distribution de probabilité entre ces deux étapes est déséquilibrée.\nLa procédure de formation est donc similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps.\nEt ici, nous utilisons également ce tau pour représenter le moment où nous devrions mettre fin à ce processus de génération.\nEt ici, l'espace est différent de séquence à séquence car il est différent à chaque pas de temps, alors que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire.\nEt cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.\nNous menons donc des expériences sur les données de problèmes de mots mathématiques, MAWPS, Math23K, MathQA et SVAMP couramment utilisées.\nEt ici, nous montrons brièvement les résultats comparés aux meilleures approches antérieures.\nDonc, notre variante la plus performante est Roberta-DeuctiveReasoner.\nEt en réalité, nous n'utilisons pas la beam search ; au contraire, toutes les approches antérieures utilisent la beam search.\nTrès bien. Ainsi, les meilleures approches sont souvent un modèle à base d'arbre.\nDonc, dans l'ensemble, notre raisonneur est capable de dépasser significativement ce modèle à base d'arbre.\nMais nous pouvons voir que les nombres absolus sur MathQA ou SVAMP ne sont pas vraiment élevés.\nNous étudions donc plus en détail les résultats sur SVAMP.\nEt ces données sont difficiles parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle TAL traitement automatique du langage naturel comme ajouter des informations non pertinentes et des quantités supplémentaires.\nDonc, dans notre prévention, nous trouvons que certaines des valeurs intermédiaires sont en réalité négatives.\nPar exemple, dans ces questions, nous demandons « combien de pommes a Jake ? »\nMais nous avons des informations supplémentaires comme « dix-sept photos de moins », et « Steven en a huit », ce qui est totalement hors de propos.\nAinsi, notre modèle effectue une prévention comme celle-ci qui produit des valeurs négatives.\nEt nous constatons que ces deux expressions ont en réalité des scores similaires.\nNous pouvons alors limiter cet espace de recherche en supprimant les résultats négatifs afin que nous puissions rendre la réponse correcte.\nNous trouvons donc que cette contrainte améliore en réalité beaucoup pour certains modèles.\nPar exemple, pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous améliorons sept points, puis pour le modèle de base Roberta, nous avons amélioré deux points.\nLe meilleur modèle de langue a donc de meilleures capacités de compréhension de la langue de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs.\nEt nous essayons également d'analyser la difficulté derrière toutes ces données.\nNous supposons que le nombre de quantités inutilisées peut être considéré ici comme une information non pertinente.\nDonc ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et les données SVAMP ont la plus grande partie.\nEt ici, nous montrons également la performance globale.\nPour ces échantillons sans quantités inutilisées, la performance est en réalité supérieure à la performance globale.\nMais avec ces échantillons ayant une quantité inutilisée, c'est en fait bien pire que la performance globale.\nPour MAWPS, nous n'avons pas vraiment trop de cas de test, alors j'ignore simplement cette partie.\nFinalement, nous voulons montrer l'interopérabilité à travers un exemple de perturbation de question.\nDonc ici, notre modèle effectue en réalité une prévention erronée à la première étape.\nAinsi, nous pouvons effectivement corréler cette expression avec la phrase ici. Très bien.\nNous pensons donc que cette phrase pourrait induire le modèle en erreur avec des préventions incorrectes.\nDonc ici, en planter trente-cinq autres fait penser au modèle qu'il devrait être un opérateur d'addition.\nNous essayons alors de réviser la phrase pour que cela soit quelque chose comme « le nombre de poiriers est inférieur de trente-cinq à celui des pommiers ».\nNous faisons en sorte de transmettre une sémantique plus précise afin que le modèle soit capable de rendre la prévention correcte.\nAinsi, cette étude montre comment les préventions interprétables nous aident à comprendre le comportement du modèle.\nDonc, pour conclure notre travail, notre modèle est en réalité assez efficace.\nEt nous sommes en mesure de fournir une procédure de résolution interprétable.\nEt nous pouvons facilement incorporer des connaissances antérieures en tant que contraintes qui peuvent aider à améliorer la performance.\nEt la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution des problèmes de réseau, mais aussi aux autres tâches qui impliquent un raisonnement à plusieurs étapes.\nNous avons également certaines limites.\nSi nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée.\nEt la deuxième chose est que, comme mentionné, puisque la distribution de probabilité est déséquilibrée entre les différentes étapes de temps, il est donc également assez difficile d'appliquer la stratégie de beam search.\nNous arrivons donc à la fin de la discussion, et vos questions sont les bienvenues. Merci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 418, "src_audio": "/acl6060/audio/en/418.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.\nI will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.\nLegal issues are an integral part of many people's lives.\nBut the majority of citizens have little to know knowledge about their rights and fundamental legal processes.\nAs a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.\nAll work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.\nSuch a system could provide a free professional legal help service for unskilled humans.\nBefore diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.\nGiven a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?\nA model is required to retrieve all relevant statutory articles from a large body of legislation.\nThis information retrieval task comes with its own set of challenges.\nFirst, it deals with two types of language.\nCommon natural language for the questions and complex legal language for the statutes.\nThis difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.\nBesides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.\nInstead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.\nLastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.\nHere, there are long documents that may be up to six thousand words.\nThe recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.\nBut statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.\nIn this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.\nOur Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.\nThese questions cover a wide range of topics from family, housing, money, to work and social security.\nEach of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.\nLet's now talk about how we collected this dataset.\nFirst, we started by compiling a large corpus of legal articles.\nWe considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.\nThen we gathered legal questions with references to relevant statutes.\nTo do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.\nWe were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.\nWe collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.\nLastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.\nThe remaining references were matched and converted to the corresponding article ids from our corpus.\nWe eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.\nIn addition, each question comes with the main category and a concatenation of subcategories.\nAnd each articles comes with a concatenation of the subsequence heading in the structure of the law.\nThis extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.\nLet's look at some characteristic of our dataset.\nThe questions are between five and forty four words long with a median of fourteen words.\nThe articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.\nThe lengthiest one being up to five thousand seven hundred and ninety words.\nAs previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.\nWhile the remaining fifteen percent concern either social security, foreigners or work.\nThe article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.\nHere's the total number of articles collected from each of these Belgian codes.\nOut of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.\nAnd around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.\nMeanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.\nWhich can be explained by the fact that those codes focused less on individuals and their concerns.\nOverall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.\nUsing all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.\nGiven a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.\nWe experiment with the standard TF-IDF and BM25 ranking functions.\nThe main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.\nTo overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.\nWe use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.\nThese embeddings typically result from a pooling operation on the output of a word embedding model.\nFirst, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.\nWe experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.\nAdditionally, we train our own CamemBERT based model ah bi-encoders on our dataset.\nNote that for training, we experiment with the two flavors of the bi-encoder architecture.\nSiamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.\nWe experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.\nHere are the result of our baseline on the test sets.\nWith the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.\nOverall, the finetuned bi-encoder significantly outperforms all the other baselines.\nThe two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.\nAlthough BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.\nRegarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.\nFurthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.\nAlthough promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.\nLet's conclude by discussing two limitations of our dataset.\nFirst, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.\nDuring the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.\nThis information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.\nSecond, we should note that not all legal questions can be answered with statutes alone.\nFor instance, the question, can I evict my tenants if they make too much noise?\nMight not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.\nInstead, the landlord should probably rely more on case law and find precedents similar to their current situation.\nFor example, the tenants makes two parties a week until two AM.\nHence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.\nWe hope that our work sparks interest in developing practical and reliable statutory article retrieval models.\nThat can help improve access to justice for all.\nYou can check out our paper, dataset and code at the following links. Thank you.", "tgt_ref": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht.\nJe vais vous présenter mon travail en commun avec Jerry, qui porte sur de nouvelles données pour l'extraction des articles de droit.\nLes questions juridiques font partie intégrante de la vie de nombreuses personnes.\nMais la majorité des citoyens ont peu de connaissances sur leurs droits et leurs processus juridiques fondamentaux.\nEn conséquence, de nombreux citoyens vulnérables qui n'ont pas les moyens de se payer l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités.\nTous les travaux visent à combler le fossé entre les personnes et la loi en développant un système d'extraction efficace pour les articles de droit.\nUn tel système pourrait fournir un service d'aide juridique professionnel gratuit aux personnes non qualifiées.\nAvant de plonger dans la principale contribution de ce travail, nous allons d'abord décrire le problème d'extraction des articles de droit.\nCompte tenu d'une simple question sur une question juridique telle que : « qu'est-ce que je risque si je viole la confidentialité professionnelle ? »\nUn modèle est nécessaire pour extraire tous les articles de droit pertinents d'un grand corpus législatif.\nCette tâche d'extraction des informations comporte son propre ensemble de défis.\nTout d'abord, elle traite de deux types de langue.\nLa langue naturelle commune pour les questions et la langue juridique complexe pour les lois.\nCette différence dans les distributions de langue rend plus difficile pour un système d'extraire les candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des lois.\nEn outre, le droit écrit n'est pas une pile d'articles indépendants qui peuvent être traités comme une source complète d'informations par eux-mêmes, contrairement aux nouvelles ou aux recettes, par exemple.\nAu lieu de cela, il s'agit d'un ensemble structuré de dispositions juridiques qui n'ont un sens entier que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les domaines et sous-domaines auxquels elles appartiennent et leur place dans la structure de la loi.\nEnfin, les articles de droit ne sont pas de petits paragraphes constituant généralement l'unité d'extraction typique dans la plupart des travaux d'extraction.\nIci, il y a de longs documents qui peuvent aller jusqu'à six mille mots.\nLes recent advances en matière de TAL traitement automatique du langage naturel ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prévention du jugement juridique ou l'examen des contrats de contacts automatisés.\nMais l'extraction de l'article de droit est restée essentiellement inchangée en raison du manque de grandes données étiquetées de haute qualité.\nDans ce travail, nous présentons de nouvelles données centrées sur le citoyen natif français pour étudier si l'extraction des modèles peut se rapprocher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche d'extraction de l'article de droit.\nNos données d'extraction de l'article de droit belge BSARD se composent de plus de mille cent questions juridiques posées par des citoyens belges.\nCes questions couvrent un wide range de sujets allant de la famille au logement, à l'argent, au travail et à la sécurité sociale.\nChacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques de codes de droit belge.\nParlons maintenant de la façon dont nous avons collecté ces données.\nTout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques.\nNous avons examiné trente-deux codes belges accessibles au public et extrait tous les articles ainsi que les titres de section correspondants.\nEnsuite, nous avons rassemblé les questions juridiques avec les références aux lois pertinentes.\nPour ce faire, nous nous associons au cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur une question juridique personnelle.\nNous avons eu la chance d'avoir accès à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges.\nNous avons recueilli des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux lois pertinentes.\nEnfin, nous avons fait passer les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avons considérés.\nLes références restantes ont été appariées et converties aux identifiants d'article correspondants de notre corpus.\nNous nous sommes finalement retrouvés avec mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de droit.\nDe plus, chaque question est accompagnée de la catégorie principale et d'un enchaînement de sous-catégories.\nEt chaque article comporte un enchaînement de la rubrique de sous-séquence dans la structure de la loi.\nCes informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour des recherches futures sur l'extraction d'informations juridiques ou la classification de textes juridiques.\nJetons un coup d'œil à certaines caractéristiques de nos données.\nLes questions comportent entre cinq et quarante-quatre mots avec une moyenne de quatorze mots.\nLes articles sont beaucoup plus longs avec une longueur moyenne de soixante-dix-sept mots, avec cent quarante-deux d'entre eux dépassant les mille mots.\nLe plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots.\nComme mentionné précédemment, les questions couvrent un wide range de sujets, dont environ quatre-vingt-cinq pour cent concernent la famille, le logement, l'argent ou la justice.\nAlors que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail.\nL'article est également très diversifié car il provient de trente-deux codes belges différents qui couvrent un grand nombre de sujets juridiques.\nVoici le nombre total d'articles collectés à partir de chacun de ces codes belges.\nSur les vingt-deux mille six cent trente-trois articles, seuls mille six cent douze sont désignés comme pertinents à au moins une question dans les données.\nEt environ quatre-vingts pour cent de ces articles cités proviennent du code civil, des codes judiciaires, des codes d'enquête criminelle ou des codes pénaux.\nPendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question.\nCe qui peut s'expliquer par le fait que ces codes se concentraient moins sur les individus et leurs préoccupations.\nDans l'ensemble, le nombre moyen de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.\nEn utilisant toutes les données, nous avons comparé plusieurs approches d'extraction, y compris l'architecture lexicale et dense.\nÉtant donné une requête et un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de requête des poids de chacun de ces termes dans cet article.\nNous expérimentons avec les fonctions de classement TF-IDF et BM25 standard.\nLe principal problème de ces approches est qu'elles ne peuvent extraire que les articles contenant des mots-clés présents dans la requête.\nPour surmonter cette limitation, nous expérimentons une architecture neuronale qui peut capturer les relations sémantiques entre les requêtes et l'article.\nNous utilisons un modèle bi-encodeur qui cartographie les requêtes et les articles en représentations vecteurs denses et calculons un score de pertinence entre une paire d'articles de requête par la similitude de leurs intégrations.\nCes intégrations résultent typiquement d'une opération de pooling sur la sortie d'un modèle d'intégration de mots.\nTout d'abord, nous étudions l'efficacité des bi-encodeurs siamois dans une configuration d'évaluation « zero shot », ce qui signifie que des modèles d'intégration de mots préformés sont appliqués immédiatement sans aucun raffinement supplémentaire.\nNous expérimentons avec l'encodeur de texte indépendant du contexte, à savoir word2vec et fastText, et des modèles d'intégration dépendants du contexte, à savoir Roberta et plus précisément CamemBERT qui est un modèle Roberta français.\nDe plus, nous formons nos propres bi-encodeurs de modèles basés sur CamemBERT sur nos données.\nNotez que pour la formation, nous expérimentons avec les deux modèles de l'architecture bi-encodeur.\nLe siamois, qui utilise un modèle d'intégration de mots unique cartographiant la requête et l'article ensemble dans un vector space dense partagé, et le modèle à deux tours, qui utilise deux modèles d'intégration de mots indépendants encodant la requête et l'article séparément dans différents espaces d'intégration.\nNous expérimentons la mise en commun moyenne, maximale et CLS, ainsi que le produit et cosinus pour calculer les similitudes.\nVoici le résultat de notre base sur les ensembles de test.\nAvec les méthodes lexicales ci-dessus, les bi-encodeurs siamois ont évalué dans une configuration zero shot au milieu, et les bi-encodeurs raffinés ci-dessous.\nDans l'ensemble, le bi-encodeur raffiné dépasse de manière significative toutes les autres bases.\nLe modèle à deux tours s'améliore par rapport à ses variantes siamoises lors du rappel à cent, mais fonctionne de la même manière sur les autres indicateurs.\nBien que BM25 ait sous-performé le bi-encodeur formé de manière significative, ses performances indiquent qu'il s'agit toujours d'une base solide pour l'extraction spécifique au domaine.\nEn ce qui concerne l'évaluation zero shot du bi-encodeur siamois, nous constatons que l'utilisation directe des intégrations d'un modèle CamemBERT préformé sans optimiser pour la tâche d'extraction d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats antérieurs.\nEn outre, nous observons que le bi-encodeur basé sur word2vec a dépassé de manière significative les modèles basés sur fastText et les Représentations d'encodeurs bidirectionnels à partir de transformateurs, suggérant que peut-être les intégrations de niveau mot préformées sont plus appropriées pour la tâche que les intégrations de niveau caractère ou sous-mot lorsqu'elles sont utilisées immédiatement.\nBien que prometteurs, ces résultats suggèrent de nombreuses possibilités d'amélioration comparé à un expert juridique qualifié qui peut éventuellement extraire tous les articles pertinents à n'importe quelle question, et ainsi obtenir des scores parfaits.\nConcluons en discutant de deux limites de nos données.\nPremièrement, le corpus d'article est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants.\nAu cours de la construction des données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions ne se retrouvent qu'avec une fraction du nombre initial d'articles pertinents.\nCette information implique donc que la réponse contenue dans les autres articles pertinents pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.\nDeuxièmement, il convient de noter qu'on ne peut pas répondre à toutes les questions juridiques uniquement par des lois.\nPar exemple, la question « puis-je expulser mes locataires s'ils font trop de bruit ? »\nElle peut ne pas avoir de réponse détaillée dans le droit écrit qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée.\nAu lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des antécédents similaires à sa situation actuelle.\nPar exemple, les locataires font la fête deux fois par semaine jusqu'à deux heures du matin.\nPar conséquent, certaines questions sont mieux adaptées que d'autres à la tâche d'extraction de l'article de droit, et le domaine des moins adaptées reste à déterminer.\nNous espérons que nos travaux susciteront l'intérêt pour l'élaboration de modèles d'extraction d'article de droit pratiques et fiables.\nCela peut aider à améliorer l'accès à la justice pour tous.\nVous pouvez consulter notre article, nos données et notre code aux liens suivants. Merci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 419, "src_audio": "/acl6060/audio/en/419.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.\nWhy did we do the trouble in setting up this benchmark?\nWell, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.\nEach one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.\nSo we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.\nBut do we know what the models have actually learned?\nWhat is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?\nAnd the low score for this one?\nDo vision and language models focus on the right thing?\nOr do they focus on biases as shown by previous work?\nTo shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.\nWe target existence, plurality, counting, spatial relations, actions and entity coreference.\nBut how do we test whether the vision and language models have captured this phenomena?\nBy foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.\nFoiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.\nAnd we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.\nFor example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.\nCounting and coreference also are pieces that have more than one instrument.\nAnd we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.\nThis is not easy to do because a foiled caption may be less likely than the original caption.\nFor example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.\nTherefore, to obtain valid foils, we must take action.\nFirst, we make use of strong language models to propose foils.\nSecond, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.\nTo test this automatically, we apply natural language inference with the following rationale.\nWe consider an image to be the premise and its caption its entailed hypothesis.\nIn addition, we consider the caption to be the premise, and the foil is its hypothesis.\nIf an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.\nIf an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.\nBut this procedure is not perfect, it is just an indicator for valid foils.\nTherefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.\nSo, after filtering and human evaluation, we have as many test instances as described in this table.\nNote that VALSE does not deliver any training data but only test data.\nSince it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.\nFinetuning would only enable models to exploit artifacts or statistical biases in the data.\nAnd we all know that these models like to cheat and take shortcuts.\nAnd as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.\nWe experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.\nTwo of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.\nPerhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.\nFor more metrics and results on them, do check out our paper.\nThe results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.\nIt's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.\nHowever, none of the remaining pieces can be reliably solved in our adversarial foiling settings.\nWe see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.\nThe relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.\nThey also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.\nFrom the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.\nAs a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.\nIf the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.\nAnd it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.\nSo to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.\nOur experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.\nWe would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.\nAnd even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.\nIf you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "tgt_ref": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE ; un indice de référence indépendant de la tâche destiné à tester les modèles de langue et de vision avec des phénomènes linguistiques spécifiques.\nPourquoi avons-nous pris la peine de mettre en place cet indice de référence ?\nEh bien, au cours des dernières années, nous avons vu une explosion de la vision basée sur le transformateur et des modèles de langue préformés sur de grandes quantités de paires de textes d'images.\nChacun de ces modèles pousse les tâches de pointe en matière de vision et de langue telles que la réponse aux questions visuelles, le raisonnement de bon sens visuel, l'extraction d'images et les bases de phrases.\nNous avons donc reçu un message : les précisions sur ces tâches et les indices de référence spécifiques augmentent régulièrement.\nMais savons-nous ce que les modèles ont réellement appris ?\nQu'est-ce qu'une vision et un transformateur de langue ont compris lors de l'attribution d'un score élevé pour que cette image et cette phrase correspondent ?\nEt le score bas pour celle-ci ?\nEst-ce que les modèles de langue et de vision se concentrent sur la bonne chose ?\nOu se concentrent-ils sur les biais comme le montre le travail antérieur ?\nPour éclairer davantage cet aspect, nous proposons une direction plus indépendante des tâches et introduisons VALSE qui teste la sensibilité des modèles de langue et de vision à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistiques et visuelles.\nNous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités.\nMais comment tester si les modèles de langue et de vision ont capturé ce phénomène ?\nEn pratiquant le foiling sur une méthode précédemment appliquée pour les modèles de langue et de vision seulement pour des phrases nominales de Ravi Shekhar et de ses collaborateurs, et en comptant par nous dans les travaux antérieurs.\nEffectuer un foiling signifie essentiellement que nous prenons la légende d'une image et produisons un foil en modifiant la légende de sorte qu'elle ne décrive plus l'image.\nEt nous apportons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous aurions trouvé plus d'une façon intéressante de créer des instances de foil.\nPar exemple, dans le cas de l'élément d'actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié avec une action différente, et un dans lequel les acteurs sont échangés.\nLe comptage et la coréférence sont également des éléments qui ont plusieurs instruments.\nEt nous créons ces foils en nous assurant qu'ils ne décrivent pas l'image et que ce sont des phrases grammaticales et autrement valides.\nCe n'est pas facile à faire car une légende ayant subi un foil peut être moins probable que la légende originale.\nPar exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que les plantes coupent un homme qu'un homme coupe les plantes, et la grande vision et les modèles de langue pourraient capter cela.\nPar conséquent, pour obtenir des foils valides, nous devons agir.\nTout d'abord, nous utilisons des modèles de langue forts pour proposer des foils.\nDeuxièmement, nous utilisons l'inférence de la langue naturelle ou la NLI courte pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image.\nPour tester cela automatiquement, nous appliquons l'inférence de la langue naturelle avec la justification suivante.\nNous considérons qu'une image est la prémisse et sa légende son hypothèse implicite.\nEn outre, nous considérons la légende comme la prémisse, et le foil est son hypothèse.\nSi un modèle NLI prévient que le foil est en contradiction ou neutre par rapport à la légende, nous considérons cela comme un indicateur d'un foil valide.\nSi une NLI prévient le foil à engendrer par la légende, cela ne peut pas être un bon foil, car par transitivité, cela donnera une description véridique de l'image et nous filtrons ces foils.\nMais cette procédure n'est pas parfaite, il s'agit simplement d'un indicateur pour les foils valides.\nPar conséquent, comme troisième mesure pour produire des foils valides, nous utilisons des annotateurs humains pour valider les données utilisées dans VALSE.\nDonc, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrit dans ce tableau.\nNotez que VALSE ne fournit pas de données de formation mais seulement des données de test.\nComme il s'agit uniquement d'un indicateur de référence de test zero shot, il est conçu pour tirer parti des capacités existantes des modèles de langue et de vision après la préformation.\nLe raffinement permettrait seulement aux modèles d'exploiter des artefacts ou des biais statistiques dans les données.\nEt nous savons tous que ces modèles aiment tricher et prendre des raccourcis.\nEt comme nous l'avons dit, cela nous intéresse d'évaluer les capacités des modèles de langue et de vision après la préformation.\nNous expérimentons avec cinq modèles de langue et de vision sur VALSE, à savoir avec CLIP, LXMert, ViLBERT, ViLBERT douze en un, et VisualBERT.\nDeux de nos indicateurs d'évaluation les plus importants sont la précision des modèles dans la classification de paires de phrases images en légendes et foils.\nPeut-être plus pertinent pour cette vidéo, nous présenterons notre indicateur plus permissif, la précision par paire, qui mesure si le score de sentence alignment d'image est plus élevé pour la bonne paire de texte image que pour sa paire ayant subi un foil.\nPour plus d'indicateurs et de résultats, consultez notre article.\nLes résultats avec une précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres indicateurs. La meilleure performance zero shot est obtenue par ViLBERT douze en un, suivi de ViLBERT, LXMert, CLIP, et enfin VisualBERT.\nIl est remarquable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par ViLBERT douze en un, en soulignant que les modèles sont capables d'identifier des objets nommés et leur présence dans les images.\nCependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de foiling antagonistes.\nNous voyons à partir des instruments de pluralité et de comptage que les modèles de langue et de vision ont du mal à distinguer les références à des objets uniques par rapport à plusieurs, ou à les compter dans une image.\nL'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image.\nIls ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité comme nous le voyons dans l'élément d'actions.\nÀ partir de l'élément de coréférence, nous découvrons que relever plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de langue et de vision.\nPour vérifier la santé mentale, et parce que c'est une expérience intéressante, nous comparons également deux modèles à texte seulement, GPT un et GPT deux, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et ayant subi un foil, sans image ici, et en prévenant l'entrée avec la plus faible perplexité.\nSi la perplexité est plus élevée pour le foil, nous considérons cela comme une indication que la légende ayant subi un foil peut souffrir de biais de plausibilité ou d'autres biais linguistiques.\nEt il est intéressant de voir que dans certains cas, les modèles GPT à texte seulement ont capturé la plausibilité du monde mieux que les modèles de langue et de vision.\nDonc, pour résumer, VALSE est un indicateur de référence qui utilise l'objectif des constructions linguistiques pour aider la communauté à améliorer les modèles de langue et de vision en testant durement leurs capacités de bases visuelles.\nNos expériences montrent que les modèles de langue et de vision identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais luttent pour ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques.\nNous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers les bases de la langue avec des modèles de langue et de vision.\nEt plus encore, VALSE pourrait être utilisé comme une évaluation indirecte des données, car les modèles pourraient être évalués avant et après la formation ou le raffinement pour voir si des données aident les modèles à améliorer l'un des aspects testés par VALSE.\nSi vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 420, "src_audio": "/acl6060/audio/en/420.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.\nI'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.\nI'll be explaining in this order.\nFirst, I will introduce automatic release note generation that we are working on in this research.\nA release note is a technical document that summarizes the changes distributed with each release of a software product.\nThe image shows a release note for version two point six point four of the vuejs library.\nRelease notes play an important role in open source development but they're time consuming to prepare manually.\nTherefore, it would be very useful to be able to automatically generate high quality release notes.\nI will defer to two previous researches on automatic release note generation.\nThe first is a system called ARENA released in twenty fourteen.\nIt takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.\nThe most notable feature of this system is the issue extractor in the upper right corner.\nWhich must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.\nIn other words, it cannot be used for many projects on GitHub.\nThe second is Glyph, recently announced in twenty twenty.\nIt is available on the internet and can be installed via pip.\nThis system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.\nThis image is a sample usage that returns a corrective or bug fixes label.\nGlyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.\nThe performance of the text classification model is not high.\nI present two related researches, but their problems are limited applicability and scarce data resources.\nOur paper solves these two problems and automatically generates high quality release notes.\nWith a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.\nThis proposed method can be used for all English repositories.\nFor the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.\nNext, I'll describe our dataset.\nHere is an example of data.\nThe left side is a commit message and the right side is the release notes.\nRelease notes are labeled as improvements or fixes, etc.\nWe have set up a task that takes the commit messages as input and outputs a labeled release notes.\nThis can be regarded as a summarization task.\nWe have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.\nThese were set based on previous research and other factors.\nThe release note on the bottom right is extracted from the release note on the bottom left.\nAt this time, it is necessary to detect the four labels that have been set up in advance.\nBut the labels are not always consistent with each repository.\nFor example, the improvements label includes improvements, enhancements, optimizations, and so on.\nWe prepared a vocabulary list of about thirty labels for each of these notational variations.\nThis is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.\nNext is a commit message.\nCommit messages are not tied to each release.\nAs shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.\nThis is a bit tedious and it is not enough to just get a list of releases and look at the before and after.\nWe created a heuristic matching rule to get the previous and next versions.\nDataset analysis.\nIn the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.\nAlso, the average number of release notes tokens is sixty three, which is quite high for a summarization task.\nAlso, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.\nThis is due to the large number of unique class or method names found in the repository.\nNext, I will explain the proposed method.\nThe classwise extractive then abstractive summarization model consists of two neural modules.\nA classifier using BERT or CodeBERT and a generator using BART.\nFirst, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.\nThe commit messages classified as other are discarded.\nThen CEAS applies the generator to the four labeled documents independently and generates release notes for each class.\nIn this task, the direct correspondences between commit messages and release notes are not known.\nTherefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.\nWe modeled the classwise abstractive summarization approach by two different methods.\nThe first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.\nThe output texts can be divided into classwise segments based on special class-specific endpoint symbols.\nThe second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.\nOkay, let me explain the experiments.\nFive methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.\nRegarding evaluation, in some cases, release notes are output in multiple sentences.\nSince it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.\nThe BLEU is penalized when the system outputs a short sentence.\nThis penalty results in a lower BLEU value in the experiment results described next.\nFinally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.\nA higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.\nHere are the results.\nSince the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.\nCEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.\nIn particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.\nThese results indicate that CEAS and CAS are significantly affected.\nCEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.\nHigh coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.\nCAS-Multi tended to yield higher ROUGE-L than CAS-Single.\nSuggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.\nHere are an error analysis.\nCAS methods tend to output shorter sentences than human reference sentences.\nIn the figure on the right, the reference sentence has three or four sentences, while CAS has only one.\nThe reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.\nFurthermore, CAS methods cannot generate accurate release notes without additional information.\nThe top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.\nThe example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.\nFinally, a conclusion.\nWe have built a new dataset for automatic release note generation.\nWe have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.\nOur experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.\nPlease check out our dataset on GitHub.\nThank you.", "tgt_ref": "Bonjour, je m'appelle Kamezawa de l'Université de Tokyo.\nJe vais vous présenter un article intitulé RNSum : des données à grande échelle pour la génération automatique de note de version via la synthèse des journaux de validation.\nJe vais vous expliquer dans cet ordre.\nTout d'abord, je vais présenter la génération de note de version automatique sur laquelle nous travaillons dans cette recherche.\nUne note de version est un document technique qui résume les modifications distribuées à chaque version d'un produit logiciel.\nL'image montre une note de version pour la version deux point six point quatre de la bibliothèque Vuejs.\nLes notes de version jouent un rôle important dans le développement open source, mais elles mettent du temps à être préparées manuellement.\nPar conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.\nJe m'en remettrai à deux recherches antérieures sur la génération automatique de notes de version.\nLa première est un système appelé ARENA sorti en deux-mille-quatorze.\nIl faut une approche basée sur des règles, par exemple, en utilisant l'extracteur de changement pour extraire toutes les différences, les changements de bibliothèque et les changements de document à partir des différences entre les versions, et enfin, en les combinant.\nLa caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit.\nLe système de suivi des problèmes, qui doit être laissé à Jira, ne peut être appliqué qu'aux projets qui utilisent Jira.\nEn d'autres mots, il ne peut pas être utilisé pour de nombreux projets sur GitHub.\nLa seconde est Glyphe, récemment annoncée en deux-mille-vingt.\nElle est disponible sur internet et peut être installée via pip.\nCe système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'une des cinq étiquettes telles que les fonctions ou corrections de bugs pour chaque message de validation de saisie.\nCette image est un exemple d'utilisation qui renvoie une étiquette de correction ou de débogage.\nLes données de formation de Glyphe sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous.\nLes performances du modèle de classification de texte ne sont pas élevées.\nJe présente deux recherches connexes, mais leurs problèmes sont l'applicabilité limitée et les ressources de données rares.\nNotre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité.\nAvec un problème d'applicabilité limitée, nous proposons une méthode de synthèse par classe de haute qualité en utilisant uniquement des messages de validation comme saisie.\nCette méthode proposée peut être utilisée pour tous les référentiels anglais.\nPour le deuxième problème de ressources de données rares, nous avons construit nos données RNSum composées d'environ quatre-vingt-deux mille éléments de données en collectant des données à partir de référentiels GitHub publics en utilisant l'API GitHub.\nEnsuite, je vais décrire nos données.\nVoici un exemple de données.\nLe côté gauche est un message de validation et le côté droit représente les notes de version.\nLes notes de version sont étiquetées comme améliorations ou correctifs, etc.\nNous avons mis en place une tâche qui prend les messages de validation comme saisie et sorties des notes de version étiquetées.\nCela peut être considéré comme une tâche de synthèse.\nNous avons prédéfini quatre étiquettes : fonctions, améliorations, corrections de bugs, suppressions de dépréciations et modifications de rupture.\nCelles-ci ont été établies sur la base de la recherche antérieure et d'autres facteurs.\nLa note de version en bas à droite est extraite de la note de version en bas à gauche.\nÀ ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été mises en place à l'avance.\nMais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel.\nPar exemple, l'étiquette d'améliorations inclut des améliorations, des perfectionnements, des optimisations, etc.\nNous avons préparé une liste de vocabulaire d'une trentaine d'étiquettes pour chacune de ces variations de notation.\nIl s'agit de détecter la classe de note de version et de collecter le texte de la version qui suit en tant que phrase de note de version pour la classe.\nEnsuite, il y a un message de validation.\nLes messages de validation ne sont pas liés à chaque version.\nComme le montre l'image ci-dessous, si la version actuelle est la version deux point cinq à dix-neuf, nous devons identifier la version deux point cinq à dix-huit antérieure et obtenir un diff.\nC'est un peu fastidieux et il ne suffit pas d'obtenir une liste de versions et de regarder l'avant et l'après.\nNous avons créé une règle de correspondance heuristique pour obtenir les versions antérieures et suivantes.\nAnalyse des données.\nEn fin de compte, sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données ont été recueillis.\nEn outre, le nombre moyen de gages de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse.\nDe même, le nombre de gages uniques est assez grand, s'élevant à huit mille huit cent trente mille.\nCela est dû au grand nombre de noms de méthode ou de classe unique trouvés dans le référentiel.\nEnsuite, je vais expliquer la méthode proposée.\nLe modèle de synthèse extractive puis abstractive par classe se compose de deux modules neuronaux.\nUn classificateur utilisant BERT ou CodeBERT et un générateur utilisant BART.\nTout d'abord, CEAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, qui utilisent des améliorations, des corrections de bogues, des dépréciations et d'autres.\nLes messages de validation classés comme autres sont supprimés.\nEnsuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.\nDans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues.\nPar conséquent, pour former le classificateur, nous avons réaffecté les enquêtes à chaque message de validation de saisie en utilisant les dix premiers caractères de chaque message de validation.\nNous avons modélisé l'approche de synthèse abstractive par classe à travers deux méthodes différentes.\nLe premier modèle, que nous appelons CAS unique, se compose d'un réseau de six à six unique et génère un seul texte de note de version donnant un enchaînement de messages de validation de saisie.\nLes textes de sortie peuvent être divisés en segments par classe sur la base de symboles de point d'extrémité particuliers, spécifiques à la classe.\nLa seconde méthode, que nous appelons CAS multiple, se compose de quatre réseaux seq2seq différents, dont chacun correspond à l'une des classes de notes de version fixes.\nOk, je vais vous expliquer les expériences.\nCinq méthodes ont été comparées : CEAS, CAS unique, CAS multiple, regroupement et étude antérieure, Glyph.\nEn ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases.\nPuisqu'il est difficile de calculer le nombre de phrases telles qu'elles sont, elles sont combinées avec des espaces et traitées comme une seule phrase longue.\nLe BLEU est pénalisé lorsque le système produit une phrase courte.\nCette pénalité se traduit par une valeur BLEU inférieure dans les résultats de l'expérience décrits ci-dessous.\nEnfin, nous calculons également la spécificité car ROUGE et BLEU ne peuvent pas être calculés si les notes de version sont vides.\nUne spécificité plus élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées être vides.\nLes résultats sont indiqués ci-après.\nÉtant donné que les données contiennent des adresses électroniques, des valeurs hachées, etc., nous avons également évalué les données nettoyées, ce qui les exclut.\nLe CEAS et le CAS ont obtenu des scores de ROUGE-L supérieurs de plus de dix points par rapport aux bases.\nEn particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les bases a atteint plus de vingt points.\nCes résultats indiquent que le CEAS et le CAS sont considérablement touchés.\nLe CEAS a obtenu un meilleur score ROUGE-L que le CAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour former le classificateur à l'aide de pseudo-étiquettes.\nUne couverture élevée du CEAS peut être obtenue probablement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.\nLe CAS multiple tendait à produire plus de ROUGE-L que le CAS unique.\nEn suggérant qu'il est également efficace de développer indépendamment et différemment des modèles de abstractive summarization pour chaque classe de note de version.\nVoici une analyse d'erreur.\nLes méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines.\nDans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que le CAS n'en a qu'une.\nLa raison de la réticence de ce modèle est que dans les données de formation, seulement trente-trois pour cent des phrases sont présentes dans l'étiquette des fonctions et quarante pour cent dans l'étiquette des améliorations.\nEn outre, les méthodes CAS ne peuvent pas générer des notes de version précises sans informations supplémentaires.\nL'exemple en haut à droite est un exemple de message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la progression ou au problème correspondant(e).\nL'exemple ci-dessous montre que les deux messages de validation dans la saisie sont liés et doivent être combinés en une phrase, mais cela n'est pas fait.\nEnfin, passons à la conclusion.\nNous avons construit de nouvelles données pour la génération automatique de notes de version.\nNous avons également formulé une tâche consistant à saisir des messages de validation et à les résumer afin qu'ils soient applicables à tous les projets écrits en anglais.\nNos expériences montrent que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les bases.\nVeuillez consulter nos données sur GitHub.\nMerci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 0, "src_audio": "/acl6060/audio/en/0.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour tout le monde. Aujourd'hui, je vais vous présenter notre travail de recherche « Apprendre à raisonner par déduction » : résolution de problèmes de mots mathématiques comme extraction de relation complexe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 1, "src_audio": "/acl6060/audio/en/1.wav", "src_ref": "I'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'appelle Allan du laboratoire d'intelligence artificielle ByteDance, et ceci est un travail en commun avec Jierui Li de l'Université du Texas à Austin et Wei Lu du SUTD.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 2, "src_audio": "/acl6060/audio/en/2.wav", "src_ref": "First, I'd like to talk about our motivation for reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, j'aimerais parler de notre motivation pour le raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 3, "src_audio": "/acl6060/audio/en/3.wav", "src_ref": "So here we show an examples where multi-step reasoning is helpful.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous montrons un exemple où le raisonnement en plusieurs étapes est utile.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 4, "src_audio": "/acl6060/audio/en/4.wav", "src_ref": "So this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce chiffre est donc tiré de l'article PaLM où ils incitent à résoudre le problème de réseau dans le scénario d'apprentissage de quelques prises de vue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 5, "src_audio": "/acl6060/audio/en/5.wav", "src_ref": "So on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, sur le côté gauche, nous pouvons voir que si nous fournissons quelques exemples avec juste des questions et des réponses, nous pourrions ne pas être en mesure d'obtenir les bonnes réponses.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 6, "src_audio": "/acl6060/audio/en/6.wav", "src_ref": "But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si nous fournissons plus de description du raisonnement, le modèle est capable de prévenir la description du raisonnement, mais aussi d'effectuer ici une prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 7, "src_audio": "/acl6060/audio/en/7.wav", "src_ref": "So it is good to have interpretable multi-step reasoning as output.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est alors bon d'avoir comme résultat un raisonnement à plusieurs étapes interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 8, "src_audio": "/acl6060/audio/en/8.wav", "src_ref": "And we also think math word problem is a straightforward application to evaluate such reasoning abilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pensons également que les problèmes de mots mathématiques sont une application directe pour évaluer de telles capacités de raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 9, "src_audio": "/acl6060/audio/en/9.wav", "src_ref": "So, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 10, "src_audio": "/acl6060/audio/en/10.wav", "src_ref": "So in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans nos données, nous recevons également l‘expression mathématique qui conduit à cette réponse particulière.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 11, "src_audio": "/acl6060/audio/en/11.wav", "src_ref": "So, certain assumptions ah also apply as in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, certaines hypothèses s'appliquent également comme dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 12, "src_audio": "/acl6060/audio/en/12.wav", "src_ref": "We assume the precision of quantities are known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que la précision des quantités est connue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 13, "src_audio": "/acl6060/audio/en/13.wav", "src_ref": "And we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 14, "src_audio": "/acl6060/audio/en/14.wav", "src_ref": "Furthermore, complicated operators can be actually decomposed into these basic operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les opérateurs compliqués peuvent effectivement être décomposés en ces opérateurs de base.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 15, "src_audio": "/acl6060/audio/en/15.wav", "src_ref": "So, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, le travail antérieur dans la résolution de problèmes de mots mathématiques peut en effet être classé dans un modèle séquence à séquence et séquence à arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 16, "src_audio": "/acl6060/audio/en/16.wav", "src_ref": "So, traditional sequence to sequence model convert the expression to a specific sequence for generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle séquence à séquence traditionnel convertit donc l'expression en une séquence spécifique pour la génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 17, "src_audio": "/acl6060/audio/en/17.wav", "src_ref": "And it is pretty easy to implement and it can generalize to many different complicated problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est assez facile à mettre en œuvre et peut être généralisé à de nombreux problèmes compliqués différents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 18, "src_audio": "/acl6060/audio/en/18.wav", "src_ref": "But the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle structuré et son manque d'interopérabilité pour la prévention.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 19, "src_audio": "/acl6060/audio/en/19.wav", "src_ref": "But actually this direction is still quite popular because of um the transformer model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, cette direction est encore très populaire en raison du modèle de conversion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 20, "src_audio": "/acl6060/audio/en/20.wav", "src_ref": "So, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans les modèles à base d'arbre, nous structurons en réalité ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 21, "src_audio": "/acl6060/audio/en/21.wav", "src_ref": "So here we keep generating the operators until we reach the leaves, which are the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous continuons à produire les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 22, "src_audio": "/acl6060/audio/en/22.wav", "src_ref": "So here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, la bonne chose est qu'en fait, cela nous donne cette structure d'arbre binaire. Mais en réalité, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur et ensuite, à la fin, nous générons les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 23, "src_audio": "/acl6060/audio/en/23.wav", "src_ref": "And the second thing is that it also contains some repetitive computations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que cela contient également des calculs répétitifs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 24, "src_audio": "/acl6060/audio/en/24.wav", "src_ref": "So here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, si nous regardons cette expression, huit fois trois plus trois est en fait généré deux fois, mais en réalité, nous devrions réutiliser les résultats.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 25, "src_audio": "/acl6060/audio/en/25.wav", "src_ref": "So, in our proposed approach we want to solve those problems in a step by step and interpretable manners.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 26, "src_audio": "/acl6060/audio/en/26.wav", "src_ref": "So for example, here in the second step, ah we can obtain these divisors which is twenty seven.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, ici dans la deuxième étape, nous pouvons obtenir ces diviseurs qui sont vingt-sept.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 27, "src_audio": "/acl6060/audio/en/27.wav", "src_ref": "And we can also refer back to the original questions to find the relevant contents.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 28, "src_audio": "/acl6060/audio/en/28.wav", "src_ref": "And in these steps we obtain the divisors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et dans ces étapes, nous obtenons les diviseurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 29, "src_audio": "/acl6060/audio/en/29.wav", "src_ref": "So, ah and then at this third step we actually get the quotient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis, à cette troisième étape, nous obtenons alors le quotient.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 30, "src_audio": "/acl6060/audio/en/30.wav", "src_ref": "Alright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Et après ces trois étapes, nous pouvons alors réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, et enfin, nous pouvons obtenir les dividendes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 31, "src_audio": "/acl6060/audio/en/31.wav", "src_ref": "So, here we actually generate the whole expression directly rather than generating a single operators or quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous générons en réalité l'expression entière directement plutôt que de produire un seul opérateur ou une seule quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 32, "src_audio": "/acl6060/audio/en/32.wav", "src_ref": "So this makes the process more accurate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela rend le processus plus précis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 33, "src_audio": "/acl6060/audio/en/33.wav", "src_ref": "So, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et incluant également une certaine constante comme notre état initial.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 34, "src_audio": "/acl6060/audio/en/34.wav", "src_ref": "So, the expression is represented by e i j o p.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, l'expression est représentée par e i j o p.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 35, "src_audio": "/acl6060/audio/en/35.wav", "src_ref": "Where we perform operator from q_i to q_j, and such expression is actually directed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Où nous effectuons l'opérateur de q_i à q_j, et cette expression est en fait dirigée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 36, "src_audio": "/acl6060/audio/en/36.wav", "src_ref": "So, we also have subtraction with words here to represent the opposite direction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, nous avons également ici la soustraction avec des mots pour représenter la direction opposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 37, "src_audio": "/acl6060/audio/en/37.wav", "src_ref": "This is quite similar to relation extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est tout à fait similaire à l'extraction de relation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 38, "src_audio": "/acl6060/audio/en/38.wav", "src_ref": "So in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans un système déductif formel, à un pas de temps t, on applique l'opérateur entre la paire q_i et q_j, puis on obtient cette nouvelle expression.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 39, "src_audio": "/acl6060/audio/en/39.wav", "src_ref": "We add it to the next state to become a new quantity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous l'ajoutons à l'état suivant pour devenir une nouvelle quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 40, "src_audio": "/acl6060/audio/en/40.wav", "src_ref": "So, these slides actually visualize the evolution of the state where we keep adding expression to the current state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, ces diapositives visualisent en réalité l'évolution de l'état où nous continuons à ajouter de l'expression à l'état actuel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 41, "src_audio": "/acl6060/audio/en/41.wav", "src_ref": "So in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langue préformée qui peut être BERT ou Roberta, puis nous encodons la phrase et obtenons ensuite ces représentations de quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 42, "src_audio": "/acl6060/audio/en/42.wav", "src_ref": "So, once we get the quantity representations, we can start to do inference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire l'inférence.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 43, "src_audio": "/acl6060/audio/en/43.wav", "src_ref": "Here we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous montrons ici un exemple de q_1 pour obtenir la représentation pour q_2 divisée par q_2 multipliée par q_3.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 44, "src_audio": "/acl6060/audio/en/44.wav", "src_ref": "First we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous obtenons la représentation de paire, qui n'est essentiellement que l'enchaînement entre q_1 et q_2, puis nous appliquons un réseau prédictif qui est paramétré par l'opérateur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 45, "src_audio": "/acl6060/audio/en/45.wav", "src_ref": "And then finally we obtain the expression representation q_1 divided by q_2.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et enfin, nous obtenons la représentation de l'expression q_1 divisée par q_2.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 46, "src_audio": "/acl6060/audio/en/46.wav", "src_ref": "But in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, dans la pratique, au stade de l'inférence, nous pourrions également obtenir une expression incorrecte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 47, "src_audio": "/acl6060/audio/en/47.wav", "src_ref": "So, here all the possible expression is equals to three times the number of operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, toute l'expression possible est égale à trois fois le nombre d'opérateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 48, "src_audio": "/acl6060/audio/en/48.wav", "src_ref": "So the nice thing here is that we can easily add constraints to control this search this search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors, ce qui est bien ici, c'est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 49, "src_audio": "/acl6060/audio/en/49.wav", "src_ref": "For example, if this expression is not allowed, we can simply remove this expression in our search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 50, "src_audio": "/acl6060/audio/en/50.wav", "src_ref": "So in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans la deuxième étape, nous faisons la même chose, mais la seule différence est que nous avons une quantité de plus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 51, "src_audio": "/acl6060/audio/en/51.wav", "src_ref": "So this quantity come from the previous calculated expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette quantité provient donc de l'expression calculée antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 52, "src_audio": "/acl6060/audio/en/52.wav", "src_ref": "So finally we can obtain this final expression q_3 times q_4.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous pouvons obtenir cette expression finale q_3 multipliée par q_4.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 53, "src_audio": "/acl6060/audio/en/53.wav", "src_ref": "And we can also see the number of all the possible ah expression is different from the previous step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 54, "src_audio": "/acl6060/audio/en/54.wav", "src_ref": "So, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une telle différence rend difficile l'application de la beam search car la distribution de probabilité entre ces deux étapes est déséquilibrée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 55, "src_audio": "/acl6060/audio/en/55.wav", "src_ref": "So the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La procédure de formation est donc similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 56, "src_audio": "/acl6060/audio/en/56.wav", "src_ref": "And here we also use this tau to represent when we should terminate this generation process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous utilisons également ce tau pour représenter le moment où nous devrions mettre fin à ce processus de génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 57, "src_audio": "/acl6060/audio/en/57.wav", "src_ref": "And here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, l'espace est différent de séquence à séquence car il est différent à chaque pas de temps, alors que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 58, "src_audio": "/acl6060/audio/en/58.wav", "src_ref": "And it also allows us to impose certain constraints from prior from prior knowledge.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 59, "src_audio": "/acl6060/audio/en/59.wav", "src_ref": "So we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous menons donc des expériences sur les données de problèmes de mots mathématiques, MAWPS, Math23K, MathQA et SVAMP couramment utilisées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 60, "src_audio": "/acl6060/audio/en/60.wav", "src_ref": "And here we briefly show the results compared with the previous best approaches.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons brièvement les résultats comparés aux meilleures approches antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 61, "src_audio": "/acl6060/audio/en/61.wav", "src_ref": "So our best performing variant is Roberta-DeductiveReasoner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, notre variante la plus performante est Roberta-DeuctiveReasoner.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 62, "src_audio": "/acl6060/audio/en/62.wav", "src_ref": "And in fact we do not use beam search, in contrast all previous approaches are using beam search.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en réalité, nous n'utilisons pas la beam search ; au contraire, toutes les approches antérieures utilisent la beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 63, "src_audio": "/acl6060/audio/en/63.wav", "src_ref": "All right. So, the best approaches are often tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Ainsi, les meilleures approches sont souvent un modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 64, "src_audio": "/acl6060/audio/en/64.wav", "src_ref": "So, overall our reasoner is able to significantl significantly outperform this tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans l'ensemble, notre raisonneur est capable de dépasser significativement ce modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 65, "src_audio": "/acl6060/audio/en/65.wav", "src_ref": "But we can see the absolute numbers on MathQA or SVAMP are not really high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons voir que les nombres absolus sur MathQA ou SVAMP ne sont pas vraiment élevés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 66, "src_audio": "/acl6060/audio/en/66.wav", "src_ref": "So we further investigate the results on SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous étudions donc plus en détail les résultats sur SVAMP.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 67, "src_audio": "/acl6060/audio/en/67.wav", "src_ref": "And this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ces données sont difficiles parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle TAL traitement automatique du langage naturel comme ajouter des informations non pertinentes et des quantités supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 68, "src_audio": "/acl6060/audio/en/68.wav", "src_ref": "So, in our prediction we find some of the intermediate values are actually negatives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans notre prévention, nous trouvons que certaines des valeurs intermédiaires sont en réalité négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 69, "src_audio": "/acl6060/audio/en/69.wav", "src_ref": "For example, um, in these questions we are asking how many apples does Jake have?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans ces questions, nous demandons « combien de pommes a Jake ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 70, "src_audio": "/acl6060/audio/en/70.wav", "src_ref": "But we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous avons des informations supplémentaires comme « dix-sept photos de moins », et « Steven en a huit », ce qui est totalement hors de propos.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 71, "src_audio": "/acl6060/audio/en/71.wav", "src_ref": "So, our model makes some prediction like this which is producing negative values.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, notre modèle effectue une prévention comme celle-ci qui produit des valeurs négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 72, "src_audio": "/acl6060/audio/en/72.wav", "src_ref": "And we observe these two expressions actually have similar scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous constatons que ces deux expressions ont en réalité des scores similaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 73, "src_audio": "/acl6060/audio/en/73.wav", "src_ref": "So, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pouvons alors limiter cet espace de recherche en supprimant les résultats négatifs afin que nous puissions rendre la réponse correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 74, "src_audio": "/acl6060/audio/en/74.wav", "src_ref": "So um we further find such constraint actually improves quite a lot for some models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous trouvons donc que cette contrainte améliore en réalité beaucoup pour certains modèles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 75, "src_audio": "/acl6060/audio/en/75.wav", "src_ref": "For example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous améliorons sept points, puis pour le modèle de base Roberta, nous avons amélioré deux points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 76, "src_audio": "/acl6060/audio/en/76.wav", "src_ref": "So better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le meilleur modèle de langue a donc de meilleures capacités de compréhension de la langue de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 77, "src_audio": "/acl6060/audio/en/77.wav", "src_ref": "And we also try to analyze the difficulty behind these behind all these datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous essayons également d'analyser la difficulté derrière toutes ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 78, "src_audio": "/acl6060/audio/en/78.wav", "src_ref": "We assume the number of unused quantities can be regarded as irrelevant information here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que le nombre de quantités inutilisées peut être considéré ici comme une information non pertinente.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 79, "src_audio": "/acl6060/audio/en/79.wav", "src_ref": "So ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et les données SVAMP ont la plus grande partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 80, "src_audio": "/acl6060/audio/en/80.wav", "src_ref": "And here we also show the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons également la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 81, "src_audio": "/acl6060/audio/en/81.wav", "src_ref": "For those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ces échantillons sans quantités inutilisées, la performance est en réalité supérieure à la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 82, "src_audio": "/acl6060/audio/en/82.wav", "src_ref": "But with those samples that with unused quantity is actually way worse than the, worse than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais avec ces échantillons ayant une quantité inutilisée, c'est en fait bien pire que la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 83, "src_audio": "/acl6060/audio/en/83.wav", "src_ref": "For MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour MAWPS, nous n'avons pas vraiment trop de cas de test, alors j'ignore simplement cette partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 84, "src_audio": "/acl6060/audio/en/84.wav", "src_ref": "So, finally we want to show the interpretability through a question perturbation example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous voulons montrer l'interopérabilité à travers un exemple de perturbation de question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 85, "src_audio": "/acl6060/audio/en/85.wav", "src_ref": "So here our model actually makes a wrong prediction at the first step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, notre modèle effectue en réalité une prévention erronée à la première étape.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 86, "src_audio": "/acl6060/audio/en/86.wav", "src_ref": "So, we can actually correlate this expression with the sentence here. Alright.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, nous pouvons effectivement corréler cette expression avec la phrase ici. Très bien.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 87, "src_audio": "/acl6060/audio/en/87.wav", "src_ref": "So, we think this sentence might be misleading the model to an incorrect predictions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pensons donc que cette phrase pourrait induire le modèle en erreur avec des préventions incorrectes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 88, "src_audio": "/acl6060/audio/en/88.wav", "src_ref": "So here planting another thirty five makes the model makes the model think it should be an addition operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, en planter trente-cinq autres fait penser au modèle qu'il devrait être un opérateur d'addition.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 89, "src_audio": "/acl6060/audio/en/89.wav", "src_ref": "So we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous essayons alors de réviser la phrase pour que cela soit quelque chose comme « le nombre de poiriers est inférieur de trente-cinq à celui des pommiers ».", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 90, "src_audio": "/acl6060/audio/en/90.wav", "src_ref": "So, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous faisons en sorte de transmettre une sémantique plus précise afin que le modèle soit capable de rendre la prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 91, "src_audio": "/acl6060/audio/en/91.wav", "src_ref": "So, this study shows how the interpretable predictions help us understand the model behavior.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, cette étude montre comment les préventions interprétables nous aident à comprendre le comportement du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 92, "src_audio": "/acl6060/audio/en/92.wav", "src_ref": "So to conclude our work, so first our model is actually pretty efficient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour conclure notre travail, notre modèle est en réalité assez efficace.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 93, "src_audio": "/acl6060/audio/en/93.wav", "src_ref": "And we are able to provide interpretable solving procedure.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous sommes en mesure de fournir une procédure de résolution interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 94, "src_audio": "/acl6060/audio/en/94.wav", "src_ref": "And we can easily incorporate some prior knowledge as constraint which can help improve the performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons facilement incorporer des connaissances antérieures en tant que contraintes qui peuvent aider à améliorer la performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 95, "src_audio": "/acl6060/audio/en/95.wav", "src_ref": "And the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution des problèmes de réseau, mais aussi aux autres tâches qui impliquent un raisonnement à plusieurs étapes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 96, "src_audio": "/acl6060/audio/en/96.wav", "src_ref": "We also have certain limitations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également certaines limites.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 97, "src_audio": "/acl6060/audio/en/97.wav", "src_ref": "Ah, if we have a large number of operators or constants, the memory consumption could be pretty high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 98, "src_audio": "/acl6060/audio/en/98.wav", "src_ref": "And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que, comme mentionné, puisque la distribution de probabilité est déséquilibrée entre les différentes étapes de temps, il est donc également assez difficile d'appliquer la stratégie de beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 99, "src_audio": "/acl6060/audio/en/99.wav", "src_ref": "So this is the end of the talk, and questions are welcomed. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous arrivons donc à la fin de la discussion, et vos questions sont les bienvenues. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 100, "src_audio": "/acl6060/audio/en/100.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 101, "src_audio": "/acl6060/audio/en/101.wav", "src_ref": "I will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter mon travail en commun avec Jerry, qui porte sur de nouvelles données pour l'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 102, "src_audio": "/acl6060/audio/en/102.wav", "src_ref": "Legal issues are an integral part of many people's lives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions juridiques font partie intégrante de la vie de nombreuses personnes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 103, "src_audio": "/acl6060/audio/en/103.wav", "src_ref": "But the majority of citizens have little to know knowledge about their rights and fundamental legal processes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais la majorité des citoyens ont peu de connaissances sur leurs droits et leurs processus juridiques fondamentaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 104, "src_audio": "/acl6060/audio/en/104.wav", "src_ref": "As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En conséquence, de nombreux citoyens vulnérables qui n'ont pas les moyens de se payer l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 105, "src_audio": "/acl6060/audio/en/105.wav", "src_ref": "All work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tous les travaux visent à combler le fossé entre les personnes et la loi en développant un système d'extraction efficace pour les articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 106, "src_audio": "/acl6060/audio/en/106.wav", "src_ref": "Such a system could provide a free professional legal help service for unskilled humans.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un tel système pourrait fournir un service d'aide juridique professionnel gratuit aux personnes non qualifiées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 107, "src_audio": "/acl6060/audio/en/107.wav", "src_ref": "Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avant de plonger dans la principale contribution de ce travail, nous allons d'abord décrire le problème d'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 108, "src_audio": "/acl6060/audio/en/108.wav", "src_ref": "Given a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Compte tenu d'une simple question sur une question juridique telle que : « qu'est-ce que je risque si je viole la confidentialité professionnelle ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 109, "src_audio": "/acl6060/audio/en/109.wav", "src_ref": "A model is required to retrieve all relevant statutory articles from a large body of legislation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un modèle est nécessaire pour extraire tous les articles de droit pertinents d'un grand corpus législatif.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 110, "src_audio": "/acl6060/audio/en/110.wav", "src_ref": "This information retrieval task comes with its own set of challenges.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette tâche d'extraction des informations comporte son propre ensemble de défis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 111, "src_audio": "/acl6060/audio/en/111.wav", "src_ref": "First, it deals with two types of language.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, elle traite de deux types de langue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 112, "src_audio": "/acl6060/audio/en/112.wav", "src_ref": "Common natural language for the questions and complex legal language for the statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La langue naturelle commune pour les questions et la langue juridique complexe pour les lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 113, "src_audio": "/acl6060/audio/en/113.wav", "src_ref": "This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette différence dans les distributions de langue rend plus difficile pour un système d'extraire les candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 114, "src_audio": "/acl6060/audio/en/114.wav", "src_ref": "Besides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le droit écrit n'est pas une pile d'articles indépendants qui peuvent être traités comme une source complète d'informations par eux-mêmes, contrairement aux nouvelles ou aux recettes, par exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 115, "src_audio": "/acl6060/audio/en/115.wav", "src_ref": "Instead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, il s'agit d'un ensemble structuré de dispositions juridiques qui n'ont un sens entier que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les domaines et sous-domaines auxquels elles appartiennent et leur place dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 116, "src_audio": "/acl6060/audio/en/116.wav", "src_ref": "Lastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, les articles de droit ne sont pas de petits paragraphes constituant généralement l'unité d'extraction typique dans la plupart des travaux d'extraction.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 117, "src_audio": "/acl6060/audio/en/117.wav", "src_ref": "Here, there are long documents that may be up to six thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ici, il y a de longs documents qui peuvent aller jusqu'à six mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 118, "src_audio": "/acl6060/audio/en/118.wav", "src_ref": "The recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les recent advances en matière de TAL traitement automatique du langage naturel ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prévention du jugement juridique ou l'examen des contrats de contacts automatisés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 119, "src_audio": "/acl6060/audio/en/119.wav", "src_ref": "But statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais l'extraction de l'article de droit est restée essentiellement inchangée en raison du manque de grandes données étiquetées de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 120, "src_audio": "/acl6060/audio/en/120.wav", "src_ref": "In this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans ce travail, nous présentons de nouvelles données centrées sur le citoyen natif français pour étudier si l'extraction des modèles peut se rapprocher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche d'extraction de l'article de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 121, "src_audio": "/acl6060/audio/en/121.wav", "src_ref": "Our Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos données d'extraction de l'article de droit belge BSARD se composent de plus de mille cent questions juridiques posées par des citoyens belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 122, "src_audio": "/acl6060/audio/en/122.wav", "src_ref": "These questions cover a wide range of topics from family, housing, money, to work and social security.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces questions couvrent un wide range de sujets allant de la famille au logement, à l'argent, au travail et à la sécurité sociale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 123, "src_audio": "/acl6060/audio/en/123.wav", "src_ref": "Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques de codes de droit belge.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 124, "src_audio": "/acl6060/audio/en/124.wav", "src_ref": "Let's now talk about how we collected this dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Parlons maintenant de la façon dont nous avons collecté ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 125, "src_audio": "/acl6060/audio/en/125.wav", "src_ref": "First, we started by compiling a large corpus of legal articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 126, "src_audio": "/acl6060/audio/en/126.wav", "src_ref": "We considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons examiné trente-deux codes belges accessibles au public et extrait tous les articles ainsi que les titres de section correspondants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 127, "src_audio": "/acl6060/audio/en/127.wav", "src_ref": "Then we gathered legal questions with references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous avons rassemblé les questions juridiques avec les références aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 128, "src_audio": "/acl6060/audio/en/128.wav", "src_ref": "To do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ce faire, nous nous associons au cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur une question juridique personnelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 129, "src_audio": "/acl6060/audio/en/129.wav", "src_ref": "We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons eu la chance d'avoir accès à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 130, "src_audio": "/acl6060/audio/en/130.wav", "src_ref": "We collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons recueilli des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 131, "src_audio": "/acl6060/audio/en/131.wav", "src_ref": "Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous avons fait passer les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avons considérés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 132, "src_audio": "/acl6060/audio/en/132.wav", "src_ref": "The remaining references were matched and converted to the corresponding article ids from our corpus.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les références restantes ont été appariées et converties aux identifiants d'article correspondants de notre corpus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 133, "src_audio": "/acl6060/audio/en/133.wav", "src_ref": "We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous nous sommes finalement retrouvés avec mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 134, "src_audio": "/acl6060/audio/en/134.wav", "src_ref": "In addition, each question comes with the main category and a concatenation of subcategories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, chaque question est accompagnée de la catégorie principale et d'un enchaînement de sous-catégories.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 135, "src_audio": "/acl6060/audio/en/135.wav", "src_ref": "And each articles comes with a concatenation of the subsequence heading in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et chaque article comporte un enchaînement de la rubrique de sous-séquence dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 136, "src_audio": "/acl6060/audio/en/136.wav", "src_ref": "This extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour des recherches futures sur l'extraction d'informations juridiques ou la classification de textes juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 137, "src_audio": "/acl6060/audio/en/137.wav", "src_ref": "Let's look at some characteristic of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jetons un coup d'œil à certaines caractéristiques de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 138, "src_audio": "/acl6060/audio/en/138.wav", "src_ref": "The questions are between five and forty four words long with a median of fourteen words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions comportent entre cinq et quarante-quatre mots avec une moyenne de quatorze mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 139, "src_audio": "/acl6060/audio/en/139.wav", "src_ref": "The articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les articles sont beaucoup plus longs avec une longueur moyenne de soixante-dix-sept mots, avec cent quarante-deux d'entre eux dépassant les mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 140, "src_audio": "/acl6060/audio/en/140.wav", "src_ref": "The lengthiest one being up to five thousand seven hundred and ninety words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 141, "src_audio": "/acl6060/audio/en/141.wav", "src_ref": "As previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme mentionné précédemment, les questions couvrent un wide range de sujets, dont environ quatre-vingt-cinq pour cent concernent la famille, le logement, l'argent ou la justice.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 142, "src_audio": "/acl6060/audio/en/142.wav", "src_ref": "While the remaining fifteen percent concern either social security, foreigners or work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 143, "src_audio": "/acl6060/audio/en/143.wav", "src_ref": "The article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'article est également très diversifié car il provient de trente-deux codes belges différents qui couvrent un grand nombre de sujets juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 144, "src_audio": "/acl6060/audio/en/144.wav", "src_ref": "Here's the total number of articles collected from each of these Belgian codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le nombre total d'articles collectés à partir de chacun de ces codes belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 145, "src_audio": "/acl6060/audio/en/145.wav", "src_ref": "Out of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sur les vingt-deux mille six cent trente-trois articles, seuls mille six cent douze sont désignés comme pertinents à au moins une question dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 146, "src_audio": "/acl6060/audio/en/146.wav", "src_ref": "And around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et environ quatre-vingts pour cent de ces articles cités proviennent du code civil, des codes judiciaires, des codes d'enquête criminelle ou des codes pénaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 147, "src_audio": "/acl6060/audio/en/147.wav", "src_ref": "Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 148, "src_audio": "/acl6060/audio/en/148.wav", "src_ref": "Which can be explained by the fact that those codes focused less on individuals and their concerns.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce qui peut s'expliquer par le fait que ces codes se concentraient moins sur les individus et leurs préoccupations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 149, "src_audio": "/acl6060/audio/en/149.wav", "src_ref": "Overall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le nombre moyen de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 150, "src_audio": "/acl6060/audio/en/150.wav", "src_ref": "Using all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En utilisant toutes les données, nous avons comparé plusieurs approches d'extraction, y compris l'architecture lexicale et dense.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 151, "src_audio": "/acl6060/audio/en/151.wav", "src_ref": "Given a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné une requête et un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de requête des poids de chacun de ces termes dans cet article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 152, "src_audio": "/acl6060/audio/en/152.wav", "src_ref": "We experiment with the standard TF-IDF and BM25 ranking functions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec les fonctions de classement TF-IDF et BM25 standard.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 153, "src_audio": "/acl6060/audio/en/153.wav", "src_ref": "The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le principal problème de ces approches est qu'elles ne peuvent extraire que les articles contenant des mots-clés présents dans la requête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 154, "src_audio": "/acl6060/audio/en/154.wav", "src_ref": "To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour surmonter cette limitation, nous expérimentons une architecture neuronale qui peut capturer les relations sémantiques entre les requêtes et l'article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 155, "src_audio": "/acl6060/audio/en/155.wav", "src_ref": "We use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons un modèle bi-encodeur qui cartographie les requêtes et les articles en représentations vecteurs denses et calculons un score de pertinence entre une paire d'articles de requête par la similitude de leurs intégrations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 156, "src_audio": "/acl6060/audio/en/156.wav", "src_ref": "These embeddings typically result from a pooling operation on the output of a word embedding model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces intégrations résultent typiquement d'une opération de pooling sur la sortie d'un modèle d'intégration de mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 157, "src_audio": "/acl6060/audio/en/157.wav", "src_ref": "First, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous étudions l'efficacité des bi-encodeurs siamois dans une configuration d'évaluation « zero shot », ce qui signifie que des modèles d'intégration de mots préformés sont appliqués immédiatement sans aucun raffinement supplémentaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 158, "src_audio": "/acl6060/audio/en/158.wav", "src_ref": "We experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec l'encodeur de texte indépendant du contexte, à savoir word2vec et fastText, et des modèles d'intégration dépendants du contexte, à savoir Roberta et plus précisément CamemBERT qui est un modèle Roberta français.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 159, "src_audio": "/acl6060/audio/en/159.wav", "src_ref": "Additionally, we train our own CamemBERT based model ah bi-encoders on our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, nous formons nos propres bi-encodeurs de modèles basés sur CamemBERT sur nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 160, "src_audio": "/acl6060/audio/en/160.wav", "src_ref": "Note that for training, we experiment with the two flavors of the bi-encoder architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que pour la formation, nous expérimentons avec les deux modèles de l'architecture bi-encodeur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 161, "src_audio": "/acl6060/audio/en/161.wav", "src_ref": "Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le siamois, qui utilise un modèle d'intégration de mots unique cartographiant la requête et l'article ensemble dans un vector space dense partagé, et le modèle à deux tours, qui utilise deux modèles d'intégration de mots indépendants encodant la requête et l'article séparément dans différents espaces d'intégration.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 162, "src_audio": "/acl6060/audio/en/162.wav", "src_ref": "We experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons la mise en commun moyenne, maximale et CLS, ainsi que le produit et cosinus pour calculer les similitudes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 163, "src_audio": "/acl6060/audio/en/163.wav", "src_ref": "Here are the result of our baseline on the test sets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le résultat de notre base sur les ensembles de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 164, "src_audio": "/acl6060/audio/en/164.wav", "src_ref": "With the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec les méthodes lexicales ci-dessus, les bi-encodeurs siamois ont évalué dans une configuration zero shot au milieu, et les bi-encodeurs raffinés ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 165, "src_audio": "/acl6060/audio/en/165.wav", "src_ref": "Overall, the finetuned bi-encoder significantly outperforms all the other baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le bi-encodeur raffiné dépasse de manière significative toutes les autres bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 166, "src_audio": "/acl6060/audio/en/166.wav", "src_ref": "The two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle à deux tours s'améliore par rapport à ses variantes siamoises lors du rappel à cent, mais fonctionne de la même manière sur les autres indicateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 167, "src_audio": "/acl6060/audio/en/167.wav", "src_ref": "Although BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que BM25 ait sous-performé le bi-encodeur formé de manière significative, ses performances indiquent qu'il s'agit toujours d'une base solide pour l'extraction spécifique au domaine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 168, "src_audio": "/acl6060/audio/en/168.wav", "src_ref": "Regarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation zero shot du bi-encodeur siamois, nous constatons que l'utilisation directe des intégrations d'un modèle CamemBERT préformé sans optimiser pour la tâche d'extraction d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 169, "src_audio": "/acl6060/audio/en/169.wav", "src_ref": "Furthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous observons que le bi-encodeur basé sur word2vec a dépassé de manière significative les modèles basés sur fastText et les Représentations d'encodeurs bidirectionnels à partir de transformateurs, suggérant que peut-être les intégrations de niveau mot préformées sont plus appropriées pour la tâche que les intégrations de niveau caractère ou sous-mot lorsqu'elles sont utilisées immédiatement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 170, "src_audio": "/acl6060/audio/en/170.wav", "src_ref": "Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que prometteurs, ces résultats suggèrent de nombreuses possibilités d'amélioration comparé à un expert juridique qualifié qui peut éventuellement extraire tous les articles pertinents à n'importe quelle question, et ainsi obtenir des scores parfaits.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 171, "src_audio": "/acl6060/audio/en/171.wav", "src_ref": "Let's conclude by discussing two limitations of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Concluons en discutant de deux limites de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 172, "src_audio": "/acl6060/audio/en/172.wav", "src_ref": "First, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Premièrement, le corpus d'article est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 173, "src_audio": "/acl6060/audio/en/173.wav", "src_ref": "During the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au cours de la construction des données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions ne se retrouvent qu'avec une fraction du nombre initial d'articles pertinents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 174, "src_audio": "/acl6060/audio/en/174.wav", "src_ref": "This information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette information implique donc que la réponse contenue dans les autres articles pertinents pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 175, "src_audio": "/acl6060/audio/en/175.wav", "src_ref": "Second, we should note that not all legal questions can be answered with statutes alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, il convient de noter qu'on ne peut pas répondre à toutes les questions juridiques uniquement par des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 176, "src_audio": "/acl6060/audio/en/176.wav", "src_ref": "For instance, the question, can I evict my tenants if they make too much noise?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, la question « puis-je expulser mes locataires s'ils font trop de bruit ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 177, "src_audio": "/acl6060/audio/en/177.wav", "src_ref": "Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle peut ne pas avoir de réponse détaillée dans le droit écrit qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 178, "src_audio": "/acl6060/audio/en/178.wav", "src_ref": "Instead, the landlord should probably rely more on case law and find precedents similar to their current situation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des antécédents similaires à sa situation actuelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 179, "src_audio": "/acl6060/audio/en/179.wav", "src_ref": "For example, the tenants makes two parties a week until two AM.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, les locataires font la fête deux fois par semaine jusqu'à deux heures du matin.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 180, "src_audio": "/acl6060/audio/en/180.wav", "src_ref": "Hence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche d'extraction de l'article de droit, et le domaine des moins adaptées reste à déterminer.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 181, "src_audio": "/acl6060/audio/en/181.wav", "src_ref": "We hope that our work sparks interest in developing practical and reliable statutory article retrieval models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous espérons que nos travaux susciteront l'intérêt pour l'élaboration de modèles d'extraction d'article de droit pratiques et fiables.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 182, "src_audio": "/acl6060/audio/en/182.wav", "src_ref": "That can help improve access to justice for all.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut aider à améliorer l'accès à la justice pour tous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 183, "src_audio": "/acl6060/audio/en/183.wav", "src_ref": "You can check out our paper, dataset and code at the following links. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez consulter notre article, nos données et notre code aux liens suivants. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 184, "src_audio": "/acl6060/audio/en/184.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE ; un indice de référence indépendant de la tâche destiné à tester les modèles de langue et de vision avec des phénomènes linguistiques spécifiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 185, "src_audio": "/acl6060/audio/en/185.wav", "src_ref": "Why did we do the trouble in setting up this benchmark?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pourquoi avons-nous pris la peine de mettre en place cet indice de référence ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 186, "src_audio": "/acl6060/audio/en/186.wav", "src_ref": "Well, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eh bien, au cours des dernières années, nous avons vu une explosion de la vision basée sur le transformateur et des modèles de langue préformés sur de grandes quantités de paires de textes d'images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 187, "src_audio": "/acl6060/audio/en/187.wav", "src_ref": "Each one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacun de ces modèles pousse les tâches de pointe en matière de vision et de langue telles que la réponse aux questions visuelles, le raisonnement de bon sens visuel, l'extraction d'images et les bases de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 188, "src_audio": "/acl6060/audio/en/188.wav", "src_ref": "So we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc reçu un message : les précisions sur ces tâches et les indices de référence spécifiques augmentent régulièrement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 189, "src_audio": "/acl6060/audio/en/189.wav", "src_ref": "But do we know what the models have actually learned?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais savons-nous ce que les modèles ont réellement appris ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 190, "src_audio": "/acl6060/audio/en/190.wav", "src_ref": "What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Qu'est-ce qu'une vision et un transformateur de langue ont compris lors de l'attribution d'un score élevé pour que cette image et cette phrase correspondent ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 191, "src_audio": "/acl6060/audio/en/191.wav", "src_ref": "And the low score for this one?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le score bas pour celle-ci ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 192, "src_audio": "/acl6060/audio/en/192.wav", "src_ref": "Do vision and language models focus on the right thing?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Est-ce que les modèles de langue et de vision se concentrent sur la bonne chose ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 193, "src_audio": "/acl6060/audio/en/193.wav", "src_ref": "Or do they focus on biases as shown by previous work?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou se concentrent-ils sur les biais comme le montre le travail antérieur ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 194, "src_audio": "/acl6060/audio/en/194.wav", "src_ref": "To shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour éclairer davantage cet aspect, nous proposons une direction plus indépendante des tâches et introduisons VALSE qui teste la sensibilité des modèles de langue et de vision à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistiques et visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 195, "src_audio": "/acl6060/audio/en/195.wav", "src_ref": "We target existence, plurality, counting, spatial relations, actions and entity coreference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 196, "src_audio": "/acl6060/audio/en/196.wav", "src_ref": "But how do we test whether the vision and language models have captured this phenomena?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais comment tester si les modèles de langue et de vision ont capturé ce phénomène ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 197, "src_audio": "/acl6060/audio/en/197.wav", "src_ref": "By foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En pratiquant le foiling sur une méthode précédemment appliquée pour les modèles de langue et de vision seulement pour des phrases nominales de Ravi Shekhar et de ses collaborateurs, et en comptant par nous dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 198, "src_audio": "/acl6060/audio/en/198.wav", "src_ref": "Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Effectuer un foiling signifie essentiellement que nous prenons la légende d'une image et produisons un foil en modifiant la légende de sorte qu'elle ne décrive plus l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 199, "src_audio": "/acl6060/audio/en/199.wav", "src_ref": "And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous apportons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous aurions trouvé plus d'une façon intéressante de créer des instances de foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 200, "src_audio": "/acl6060/audio/en/200.wav", "src_ref": "For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans le cas de l'élément d'actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié avec une action différente, et un dans lequel les acteurs sont échangés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 201, "src_audio": "/acl6060/audio/en/201.wav", "src_ref": "Counting and coreference also are pieces that have more than one instrument.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le comptage et la coréférence sont également des éléments qui ont plusieurs instruments.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 202, "src_audio": "/acl6060/audio/en/202.wav", "src_ref": "And we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous créons ces foils en nous assurant qu'ils ne décrivent pas l'image et que ce sont des phrases grammaticales et autrement valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 203, "src_audio": "/acl6060/audio/en/203.wav", "src_ref": "This is not easy to do because a foiled caption may be less likely than the original caption.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce n'est pas facile à faire car une légende ayant subi un foil peut être moins probable que la légende originale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 204, "src_audio": "/acl6060/audio/en/204.wav", "src_ref": "For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que les plantes coupent un homme qu'un homme coupe les plantes, et la grande vision et les modèles de langue pourraient capter cela.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 205, "src_audio": "/acl6060/audio/en/205.wav", "src_ref": "Therefore, to obtain valid foils, we must take action.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour obtenir des foils valides, nous devons agir.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 206, "src_audio": "/acl6060/audio/en/206.wav", "src_ref": "First, we make use of strong language models to propose foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous utilisons des modèles de langue forts pour proposer des foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 207, "src_audio": "/acl6060/audio/en/207.wav", "src_ref": "Second, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, nous utilisons l'inférence de la langue naturelle ou la NLI courte pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 208, "src_audio": "/acl6060/audio/en/208.wav", "src_ref": "To test this automatically, we apply natural language inference with the following rationale.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour tester cela automatiquement, nous appliquons l'inférence de la langue naturelle avec la justification suivante.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 209, "src_audio": "/acl6060/audio/en/209.wav", "src_ref": "We consider an image to be the premise and its caption its entailed hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous considérons qu'une image est la prémisse et sa légende son hypothèse implicite.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 210, "src_audio": "/acl6060/audio/en/210.wav", "src_ref": "In addition, we consider the caption to be the premise, and the foil is its hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous considérons la légende comme la prémisse, et le foil est son hypothèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 211, "src_audio": "/acl6060/audio/en/211.wav", "src_ref": "If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si un modèle NLI prévient que le foil est en contradiction ou neutre par rapport à la légende, nous considérons cela comme un indicateur d'un foil valide.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 212, "src_audio": "/acl6060/audio/en/212.wav", "src_ref": "If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si une NLI prévient le foil à engendrer par la légende, cela ne peut pas être un bon foil, car par transitivité, cela donnera une description véridique de l'image et nous filtrons ces foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 213, "src_audio": "/acl6060/audio/en/213.wav", "src_ref": "But this procedure is not perfect, it is just an indicator for valid foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cette procédure n'est pas parfaite, il s'agit simplement d'un indicateur pour les foils valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 214, "src_audio": "/acl6060/audio/en/214.wav", "src_ref": "Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, comme troisième mesure pour produire des foils valides, nous utilisons des annotateurs humains pour valider les données utilisées dans VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 215, "src_audio": "/acl6060/audio/en/215.wav", "src_ref": "So, after filtering and human evaluation, we have as many test instances as described in this table.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrit dans ce tableau.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 216, "src_audio": "/acl6060/audio/en/216.wav", "src_ref": "Note that VALSE does not deliver any training data but only test data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que VALSE ne fournit pas de données de formation mais seulement des données de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 217, "src_audio": "/acl6060/audio/en/217.wav", "src_ref": "Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme il s'agit uniquement d'un indicateur de référence de test zero shot, il est conçu pour tirer parti des capacités existantes des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 218, "src_audio": "/acl6060/audio/en/218.wav", "src_ref": "Finetuning would only enable models to exploit artifacts or statistical biases in the data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement permettrait seulement aux modèles d'exploiter des artefacts ou des biais statistiques dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 219, "src_audio": "/acl6060/audio/en/219.wav", "src_ref": "And we all know that these models like to cheat and take shortcuts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 220, "src_audio": "/acl6060/audio/en/220.wav", "src_ref": "And as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et comme nous l'avons dit, cela nous intéresse d'évaluer les capacités des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 221, "src_audio": "/acl6060/audio/en/221.wav", "src_ref": "We experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec cinq modèles de langue et de vision sur VALSE, à savoir avec CLIP, LXMert, ViLBERT, ViLBERT douze en un, et VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 222, "src_audio": "/acl6060/audio/en/222.wav", "src_ref": "Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deux de nos indicateurs d'évaluation les plus importants sont la précision des modèles dans la classification de paires de phrases images en légendes et foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 223, "src_audio": "/acl6060/audio/en/223.wav", "src_ref": "Perhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Peut-être plus pertinent pour cette vidéo, nous présenterons notre indicateur plus permissif, la précision par paire, qui mesure si le score de sentence alignment d'image est plus élevé pour la bonne paire de texte image que pour sa paire ayant subi un foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 224, "src_audio": "/acl6060/audio/en/224.wav", "src_ref": "For more metrics and results on them, do check out our paper.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour plus d'indicateurs et de résultats, consultez notre article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 225, "src_audio": "/acl6060/audio/en/225.wav", "src_ref": "The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats avec une précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres indicateurs. La meilleure performance zero shot est obtenue par ViLBERT douze en un, suivi de ViLBERT, LXMert, CLIP, et enfin VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 226, "src_audio": "/acl6060/audio/en/226.wav", "src_ref": "It's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est remarquable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par ViLBERT douze en un, en soulignant que les modèles sont capables d'identifier des objets nommés et leur présence dans les images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 227, "src_audio": "/acl6060/audio/en/227.wav", "src_ref": "However, none of the remaining pieces can be reliably solved in our adversarial foiling settings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de foiling antagonistes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 228, "src_audio": "/acl6060/audio/en/228.wav", "src_ref": "We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous voyons à partir des instruments de pluralité et de comptage que les modèles de langue et de vision ont du mal à distinguer les références à des objets uniques par rapport à plusieurs, ou à les compter dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 229, "src_audio": "/acl6060/audio/en/229.wav", "src_ref": "The relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 230, "src_audio": "/acl6060/audio/en/230.wav", "src_ref": "They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ils ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité comme nous le voyons dans l'élément d'actions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 231, "src_audio": "/acl6060/audio/en/231.wav", "src_ref": "From the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À partir de l'élément de coréférence, nous découvrons que relever plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 232, "src_audio": "/acl6060/audio/en/232.wav", "src_ref": "As a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour vérifier la santé mentale, et parce que c'est une expérience intéressante, nous comparons également deux modèles à texte seulement, GPT un et GPT deux, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et ayant subi un foil, sans image ici, et en prévenant l'entrée avec la plus faible perplexité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 233, "src_audio": "/acl6060/audio/en/233.wav", "src_ref": "If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si la perplexité est plus élevée pour le foil, nous considérons cela comme une indication que la légende ayant subi un foil peut souffrir de biais de plausibilité ou d'autres biais linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 234, "src_audio": "/acl6060/audio/en/234.wav", "src_ref": "And it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il est intéressant de voir que dans certains cas, les modèles GPT à texte seulement ont capturé la plausibilité du monde mieux que les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 235, "src_audio": "/acl6060/audio/en/235.wav", "src_ref": "So to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour résumer, VALSE est un indicateur de référence qui utilise l'objectif des constructions linguistiques pour aider la communauté à améliorer les modèles de langue et de vision en testant durement leurs capacités de bases visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 236, "src_audio": "/acl6060/audio/en/236.wav", "src_ref": "Our experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que les modèles de langue et de vision identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais luttent pour ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 237, "src_audio": "/acl6060/audio/en/237.wav", "src_ref": "We would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers les bases de la langue avec des modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 238, "src_audio": "/acl6060/audio/en/238.wav", "src_ref": "And even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et plus encore, VALSE pourrait être utilisé comme une évaluation indirecte des données, car les modèles pourraient être évalués avant et après la formation ou le raffinement pour voir si des données aident les modèles à améliorer l'un des aspects testés par VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 239, "src_audio": "/acl6060/audio/en/239.wav", "src_ref": "If you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 240, "src_audio": "/acl6060/audio/en/240.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Kamezawa de l'Université de Tokyo.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 241, "src_audio": "/acl6060/audio/en/241.wav", "src_ref": "I'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter un article intitulé RNSum : des données à grande échelle pour la génération automatique de note de version via la synthèse des journaux de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 242, "src_audio": "/acl6060/audio/en/242.wav", "src_ref": "I'll be explaining in this order.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous expliquer dans cet ordre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 243, "src_audio": "/acl6060/audio/en/243.wav", "src_ref": "First, I will introduce automatic release note generation that we are working on in this research.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, je vais présenter la génération de note de version automatique sur laquelle nous travaillons dans cette recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 244, "src_audio": "/acl6060/audio/en/244.wav", "src_ref": "A release note is a technical document that summarizes the changes distributed with each release of a software product.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une note de version est un document technique qui résume les modifications distribuées à chaque version d'un produit logiciel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 245, "src_audio": "/acl6060/audio/en/245.wav", "src_ref": "The image shows a release note for version two point six point four of the vuejs library.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'image montre une note de version pour la version deux point six point quatre de la bibliothèque Vuejs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 246, "src_audio": "/acl6060/audio/en/246.wav", "src_ref": "Release notes play an important role in open source development but they're time consuming to prepare manually.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version jouent un rôle important dans le développement open source, mais elles mettent du temps à être préparées manuellement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 247, "src_audio": "/acl6060/audio/en/247.wav", "src_ref": "Therefore, it would be very useful to be able to automatically generate high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 248, "src_audio": "/acl6060/audio/en/248.wav", "src_ref": "I will defer to two previous researches on automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'en remettrai à deux recherches antérieures sur la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 249, "src_audio": "/acl6060/audio/en/249.wav", "src_ref": "The first is a system called ARENA released in twenty fourteen.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première est un système appelé ARENA sorti en deux-mille-quatorze.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 250, "src_audio": "/acl6060/audio/en/250.wav", "src_ref": "It takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il faut une approche basée sur des règles, par exemple, en utilisant l'extracteur de changement pour extraire toutes les différences, les changements de bibliothèque et les changements de document à partir des différences entre les versions, et enfin, en les combinant.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 251, "src_audio": "/acl6060/audio/en/251.wav", "src_ref": "The most notable feature of this system is the issue extractor in the upper right corner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 252, "src_audio": "/acl6060/audio/en/252.wav", "src_ref": "Which must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le système de suivi des problèmes, qui doit être laissé à Jira, ne peut être appliqué qu'aux projets qui utilisent Jira.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 253, "src_audio": "/acl6060/audio/en/253.wav", "src_ref": "In other words, it cannot be used for many projects on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En d'autres mots, il ne peut pas être utilisé pour de nombreux projets sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 254, "src_audio": "/acl6060/audio/en/254.wav", "src_ref": "The second is Glyph, recently announced in twenty twenty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde est Glyphe, récemment annoncée en deux-mille-vingt.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 255, "src_audio": "/acl6060/audio/en/255.wav", "src_ref": "It is available on the internet and can be installed via pip.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle est disponible sur internet et peut être installée via pip.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 256, "src_audio": "/acl6060/audio/en/256.wav", "src_ref": "This system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'une des cinq étiquettes telles que les fonctions ou corrections de bugs pour chaque message de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 257, "src_audio": "/acl6060/audio/en/257.wav", "src_ref": "This image is a sample usage that returns a corrective or bug fixes label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette image est un exemple d'utilisation qui renvoie une étiquette de correction ou de débogage.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 258, "src_audio": "/acl6060/audio/en/258.wav", "src_ref": "Glyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données de formation de Glyphe sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 259, "src_audio": "/acl6060/audio/en/259.wav", "src_ref": "The performance of the text classification model is not high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les performances du modèle de classification de texte ne sont pas élevées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 260, "src_audio": "/acl6060/audio/en/260.wav", "src_ref": "I present two related researches, but their problems are limited applicability and scarce data resources.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je présente deux recherches connexes, mais leurs problèmes sont l'applicabilité limitée et les ressources de données rares.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 261, "src_audio": "/acl6060/audio/en/261.wav", "src_ref": "Our paper solves these two problems and automatically generates high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 262, "src_audio": "/acl6060/audio/en/262.wav", "src_ref": "With a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec un problème d'applicabilité limitée, nous proposons une méthode de synthèse par classe de haute qualité en utilisant uniquement des messages de validation comme saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 263, "src_audio": "/acl6060/audio/en/263.wav", "src_ref": "This proposed method can be used for all English repositories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette méthode proposée peut être utilisée pour tous les référentiels anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 264, "src_audio": "/acl6060/audio/en/264.wav", "src_ref": "For the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour le deuxième problème de ressources de données rares, nous avons construit nos données RNSum composées d'environ quatre-vingt-deux mille éléments de données en collectant des données à partir de référentiels GitHub publics en utilisant l'API GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 265, "src_audio": "/acl6060/audio/en/265.wav", "src_ref": "Next, I'll describe our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais décrire nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 266, "src_audio": "/acl6060/audio/en/266.wav", "src_ref": "Here is an example of data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici un exemple de données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 267, "src_audio": "/acl6060/audio/en/267.wav", "src_ref": "The left side is a commit message and the right side is the release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le côté gauche est un message de validation et le côté droit représente les notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 268, "src_audio": "/acl6060/audio/en/268.wav", "src_ref": "Release notes are labeled as improvements or fixes, etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version sont étiquetées comme améliorations ou correctifs, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 269, "src_audio": "/acl6060/audio/en/269.wav", "src_ref": "We have set up a task that takes the commit messages as input and outputs a labeled release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons mis en place une tâche qui prend les messages de validation comme saisie et sorties des notes de version étiquetées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 270, "src_audio": "/acl6060/audio/en/270.wav", "src_ref": "This can be regarded as a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut être considéré comme une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 271, "src_audio": "/acl6060/audio/en/271.wav", "src_ref": "We have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons prédéfini quatre étiquettes : fonctions, améliorations, corrections de bugs, suppressions de dépréciations et modifications de rupture.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 272, "src_audio": "/acl6060/audio/en/272.wav", "src_ref": "These were set based on previous research and other factors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Celles-ci ont été établies sur la base de la recherche antérieure et d'autres facteurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 273, "src_audio": "/acl6060/audio/en/273.wav", "src_ref": "The release note on the bottom right is extracted from the release note on the bottom left.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La note de version en bas à droite est extraite de la note de version en bas à gauche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 274, "src_audio": "/acl6060/audio/en/274.wav", "src_ref": "At this time, it is necessary to detect the four labels that have been set up in advance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été mises en place à l'avance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 275, "src_audio": "/acl6060/audio/en/275.wav", "src_ref": "But the labels are not always consistent with each repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 276, "src_audio": "/acl6060/audio/en/276.wav", "src_ref": "For example, the improvements label includes improvements, enhancements, optimizations, and so on.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, l'étiquette d'améliorations inclut des améliorations, des perfectionnements, des optimisations, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 277, "src_audio": "/acl6060/audio/en/277.wav", "src_ref": "We prepared a vocabulary list of about thirty labels for each of these notational variations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons préparé une liste de vocabulaire d'une trentaine d'étiquettes pour chacune de ces variations de notation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 278, "src_audio": "/acl6060/audio/en/278.wav", "src_ref": "This is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de détecter la classe de note de version et de collecter le texte de la version qui suit en tant que phrase de note de version pour la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 279, "src_audio": "/acl6060/audio/en/279.wav", "src_ref": "Next is a commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il y a un message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 280, "src_audio": "/acl6060/audio/en/280.wav", "src_ref": "Commit messages are not tied to each release.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation ne sont pas liés à chaque version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 281, "src_audio": "/acl6060/audio/en/281.wav", "src_ref": "As shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme le montre l'image ci-dessous, si la version actuelle est la version deux point cinq à dix-neuf, nous devons identifier la version deux point cinq à dix-huit antérieure et obtenir un diff.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 282, "src_audio": "/acl6060/audio/en/282.wav", "src_ref": "This is a bit tedious and it is not enough to just get a list of releases and look at the before and after.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est un peu fastidieux et il ne suffit pas d'obtenir une liste de versions et de regarder l'avant et l'après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 283, "src_audio": "/acl6060/audio/en/283.wav", "src_ref": "We created a heuristic matching rule to get the previous and next versions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons créé une règle de correspondance heuristique pour obtenir les versions antérieures et suivantes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 284, "src_audio": "/acl6060/audio/en/284.wav", "src_ref": "Dataset analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Analyse des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 285, "src_audio": "/acl6060/audio/en/285.wav", "src_ref": "In the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En fin de compte, sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données ont été recueillis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 286, "src_audio": "/acl6060/audio/en/286.wav", "src_ref": "Also, the average number of release notes tokens is sixty three, which is quite high for a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le nombre moyen de gages de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 287, "src_audio": "/acl6060/audio/en/287.wav", "src_ref": "Also, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De même, le nombre de gages uniques est assez grand, s'élevant à huit mille huit cent trente mille.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 288, "src_audio": "/acl6060/audio/en/288.wav", "src_ref": "This is due to the large number of unique class or method names found in the repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela est dû au grand nombre de noms de méthode ou de classe unique trouvés dans le référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 289, "src_audio": "/acl6060/audio/en/289.wav", "src_ref": "Next, I will explain the proposed method.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais expliquer la méthode proposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 290, "src_audio": "/acl6060/audio/en/290.wav", "src_ref": "The classwise extractive then abstractive summarization model consists of two neural modules.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle de synthèse extractive puis abstractive par classe se compose de deux modules neuronaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 291, "src_audio": "/acl6060/audio/en/291.wav", "src_ref": "A classifier using BERT or CodeBERT and a generator using BART.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un classificateur utilisant BERT ou CodeBERT et un générateur utilisant BART.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 292, "src_audio": "/acl6060/audio/en/292.wav", "src_ref": "First, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, CEAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, qui utilisent des améliorations, des corrections de bogues, des dépréciations et d'autres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 293, "src_audio": "/acl6060/audio/en/293.wav", "src_ref": "The commit messages classified as other are discarded.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation classés comme autres sont supprimés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 294, "src_audio": "/acl6060/audio/en/294.wav", "src_ref": "Then CEAS applies the generator to the four labeled documents independently and generates release notes for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 295, "src_audio": "/acl6060/audio/en/295.wav", "src_ref": "In this task, the direct correspondences between commit messages and release notes are not known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 296, "src_audio": "/acl6060/audio/en/296.wav", "src_ref": "Therefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour former le classificateur, nous avons réaffecté les enquêtes à chaque message de validation de saisie en utilisant les dix premiers caractères de chaque message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 297, "src_audio": "/acl6060/audio/en/297.wav", "src_ref": "We modeled the classwise abstractive summarization approach by two different methods.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons modélisé l'approche de synthèse abstractive par classe à travers deux méthodes différentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 298, "src_audio": "/acl6060/audio/en/298.wav", "src_ref": "The first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le premier modèle, que nous appelons CAS unique, se compose d'un réseau de six à six unique et génère un seul texte de note de version donnant un enchaînement de messages de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 299, "src_audio": "/acl6060/audio/en/299.wav", "src_ref": "The output texts can be divided into classwise segments based on special class-specific endpoint symbols.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les textes de sortie peuvent être divisés en segments par classe sur la base de symboles de point d'extrémité particuliers, spécifiques à la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 300, "src_audio": "/acl6060/audio/en/300.wav", "src_ref": "The second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde méthode, que nous appelons CAS multiple, se compose de quatre réseaux seq2seq différents, dont chacun correspond à l'une des classes de notes de version fixes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 301, "src_audio": "/acl6060/audio/en/301.wav", "src_ref": "Okay, let me explain the experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ok, je vais vous expliquer les expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 302, "src_audio": "/acl6060/audio/en/302.wav", "src_ref": "Five methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cinq méthodes ont été comparées : CEAS, CAS unique, CAS multiple, regroupement et étude antérieure, Glyph.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 303, "src_audio": "/acl6060/audio/en/303.wav", "src_ref": "Regarding evaluation, in some cases, release notes are output in multiple sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 304, "src_audio": "/acl6060/audio/en/304.wav", "src_ref": "Since it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisqu'il est difficile de calculer le nombre de phrases telles qu'elles sont, elles sont combinées avec des espaces et traitées comme une seule phrase longue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 305, "src_audio": "/acl6060/audio/en/305.wav", "src_ref": "The BLEU is penalized when the system outputs a short sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le BLEU est pénalisé lorsque le système produit une phrase courte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 306, "src_audio": "/acl6060/audio/en/306.wav", "src_ref": "This penalty results in a lower BLEU value in the experiment results described next.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette pénalité se traduit par une valeur BLEU inférieure dans les résultats de l'expérience décrits ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 307, "src_audio": "/acl6060/audio/en/307.wav", "src_ref": "Finally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous calculons également la spécificité car ROUGE et BLEU ne peuvent pas être calculés si les notes de version sont vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 308, "src_audio": "/acl6060/audio/en/308.wav", "src_ref": "A higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une spécificité plus élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées être vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 309, "src_audio": "/acl6060/audio/en/309.wav", "src_ref": "Here are the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats sont indiqués ci-après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 310, "src_audio": "/acl6060/audio/en/310.wav", "src_ref": "Since the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné que les données contiennent des adresses électroniques, des valeurs hachées, etc., nous avons également évalué les données nettoyées, ce qui les exclut.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 311, "src_audio": "/acl6060/audio/en/311.wav", "src_ref": "CEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS et le CAS ont obtenu des scores de ROUGE-L supérieurs de plus de dix points par rapport aux bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 312, "src_audio": "/acl6060/audio/en/312.wav", "src_ref": "In particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les bases a atteint plus de vingt points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 313, "src_audio": "/acl6060/audio/en/313.wav", "src_ref": "These results indicate that CEAS and CAS are significantly affected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces résultats indiquent que le CEAS et le CAS sont considérablement touchés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 314, "src_audio": "/acl6060/audio/en/314.wav", "src_ref": "CEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS a obtenu un meilleur score ROUGE-L que le CAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour former le classificateur à l'aide de pseudo-étiquettes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 315, "src_audio": "/acl6060/audio/en/315.wav", "src_ref": "High coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une couverture élevée du CEAS peut être obtenue probablement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 316, "src_audio": "/acl6060/audio/en/316.wav", "src_ref": "CAS-Multi tended to yield higher ROUGE-L than CAS-Single.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CAS multiple tendait à produire plus de ROUGE-L que le CAS unique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 317, "src_audio": "/acl6060/audio/en/317.wav", "src_ref": "Suggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En suggérant qu'il est également efficace de développer indépendamment et différemment des modèles de abstractive summarization pour chaque classe de note de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 318, "src_audio": "/acl6060/audio/en/318.wav", "src_ref": "Here are an error analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici une analyse d'erreur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 319, "src_audio": "/acl6060/audio/en/319.wav", "src_ref": "CAS methods tend to output shorter sentences than human reference sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 320, "src_audio": "/acl6060/audio/en/320.wav", "src_ref": "In the figure on the right, the reference sentence has three or four sentences, while CAS has only one.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que le CAS n'en a qu'une.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 321, "src_audio": "/acl6060/audio/en/321.wav", "src_ref": "The reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La raison de la réticence de ce modèle est que dans les données de formation, seulement trente-trois pour cent des phrases sont présentes dans l'étiquette des fonctions et quarante pour cent dans l'étiquette des améliorations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 322, "src_audio": "/acl6060/audio/en/322.wav", "src_ref": "Furthermore, CAS methods cannot generate accurate release notes without additional information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les méthodes CAS ne peuvent pas générer des notes de version précises sans informations supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 323, "src_audio": "/acl6060/audio/en/323.wav", "src_ref": "The top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple en haut à droite est un exemple de message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la progression ou au problème correspondant(e).", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 324, "src_audio": "/acl6060/audio/en/324.wav", "src_ref": "The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple ci-dessous montre que les deux messages de validation dans la saisie sont liés et doivent être combinés en une phrase, mais cela n'est pas fait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 325, "src_audio": "/acl6060/audio/en/325.wav", "src_ref": "Finally, a conclusion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, passons à la conclusion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 326, "src_audio": "/acl6060/audio/en/326.wav", "src_ref": "We have built a new dataset for automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons construit de nouvelles données pour la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 327, "src_audio": "/acl6060/audio/en/327.wav", "src_ref": "We have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également formulé une tâche consistant à saisir des messages de validation et à les résumer afin qu'ils soient applicables à tous les projets écrits en anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 328, "src_audio": "/acl6060/audio/en/328.wav", "src_ref": "Our experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 329, "src_audio": "/acl6060/audio/en/329.wav", "src_ref": "Please check out our dataset on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Veuillez consulter nos données sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 330, "src_audio": "/acl6060/audio/en/330.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 331, "src_audio": "/acl6060/audio/en/331.wav", "src_ref": "Hello. My name is Asaf Harari.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour. Je m'appelle Asaf Harari.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 332, "src_audio": "/acl6060/audio/en/332.wav", "src_ref": "And I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et je vais vous présenter notre article : enrichissement de données tabulaires en quelques coups à l'aide d'architectures de transformateurs raffinées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 333, "src_audio": "/acl6060/audio/en/333.wav", "src_ref": "Data scientists analyze data and mainly focus on the manipulating the data's existing features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les scientifiques analysent les données et se concentrent principalement sur la manipulation des fonctions existantes des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 334, "src_audio": "/acl6060/audio/en/334.wav", "src_ref": "But sometimes, these features are limited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais parfois, ces fonctions sont limitées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 335, "src_audio": "/acl6060/audio/en/335.wav", "src_ref": "Feature generation using another data source may add substantial information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La génération de fonctions en utilisant une autre source de données peut ajouter des informations substantielles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 336, "src_audio": "/acl6060/audio/en/336.wav", "src_ref": "Our research goal is automatic tabular data enrichment using external sources' free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des textes libres de sources externes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 337, "src_audio": "/acl6060/audio/en/337.wav", "src_ref": "Assume we have a tabular dataset and a knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Supposons que nous ayons des données tabulaires et une base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 338, "src_audio": "/acl6060/audio/en/338.wav", "src_ref": "We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles fonctions du texte libre de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 339, "src_audio": "/acl6060/audio/en/339.wav", "src_ref": "Our framework FeSTE is exactly this automatic process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre cadre FeSTE est exactement ce processus automatique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 340, "src_audio": "/acl6060/audio/en/340.wav", "src_ref": "So let's see an example in a dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons donc un exemple dans des données introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 341, "src_audio": "/acl6060/audio/en/341.wav", "src_ref": "In this example, the dataset is university dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données sont des données universitaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 342, "src_audio": "/acl6060/audio/en/342.wav", "src_ref": "When its goal is to classify universities into low ranking universities and high-ranking universities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Quand l'objectif est de classer les universités en universités de bas rang et en universités de haut rang.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 343, "src_audio": "/acl6060/audio/en/343.wav", "src_ref": "As knowledge base, we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 344, "src_audio": "/acl6060/audio/en/344.wav", "src_ref": "The first phase of FeSTE is entity linking.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première phase de FeSTE est la liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 345, "src_audio": "/acl6060/audio/en/345.wav", "src_ref": "When each entity, in this example the university name, is linked to an entity within the knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 346, "src_audio": "/acl6060/audio/en/346.wav", "src_ref": "And and the text of the entities of the knowledge base is extracted and added to the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le texte des entités de la base de connaissances est extrait et ajouté aux données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 347, "src_audio": "/acl6060/audio/en/347.wav", "src_ref": "In this example, the text is the Wikipedia page's abstract.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, le texte est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 348, "src_audio": "/acl6060/audio/en/348.wav", "src_ref": "Now, we need to generate or extract features from the retrieved text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Maintenant, nous devons générer ou extraire des fonctions à partir du texte extrait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 349, "src_audio": "/acl6060/audio/en/349.wav", "src_ref": "So, we need to ah feature extraction phase ah which includes text analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc besoin de la phase d'extraction des fonctions qui comprend l'analyse de texte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 350, "src_audio": "/acl6060/audio/en/350.wav", "src_ref": "And this is the main novelty of this paper and I will deep dive into it in the next slides.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de la principale nouveauté de cet article et je vais mieux l'expliquer dans les prochaines diapositives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 351, "src_audio": "/acl6060/audio/en/351.wav", "src_ref": "After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Après la phase d'extraction des fonctions, il y a une phase de génération de fonctions lorsque nous utilisons les fonctions extraites pour générer un petit nombre de nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 352, "src_audio": "/acl6060/audio/en/352.wav", "src_ref": "First generate ah features in the number of classes of the original dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Générez d'abord les fonctions dans le nombre de classes des données d'origine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 353, "src_audio": "/acl6060/audio/en/353.wav", "src_ref": "In this example, the original dataset has two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données d'origine ont deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 354, "src_audio": "/acl6060/audio/en/354.wav", "src_ref": "So, FeSTE generates two new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, FeSTE génère deux nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 355, "src_audio": "/acl6060/audio/en/355.wav", "src_ref": "But if the dataset has five classes, FeSTE generates five new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si les données ont cinq classes, FeSTE génère cinq nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 356, "src_audio": "/acl6060/audio/en/356.wav", "src_ref": "Each feature represents the likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chaque fonction représente la probabilité pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 357, "src_audio": "/acl6060/audio/en/357.wav", "src_ref": "To analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour analyser le texte, nous utilisons les analyses de texte de pointe, qui sont des modèles de langue basés sur la conversion comme BERT, GPT, XLNet etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 358, "src_audio": "/acl6060/audio/en/358.wav", "src_ref": "It is but it is not likely that we can train language models using the input datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais il est peu probable que nous puissions former des modèles de langue en utilisant les données de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 359, "src_audio": "/acl6060/audio/en/359.wav", "src_ref": "So a naive approach will be ah target task finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une approche naïve sera le raffinement de la tâche cible.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 360, "src_audio": "/acl6060/audio/en/360.wav", "src_ref": "So, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la phase d'extraction des fonctions, nous pouvons télécharger des modèles de langue préformée et raffiner le modèle de langue sur les données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 361, "src_audio": "/acl6060/audio/en/361.wav", "src_ref": "In this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, raffiner le modèle de langue, classer le texte en classes, résumer en classes, basses ou hautes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 362, "src_audio": "/acl6060/audio/en/362.wav", "src_ref": "Receive the language model output, which is the likelihood for each class and use as new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Recevoir les résultats du modèle de langue, qui sont la probabilité pour chaque classe et les utiliser comme nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 363, "src_audio": "/acl6060/audio/en/363.wav", "src_ref": "The problem with this approach is datasets may have few distinct entities / texts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le problème avec cette approche est que les données peuvent avoir peu d'entités / de textes distinct(e)s.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 364, "src_audio": "/acl6060/audio/en/364.wav", "src_ref": "In our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre expérience, près de la moitié des données contiennent moins de quatre cents échantillons et les plus petites données contiennent trente-cinq échantillons dans un ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 365, "src_audio": "/acl6060/audio/en/365.wav", "src_ref": "So to finetune a language model over ah this dataset will be ineffective.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour raffiner un modèle de langue sur cela, les données seront inefficaces.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 366, "src_audio": "/acl6060/audio/en/366.wav", "src_ref": "But we can use prior knowledge about pre-analyzed datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons utiliser des connaissances préalables sur des données préanalysées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 367, "src_audio": "/acl6060/audio/en/367.wav", "src_ref": "Because FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisque nous appliquons FeSTE sur des données multiples, nous pouvons utiliser les données n moins un pour recueillir des informations sur les données n moins un, et utiliser ces informations lorsque nous analysons les nièmes données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 368, "src_audio": "/acl6060/audio/en/368.wav", "src_ref": "What we, what we suggest is to add, to add another finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce que nous suggérons, c'est d'ajouter une autre phase de raffinement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 369, "src_audio": "/acl6060/audio/en/369.wav", "src_ref": "A preliminary multitask finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une phase de raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 370, "src_audio": "/acl6060/audio/en/370.wav", "src_ref": "When you finetune the language model over the n minus one datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque vous raffinez le modèle de langue sur les données n moins un.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 371, "src_audio": "/acl6060/audio/en/371.wav", "src_ref": "And, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ensuite, nous exécutons une autre phase de raffinement qui est un raffinement de tâche cible, lorsque nous raffinons le modèle de langue sur les nièmes données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 372, "src_audio": "/acl6060/audio/en/372.wav", "src_ref": "The state-of-the-art in multitask ah multitask finetuning called MTDNN.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement multitâche de pointe appelé MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 373, "src_audio": "/acl6060/audio/en/373.wav", "src_ref": "In MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN maintient les têtes dans le nombre de tâches dans l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 374, "src_audio": "/acl6060/audio/en/374.wav", "src_ref": "So, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans cet exemple, il y a quatre tâches dans l'ensemble de formation. Le MTDNN maintient alors quatre têtes comme vous pouvez le voir sur l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 375, "src_audio": "/acl6060/audio/en/375.wav", "src_ref": "And it samples a random batch from ah from the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il échantillonne un lot aléatoire de l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 376, "src_audio": "/acl6060/audio/en/376.wav", "src_ref": "And if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si elles appartiennent à un lot aléatoire, par exemple, une seule tâche de classification de phrases, il exécute des chemins d'aller et retour à travers la première tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 377, "src_audio": "/acl6060/audio/en/377.wav", "src_ref": "And if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si le lot aléatoire appartient à la tâche de classement par paire, il exécute un chemin d'aller et retour dans la dernière tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 378, "src_audio": "/acl6060/audio/en/378.wav", "src_ref": "In our scenario, ah tabular datasets vary in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre scénario, les données tabulaires varient dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 379, "src_audio": "/acl6060/audio/en/379.wav", "src_ref": "So there are many tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il y a donc beaucoup de tâches.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 380, "src_audio": "/acl6060/audio/en/380.wav", "src_ref": "MTDNN maintained number of classes, heads, output layers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN a maintenu le nombre de classes, de têtes et de couches de sortie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 381, "src_audio": "/acl6060/audio/en/381.wav", "src_ref": "And the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en outre, le MTDNN doit initialiser de nouvelles têtes pour de nouvelles données avec une nouvelle tâche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 382, "src_audio": "/acl6060/audio/en/382.wav", "src_ref": "Our approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche, appelée raffinement de reformulation de tâche, est dans notre raffinement de reformulation de tâche d'approche. Au lieu de maintenir plusieurs têtes, nous reformulons chaque donnée dans une phrase par problème de classification, étant des tâches de deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 383, "src_audio": "/acl6060/audio/en/383.wav", "src_ref": "So let's see an example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Prenons donc un exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 384, "src_audio": "/acl6060/audio/en/384.wav", "src_ref": "Here is the our input dataset which consists of entities, features, text and classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici nos données de saisie qui se composent d'entités, de fonctions, de texte et de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 385, "src_audio": "/acl6060/audio/en/385.wav", "src_ref": "And, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous reformulons la tâche à partir d'une classification du texte en bas ou haut pour classer le texte, le résumé et la classe en vrai ou faux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 386, "src_audio": "/acl6060/audio/en/386.wav", "src_ref": "Or in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou en d'autres mots, nous avons formé le modèle de langue pour classer un résumé et une classe en résumé et classe, si le résumé appartient à la classe ou non.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 387, "src_audio": "/acl6060/audio/en/387.wav", "src_ref": "So the label vector in this case stays always ah which consists always with two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc dans ce cas, le vecteur d'étiquette consiste toujours en deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 388, "src_audio": "/acl6060/audio/en/388.wav", "src_ref": "And this is the ah algorithm for our fine, reformulated finetuning approach.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il s'agit de l'algorithme pour notre très bonne approche de raffinement reformulée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 389, "src_audio": "/acl6060/audio/en/389.wav", "src_ref": "So let's see the full framework.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons le cadre complet.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 390, "src_audio": "/acl6060/audio/en/390.wav", "src_ref": "Dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données sont introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 391, "src_audio": "/acl6060/audio/en/391.wav", "src_ref": "And then ah FeSTE executes entity linking phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis FeSTE exécute la phase de liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 392, "src_audio": "/acl6060/audio/en/392.wav", "src_ref": "It ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 393, "src_audio": "/acl6060/audio/en/393.wav", "src_ref": "Then it reformulated the task into a pairwise sentence classification task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il a reformulé la tâche en une tâche de classification des phrases par paire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 394, "src_audio": "/acl6060/audio/en/394.wav", "src_ref": "Applied the language model to the new task and the output likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il a appliqué le modèle de langue à la nouvelle tâche et la probabilité de sortie pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 395, "src_audio": "/acl6060/audio/en/395.wav", "src_ref": "And now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et maintenant, le modèle de langue est déjà raffiné sur des données n moins un en utilisant un raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 396, "src_audio": "/acl6060/audio/en/396.wav", "src_ref": "Then we use the output vector of the language model as a newly generated feature in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous utilisons le vecteur de sortie du modèle de langue comme une fonction nouvellement générée dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 397, "src_audio": "/acl6060/audio/en/397.wav", "src_ref": "To evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour évaluer notre cadre, nous utilisons dix-sept données de classification tabulaires qui varient en taille, fonctions, équilibre, domaine et performance initiale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 398, "src_audio": "/acl6060/audio/en/398.wav", "src_ref": "And as knowledge base we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 399, "src_audio": "/acl6060/audio/en/399.wav", "src_ref": "We design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous concevons notre expérience comme une évaluation où nous formons FeSTe sur seize données et l'appliquons à la dix-septième donnée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 400, "src_audio": "/acl6060/audio/en/400.wav", "src_ref": "We also, we also split each dataset into four folds and apply four folds cross validation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous divisons également chaque donnée en quatre plis et appliquons une validation croisée de quatre plis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 401, "src_audio": "/acl6060/audio/en/401.wav", "src_ref": "Then, we generate the new features and evaluate them using five evaluation classifiers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous générons les nouvelles fonctions et les évaluons en utilisant cinq classificateurs d'évaluation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 402, "src_audio": "/acl6060/audio/en/402.wav", "src_ref": "We use in our experiments base BERT base architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons dans nos expériences l'architecture de base des Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 403, "src_audio": "/acl6060/audio/en/403.wav", "src_ref": "Here are the results for our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici les résultats de nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 404, "src_audio": "/acl6060/audio/en/404.wav", "src_ref": "You can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez voir que nous comparons notre cadre au raffinement des données cibles, au raffinement de la tâche cible et à un raffinement préliminaire MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 405, "src_audio": "/acl6060/audio/en/405.wav", "src_ref": "And our reformulated finetuning achieves the best result, the best performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et notre raffinement reformulé atteint le meilleur résultat et la meilleure performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 406, "src_audio": "/acl6060/audio/en/406.wav", "src_ref": "While MTDNN achieved two percent improvement over the target dataset finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que le MTDNN a obtenu une amélioration de deux pour cent par rapport au raffinement des données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 407, "src_audio": "/acl6060/audio/en/407.wav", "src_ref": "Our approach achieved six percent improvement.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche a obtenu une amélioration de six pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 408, "src_audio": "/acl6060/audio/en/408.wav", "src_ref": "When we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque nous regardons les petites données, nous pouvons voir que la performance du MTDNN diminue et l'amélioration de la phase de raffinement multitâche préliminaire diminue aussi à un virgule cinq pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 409, "src_audio": "/acl6060/audio/en/409.wav", "src_ref": "But our performance increased to eleven percent compared to the target task finetuning alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais notre performance a augmenté à onze pour cent comparé au raffinement de tâche cible seul.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 410, "src_audio": "/acl6060/audio/en/410.wav", "src_ref": "For summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour résumer, FeSTE permet un enrichissement en quelques coups à partir de trente-cinq échantillons dans nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 411, "src_audio": "/acl6060/audio/en/411.wav", "src_ref": "It uses one architecture for all tasks and datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il utilise une architecture pour toutes les tâches et données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 412, "src_audio": "/acl6060/audio/en/412.wav", "src_ref": "And it keeps the head of ah of the model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il garde la tête du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 413, "src_audio": "/acl6060/audio/en/413.wav", "src_ref": "But it adds reformulation phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cela ajoute une phase de reformulation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 414, "src_audio": "/acl6060/audio/en/414.wav", "src_ref": "It augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il augmente l'ensemble de formation et a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'introduire dans le modèle de langue et l'utiliser dans le problème de classification par paire de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 415, "src_audio": "/acl6060/audio/en/415.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 416, "src_audio": "/acl6060/audio/en/416.wav", "src_ref": "Hello. My name is Asaf Harari.\nAnd I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.\nData scientists analyze data and mainly focus on the manipulating the data's existing features.\nBut sometimes, these features are limited.\nFeature generation using another data source may add substantial information.\nOur research goal is automatic tabular data enrichment using external sources' free text.\nAssume we have a tabular dataset and a knowledge base.\nWe need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.\nOur framework FeSTE is exactly this automatic process.\nSo let's see an example in a dataset fed into FeSTE.\nIn this example, the dataset is university dataset.\nWhen its goal is to classify universities into low ranking universities and high-ranking universities.\nAs knowledge base, we use Wikipedia.\nThe first phase of FeSTE is entity linking.\nWhen each entity, in this example the university name, is linked to an entity within the knowledge base.\nAnd and the text of the entities of the knowledge base is extracted and added to the dataset.\nIn this example, the text is the Wikipedia page's abstract.\nNow, we need to generate or extract features from the retrieved text.\nSo, we need to ah feature extraction phase ah which includes text analysis.\nAnd this is the main novelty of this paper and I will deep dive into it in the next slides.\nAfter the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.\nFirst generate ah features in the number of classes of the original dataset.\nIn this example, the original dataset has two classes.\nSo, FeSTE generates two new features.\nBut if the dataset has five classes, FeSTE generates five new features.\nEach feature represents the likelihood for each class.\nTo analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.\nIt is but it is not likely that we can train language models using the input datasets.\nSo a naive approach will be ah target task finetuning.\nSo, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.\nIn this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.\nReceive the language model output, which is the likelihood for each class and use as new features.\nThe problem with this approach is datasets may have few distinct entities / texts.\nIn our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.\nSo to finetune a language model over ah this dataset will be ineffective.\nBut we can use prior knowledge about pre-analyzed datasets.\nBecause FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.\nWhat we, what we suggest is to add, to add another finetuning phase.\nA preliminary multitask finetuning phase.\nWhen you finetune the language model over the n minus one datasets.\nAnd, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.\nThe state-of-the-art in multitask ah multitask finetuning called MTDNN.\nIn MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.\nSo, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.\nAnd it samples a random batch from ah from the training set.\nAnd if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.\nAnd if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.\nIn our scenario, ah tabular datasets vary in the number of classes.\nSo there are many tasks.\nMTDNN maintained number of classes, heads, output layers.\nAnd the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.\nOur approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.\nSo let's see an example.\nHere is the our input dataset which consists of entities, features, text and classes.\nAnd, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.\nOr in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.\nSo the label vector in this case stays always ah which consists always with two classes.\nAnd this is the ah algorithm for our fine, reformulated finetuning approach.\nSo let's see the full framework.\nDataset fed into FeSTE.\nAnd then ah FeSTE executes entity linking phase.\nIt ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.\nThen it reformulated the task into a pairwise sentence classification task.\nApplied the language model to the new task and the output likelihood for each class.\nAnd now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.\nThen we use the output vector of the language model as a newly generated feature in the number of classes.\nTo evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.\nAnd as knowledge base we use Wikipedia.\nWe design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.\nWe also, we also split each dataset into four folds and apply four folds cross validation.\nThen, we generate the new features and evaluate them using five evaluation classifiers.\nWe use in our experiments base BERT base architecture.\nHere are the results for our experiments.\nYou can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.\nAnd our reformulated finetuning achieves the best result, the best performance.\nWhile MTDNN achieved two percent improvement over the target dataset finetuning.\nOur approach achieved six percent improvement.\nWhen we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.\nBut our performance increased to eleven percent compared to the target task finetuning alone.\nFor summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.\nIt uses one architecture for all tasks and datasets.\nAnd it keeps the head of ah of the model.\nBut it adds reformulation phase.\nIt augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.\nThank you.", "tgt_ref": "Bonjour. Je m'appelle Asaf Harari.\nEt je vais vous présenter notre article : enrichissement de données tabulaires en quelques coups à l'aide d'architectures de transformateurs raffinées.\nLes scientifiques analysent les données et se concentrent principalement sur la manipulation des fonctions existantes des données.\nMais parfois, ces fonctions sont limitées.\nLa génération de fonctions en utilisant une autre source de données peut ajouter des informations substantielles.\nNotre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des textes libres de sources externes.\nSupposons que nous ayons des données tabulaires et une base de connaissances.\nNous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles fonctions du texte libre de la base de connaissances.\nNotre cadre FeSTE est exactement ce processus automatique.\nVoyons donc un exemple dans des données introduites dans FeSTE.\nDans cet exemple, les données sont des données universitaires.\nQuand l'objectif est de classer les universités en universités de bas rang et en universités de haut rang.\nEn tant que base de connaissances, nous utilisons Wikipédia.\nLa première phase de FeSTE est la liaison d'entités.\nLorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances.\nEt le texte des entités de la base de connaissances est extrait et ajouté aux données.\nDans cet exemple, le texte est le résumé de la page Wikipédia.\nMaintenant, nous devons générer ou extraire des fonctions à partir du texte extrait.\nNous avons donc besoin de la phase d'extraction des fonctions qui comprend l'analyse de texte.\nIl s'agit de la principale nouveauté de cet article et je vais mieux l'expliquer dans les prochaines diapositives.\nAprès la phase d'extraction des fonctions, il y a une phase de génération de fonctions lorsque nous utilisons les fonctions extraites pour générer un petit nombre de nouvelles fonctions.\nGénérez d'abord les fonctions dans le nombre de classes des données d'origine.\nDans cet exemple, les données d'origine ont deux classes.\nAinsi, FeSTE génère deux nouvelles fonctions.\nMais si les données ont cinq classes, FeSTE génère cinq nouvelles fonctions.\nChaque fonction représente la probabilité pour chaque classe.\nPour analyser le texte, nous utilisons les analyses de texte de pointe, qui sont des modèles de langue basés sur la conversion comme BERT, GPT, XLNet etc.\nMais il est peu probable que nous puissions former des modèles de langue en utilisant les données de saisie.\nAinsi, une approche naïve sera le raffinement de la tâche cible.\nDans la phase d'extraction des fonctions, nous pouvons télécharger des modèles de langue préformée et raffiner le modèle de langue sur les données cibles.\nDans cet exemple, raffiner le modèle de langue, classer le texte en classes, résumer en classes, basses ou hautes.\nRecevoir les résultats du modèle de langue, qui sont la probabilité pour chaque classe et les utiliser comme nouvelles fonctions.\nLe problème avec cette approche est que les données peuvent avoir peu d'entités / de textes distinct(e)s.\nDans notre expérience, près de la moitié des données contiennent moins de quatre cents échantillons et les plus petites données contiennent trente-cinq échantillons dans un ensemble de formation.\nDonc, pour raffiner un modèle de langue sur cela, les données seront inefficaces.\nMais nous pouvons utiliser des connaissances préalables sur des données préanalysées.\nPuisque nous appliquons FeSTE sur des données multiples, nous pouvons utiliser les données n moins un pour recueillir des informations sur les données n moins un, et utiliser ces informations lorsque nous analysons les nièmes données.\nCe que nous suggérons, c'est d'ajouter une autre phase de raffinement.\nUne phase de raffinement multitâche préliminaire.\nLorsque vous raffinez le modèle de langue sur les données n moins un.\nEt ensuite, nous exécutons une autre phase de raffinement qui est un raffinement de tâche cible, lorsque nous raffinons le modèle de langue sur les nièmes données cibles.\nLe raffinement multitâche de pointe appelé MTDNN.\nLe MTDNN maintient les têtes dans le nombre de tâches dans l'ensemble de formation.\nDonc, dans cet exemple, il y a quatre tâches dans l'ensemble de formation. Le MTDNN maintient alors quatre têtes comme vous pouvez le voir sur l'image.\nEt il échantillonne un lot aléatoire de l'ensemble de formation.\nEt si elles appartiennent à un lot aléatoire, par exemple, une seule tâche de classification de phrases, il exécute des chemins d'aller et retour à travers la première tête.\nEt si le lot aléatoire appartient à la tâche de classement par paire, il exécute un chemin d'aller et retour dans la dernière tête.\nDans notre scénario, les données tabulaires varient dans le nombre de classes.\nIl y a donc beaucoup de tâches.\nLe MTDNN a maintenu le nombre de classes, de têtes et de couches de sortie.\nEt en outre, le MTDNN doit initialiser de nouvelles têtes pour de nouvelles données avec une nouvelle tâche.\nNotre approche, appelée raffinement de reformulation de tâche, est dans notre raffinement de reformulation de tâche d'approche. Au lieu de maintenir plusieurs têtes, nous reformulons chaque donnée dans une phrase par problème de classification, étant des tâches de deux classes.\nPrenons donc un exemple.\nVoici nos données de saisie qui se composent d'entités, de fonctions, de texte et de classes.\nEt nous reformulons la tâche à partir d'une classification du texte en bas ou haut pour classer le texte, le résumé et la classe en vrai ou faux.\nOu en d'autres mots, nous avons formé le modèle de langue pour classer un résumé et une classe en résumé et classe, si le résumé appartient à la classe ou non.\nDonc dans ce cas, le vecteur d'étiquette consiste toujours en deux classes.\nEt il s'agit de l'algorithme pour notre très bonne approche de raffinement reformulée.\nVoyons le cadre complet.\nLes données sont introduites dans FeSTE.\nPuis FeSTE exécute la phase de liaison d'entités.\nIl extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia.\nEnsuite, il a reformulé la tâche en une tâche de classification des phrases par paire.\nIl a appliqué le modèle de langue à la nouvelle tâche et la probabilité de sortie pour chaque classe.\nEt maintenant, le modèle de langue est déjà raffiné sur des données n moins un en utilisant un raffinement multitâche préliminaire.\nEnsuite, nous utilisons le vecteur de sortie du modèle de langue comme une fonction nouvellement générée dans le nombre de classes.\nPour évaluer notre cadre, nous utilisons dix-sept données de classification tabulaires qui varient en taille, fonctions, équilibre, domaine et performance initiale.\nEt en tant que base de connaissances, nous utilisons Wikipédia.\nNous concevons notre expérience comme une évaluation où nous formons FeSTe sur seize données et l'appliquons à la dix-septième donnée.\nNous divisons également chaque donnée en quatre plis et appliquons une validation croisée de quatre plis.\nEnsuite, nous générons les nouvelles fonctions et les évaluons en utilisant cinq classificateurs d'évaluation.\nNous utilisons dans nos expériences l'architecture de base des Représentations d'encodeurs bidirectionnels à partir de transformateurs.\nVoici les résultats de nos expériences.\nVous pouvez voir que nous comparons notre cadre au raffinement des données cibles, au raffinement de la tâche cible et à un raffinement préliminaire MTDNN.\nEt notre raffinement reformulé atteint le meilleur résultat et la meilleure performance.\nAlors que le MTDNN a obtenu une amélioration de deux pour cent par rapport au raffinement des données cibles.\nNotre approche a obtenu une amélioration de six pour cent.\nLorsque nous regardons les petites données, nous pouvons voir que la performance du MTDNN diminue et l'amélioration de la phase de raffinement multitâche préliminaire diminue aussi à un virgule cinq pour cent.\nMais notre performance a augmenté à onze pour cent comparé au raffinement de tâche cible seul.\nPour résumer, FeSTE permet un enrichissement en quelques coups à partir de trente-cinq échantillons dans nos expériences.\nIl utilise une architecture pour toutes les tâches et données.\nEt il garde la tête du modèle.\nMais cela ajoute une phase de reformulation.\nIl augmente l'ensemble de formation et a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'introduire dans le modèle de langue et l'utiliser dans le problème de classification par paire de phrases.\nMerci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 417, "src_audio": "/acl6060/audio/en/417.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.\nI'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.\nFirst, I'd like to talk about our motivation for reasoning.\nSo here we show an examples where multi-step reasoning is helpful.\nSo this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.\nSo on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.\nBut if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.\nSo it is good to have interpretable multi-step reasoning as output.\nAnd we also think math word problem is a straightforward application to evaluate such reasoning abilities.\nSo, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.\nSo in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.\nSo, certain assumptions ah also apply as in previous work.\nWe assume the precision of quantities are known.\nAnd we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.\nFurthermore, complicated operators can be actually decomposed into these basic operators.\nSo, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.\nSo, traditional sequence to sequence model convert the expression to a specific sequence for generation.\nAnd it is pretty easy to implement and it can generalize to many different complicated problem.\nBut the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.\nBut actually this direction is still quite popular because of um the transformer model.\nSo, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.\nSo here we keep generating the operators until we reach the leaves, which are the quantities.\nSo here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.\nAnd the second thing is that it also contains some repetitive computations.\nSo here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.\nSo, in our proposed approach we want to solve those problems in a step by step and interpretable manners.\nSo for example, here in the second step, ah we can obtain these divisors which is twenty seven.\nAnd we can also refer back to the original questions to find the relevant contents.\nAnd in these steps we obtain the divisors.\nSo, ah and then at this third step we actually get the quotient.\nAlright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.\nSo, here we actually generate the whole expression directly rather than generating a single operators or quantities.\nSo this makes the process more accurate.\nSo, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.\nSo, the expression is represented by e i j o p.\nWhere we perform operator from q_i to q_j, and such expression is actually directed.\nSo, we also have subtraction with words here to represent the opposite direction.\nThis is quite similar to relation extraction.\nSo in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.\nWe add it to the next state to become a new quantity.\nSo, these slides actually visualize the evolution of the state where we keep adding expression to the current state.\nSo in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.\nSo, once we get the quantity representations, we can start to do inference.\nHere we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.\nFirst we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.\nAnd then finally we obtain the expression representation q_1 divided by q_2.\nBut in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.\nSo, here all the possible expression is equals to three times the number of operators.\nSo the nice thing here is that we can easily add constraints to control this search this search space.\nFor example, if this expression is not allowed, we can simply remove this expression in our search space.\nSo in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.\nSo this quantity come from the previous calculated expression.\nSo finally we can obtain this final expression q_3 times q_4.\nAnd we can also see the number of all the possible ah expression is different from the previous step.\nSo, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.\nSo the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.\nAnd here we also use this tau to represent when we should terminate this generation process.\nAnd here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.\nAnd it also allows us to impose certain constraints from prior from prior knowledge.\nSo we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.\nAnd here we briefly show the results compared with the previous best approaches.\nSo our best performing variant is Roberta-DeductiveReasoner.\nAnd in fact we do not use beam search, in contrast all previous approaches are using beam search.\nAll right. So, the best approaches are often tree based model.\nSo, overall our reasoner is able to significantl significantly outperform this tree based model.\nBut we can see the absolute numbers on MathQA or SVAMP are not really high.\nSo we further investigate the results on SVAMP.\nAnd this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.\nSo, in our prediction we find some of the intermediate values are actually negatives.\nFor example, um, in these questions we are asking how many apples does Jake have?\nBut we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.\nSo, our model makes some prediction like this which is producing negative values.\nAnd we observe these two expressions actually have similar scores.\nSo, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.\nSo um we further find such constraint actually improves quite a lot for some models.\nFor example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.\nSo better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.\nAnd we also try to analyze the difficulty behind these behind all these datasets.\nWe assume the number of unused quantities can be regarded as irrelevant information here.\nSo ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.\nAnd here we also show the overall performance.\nFor those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.\nBut with those samples that with unused quantity is actually way worse than the, worse than the overall performance.\nFor MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.\nSo, finally we want to show the interpretability through a question perturbation example.\nSo here our model actually makes a wrong prediction at the first step.\nSo, we can actually correlate this expression with the sentence here. Alright.\nSo, we think this sentence might be misleading the model to an incorrect predictions.\nSo here planting another thirty five makes the model makes the model think it should be an addition operator.\nSo we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.\nSo, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.\nSo, this study shows how the interpretable predictions help us understand the model behavior.\nSo to conclude our work, so first our model is actually pretty efficient.\nAnd we are able to provide interpretable solving procedure.\nAnd we can easily incorporate some prior knowledge as constraint which can help improve the performance.\nAnd the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.\nWe also have certain limitations.\nAh, if we have a large number of operators or constants, the memory consumption could be pretty high.\nAnd the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.\nSo this is the end of the talk, and questions are welcomed. Thank you.", "tgt_ref": "Bonjour tout le monde. Aujourd'hui, je vais vous présenter notre travail de recherche « Apprendre à raisonner par déduction » : résolution de problèmes de mots mathématiques comme extraction de relation complexe.\nJe m'appelle Allan du laboratoire d'intelligence artificielle ByteDance, et ceci est un travail en commun avec Jierui Li de l'Université du Texas à Austin et Wei Lu du SUTD.\nTout d'abord, j'aimerais parler de notre motivation pour le raisonnement.\nDonc ici, nous montrons un exemple où le raisonnement en plusieurs étapes est utile.\nCe chiffre est donc tiré de l'article PaLM où ils incitent à résoudre le problème de réseau dans le scénario d'apprentissage de quelques prises de vue.\nDonc, sur le côté gauche, nous pouvons voir que si nous fournissons quelques exemples avec juste des questions et des réponses, nous pourrions ne pas être en mesure d'obtenir les bonnes réponses.\nMais si nous fournissons plus de description du raisonnement, le modèle est capable de prévenir la description du raisonnement, mais aussi d'effectuer ici une prévention correcte.\nIl est alors bon d'avoir comme résultat un raisonnement à plusieurs étapes interprétable.\nEt nous pensons également que les problèmes de mots mathématiques sont une application directe pour évaluer de telles capacités de raisonnement.\nDonc ici, dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques.\nAinsi, dans nos données, nous recevons également l‘expression mathématique qui conduit à cette réponse particulière.\nDonc, certaines hypothèses s'appliquent également comme dans les travaux antérieurs.\nNous supposons que la précision des quantités est connue.\nEt nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle.\nEn outre, les opérateurs compliqués peuvent effectivement être décomposés en ces opérateurs de base.\nAinsi, le travail antérieur dans la résolution de problèmes de mots mathématiques peut en effet être classé dans un modèle séquence à séquence et séquence à arbre.\nLe modèle séquence à séquence traditionnel convertit donc l'expression en une séquence spécifique pour la génération.\nIl est assez facile à mettre en œuvre et peut être généralisé à de nombreux problèmes compliqués différents.\nMais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle structuré et son manque d'interopérabilité pour la prévention.\nMais en réalité, cette direction est encore très populaire en raison du modèle de conversion.\nDonc, dans les modèles à base d'arbre, nous structurons en réalité ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres.\nDonc ici, nous continuons à produire les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités.\nDonc ici, la bonne chose est qu'en fait, cela nous donne cette structure d'arbre binaire. Mais en réalité, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur et ensuite, à la fin, nous générons les quantités.\nEt la deuxième chose est que cela contient également des calculs répétitifs.\nDonc ici, si nous regardons cette expression, huit fois trois plus trois est en fait généré deux fois, mais en réalité, nous devrions réutiliser les résultats.\nAinsi, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable.\nPar exemple, ici dans la deuxième étape, nous pouvons obtenir ces diviseurs qui sont vingt-sept.\nEt nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent.\nEt dans ces étapes, nous obtenons les diviseurs.\nPuis, à cette troisième étape, nous obtenons alors le quotient.\nTrès bien. Et après ces trois étapes, nous pouvons alors réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, et enfin, nous pouvons obtenir les dividendes.\nDonc ici, nous générons en réalité l'expression entière directement plutôt que de produire un seul opérateur ou une seule quantité.\nCela rend le processus plus précis.\nAinsi, dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et incluant également une certaine constante comme notre état initial.\nAinsi, l'expression est représentée par e i j o p.\nOù nous effectuons l'opérateur de q_i à q_j, et cette expression est en fait dirigée.\nDonc, nous avons également ici la soustraction avec des mots pour représenter la direction opposée.\nC'est tout à fait similaire à l'extraction de relation.\nDonc, dans un système déductif formel, à un pas de temps t, on applique l'opérateur entre la paire q_i et q_j, puis on obtient cette nouvelle expression.\nNous l'ajoutons à l'état suivant pour devenir une nouvelle quantité.\nAinsi, ces diapositives visualisent en réalité l'évolution de l'état où nous continuons à ajouter de l'expression à l'état actuel.\nDonc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langue préformée qui peut être BERT ou Roberta, puis nous encodons la phrase et obtenons ensuite ces représentations de quantité.\nDonc, une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire l'inférence.\nNous montrons ici un exemple de q_1 pour obtenir la représentation pour q_2 divisée par q_2 multipliée par q_3.\nTout d'abord, nous obtenons la représentation de paire, qui n'est essentiellement que l'enchaînement entre q_1 et q_2, puis nous appliquons un réseau prédictif qui est paramétré par l'opérateur.\nEt enfin, nous obtenons la représentation de l'expression q_1 divisée par q_2.\nMais en réalité, dans la pratique, au stade de l'inférence, nous pourrions également obtenir une expression incorrecte.\nDonc ici, toute l'expression possible est égale à trois fois le nombre d'opérateurs.\nAlors, ce qui est bien ici, c'est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche.\nPar exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.\nDonc, dans la deuxième étape, nous faisons la même chose, mais la seule différence est que nous avons une quantité de plus.\nCette quantité provient donc de l'expression calculée antérieure.\nFinalement, nous pouvons obtenir cette expression finale q_3 multipliée par q_4.\nEt nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape antérieure.\nAinsi, une telle différence rend difficile l'application de la beam search car la distribution de probabilité entre ces deux étapes est déséquilibrée.\nLa procédure de formation est donc similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps.\nEt ici, nous utilisons également ce tau pour représenter le moment où nous devrions mettre fin à ce processus de génération.\nEt ici, l'espace est différent de séquence à séquence car il est différent à chaque pas de temps, alors que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire.\nEt cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.\nNous menons donc des expériences sur les données de problèmes de mots mathématiques, MAWPS, Math23K, MathQA et SVAMP couramment utilisées.\nEt ici, nous montrons brièvement les résultats comparés aux meilleures approches antérieures.\nDonc, notre variante la plus performante est Roberta-DeuctiveReasoner.\nEt en réalité, nous n'utilisons pas la beam search ; au contraire, toutes les approches antérieures utilisent la beam search.\nTrès bien. Ainsi, les meilleures approches sont souvent un modèle à base d'arbre.\nDonc, dans l'ensemble, notre raisonneur est capable de dépasser significativement ce modèle à base d'arbre.\nMais nous pouvons voir que les nombres absolus sur MathQA ou SVAMP ne sont pas vraiment élevés.\nNous étudions donc plus en détail les résultats sur SVAMP.\nEt ces données sont difficiles parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle TAL traitement automatique du langage naturel comme ajouter des informations non pertinentes et des quantités supplémentaires.\nDonc, dans notre prévention, nous trouvons que certaines des valeurs intermédiaires sont en réalité négatives.\nPar exemple, dans ces questions, nous demandons « combien de pommes a Jake ? »\nMais nous avons des informations supplémentaires comme « dix-sept photos de moins », et « Steven en a huit », ce qui est totalement hors de propos.\nAinsi, notre modèle effectue une prévention comme celle-ci qui produit des valeurs négatives.\nEt nous constatons que ces deux expressions ont en réalité des scores similaires.\nNous pouvons alors limiter cet espace de recherche en supprimant les résultats négatifs afin que nous puissions rendre la réponse correcte.\nNous trouvons donc que cette contrainte améliore en réalité beaucoup pour certains modèles.\nPar exemple, pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous améliorons sept points, puis pour le modèle de base Roberta, nous avons amélioré deux points.\nLe meilleur modèle de langue a donc de meilleures capacités de compréhension de la langue de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs.\nEt nous essayons également d'analyser la difficulté derrière toutes ces données.\nNous supposons que le nombre de quantités inutilisées peut être considéré ici comme une information non pertinente.\nDonc ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et les données SVAMP ont la plus grande partie.\nEt ici, nous montrons également la performance globale.\nPour ces échantillons sans quantités inutilisées, la performance est en réalité supérieure à la performance globale.\nMais avec ces échantillons ayant une quantité inutilisée, c'est en fait bien pire que la performance globale.\nPour MAWPS, nous n'avons pas vraiment trop de cas de test, alors j'ignore simplement cette partie.\nFinalement, nous voulons montrer l'interopérabilité à travers un exemple de perturbation de question.\nDonc ici, notre modèle effectue en réalité une prévention erronée à la première étape.\nAinsi, nous pouvons effectivement corréler cette expression avec la phrase ici. Très bien.\nNous pensons donc que cette phrase pourrait induire le modèle en erreur avec des préventions incorrectes.\nDonc ici, en planter trente-cinq autres fait penser au modèle qu'il devrait être un opérateur d'addition.\nNous essayons alors de réviser la phrase pour que cela soit quelque chose comme « le nombre de poiriers est inférieur de trente-cinq à celui des pommiers ».\nNous faisons en sorte de transmettre une sémantique plus précise afin que le modèle soit capable de rendre la prévention correcte.\nAinsi, cette étude montre comment les préventions interprétables nous aident à comprendre le comportement du modèle.\nDonc, pour conclure notre travail, notre modèle est en réalité assez efficace.\nEt nous sommes en mesure de fournir une procédure de résolution interprétable.\nEt nous pouvons facilement incorporer des connaissances antérieures en tant que contraintes qui peuvent aider à améliorer la performance.\nEt la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution des problèmes de réseau, mais aussi aux autres tâches qui impliquent un raisonnement à plusieurs étapes.\nNous avons également certaines limites.\nSi nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée.\nEt la deuxième chose est que, comme mentionné, puisque la distribution de probabilité est déséquilibrée entre les différentes étapes de temps, il est donc également assez difficile d'appliquer la stratégie de beam search.\nNous arrivons donc à la fin de la discussion, et vos questions sont les bienvenues. Merci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 418, "src_audio": "/acl6060/audio/en/418.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.\nI will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.\nLegal issues are an integral part of many people's lives.\nBut the majority of citizens have little to know knowledge about their rights and fundamental legal processes.\nAs a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.\nAll work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.\nSuch a system could provide a free professional legal help service for unskilled humans.\nBefore diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.\nGiven a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?\nA model is required to retrieve all relevant statutory articles from a large body of legislation.\nThis information retrieval task comes with its own set of challenges.\nFirst, it deals with two types of language.\nCommon natural language for the questions and complex legal language for the statutes.\nThis difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.\nBesides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.\nInstead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.\nLastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.\nHere, there are long documents that may be up to six thousand words.\nThe recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.\nBut statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.\nIn this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.\nOur Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.\nThese questions cover a wide range of topics from family, housing, money, to work and social security.\nEach of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.\nLet's now talk about how we collected this dataset.\nFirst, we started by compiling a large corpus of legal articles.\nWe considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.\nThen we gathered legal questions with references to relevant statutes.\nTo do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.\nWe were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.\nWe collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.\nLastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.\nThe remaining references were matched and converted to the corresponding article ids from our corpus.\nWe eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.\nIn addition, each question comes with the main category and a concatenation of subcategories.\nAnd each articles comes with a concatenation of the subsequence heading in the structure of the law.\nThis extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.\nLet's look at some characteristic of our dataset.\nThe questions are between five and forty four words long with a median of fourteen words.\nThe articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.\nThe lengthiest one being up to five thousand seven hundred and ninety words.\nAs previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.\nWhile the remaining fifteen percent concern either social security, foreigners or work.\nThe article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.\nHere's the total number of articles collected from each of these Belgian codes.\nOut of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.\nAnd around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.\nMeanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.\nWhich can be explained by the fact that those codes focused less on individuals and their concerns.\nOverall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.\nUsing all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.\nGiven a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.\nWe experiment with the standard TF-IDF and BM25 ranking functions.\nThe main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.\nTo overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.\nWe use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.\nThese embeddings typically result from a pooling operation on the output of a word embedding model.\nFirst, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.\nWe experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.\nAdditionally, we train our own CamemBERT based model ah bi-encoders on our dataset.\nNote that for training, we experiment with the two flavors of the bi-encoder architecture.\nSiamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.\nWe experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.\nHere are the result of our baseline on the test sets.\nWith the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.\nOverall, the finetuned bi-encoder significantly outperforms all the other baselines.\nThe two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.\nAlthough BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.\nRegarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.\nFurthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.\nAlthough promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.\nLet's conclude by discussing two limitations of our dataset.\nFirst, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.\nDuring the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.\nThis information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.\nSecond, we should note that not all legal questions can be answered with statutes alone.\nFor instance, the question, can I evict my tenants if they make too much noise?\nMight not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.\nInstead, the landlord should probably rely more on case law and find precedents similar to their current situation.\nFor example, the tenants makes two parties a week until two AM.\nHence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.\nWe hope that our work sparks interest in developing practical and reliable statutory article retrieval models.\nThat can help improve access to justice for all.\nYou can check out our paper, dataset and code at the following links. Thank you.", "tgt_ref": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht.\nJe vais vous présenter mon travail en commun avec Jerry, qui porte sur de nouvelles données pour l'extraction des articles de droit.\nLes questions juridiques font partie intégrante de la vie de nombreuses personnes.\nMais la majorité des citoyens ont peu de connaissances sur leurs droits et leurs processus juridiques fondamentaux.\nEn conséquence, de nombreux citoyens vulnérables qui n'ont pas les moyens de se payer l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités.\nTous les travaux visent à combler le fossé entre les personnes et la loi en développant un système d'extraction efficace pour les articles de droit.\nUn tel système pourrait fournir un service d'aide juridique professionnel gratuit aux personnes non qualifiées.\nAvant de plonger dans la principale contribution de ce travail, nous allons d'abord décrire le problème d'extraction des articles de droit.\nCompte tenu d'une simple question sur une question juridique telle que : « qu'est-ce que je risque si je viole la confidentialité professionnelle ? »\nUn modèle est nécessaire pour extraire tous les articles de droit pertinents d'un grand corpus législatif.\nCette tâche d'extraction des informations comporte son propre ensemble de défis.\nTout d'abord, elle traite de deux types de langue.\nLa langue naturelle commune pour les questions et la langue juridique complexe pour les lois.\nCette différence dans les distributions de langue rend plus difficile pour un système d'extraire les candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des lois.\nEn outre, le droit écrit n'est pas une pile d'articles indépendants qui peuvent être traités comme une source complète d'informations par eux-mêmes, contrairement aux nouvelles ou aux recettes, par exemple.\nAu lieu de cela, il s'agit d'un ensemble structuré de dispositions juridiques qui n'ont un sens entier que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les domaines et sous-domaines auxquels elles appartiennent et leur place dans la structure de la loi.\nEnfin, les articles de droit ne sont pas de petits paragraphes constituant généralement l'unité d'extraction typique dans la plupart des travaux d'extraction.\nIci, il y a de longs documents qui peuvent aller jusqu'à six mille mots.\nLes recent advances en matière de TAL traitement automatique du langage naturel ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prévention du jugement juridique ou l'examen des contrats de contacts automatisés.\nMais l'extraction de l'article de droit est restée essentiellement inchangée en raison du manque de grandes données étiquetées de haute qualité.\nDans ce travail, nous présentons de nouvelles données centrées sur le citoyen natif français pour étudier si l'extraction des modèles peut se rapprocher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche d'extraction de l'article de droit.\nNos données d'extraction de l'article de droit belge BSARD se composent de plus de mille cent questions juridiques posées par des citoyens belges.\nCes questions couvrent un wide range de sujets allant de la famille au logement, à l'argent, au travail et à la sécurité sociale.\nChacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques de codes de droit belge.\nParlons maintenant de la façon dont nous avons collecté ces données.\nTout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques.\nNous avons examiné trente-deux codes belges accessibles au public et extrait tous les articles ainsi que les titres de section correspondants.\nEnsuite, nous avons rassemblé les questions juridiques avec les références aux lois pertinentes.\nPour ce faire, nous nous associons au cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur une question juridique personnelle.\nNous avons eu la chance d'avoir accès à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges.\nNous avons recueilli des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux lois pertinentes.\nEnfin, nous avons fait passer les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avons considérés.\nLes références restantes ont été appariées et converties aux identifiants d'article correspondants de notre corpus.\nNous nous sommes finalement retrouvés avec mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de droit.\nDe plus, chaque question est accompagnée de la catégorie principale et d'un enchaînement de sous-catégories.\nEt chaque article comporte un enchaînement de la rubrique de sous-séquence dans la structure de la loi.\nCes informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour des recherches futures sur l'extraction d'informations juridiques ou la classification de textes juridiques.\nJetons un coup d'œil à certaines caractéristiques de nos données.\nLes questions comportent entre cinq et quarante-quatre mots avec une moyenne de quatorze mots.\nLes articles sont beaucoup plus longs avec une longueur moyenne de soixante-dix-sept mots, avec cent quarante-deux d'entre eux dépassant les mille mots.\nLe plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots.\nComme mentionné précédemment, les questions couvrent un wide range de sujets, dont environ quatre-vingt-cinq pour cent concernent la famille, le logement, l'argent ou la justice.\nAlors que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail.\nL'article est également très diversifié car il provient de trente-deux codes belges différents qui couvrent un grand nombre de sujets juridiques.\nVoici le nombre total d'articles collectés à partir de chacun de ces codes belges.\nSur les vingt-deux mille six cent trente-trois articles, seuls mille six cent douze sont désignés comme pertinents à au moins une question dans les données.\nEt environ quatre-vingts pour cent de ces articles cités proviennent du code civil, des codes judiciaires, des codes d'enquête criminelle ou des codes pénaux.\nPendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question.\nCe qui peut s'expliquer par le fait que ces codes se concentraient moins sur les individus et leurs préoccupations.\nDans l'ensemble, le nombre moyen de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.\nEn utilisant toutes les données, nous avons comparé plusieurs approches d'extraction, y compris l'architecture lexicale et dense.\nÉtant donné une requête et un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de requête des poids de chacun de ces termes dans cet article.\nNous expérimentons avec les fonctions de classement TF-IDF et BM25 standard.\nLe principal problème de ces approches est qu'elles ne peuvent extraire que les articles contenant des mots-clés présents dans la requête.\nPour surmonter cette limitation, nous expérimentons une architecture neuronale qui peut capturer les relations sémantiques entre les requêtes et l'article.\nNous utilisons un modèle bi-encodeur qui cartographie les requêtes et les articles en représentations vecteurs denses et calculons un score de pertinence entre une paire d'articles de requête par la similitude de leurs intégrations.\nCes intégrations résultent typiquement d'une opération de pooling sur la sortie d'un modèle d'intégration de mots.\nTout d'abord, nous étudions l'efficacité des bi-encodeurs siamois dans une configuration d'évaluation « zero shot », ce qui signifie que des modèles d'intégration de mots préformés sont appliqués immédiatement sans aucun raffinement supplémentaire.\nNous expérimentons avec l'encodeur de texte indépendant du contexte, à savoir word2vec et fastText, et des modèles d'intégration dépendants du contexte, à savoir Roberta et plus précisément CamemBERT qui est un modèle Roberta français.\nDe plus, nous formons nos propres bi-encodeurs de modèles basés sur CamemBERT sur nos données.\nNotez que pour la formation, nous expérimentons avec les deux modèles de l'architecture bi-encodeur.\nLe siamois, qui utilise un modèle d'intégration de mots unique cartographiant la requête et l'article ensemble dans un vector space dense partagé, et le modèle à deux tours, qui utilise deux modèles d'intégration de mots indépendants encodant la requête et l'article séparément dans différents espaces d'intégration.\nNous expérimentons la mise en commun moyenne, maximale et CLS, ainsi que le produit et cosinus pour calculer les similitudes.\nVoici le résultat de notre base sur les ensembles de test.\nAvec les méthodes lexicales ci-dessus, les bi-encodeurs siamois ont évalué dans une configuration zero shot au milieu, et les bi-encodeurs raffinés ci-dessous.\nDans l'ensemble, le bi-encodeur raffiné dépasse de manière significative toutes les autres bases.\nLe modèle à deux tours s'améliore par rapport à ses variantes siamoises lors du rappel à cent, mais fonctionne de la même manière sur les autres indicateurs.\nBien que BM25 ait sous-performé le bi-encodeur formé de manière significative, ses performances indiquent qu'il s'agit toujours d'une base solide pour l'extraction spécifique au domaine.\nEn ce qui concerne l'évaluation zero shot du bi-encodeur siamois, nous constatons que l'utilisation directe des intégrations d'un modèle CamemBERT préformé sans optimiser pour la tâche d'extraction d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats antérieurs.\nEn outre, nous observons que le bi-encodeur basé sur word2vec a dépassé de manière significative les modèles basés sur fastText et les Représentations d'encodeurs bidirectionnels à partir de transformateurs, suggérant que peut-être les intégrations de niveau mot préformées sont plus appropriées pour la tâche que les intégrations de niveau caractère ou sous-mot lorsqu'elles sont utilisées immédiatement.\nBien que prometteurs, ces résultats suggèrent de nombreuses possibilités d'amélioration comparé à un expert juridique qualifié qui peut éventuellement extraire tous les articles pertinents à n'importe quelle question, et ainsi obtenir des scores parfaits.\nConcluons en discutant de deux limites de nos données.\nPremièrement, le corpus d'article est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants.\nAu cours de la construction des données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions ne se retrouvent qu'avec une fraction du nombre initial d'articles pertinents.\nCette information implique donc que la réponse contenue dans les autres articles pertinents pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.\nDeuxièmement, il convient de noter qu'on ne peut pas répondre à toutes les questions juridiques uniquement par des lois.\nPar exemple, la question « puis-je expulser mes locataires s'ils font trop de bruit ? »\nElle peut ne pas avoir de réponse détaillée dans le droit écrit qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée.\nAu lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des antécédents similaires à sa situation actuelle.\nPar exemple, les locataires font la fête deux fois par semaine jusqu'à deux heures du matin.\nPar conséquent, certaines questions sont mieux adaptées que d'autres à la tâche d'extraction de l'article de droit, et le domaine des moins adaptées reste à déterminer.\nNous espérons que nos travaux susciteront l'intérêt pour l'élaboration de modèles d'extraction d'article de droit pratiques et fiables.\nCela peut aider à améliorer l'accès à la justice pour tous.\nVous pouvez consulter notre article, nos données et notre code aux liens suivants. Merci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 419, "src_audio": "/acl6060/audio/en/419.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.\nWhy did we do the trouble in setting up this benchmark?\nWell, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.\nEach one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.\nSo we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.\nBut do we know what the models have actually learned?\nWhat is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?\nAnd the low score for this one?\nDo vision and language models focus on the right thing?\nOr do they focus on biases as shown by previous work?\nTo shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.\nWe target existence, plurality, counting, spatial relations, actions and entity coreference.\nBut how do we test whether the vision and language models have captured this phenomena?\nBy foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.\nFoiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.\nAnd we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.\nFor example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.\nCounting and coreference also are pieces that have more than one instrument.\nAnd we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.\nThis is not easy to do because a foiled caption may be less likely than the original caption.\nFor example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.\nTherefore, to obtain valid foils, we must take action.\nFirst, we make use of strong language models to propose foils.\nSecond, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.\nTo test this automatically, we apply natural language inference with the following rationale.\nWe consider an image to be the premise and its caption its entailed hypothesis.\nIn addition, we consider the caption to be the premise, and the foil is its hypothesis.\nIf an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.\nIf an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.\nBut this procedure is not perfect, it is just an indicator for valid foils.\nTherefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.\nSo, after filtering and human evaluation, we have as many test instances as described in this table.\nNote that VALSE does not deliver any training data but only test data.\nSince it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.\nFinetuning would only enable models to exploit artifacts or statistical biases in the data.\nAnd we all know that these models like to cheat and take shortcuts.\nAnd as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.\nWe experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.\nTwo of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.\nPerhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.\nFor more metrics and results on them, do check out our paper.\nThe results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.\nIt's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.\nHowever, none of the remaining pieces can be reliably solved in our adversarial foiling settings.\nWe see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.\nThe relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.\nThey also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.\nFrom the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.\nAs a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.\nIf the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.\nAnd it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.\nSo to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.\nOur experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.\nWe would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.\nAnd even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.\nIf you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "tgt_ref": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE ; un indice de référence indépendant de la tâche destiné à tester les modèles de langue et de vision avec des phénomènes linguistiques spécifiques.\nPourquoi avons-nous pris la peine de mettre en place cet indice de référence ?\nEh bien, au cours des dernières années, nous avons vu une explosion de la vision basée sur le transformateur et des modèles de langue préformés sur de grandes quantités de paires de textes d'images.\nChacun de ces modèles pousse les tâches de pointe en matière de vision et de langue telles que la réponse aux questions visuelles, le raisonnement de bon sens visuel, l'extraction d'images et les bases de phrases.\nNous avons donc reçu un message : les précisions sur ces tâches et les indices de référence spécifiques augmentent régulièrement.\nMais savons-nous ce que les modèles ont réellement appris ?\nQu'est-ce qu'une vision et un transformateur de langue ont compris lors de l'attribution d'un score élevé pour que cette image et cette phrase correspondent ?\nEt le score bas pour celle-ci ?\nEst-ce que les modèles de langue et de vision se concentrent sur la bonne chose ?\nOu se concentrent-ils sur les biais comme le montre le travail antérieur ?\nPour éclairer davantage cet aspect, nous proposons une direction plus indépendante des tâches et introduisons VALSE qui teste la sensibilité des modèles de langue et de vision à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistiques et visuelles.\nNous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités.\nMais comment tester si les modèles de langue et de vision ont capturé ce phénomène ?\nEn pratiquant le foiling sur une méthode précédemment appliquée pour les modèles de langue et de vision seulement pour des phrases nominales de Ravi Shekhar et de ses collaborateurs, et en comptant par nous dans les travaux antérieurs.\nEffectuer un foiling signifie essentiellement que nous prenons la légende d'une image et produisons un foil en modifiant la légende de sorte qu'elle ne décrive plus l'image.\nEt nous apportons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous aurions trouvé plus d'une façon intéressante de créer des instances de foil.\nPar exemple, dans le cas de l'élément d'actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié avec une action différente, et un dans lequel les acteurs sont échangés.\nLe comptage et la coréférence sont également des éléments qui ont plusieurs instruments.\nEt nous créons ces foils en nous assurant qu'ils ne décrivent pas l'image et que ce sont des phrases grammaticales et autrement valides.\nCe n'est pas facile à faire car une légende ayant subi un foil peut être moins probable que la légende originale.\nPar exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que les plantes coupent un homme qu'un homme coupe les plantes, et la grande vision et les modèles de langue pourraient capter cela.\nPar conséquent, pour obtenir des foils valides, nous devons agir.\nTout d'abord, nous utilisons des modèles de langue forts pour proposer des foils.\nDeuxièmement, nous utilisons l'inférence de la langue naturelle ou la NLI courte pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image.\nPour tester cela automatiquement, nous appliquons l'inférence de la langue naturelle avec la justification suivante.\nNous considérons qu'une image est la prémisse et sa légende son hypothèse implicite.\nEn outre, nous considérons la légende comme la prémisse, et le foil est son hypothèse.\nSi un modèle NLI prévient que le foil est en contradiction ou neutre par rapport à la légende, nous considérons cela comme un indicateur d'un foil valide.\nSi une NLI prévient le foil à engendrer par la légende, cela ne peut pas être un bon foil, car par transitivité, cela donnera une description véridique de l'image et nous filtrons ces foils.\nMais cette procédure n'est pas parfaite, il s'agit simplement d'un indicateur pour les foils valides.\nPar conséquent, comme troisième mesure pour produire des foils valides, nous utilisons des annotateurs humains pour valider les données utilisées dans VALSE.\nDonc, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrit dans ce tableau.\nNotez que VALSE ne fournit pas de données de formation mais seulement des données de test.\nComme il s'agit uniquement d'un indicateur de référence de test zero shot, il est conçu pour tirer parti des capacités existantes des modèles de langue et de vision après la préformation.\nLe raffinement permettrait seulement aux modèles d'exploiter des artefacts ou des biais statistiques dans les données.\nEt nous savons tous que ces modèles aiment tricher et prendre des raccourcis.\nEt comme nous l'avons dit, cela nous intéresse d'évaluer les capacités des modèles de langue et de vision après la préformation.\nNous expérimentons avec cinq modèles de langue et de vision sur VALSE, à savoir avec CLIP, LXMert, ViLBERT, ViLBERT douze en un, et VisualBERT.\nDeux de nos indicateurs d'évaluation les plus importants sont la précision des modèles dans la classification de paires de phrases images en légendes et foils.\nPeut-être plus pertinent pour cette vidéo, nous présenterons notre indicateur plus permissif, la précision par paire, qui mesure si le score de sentence alignment d'image est plus élevé pour la bonne paire de texte image que pour sa paire ayant subi un foil.\nPour plus d'indicateurs et de résultats, consultez notre article.\nLes résultats avec une précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres indicateurs. La meilleure performance zero shot est obtenue par ViLBERT douze en un, suivi de ViLBERT, LXMert, CLIP, et enfin VisualBERT.\nIl est remarquable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par ViLBERT douze en un, en soulignant que les modèles sont capables d'identifier des objets nommés et leur présence dans les images.\nCependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de foiling antagonistes.\nNous voyons à partir des instruments de pluralité et de comptage que les modèles de langue et de vision ont du mal à distinguer les références à des objets uniques par rapport à plusieurs, ou à les compter dans une image.\nL'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image.\nIls ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité comme nous le voyons dans l'élément d'actions.\nÀ partir de l'élément de coréférence, nous découvrons que relever plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de langue et de vision.\nPour vérifier la santé mentale, et parce que c'est une expérience intéressante, nous comparons également deux modèles à texte seulement, GPT un et GPT deux, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et ayant subi un foil, sans image ici, et en prévenant l'entrée avec la plus faible perplexité.\nSi la perplexité est plus élevée pour le foil, nous considérons cela comme une indication que la légende ayant subi un foil peut souffrir de biais de plausibilité ou d'autres biais linguistiques.\nEt il est intéressant de voir que dans certains cas, les modèles GPT à texte seulement ont capturé la plausibilité du monde mieux que les modèles de langue et de vision.\nDonc, pour résumer, VALSE est un indicateur de référence qui utilise l'objectif des constructions linguistiques pour aider la communauté à améliorer les modèles de langue et de vision en testant durement leurs capacités de bases visuelles.\nNos expériences montrent que les modèles de langue et de vision identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais luttent pour ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques.\nNous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers les bases de la langue avec des modèles de langue et de vision.\nEt plus encore, VALSE pourrait être utilisé comme une évaluation indirecte des données, car les modèles pourraient être évalués avant et après la formation ou le raffinement pour voir si des données aident les modèles à améliorer l'un des aspects testés par VALSE.\nSi vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 420, "src_audio": "/acl6060/audio/en/420.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.\nI'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.\nI'll be explaining in this order.\nFirst, I will introduce automatic release note generation that we are working on in this research.\nA release note is a technical document that summarizes the changes distributed with each release of a software product.\nThe image shows a release note for version two point six point four of the vuejs library.\nRelease notes play an important role in open source development but they're time consuming to prepare manually.\nTherefore, it would be very useful to be able to automatically generate high quality release notes.\nI will defer to two previous researches on automatic release note generation.\nThe first is a system called ARENA released in twenty fourteen.\nIt takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.\nThe most notable feature of this system is the issue extractor in the upper right corner.\nWhich must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.\nIn other words, it cannot be used for many projects on GitHub.\nThe second is Glyph, recently announced in twenty twenty.\nIt is available on the internet and can be installed via pip.\nThis system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.\nThis image is a sample usage that returns a corrective or bug fixes label.\nGlyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.\nThe performance of the text classification model is not high.\nI present two related researches, but their problems are limited applicability and scarce data resources.\nOur paper solves these two problems and automatically generates high quality release notes.\nWith a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.\nThis proposed method can be used for all English repositories.\nFor the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.\nNext, I'll describe our dataset.\nHere is an example of data.\nThe left side is a commit message and the right side is the release notes.\nRelease notes are labeled as improvements or fixes, etc.\nWe have set up a task that takes the commit messages as input and outputs a labeled release notes.\nThis can be regarded as a summarization task.\nWe have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.\nThese were set based on previous research and other factors.\nThe release note on the bottom right is extracted from the release note on the bottom left.\nAt this time, it is necessary to detect the four labels that have been set up in advance.\nBut the labels are not always consistent with each repository.\nFor example, the improvements label includes improvements, enhancements, optimizations, and so on.\nWe prepared a vocabulary list of about thirty labels for each of these notational variations.\nThis is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.\nNext is a commit message.\nCommit messages are not tied to each release.\nAs shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.\nThis is a bit tedious and it is not enough to just get a list of releases and look at the before and after.\nWe created a heuristic matching rule to get the previous and next versions.\nDataset analysis.\nIn the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.\nAlso, the average number of release notes tokens is sixty three, which is quite high for a summarization task.\nAlso, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.\nThis is due to the large number of unique class or method names found in the repository.\nNext, I will explain the proposed method.\nThe classwise extractive then abstractive summarization model consists of two neural modules.\nA classifier using BERT or CodeBERT and a generator using BART.\nFirst, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.\nThe commit messages classified as other are discarded.\nThen CEAS applies the generator to the four labeled documents independently and generates release notes for each class.\nIn this task, the direct correspondences between commit messages and release notes are not known.\nTherefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.\nWe modeled the classwise abstractive summarization approach by two different methods.\nThe first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.\nThe output texts can be divided into classwise segments based on special class-specific endpoint symbols.\nThe second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.\nOkay, let me explain the experiments.\nFive methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.\nRegarding evaluation, in some cases, release notes are output in multiple sentences.\nSince it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.\nThe BLEU is penalized when the system outputs a short sentence.\nThis penalty results in a lower BLEU value in the experiment results described next.\nFinally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.\nA higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.\nHere are the results.\nSince the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.\nCEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.\nIn particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.\nThese results indicate that CEAS and CAS are significantly affected.\nCEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.\nHigh coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.\nCAS-Multi tended to yield higher ROUGE-L than CAS-Single.\nSuggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.\nHere are an error analysis.\nCAS methods tend to output shorter sentences than human reference sentences.\nIn the figure on the right, the reference sentence has three or four sentences, while CAS has only one.\nThe reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.\nFurthermore, CAS methods cannot generate accurate release notes without additional information.\nThe top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.\nThe example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.\nFinally, a conclusion.\nWe have built a new dataset for automatic release note generation.\nWe have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.\nOur experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.\nPlease check out our dataset on GitHub.\nThank you.", "tgt_ref": "Bonjour, je m'appelle Kamezawa de l'Université de Tokyo.\nJe vais vous présenter un article intitulé RNSum : des données à grande échelle pour la génération automatique de note de version via la synthèse des journaux de validation.\nJe vais vous expliquer dans cet ordre.\nTout d'abord, je vais présenter la génération de note de version automatique sur laquelle nous travaillons dans cette recherche.\nUne note de version est un document technique qui résume les modifications distribuées à chaque version d'un produit logiciel.\nL'image montre une note de version pour la version deux point six point quatre de la bibliothèque Vuejs.\nLes notes de version jouent un rôle important dans le développement open source, mais elles mettent du temps à être préparées manuellement.\nPar conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.\nJe m'en remettrai à deux recherches antérieures sur la génération automatique de notes de version.\nLa première est un système appelé ARENA sorti en deux-mille-quatorze.\nIl faut une approche basée sur des règles, par exemple, en utilisant l'extracteur de changement pour extraire toutes les différences, les changements de bibliothèque et les changements de document à partir des différences entre les versions, et enfin, en les combinant.\nLa caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit.\nLe système de suivi des problèmes, qui doit être laissé à Jira, ne peut être appliqué qu'aux projets qui utilisent Jira.\nEn d'autres mots, il ne peut pas être utilisé pour de nombreux projets sur GitHub.\nLa seconde est Glyphe, récemment annoncée en deux-mille-vingt.\nElle est disponible sur internet et peut être installée via pip.\nCe système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'une des cinq étiquettes telles que les fonctions ou corrections de bugs pour chaque message de validation de saisie.\nCette image est un exemple d'utilisation qui renvoie une étiquette de correction ou de débogage.\nLes données de formation de Glyphe sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous.\nLes performances du modèle de classification de texte ne sont pas élevées.\nJe présente deux recherches connexes, mais leurs problèmes sont l'applicabilité limitée et les ressources de données rares.\nNotre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité.\nAvec un problème d'applicabilité limitée, nous proposons une méthode de synthèse par classe de haute qualité en utilisant uniquement des messages de validation comme saisie.\nCette méthode proposée peut être utilisée pour tous les référentiels anglais.\nPour le deuxième problème de ressources de données rares, nous avons construit nos données RNSum composées d'environ quatre-vingt-deux mille éléments de données en collectant des données à partir de référentiels GitHub publics en utilisant l'API GitHub.\nEnsuite, je vais décrire nos données.\nVoici un exemple de données.\nLe côté gauche est un message de validation et le côté droit représente les notes de version.\nLes notes de version sont étiquetées comme améliorations ou correctifs, etc.\nNous avons mis en place une tâche qui prend les messages de validation comme saisie et sorties des notes de version étiquetées.\nCela peut être considéré comme une tâche de synthèse.\nNous avons prédéfini quatre étiquettes : fonctions, améliorations, corrections de bugs, suppressions de dépréciations et modifications de rupture.\nCelles-ci ont été établies sur la base de la recherche antérieure et d'autres facteurs.\nLa note de version en bas à droite est extraite de la note de version en bas à gauche.\nÀ ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été mises en place à l'avance.\nMais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel.\nPar exemple, l'étiquette d'améliorations inclut des améliorations, des perfectionnements, des optimisations, etc.\nNous avons préparé une liste de vocabulaire d'une trentaine d'étiquettes pour chacune de ces variations de notation.\nIl s'agit de détecter la classe de note de version et de collecter le texte de la version qui suit en tant que phrase de note de version pour la classe.\nEnsuite, il y a un message de validation.\nLes messages de validation ne sont pas liés à chaque version.\nComme le montre l'image ci-dessous, si la version actuelle est la version deux point cinq à dix-neuf, nous devons identifier la version deux point cinq à dix-huit antérieure et obtenir un diff.\nC'est un peu fastidieux et il ne suffit pas d'obtenir une liste de versions et de regarder l'avant et l'après.\nNous avons créé une règle de correspondance heuristique pour obtenir les versions antérieures et suivantes.\nAnalyse des données.\nEn fin de compte, sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données ont été recueillis.\nEn outre, le nombre moyen de gages de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse.\nDe même, le nombre de gages uniques est assez grand, s'élevant à huit mille huit cent trente mille.\nCela est dû au grand nombre de noms de méthode ou de classe unique trouvés dans le référentiel.\nEnsuite, je vais expliquer la méthode proposée.\nLe modèle de synthèse extractive puis abstractive par classe se compose de deux modules neuronaux.\nUn classificateur utilisant BERT ou CodeBERT et un générateur utilisant BART.\nTout d'abord, CEAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, qui utilisent des améliorations, des corrections de bogues, des dépréciations et d'autres.\nLes messages de validation classés comme autres sont supprimés.\nEnsuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.\nDans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues.\nPar conséquent, pour former le classificateur, nous avons réaffecté les enquêtes à chaque message de validation de saisie en utilisant les dix premiers caractères de chaque message de validation.\nNous avons modélisé l'approche de synthèse abstractive par classe à travers deux méthodes différentes.\nLe premier modèle, que nous appelons CAS unique, se compose d'un réseau de six à six unique et génère un seul texte de note de version donnant un enchaînement de messages de validation de saisie.\nLes textes de sortie peuvent être divisés en segments par classe sur la base de symboles de point d'extrémité particuliers, spécifiques à la classe.\nLa seconde méthode, que nous appelons CAS multiple, se compose de quatre réseaux seq2seq différents, dont chacun correspond à l'une des classes de notes de version fixes.\nOk, je vais vous expliquer les expériences.\nCinq méthodes ont été comparées : CEAS, CAS unique, CAS multiple, regroupement et étude antérieure, Glyph.\nEn ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases.\nPuisqu'il est difficile de calculer le nombre de phrases telles qu'elles sont, elles sont combinées avec des espaces et traitées comme une seule phrase longue.\nLe BLEU est pénalisé lorsque le système produit une phrase courte.\nCette pénalité se traduit par une valeur BLEU inférieure dans les résultats de l'expérience décrits ci-dessous.\nEnfin, nous calculons également la spécificité car ROUGE et BLEU ne peuvent pas être calculés si les notes de version sont vides.\nUne spécificité plus élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées être vides.\nLes résultats sont indiqués ci-après.\nÉtant donné que les données contiennent des adresses électroniques, des valeurs hachées, etc., nous avons également évalué les données nettoyées, ce qui les exclut.\nLe CEAS et le CAS ont obtenu des scores de ROUGE-L supérieurs de plus de dix points par rapport aux bases.\nEn particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les bases a atteint plus de vingt points.\nCes résultats indiquent que le CEAS et le CAS sont considérablement touchés.\nLe CEAS a obtenu un meilleur score ROUGE-L que le CAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour former le classificateur à l'aide de pseudo-étiquettes.\nUne couverture élevée du CEAS peut être obtenue probablement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.\nLe CAS multiple tendait à produire plus de ROUGE-L que le CAS unique.\nEn suggérant qu'il est également efficace de développer indépendamment et différemment des modèles de abstractive summarization pour chaque classe de note de version.\nVoici une analyse d'erreur.\nLes méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines.\nDans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que le CAS n'en a qu'une.\nLa raison de la réticence de ce modèle est que dans les données de formation, seulement trente-trois pour cent des phrases sont présentes dans l'étiquette des fonctions et quarante pour cent dans l'étiquette des améliorations.\nEn outre, les méthodes CAS ne peuvent pas générer des notes de version précises sans informations supplémentaires.\nL'exemple en haut à droite est un exemple de message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la progression ou au problème correspondant(e).\nL'exemple ci-dessous montre que les deux messages de validation dans la saisie sont liés et doivent être combinés en une phrase, mais cela n'est pas fait.\nEnfin, passons à la conclusion.\nNous avons construit de nouvelles données pour la génération automatique de notes de version.\nNous avons également formulé une tâche consistant à saisir des messages de validation et à les résumer afin qu'ils soient applicables à tous les projets écrits en anglais.\nNos expériences montrent que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les bases.\nVeuillez consulter nos données sur GitHub.\nMerci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 0, "src_audio": "/acl6060/audio/en/0.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour tout le monde. Aujourd'hui, je vais vous présenter notre travail de recherche « Apprendre à raisonner par déduction » : résolution de problèmes de mots mathématiques comme extraction de relation complexe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": null, "sample_id": 1, "src_audio": "/acl6060/audio/en/1.wav", "src_ref": "I'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'appelle Allan du laboratoire d'intelligence artificielle ByteDance, et ceci est un travail en commun avec Jierui Li de l'Université du Texas à Austin et Wei Lu du SUTD.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 2, "src_audio": "/acl6060/audio/en/2.wav", "src_ref": "First, I'd like to talk about our motivation for reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, j'aimerais parler de notre motivation pour le raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 3, "src_audio": "/acl6060/audio/en/3.wav", "src_ref": "So here we show an examples where multi-step reasoning is helpful.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous montrons un exemple où le raisonnement en plusieurs étapes est utile.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 4, "src_audio": "/acl6060/audio/en/4.wav", "src_ref": "So this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce chiffre est donc tiré de l'article PaLM où ils incitent à résoudre le problème de réseau dans le scénario d'apprentissage de quelques prises de vue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 5, "src_audio": "/acl6060/audio/en/5.wav", "src_ref": "So on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, sur le côté gauche, nous pouvons voir que si nous fournissons quelques exemples avec juste des questions et des réponses, nous pourrions ne pas être en mesure d'obtenir les bonnes réponses.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 6, "src_audio": "/acl6060/audio/en/6.wav", "src_ref": "But if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si nous fournissons plus de description du raisonnement, le modèle est capable de prévenir la description du raisonnement, mais aussi d'effectuer ici une prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 7, "src_audio": "/acl6060/audio/en/7.wav", "src_ref": "So it is good to have interpretable multi-step reasoning as output.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est alors bon d'avoir comme résultat un raisonnement à plusieurs étapes interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 8, "src_audio": "/acl6060/audio/en/8.wav", "src_ref": "And we also think math word problem is a straightforward application to evaluate such reasoning abilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pensons également que les problèmes de mots mathématiques sont une application directe pour évaluer de telles capacités de raisonnement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 9, "src_audio": "/acl6060/audio/en/9.wav", "src_ref": "So, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 10, "src_audio": "/acl6060/audio/en/10.wav", "src_ref": "So in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans nos données, nous recevons également l‘expression mathématique qui conduit à cette réponse particulière.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 11, "src_audio": "/acl6060/audio/en/11.wav", "src_ref": "So, certain assumptions ah also apply as in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, certaines hypothèses s'appliquent également comme dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 12, "src_audio": "/acl6060/audio/en/12.wav", "src_ref": "We assume the precision of quantities are known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que la précision des quantités est connue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 13, "src_audio": "/acl6060/audio/en/13.wav", "src_ref": "And we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 14, "src_audio": "/acl6060/audio/en/14.wav", "src_ref": "Furthermore, complicated operators can be actually decomposed into these basic operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les opérateurs compliqués peuvent effectivement être décomposés en ces opérateurs de base.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 15, "src_audio": "/acl6060/audio/en/15.wav", "src_ref": "So, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, le travail antérieur dans la résolution de problèmes de mots mathématiques peut en effet être classé dans un modèle séquence à séquence et séquence à arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 16, "src_audio": "/acl6060/audio/en/16.wav", "src_ref": "So, traditional sequence to sequence model convert the expression to a specific sequence for generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle séquence à séquence traditionnel convertit donc l'expression en une séquence spécifique pour la génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 17, "src_audio": "/acl6060/audio/en/17.wav", "src_ref": "And it is pretty easy to implement and it can generalize to many different complicated problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est assez facile à mettre en œuvre et peut être généralisé à de nombreux problèmes compliqués différents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 18, "src_audio": "/acl6060/audio/en/18.wav", "src_ref": "But the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle structuré et son manque d'interopérabilité pour la prévention.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 19, "src_audio": "/acl6060/audio/en/19.wav", "src_ref": "But actually this direction is still quite popular because of um the transformer model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, cette direction est encore très populaire en raison du modèle de conversion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 20, "src_audio": "/acl6060/audio/en/20.wav", "src_ref": "So, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans les modèles à base d'arbre, nous structurons en réalité ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 21, "src_audio": "/acl6060/audio/en/21.wav", "src_ref": "So here we keep generating the operators until we reach the leaves, which are the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous continuons à produire les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 22, "src_audio": "/acl6060/audio/en/22.wav", "src_ref": "So here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, la bonne chose est qu'en fait, cela nous donne cette structure d'arbre binaire. Mais en réalité, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur et ensuite, à la fin, nous générons les quantités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 23, "src_audio": "/acl6060/audio/en/23.wav", "src_ref": "And the second thing is that it also contains some repetitive computations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que cela contient également des calculs répétitifs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 24, "src_audio": "/acl6060/audio/en/24.wav", "src_ref": "So here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, si nous regardons cette expression, huit fois trois plus trois est en fait généré deux fois, mais en réalité, nous devrions réutiliser les résultats.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 25, "src_audio": "/acl6060/audio/en/25.wav", "src_ref": "So, in our proposed approach we want to solve those problems in a step by step and interpretable manners.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 26, "src_audio": "/acl6060/audio/en/26.wav", "src_ref": "So for example, here in the second step, ah we can obtain these divisors which is twenty seven.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, ici dans la deuxième étape, nous pouvons obtenir ces diviseurs qui sont vingt-sept.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 27, "src_audio": "/acl6060/audio/en/27.wav", "src_ref": "And we can also refer back to the original questions to find the relevant contents.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 28, "src_audio": "/acl6060/audio/en/28.wav", "src_ref": "And in these steps we obtain the divisors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et dans ces étapes, nous obtenons les diviseurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 29, "src_audio": "/acl6060/audio/en/29.wav", "src_ref": "So, ah and then at this third step we actually get the quotient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis, à cette troisième étape, nous obtenons alors le quotient.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 30, "src_audio": "/acl6060/audio/en/30.wav", "src_ref": "Alright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Et après ces trois étapes, nous pouvons alors réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, et enfin, nous pouvons obtenir les dividendes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 31, "src_audio": "/acl6060/audio/en/31.wav", "src_ref": "So, here we actually generate the whole expression directly rather than generating a single operators or quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous générons en réalité l'expression entière directement plutôt que de produire un seul opérateur ou une seule quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 32, "src_audio": "/acl6060/audio/en/32.wav", "src_ref": "So this makes the process more accurate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela rend le processus plus précis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 33, "src_audio": "/acl6060/audio/en/33.wav", "src_ref": "So, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et incluant également une certaine constante comme notre état initial.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 34, "src_audio": "/acl6060/audio/en/34.wav", "src_ref": "So, the expression is represented by e i j o p.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, l'expression est représentée par e i j o p.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 35, "src_audio": "/acl6060/audio/en/35.wav", "src_ref": "Where we perform operator from q_i to q_j, and such expression is actually directed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Où nous effectuons l'opérateur de q_i à q_j, et cette expression est en fait dirigée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 36, "src_audio": "/acl6060/audio/en/36.wav", "src_ref": "So, we also have subtraction with words here to represent the opposite direction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, nous avons également ici la soustraction avec des mots pour représenter la direction opposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 37, "src_audio": "/acl6060/audio/en/37.wav", "src_ref": "This is quite similar to relation extraction.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est tout à fait similaire à l'extraction de relation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 38, "src_audio": "/acl6060/audio/en/38.wav", "src_ref": "So in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans un système déductif formel, à un pas de temps t, on applique l'opérateur entre la paire q_i et q_j, puis on obtient cette nouvelle expression.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 39, "src_audio": "/acl6060/audio/en/39.wav", "src_ref": "We add it to the next state to become a new quantity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous l'ajoutons à l'état suivant pour devenir une nouvelle quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 40, "src_audio": "/acl6060/audio/en/40.wav", "src_ref": "So, these slides actually visualize the evolution of the state where we keep adding expression to the current state.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, ces diapositives visualisent en réalité l'évolution de l'état où nous continuons à ajouter de l'expression à l'état actuel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 41, "src_audio": "/acl6060/audio/en/41.wav", "src_ref": "So in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langue préformée qui peut être BERT ou Roberta, puis nous encodons la phrase et obtenons ensuite ces représentations de quantité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 42, "src_audio": "/acl6060/audio/en/42.wav", "src_ref": "So, once we get the quantity representations, we can start to do inference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire l'inférence.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 43, "src_audio": "/acl6060/audio/en/43.wav", "src_ref": "Here we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous montrons ici un exemple de q_1 pour obtenir la représentation pour q_2 divisée par q_2 multipliée par q_3.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 44, "src_audio": "/acl6060/audio/en/44.wav", "src_ref": "First we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous obtenons la représentation de paire, qui n'est essentiellement que l'enchaînement entre q_1 et q_2, puis nous appliquons un réseau prédictif qui est paramétré par l'opérateur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 45, "src_audio": "/acl6060/audio/en/45.wav", "src_ref": "And then finally we obtain the expression representation q_1 divided by q_2.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et enfin, nous obtenons la représentation de l'expression q_1 divisée par q_2.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 46, "src_audio": "/acl6060/audio/en/46.wav", "src_ref": "But in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais en réalité, dans la pratique, au stade de l'inférence, nous pourrions également obtenir une expression incorrecte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 47, "src_audio": "/acl6060/audio/en/47.wav", "src_ref": "So, here all the possible expression is equals to three times the number of operators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, toute l'expression possible est égale à trois fois le nombre d'opérateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 48, "src_audio": "/acl6060/audio/en/48.wav", "src_ref": "So the nice thing here is that we can easily add constraints to control this search this search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors, ce qui est bien ici, c'est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 49, "src_audio": "/acl6060/audio/en/49.wav", "src_ref": "For example, if this expression is not allowed, we can simply remove this expression in our search space.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 50, "src_audio": "/acl6060/audio/en/50.wav", "src_ref": "So in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans la deuxième étape, nous faisons la même chose, mais la seule différence est que nous avons une quantité de plus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 51, "src_audio": "/acl6060/audio/en/51.wav", "src_ref": "So this quantity come from the previous calculated expression.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette quantité provient donc de l'expression calculée antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 52, "src_audio": "/acl6060/audio/en/52.wav", "src_ref": "So finally we can obtain this final expression q_3 times q_4.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous pouvons obtenir cette expression finale q_3 multipliée par q_4.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 53, "src_audio": "/acl6060/audio/en/53.wav", "src_ref": "And we can also see the number of all the possible ah expression is different from the previous step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape antérieure.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 54, "src_audio": "/acl6060/audio/en/54.wav", "src_ref": "So, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une telle différence rend difficile l'application de la beam search car la distribution de probabilité entre ces deux étapes est déséquilibrée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 55, "src_audio": "/acl6060/audio/en/55.wav", "src_ref": "So the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La procédure de formation est donc similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 56, "src_audio": "/acl6060/audio/en/56.wav", "src_ref": "And here we also use this tau to represent when we should terminate this generation process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous utilisons également ce tau pour représenter le moment où nous devrions mettre fin à ce processus de génération.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 57, "src_audio": "/acl6060/audio/en/57.wav", "src_ref": "And here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, l'espace est différent de séquence à séquence car il est différent à chaque pas de temps, alors que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 58, "src_audio": "/acl6060/audio/en/58.wav", "src_ref": "And it also allows us to impose certain constraints from prior from prior knowledge.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 59, "src_audio": "/acl6060/audio/en/59.wav", "src_ref": "So we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous menons donc des expériences sur les données de problèmes de mots mathématiques, MAWPS, Math23K, MathQA et SVAMP couramment utilisées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 60, "src_audio": "/acl6060/audio/en/60.wav", "src_ref": "And here we briefly show the results compared with the previous best approaches.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons brièvement les résultats comparés aux meilleures approches antérieures.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 61, "src_audio": "/acl6060/audio/en/61.wav", "src_ref": "So our best performing variant is Roberta-DeductiveReasoner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, notre variante la plus performante est Roberta-DeuctiveReasoner.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 62, "src_audio": "/acl6060/audio/en/62.wav", "src_ref": "And in fact we do not use beam search, in contrast all previous approaches are using beam search.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en réalité, nous n'utilisons pas la beam search ; au contraire, toutes les approches antérieures utilisent la beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 63, "src_audio": "/acl6060/audio/en/63.wav", "src_ref": "All right. So, the best approaches are often tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Très bien. Ainsi, les meilleures approches sont souvent un modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 64, "src_audio": "/acl6060/audio/en/64.wav", "src_ref": "So, overall our reasoner is able to significantl significantly outperform this tree based model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans l'ensemble, notre raisonneur est capable de dépasser significativement ce modèle à base d'arbre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 65, "src_audio": "/acl6060/audio/en/65.wav", "src_ref": "But we can see the absolute numbers on MathQA or SVAMP are not really high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons voir que les nombres absolus sur MathQA ou SVAMP ne sont pas vraiment élevés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 66, "src_audio": "/acl6060/audio/en/66.wav", "src_ref": "So we further investigate the results on SVAMP.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous étudions donc plus en détail les résultats sur SVAMP.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 67, "src_audio": "/acl6060/audio/en/67.wav", "src_ref": "And this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ces données sont difficiles parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle TAL traitement automatique du langage naturel comme ajouter des informations non pertinentes et des quantités supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 68, "src_audio": "/acl6060/audio/en/68.wav", "src_ref": "So, in our prediction we find some of the intermediate values are actually negatives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans notre prévention, nous trouvons que certaines des valeurs intermédiaires sont en réalité négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 69, "src_audio": "/acl6060/audio/en/69.wav", "src_ref": "For example, um, in these questions we are asking how many apples does Jake have?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans ces questions, nous demandons « combien de pommes a Jake ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 70, "src_audio": "/acl6060/audio/en/70.wav", "src_ref": "But we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous avons des informations supplémentaires comme « dix-sept photos de moins », et « Steven en a huit », ce qui est totalement hors de propos.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 71, "src_audio": "/acl6060/audio/en/71.wav", "src_ref": "So, our model makes some prediction like this which is producing negative values.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, notre modèle effectue une prévention comme celle-ci qui produit des valeurs négatives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 72, "src_audio": "/acl6060/audio/en/72.wav", "src_ref": "And we observe these two expressions actually have similar scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous constatons que ces deux expressions ont en réalité des scores similaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 73, "src_audio": "/acl6060/audio/en/73.wav", "src_ref": "So, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pouvons alors limiter cet espace de recherche en supprimant les résultats négatifs afin que nous puissions rendre la réponse correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 74, "src_audio": "/acl6060/audio/en/74.wav", "src_ref": "So um we further find such constraint actually improves quite a lot for some models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous trouvons donc que cette contrainte améliore en réalité beaucoup pour certains modèles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 75, "src_audio": "/acl6060/audio/en/75.wav", "src_ref": "For example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous améliorons sept points, puis pour le modèle de base Roberta, nous avons amélioré deux points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 76, "src_audio": "/acl6060/audio/en/76.wav", "src_ref": "So better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le meilleur modèle de langue a donc de meilleures capacités de compréhension de la langue de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 77, "src_audio": "/acl6060/audio/en/77.wav", "src_ref": "And we also try to analyze the difficulty behind these behind all these datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous essayons également d'analyser la difficulté derrière toutes ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 78, "src_audio": "/acl6060/audio/en/78.wav", "src_ref": "We assume the number of unused quantities can be regarded as irrelevant information here.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous supposons que le nombre de quantités inutilisées peut être considéré ici comme une information non pertinente.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 79, "src_audio": "/acl6060/audio/en/79.wav", "src_ref": "So ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et les données SVAMP ont la plus grande partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 80, "src_audio": "/acl6060/audio/en/80.wav", "src_ref": "And here we also show the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ici, nous montrons également la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 81, "src_audio": "/acl6060/audio/en/81.wav", "src_ref": "For those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ces échantillons sans quantités inutilisées, la performance est en réalité supérieure à la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 82, "src_audio": "/acl6060/audio/en/82.wav", "src_ref": "But with those samples that with unused quantity is actually way worse than the, worse than the overall performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais avec ces échantillons ayant une quantité inutilisée, c'est en fait bien pire que la performance globale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 83, "src_audio": "/acl6060/audio/en/83.wav", "src_ref": "For MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour MAWPS, nous n'avons pas vraiment trop de cas de test, alors j'ignore simplement cette partie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 84, "src_audio": "/acl6060/audio/en/84.wav", "src_ref": "So, finally we want to show the interpretability through a question perturbation example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Finalement, nous voulons montrer l'interopérabilité à travers un exemple de perturbation de question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 85, "src_audio": "/acl6060/audio/en/85.wav", "src_ref": "So here our model actually makes a wrong prediction at the first step.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, notre modèle effectue en réalité une prévention erronée à la première étape.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 86, "src_audio": "/acl6060/audio/en/86.wav", "src_ref": "So, we can actually correlate this expression with the sentence here. Alright.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, nous pouvons effectivement corréler cette expression avec la phrase ici. Très bien.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 87, "src_audio": "/acl6060/audio/en/87.wav", "src_ref": "So, we think this sentence might be misleading the model to an incorrect predictions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous pensons donc que cette phrase pourrait induire le modèle en erreur avec des préventions incorrectes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 88, "src_audio": "/acl6060/audio/en/88.wav", "src_ref": "So here planting another thirty five makes the model makes the model think it should be an addition operator.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc ici, en planter trente-cinq autres fait penser au modèle qu'il devrait être un opérateur d'addition.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 89, "src_audio": "/acl6060/audio/en/89.wav", "src_ref": "So we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous essayons alors de réviser la phrase pour que cela soit quelque chose comme « le nombre de poiriers est inférieur de trente-cinq à celui des pommiers ».", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 90, "src_audio": "/acl6060/audio/en/90.wav", "src_ref": "So, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous faisons en sorte de transmettre une sémantique plus précise afin que le modèle soit capable de rendre la prévention correcte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 91, "src_audio": "/acl6060/audio/en/91.wav", "src_ref": "So, this study shows how the interpretable predictions help us understand the model behavior.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, cette étude montre comment les préventions interprétables nous aident à comprendre le comportement du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 92, "src_audio": "/acl6060/audio/en/92.wav", "src_ref": "So to conclude our work, so first our model is actually pretty efficient.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour conclure notre travail, notre modèle est en réalité assez efficace.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 93, "src_audio": "/acl6060/audio/en/93.wav", "src_ref": "And we are able to provide interpretable solving procedure.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous sommes en mesure de fournir une procédure de résolution interprétable.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 94, "src_audio": "/acl6060/audio/en/94.wav", "src_ref": "And we can easily incorporate some prior knowledge as constraint which can help improve the performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous pouvons facilement incorporer des connaissances antérieures en tant que contraintes qui peuvent aider à améliorer la performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 95, "src_audio": "/acl6060/audio/en/95.wav", "src_ref": "And the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution des problèmes de réseau, mais aussi aux autres tâches qui impliquent un raisonnement à plusieurs étapes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 96, "src_audio": "/acl6060/audio/en/96.wav", "src_ref": "We also have certain limitations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également certaines limites.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 97, "src_audio": "/acl6060/audio/en/97.wav", "src_ref": "Ah, if we have a large number of operators or constants, the memory consumption could be pretty high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 98, "src_audio": "/acl6060/audio/en/98.wav", "src_ref": "And the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et la deuxième chose est que, comme mentionné, puisque la distribution de probabilité est déséquilibrée entre les différentes étapes de temps, il est donc également assez difficile d'appliquer la stratégie de beam search.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 99, "src_audio": "/acl6060/audio/en/99.wav", "src_ref": "So this is the end of the talk, and questions are welcomed. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous arrivons donc à la fin de la discussion, et vos questions sont les bienvenues. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 100, "src_audio": "/acl6060/audio/en/100.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 101, "src_audio": "/acl6060/audio/en/101.wav", "src_ref": "I will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter mon travail en commun avec Jerry, qui porte sur de nouvelles données pour l'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 102, "src_audio": "/acl6060/audio/en/102.wav", "src_ref": "Legal issues are an integral part of many people's lives.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions juridiques font partie intégrante de la vie de nombreuses personnes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 103, "src_audio": "/acl6060/audio/en/103.wav", "src_ref": "But the majority of citizens have little to know knowledge about their rights and fundamental legal processes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais la majorité des citoyens ont peu de connaissances sur leurs droits et leurs processus juridiques fondamentaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 104, "src_audio": "/acl6060/audio/en/104.wav", "src_ref": "As a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En conséquence, de nombreux citoyens vulnérables qui n'ont pas les moyens de se payer l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 105, "src_audio": "/acl6060/audio/en/105.wav", "src_ref": "All work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tous les travaux visent à combler le fossé entre les personnes et la loi en développant un système d'extraction efficace pour les articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 106, "src_audio": "/acl6060/audio/en/106.wav", "src_ref": "Such a system could provide a free professional legal help service for unskilled humans.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un tel système pourrait fournir un service d'aide juridique professionnel gratuit aux personnes non qualifiées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 107, "src_audio": "/acl6060/audio/en/107.wav", "src_ref": "Before diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avant de plonger dans la principale contribution de ce travail, nous allons d'abord décrire le problème d'extraction des articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 108, "src_audio": "/acl6060/audio/en/108.wav", "src_ref": "Given a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Compte tenu d'une simple question sur une question juridique telle que : « qu'est-ce que je risque si je viole la confidentialité professionnelle ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 109, "src_audio": "/acl6060/audio/en/109.wav", "src_ref": "A model is required to retrieve all relevant statutory articles from a large body of legislation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un modèle est nécessaire pour extraire tous les articles de droit pertinents d'un grand corpus législatif.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 110, "src_audio": "/acl6060/audio/en/110.wav", "src_ref": "This information retrieval task comes with its own set of challenges.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette tâche d'extraction des informations comporte son propre ensemble de défis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 111, "src_audio": "/acl6060/audio/en/111.wav", "src_ref": "First, it deals with two types of language.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, elle traite de deux types de langue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 112, "src_audio": "/acl6060/audio/en/112.wav", "src_ref": "Common natural language for the questions and complex legal language for the statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La langue naturelle commune pour les questions et la langue juridique complexe pour les lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 113, "src_audio": "/acl6060/audio/en/113.wav", "src_ref": "This difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette différence dans les distributions de langue rend plus difficile pour un système d'extraire les candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 114, "src_audio": "/acl6060/audio/en/114.wav", "src_ref": "Besides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le droit écrit n'est pas une pile d'articles indépendants qui peuvent être traités comme une source complète d'informations par eux-mêmes, contrairement aux nouvelles ou aux recettes, par exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 115, "src_audio": "/acl6060/audio/en/115.wav", "src_ref": "Instead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, il s'agit d'un ensemble structuré de dispositions juridiques qui n'ont un sens entier que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les domaines et sous-domaines auxquels elles appartiennent et leur place dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 116, "src_audio": "/acl6060/audio/en/116.wav", "src_ref": "Lastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, les articles de droit ne sont pas de petits paragraphes constituant généralement l'unité d'extraction typique dans la plupart des travaux d'extraction.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 117, "src_audio": "/acl6060/audio/en/117.wav", "src_ref": "Here, there are long documents that may be up to six thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ici, il y a de longs documents qui peuvent aller jusqu'à six mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 118, "src_audio": "/acl6060/audio/en/118.wav", "src_ref": "The recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les recent advances en matière de TAL traitement automatique du langage naturel ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prévention du jugement juridique ou l'examen des contrats de contacts automatisés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 119, "src_audio": "/acl6060/audio/en/119.wav", "src_ref": "But statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais l'extraction de l'article de droit est restée essentiellement inchangée en raison du manque de grandes données étiquetées de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 120, "src_audio": "/acl6060/audio/en/120.wav", "src_ref": "In this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans ce travail, nous présentons de nouvelles données centrées sur le citoyen natif français pour étudier si l'extraction des modèles peut se rapprocher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche d'extraction de l'article de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 121, "src_audio": "/acl6060/audio/en/121.wav", "src_ref": "Our Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos données d'extraction de l'article de droit belge BSARD se composent de plus de mille cent questions juridiques posées par des citoyens belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 122, "src_audio": "/acl6060/audio/en/122.wav", "src_ref": "These questions cover a wide range of topics from family, housing, money, to work and social security.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces questions couvrent un wide range de sujets allant de la famille au logement, à l'argent, au travail et à la sécurité sociale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 123, "src_audio": "/acl6060/audio/en/123.wav", "src_ref": "Each of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques de codes de droit belge.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 124, "src_audio": "/acl6060/audio/en/124.wav", "src_ref": "Let's now talk about how we collected this dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Parlons maintenant de la façon dont nous avons collecté ces données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 125, "src_audio": "/acl6060/audio/en/125.wav", "src_ref": "First, we started by compiling a large corpus of legal articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 126, "src_audio": "/acl6060/audio/en/126.wav", "src_ref": "We considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons examiné trente-deux codes belges accessibles au public et extrait tous les articles ainsi que les titres de section correspondants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 127, "src_audio": "/acl6060/audio/en/127.wav", "src_ref": "Then we gathered legal questions with references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous avons rassemblé les questions juridiques avec les références aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 128, "src_audio": "/acl6060/audio/en/128.wav", "src_ref": "To do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour ce faire, nous nous associons au cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur une question juridique personnelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 129, "src_audio": "/acl6060/audio/en/129.wav", "src_ref": "We were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons eu la chance d'avoir accès à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 130, "src_audio": "/acl6060/audio/en/130.wav", "src_ref": "We collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons recueilli des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux lois pertinentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 131, "src_audio": "/acl6060/audio/en/131.wav", "src_ref": "Lastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous avons fait passer les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avons considérés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 132, "src_audio": "/acl6060/audio/en/132.wav", "src_ref": "The remaining references were matched and converted to the corresponding article ids from our corpus.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les références restantes ont été appariées et converties aux identifiants d'article correspondants de notre corpus.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 133, "src_audio": "/acl6060/audio/en/133.wav", "src_ref": "We eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous nous sommes finalement retrouvés avec mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 134, "src_audio": "/acl6060/audio/en/134.wav", "src_ref": "In addition, each question comes with the main category and a concatenation of subcategories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, chaque question est accompagnée de la catégorie principale et d'un enchaînement de sous-catégories.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 135, "src_audio": "/acl6060/audio/en/135.wav", "src_ref": "And each articles comes with a concatenation of the subsequence heading in the structure of the law.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et chaque article comporte un enchaînement de la rubrique de sous-séquence dans la structure de la loi.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 136, "src_audio": "/acl6060/audio/en/136.wav", "src_ref": "This extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour des recherches futures sur l'extraction d'informations juridiques ou la classification de textes juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 137, "src_audio": "/acl6060/audio/en/137.wav", "src_ref": "Let's look at some characteristic of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Jetons un coup d'œil à certaines caractéristiques de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 138, "src_audio": "/acl6060/audio/en/138.wav", "src_ref": "The questions are between five and forty four words long with a median of fourteen words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les questions comportent entre cinq et quarante-quatre mots avec une moyenne de quatorze mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 139, "src_audio": "/acl6060/audio/en/139.wav", "src_ref": "The articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les articles sont beaucoup plus longs avec une longueur moyenne de soixante-dix-sept mots, avec cent quarante-deux d'entre eux dépassant les mille mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 140, "src_audio": "/acl6060/audio/en/140.wav", "src_ref": "The lengthiest one being up to five thousand seven hundred and ninety words.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 141, "src_audio": "/acl6060/audio/en/141.wav", "src_ref": "As previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme mentionné précédemment, les questions couvrent un wide range de sujets, dont environ quatre-vingt-cinq pour cent concernent la famille, le logement, l'argent ou la justice.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 142, "src_audio": "/acl6060/audio/en/142.wav", "src_ref": "While the remaining fifteen percent concern either social security, foreigners or work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 143, "src_audio": "/acl6060/audio/en/143.wav", "src_ref": "The article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'article est également très diversifié car il provient de trente-deux codes belges différents qui couvrent un grand nombre de sujets juridiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 144, "src_audio": "/acl6060/audio/en/144.wav", "src_ref": "Here's the total number of articles collected from each of these Belgian codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le nombre total d'articles collectés à partir de chacun de ces codes belges.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 145, "src_audio": "/acl6060/audio/en/145.wav", "src_ref": "Out of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Sur les vingt-deux mille six cent trente-trois articles, seuls mille six cent douze sont désignés comme pertinents à au moins une question dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 146, "src_audio": "/acl6060/audio/en/146.wav", "src_ref": "And around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et environ quatre-vingts pour cent de ces articles cités proviennent du code civil, des codes judiciaires, des codes d'enquête criminelle ou des codes pénaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 147, "src_audio": "/acl6060/audio/en/147.wav", "src_ref": "Meanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 148, "src_audio": "/acl6060/audio/en/148.wav", "src_ref": "Which can be explained by the fact that those codes focused less on individuals and their concerns.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce qui peut s'expliquer par le fait que ces codes se concentraient moins sur les individus et leurs préoccupations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 149, "src_audio": "/acl6060/audio/en/149.wav", "src_ref": "Overall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le nombre moyen de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 150, "src_audio": "/acl6060/audio/en/150.wav", "src_ref": "Using all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En utilisant toutes les données, nous avons comparé plusieurs approches d'extraction, y compris l'architecture lexicale et dense.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 151, "src_audio": "/acl6060/audio/en/151.wav", "src_ref": "Given a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné une requête et un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de requête des poids de chacun de ces termes dans cet article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 152, "src_audio": "/acl6060/audio/en/152.wav", "src_ref": "We experiment with the standard TF-IDF and BM25 ranking functions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec les fonctions de classement TF-IDF et BM25 standard.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 153, "src_audio": "/acl6060/audio/en/153.wav", "src_ref": "The main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le principal problème de ces approches est qu'elles ne peuvent extraire que les articles contenant des mots-clés présents dans la requête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 154, "src_audio": "/acl6060/audio/en/154.wav", "src_ref": "To overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour surmonter cette limitation, nous expérimentons une architecture neuronale qui peut capturer les relations sémantiques entre les requêtes et l'article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 155, "src_audio": "/acl6060/audio/en/155.wav", "src_ref": "We use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons un modèle bi-encodeur qui cartographie les requêtes et les articles en représentations vecteurs denses et calculons un score de pertinence entre une paire d'articles de requête par la similitude de leurs intégrations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 156, "src_audio": "/acl6060/audio/en/156.wav", "src_ref": "These embeddings typically result from a pooling operation on the output of a word embedding model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces intégrations résultent typiquement d'une opération de pooling sur la sortie d'un modèle d'intégration de mots.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 157, "src_audio": "/acl6060/audio/en/157.wav", "src_ref": "First, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous étudions l'efficacité des bi-encodeurs siamois dans une configuration d'évaluation « zero shot », ce qui signifie que des modèles d'intégration de mots préformés sont appliqués immédiatement sans aucun raffinement supplémentaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 158, "src_audio": "/acl6060/audio/en/158.wav", "src_ref": "We experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec l'encodeur de texte indépendant du contexte, à savoir word2vec et fastText, et des modèles d'intégration dépendants du contexte, à savoir Roberta et plus précisément CamemBERT qui est un modèle Roberta français.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 159, "src_audio": "/acl6060/audio/en/159.wav", "src_ref": "Additionally, we train our own CamemBERT based model ah bi-encoders on our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De plus, nous formons nos propres bi-encodeurs de modèles basés sur CamemBERT sur nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 160, "src_audio": "/acl6060/audio/en/160.wav", "src_ref": "Note that for training, we experiment with the two flavors of the bi-encoder architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que pour la formation, nous expérimentons avec les deux modèles de l'architecture bi-encodeur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 161, "src_audio": "/acl6060/audio/en/161.wav", "src_ref": "Siamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le siamois, qui utilise un modèle d'intégration de mots unique cartographiant la requête et l'article ensemble dans un vector space dense partagé, et le modèle à deux tours, qui utilise deux modèles d'intégration de mots indépendants encodant la requête et l'article séparément dans différents espaces d'intégration.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 162, "src_audio": "/acl6060/audio/en/162.wav", "src_ref": "We experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons la mise en commun moyenne, maximale et CLS, ainsi que le produit et cosinus pour calculer les similitudes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 163, "src_audio": "/acl6060/audio/en/163.wav", "src_ref": "Here are the result of our baseline on the test sets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici le résultat de notre base sur les ensembles de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 164, "src_audio": "/acl6060/audio/en/164.wav", "src_ref": "With the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec les méthodes lexicales ci-dessus, les bi-encodeurs siamois ont évalué dans une configuration zero shot au milieu, et les bi-encodeurs raffinés ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 165, "src_audio": "/acl6060/audio/en/165.wav", "src_ref": "Overall, the finetuned bi-encoder significantly outperforms all the other baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans l'ensemble, le bi-encodeur raffiné dépasse de manière significative toutes les autres bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 166, "src_audio": "/acl6060/audio/en/166.wav", "src_ref": "The two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle à deux tours s'améliore par rapport à ses variantes siamoises lors du rappel à cent, mais fonctionne de la même manière sur les autres indicateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 167, "src_audio": "/acl6060/audio/en/167.wav", "src_ref": "Although BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que BM25 ait sous-performé le bi-encodeur formé de manière significative, ses performances indiquent qu'il s'agit toujours d'une base solide pour l'extraction spécifique au domaine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 168, "src_audio": "/acl6060/audio/en/168.wav", "src_ref": "Regarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation zero shot du bi-encodeur siamois, nous constatons que l'utilisation directe des intégrations d'un modèle CamemBERT préformé sans optimiser pour la tâche d'extraction d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 169, "src_audio": "/acl6060/audio/en/169.wav", "src_ref": "Furthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous observons que le bi-encodeur basé sur word2vec a dépassé de manière significative les modèles basés sur fastText et les Représentations d'encodeurs bidirectionnels à partir de transformateurs, suggérant que peut-être les intégrations de niveau mot préformées sont plus appropriées pour la tâche que les intégrations de niveau caractère ou sous-mot lorsqu'elles sont utilisées immédiatement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 170, "src_audio": "/acl6060/audio/en/170.wav", "src_ref": "Although promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bien que prometteurs, ces résultats suggèrent de nombreuses possibilités d'amélioration comparé à un expert juridique qualifié qui peut éventuellement extraire tous les articles pertinents à n'importe quelle question, et ainsi obtenir des scores parfaits.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 171, "src_audio": "/acl6060/audio/en/171.wav", "src_ref": "Let's conclude by discussing two limitations of our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Concluons en discutant de deux limites de nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 172, "src_audio": "/acl6060/audio/en/172.wav", "src_ref": "First, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Premièrement, le corpus d'article est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 173, "src_audio": "/acl6060/audio/en/173.wav", "src_ref": "During the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au cours de la construction des données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions ne se retrouvent qu'avec une fraction du nombre initial d'articles pertinents.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 174, "src_audio": "/acl6060/audio/en/174.wav", "src_ref": "This information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette information implique donc que la réponse contenue dans les autres articles pertinents pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 175, "src_audio": "/acl6060/audio/en/175.wav", "src_ref": "Second, we should note that not all legal questions can be answered with statutes alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, il convient de noter qu'on ne peut pas répondre à toutes les questions juridiques uniquement par des lois.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 176, "src_audio": "/acl6060/audio/en/176.wav", "src_ref": "For instance, the question, can I evict my tenants if they make too much noise?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, la question « puis-je expulser mes locataires s'ils font trop de bruit ? »", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 177, "src_audio": "/acl6060/audio/en/177.wav", "src_ref": "Might not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle peut ne pas avoir de réponse détaillée dans le droit écrit qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 178, "src_audio": "/acl6060/audio/en/178.wav", "src_ref": "Instead, the landlord should probably rely more on case law and find precedents similar to their current situation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des antécédents similaires à sa situation actuelle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 179, "src_audio": "/acl6060/audio/en/179.wav", "src_ref": "For example, the tenants makes two parties a week until two AM.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, les locataires font la fête deux fois par semaine jusqu'à deux heures du matin.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 180, "src_audio": "/acl6060/audio/en/180.wav", "src_ref": "Hence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche d'extraction de l'article de droit, et le domaine des moins adaptées reste à déterminer.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 181, "src_audio": "/acl6060/audio/en/181.wav", "src_ref": "We hope that our work sparks interest in developing practical and reliable statutory article retrieval models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous espérons que nos travaux susciteront l'intérêt pour l'élaboration de modèles d'extraction d'article de droit pratiques et fiables.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 182, "src_audio": "/acl6060/audio/en/182.wav", "src_ref": "That can help improve access to justice for all.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut aider à améliorer l'accès à la justice pour tous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 183, "src_audio": "/acl6060/audio/en/183.wav", "src_ref": "You can check out our paper, dataset and code at the following links. Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez consulter notre article, nos données et notre code aux liens suivants. Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 184, "src_audio": "/acl6060/audio/en/184.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE ; un indice de référence indépendant de la tâche destiné à tester les modèles de langue et de vision avec des phénomènes linguistiques spécifiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 185, "src_audio": "/acl6060/audio/en/185.wav", "src_ref": "Why did we do the trouble in setting up this benchmark?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pourquoi avons-nous pris la peine de mettre en place cet indice de référence ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 186, "src_audio": "/acl6060/audio/en/186.wav", "src_ref": "Well, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Eh bien, au cours des dernières années, nous avons vu une explosion de la vision basée sur le transformateur et des modèles de langue préformés sur de grandes quantités de paires de textes d'images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 187, "src_audio": "/acl6060/audio/en/187.wav", "src_ref": "Each one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chacun de ces modèles pousse les tâches de pointe en matière de vision et de langue telles que la réponse aux questions visuelles, le raisonnement de bon sens visuel, l'extraction d'images et les bases de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 188, "src_audio": "/acl6060/audio/en/188.wav", "src_ref": "So we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc reçu un message : les précisions sur ces tâches et les indices de référence spécifiques augmentent régulièrement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 189, "src_audio": "/acl6060/audio/en/189.wav", "src_ref": "But do we know what the models have actually learned?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais savons-nous ce que les modèles ont réellement appris ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 190, "src_audio": "/acl6060/audio/en/190.wav", "src_ref": "What is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Qu'est-ce qu'une vision et un transformateur de langue ont compris lors de l'attribution d'un score élevé pour que cette image et cette phrase correspondent ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 191, "src_audio": "/acl6060/audio/en/191.wav", "src_ref": "And the low score for this one?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le score bas pour celle-ci ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 192, "src_audio": "/acl6060/audio/en/192.wav", "src_ref": "Do vision and language models focus on the right thing?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Est-ce que les modèles de langue et de vision se concentrent sur la bonne chose ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 193, "src_audio": "/acl6060/audio/en/193.wav", "src_ref": "Or do they focus on biases as shown by previous work?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou se concentrent-ils sur les biais comme le montre le travail antérieur ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 194, "src_audio": "/acl6060/audio/en/194.wav", "src_ref": "To shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour éclairer davantage cet aspect, nous proposons une direction plus indépendante des tâches et introduisons VALSE qui teste la sensibilité des modèles de langue et de vision à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistiques et visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 195, "src_audio": "/acl6060/audio/en/195.wav", "src_ref": "We target existence, plurality, counting, spatial relations, actions and entity coreference.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 196, "src_audio": "/acl6060/audio/en/196.wav", "src_ref": "But how do we test whether the vision and language models have captured this phenomena?", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais comment tester si les modèles de langue et de vision ont capturé ce phénomène ?", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 197, "src_audio": "/acl6060/audio/en/197.wav", "src_ref": "By foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En pratiquant le foiling sur une méthode précédemment appliquée pour les modèles de langue et de vision seulement pour des phrases nominales de Ravi Shekhar et de ses collaborateurs, et en comptant par nous dans les travaux antérieurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 198, "src_audio": "/acl6060/audio/en/198.wav", "src_ref": "Foiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Effectuer un foiling signifie essentiellement que nous prenons la légende d'une image et produisons un foil en modifiant la légende de sorte qu'elle ne décrive plus l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 199, "src_audio": "/acl6060/audio/en/199.wav", "src_ref": "And we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous apportons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous aurions trouvé plus d'une façon intéressante de créer des instances de foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 200, "src_audio": "/acl6060/audio/en/200.wav", "src_ref": "For example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, dans le cas de l'élément d'actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié avec une action différente, et un dans lequel les acteurs sont échangés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 201, "src_audio": "/acl6060/audio/en/201.wav", "src_ref": "Counting and coreference also are pieces that have more than one instrument.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le comptage et la coréférence sont également des éléments qui ont plusieurs instruments.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 202, "src_audio": "/acl6060/audio/en/202.wav", "src_ref": "And we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous créons ces foils en nous assurant qu'ils ne décrivent pas l'image et que ce sont des phrases grammaticales et autrement valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 203, "src_audio": "/acl6060/audio/en/203.wav", "src_ref": "This is not easy to do because a foiled caption may be less likely than the original caption.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce n'est pas facile à faire car une légende ayant subi un foil peut être moins probable que la légende originale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 204, "src_audio": "/acl6060/audio/en/204.wav", "src_ref": "For example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que les plantes coupent un homme qu'un homme coupe les plantes, et la grande vision et les modèles de langue pourraient capter cela.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 205, "src_audio": "/acl6060/audio/en/205.wav", "src_ref": "Therefore, to obtain valid foils, we must take action.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour obtenir des foils valides, nous devons agir.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 206, "src_audio": "/acl6060/audio/en/206.wav", "src_ref": "First, we make use of strong language models to propose foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, nous utilisons des modèles de langue forts pour proposer des foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 207, "src_audio": "/acl6060/audio/en/207.wav", "src_ref": "Second, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deuxièmement, nous utilisons l'inférence de la langue naturelle ou la NLI courte pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 208, "src_audio": "/acl6060/audio/en/208.wav", "src_ref": "To test this automatically, we apply natural language inference with the following rationale.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour tester cela automatiquement, nous appliquons l'inférence de la langue naturelle avec la justification suivante.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 209, "src_audio": "/acl6060/audio/en/209.wav", "src_ref": "We consider an image to be the premise and its caption its entailed hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous considérons qu'une image est la prémisse et sa légende son hypothèse implicite.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 210, "src_audio": "/acl6060/audio/en/210.wav", "src_ref": "In addition, we consider the caption to be the premise, and the foil is its hypothesis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, nous considérons la légende comme la prémisse, et le foil est son hypothèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 211, "src_audio": "/acl6060/audio/en/211.wav", "src_ref": "If an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si un modèle NLI prévient que le foil est en contradiction ou neutre par rapport à la légende, nous considérons cela comme un indicateur d'un foil valide.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 212, "src_audio": "/acl6060/audio/en/212.wav", "src_ref": "If an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si une NLI prévient le foil à engendrer par la légende, cela ne peut pas être un bon foil, car par transitivité, cela donnera une description véridique de l'image et nous filtrons ces foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 213, "src_audio": "/acl6060/audio/en/213.wav", "src_ref": "But this procedure is not perfect, it is just an indicator for valid foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cette procédure n'est pas parfaite, il s'agit simplement d'un indicateur pour les foils valides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 214, "src_audio": "/acl6060/audio/en/214.wav", "src_ref": "Therefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, comme troisième mesure pour produire des foils valides, nous utilisons des annotateurs humains pour valider les données utilisées dans VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 215, "src_audio": "/acl6060/audio/en/215.wav", "src_ref": "So, after filtering and human evaluation, we have as many test instances as described in this table.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrit dans ce tableau.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 216, "src_audio": "/acl6060/audio/en/216.wav", "src_ref": "Note that VALSE does not deliver any training data but only test data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notez que VALSE ne fournit pas de données de formation mais seulement des données de test.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 217, "src_audio": "/acl6060/audio/en/217.wav", "src_ref": "Since it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme il s'agit uniquement d'un indicateur de référence de test zero shot, il est conçu pour tirer parti des capacités existantes des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 218, "src_audio": "/acl6060/audio/en/218.wav", "src_ref": "Finetuning would only enable models to exploit artifacts or statistical biases in the data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement permettrait seulement aux modèles d'exploiter des artefacts ou des biais statistiques dans les données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 219, "src_audio": "/acl6060/audio/en/219.wav", "src_ref": "And we all know that these models like to cheat and take shortcuts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 220, "src_audio": "/acl6060/audio/en/220.wav", "src_ref": "And as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et comme nous l'avons dit, cela nous intéresse d'évaluer les capacités des modèles de langue et de vision après la préformation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 221, "src_audio": "/acl6060/audio/en/221.wav", "src_ref": "We experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous expérimentons avec cinq modèles de langue et de vision sur VALSE, à savoir avec CLIP, LXMert, ViLBERT, ViLBERT douze en un, et VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 222, "src_audio": "/acl6060/audio/en/222.wav", "src_ref": "Two of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Deux de nos indicateurs d'évaluation les plus importants sont la précision des modèles dans la classification de paires de phrases images en légendes et foils.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 223, "src_audio": "/acl6060/audio/en/223.wav", "src_ref": "Perhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Peut-être plus pertinent pour cette vidéo, nous présenterons notre indicateur plus permissif, la précision par paire, qui mesure si le score de sentence alignment d'image est plus élevé pour la bonne paire de texte image que pour sa paire ayant subi un foil.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 224, "src_audio": "/acl6060/audio/en/224.wav", "src_ref": "For more metrics and results on them, do check out our paper.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour plus d'indicateurs et de résultats, consultez notre article.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 225, "src_audio": "/acl6060/audio/en/225.wav", "src_ref": "The results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats avec une précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres indicateurs. La meilleure performance zero shot est obtenue par ViLBERT douze en un, suivi de ViLBERT, LXMert, CLIP, et enfin VisualBERT.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 226, "src_audio": "/acl6060/audio/en/226.wav", "src_ref": "It's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il est remarquable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par ViLBERT douze en un, en soulignant que les modèles sont capables d'identifier des objets nommés et leur présence dans les images.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 227, "src_audio": "/acl6060/audio/en/227.wav", "src_ref": "However, none of the remaining pieces can be reliably solved in our adversarial foiling settings.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de foiling antagonistes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 228, "src_audio": "/acl6060/audio/en/228.wav", "src_ref": "We see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous voyons à partir des instruments de pluralité et de comptage que les modèles de langue et de vision ont du mal à distinguer les références à des objets uniques par rapport à plusieurs, ou à les compter dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 229, "src_audio": "/acl6060/audio/en/229.wav", "src_ref": "The relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 230, "src_audio": "/acl6060/audio/en/230.wav", "src_ref": "They also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ils ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité comme nous le voyons dans l'élément d'actions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 231, "src_audio": "/acl6060/audio/en/231.wav", "src_ref": "From the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À partir de l'élément de coréférence, nous découvrons que relever plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 232, "src_audio": "/acl6060/audio/en/232.wav", "src_ref": "As a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour vérifier la santé mentale, et parce que c'est une expérience intéressante, nous comparons également deux modèles à texte seulement, GPT un et GPT deux, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et ayant subi un foil, sans image ici, et en prévenant l'entrée avec la plus faible perplexité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 233, "src_audio": "/acl6060/audio/en/233.wav", "src_ref": "If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si la perplexité est plus élevée pour le foil, nous considérons cela comme une indication que la légende ayant subi un foil peut souffrir de biais de plausibilité ou d'autres biais linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 234, "src_audio": "/acl6060/audio/en/234.wav", "src_ref": "And it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il est intéressant de voir que dans certains cas, les modèles GPT à texte seulement ont capturé la plausibilité du monde mieux que les modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 235, "src_audio": "/acl6060/audio/en/235.wav", "src_ref": "So to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour résumer, VALSE est un indicateur de référence qui utilise l'objectif des constructions linguistiques pour aider la communauté à améliorer les modèles de langue et de vision en testant durement leurs capacités de bases visuelles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 236, "src_audio": "/acl6060/audio/en/236.wav", "src_ref": "Our experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que les modèles de langue et de vision identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais luttent pour ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 237, "src_audio": "/acl6060/audio/en/237.wav", "src_ref": "We would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers les bases de la langue avec des modèles de langue et de vision.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 238, "src_audio": "/acl6060/audio/en/238.wav", "src_ref": "And even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et plus encore, VALSE pourrait être utilisé comme une évaluation indirecte des données, car les modèles pourraient être évalués avant et après la formation ou le raffinement pour voir si des données aident les modèles à améliorer l'un des aspects testés par VALSE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 239, "src_audio": "/acl6060/audio/en/239.wav", "src_ref": "If you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Si vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 240, "src_audio": "/acl6060/audio/en/240.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour, je m'appelle Kamezawa de l'Université de Tokyo.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 241, "src_audio": "/acl6060/audio/en/241.wav", "src_ref": "I'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous présenter un article intitulé RNSum : des données à grande échelle pour la génération automatique de note de version via la synthèse des journaux de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 242, "src_audio": "/acl6060/audio/en/242.wav", "src_ref": "I'll be explaining in this order.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je vais vous expliquer dans cet ordre.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 243, "src_audio": "/acl6060/audio/en/243.wav", "src_ref": "First, I will introduce automatic release note generation that we are working on in this research.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, je vais présenter la génération de note de version automatique sur laquelle nous travaillons dans cette recherche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 244, "src_audio": "/acl6060/audio/en/244.wav", "src_ref": "A release note is a technical document that summarizes the changes distributed with each release of a software product.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une note de version est un document technique qui résume les modifications distribuées à chaque version d'un produit logiciel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 245, "src_audio": "/acl6060/audio/en/245.wav", "src_ref": "The image shows a release note for version two point six point four of the vuejs library.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'image montre une note de version pour la version deux point six point quatre de la bibliothèque Vuejs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 246, "src_audio": "/acl6060/audio/en/246.wav", "src_ref": "Release notes play an important role in open source development but they're time consuming to prepare manually.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version jouent un rôle important dans le développement open source, mais elles mettent du temps à être préparées manuellement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 247, "src_audio": "/acl6060/audio/en/247.wav", "src_ref": "Therefore, it would be very useful to be able to automatically generate high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 248, "src_audio": "/acl6060/audio/en/248.wav", "src_ref": "I will defer to two previous researches on automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je m'en remettrai à deux recherches antérieures sur la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 249, "src_audio": "/acl6060/audio/en/249.wav", "src_ref": "The first is a system called ARENA released in twenty fourteen.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première est un système appelé ARENA sorti en deux-mille-quatorze.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 250, "src_audio": "/acl6060/audio/en/250.wav", "src_ref": "It takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il faut une approche basée sur des règles, par exemple, en utilisant l'extracteur de changement pour extraire toutes les différences, les changements de bibliothèque et les changements de document à partir des différences entre les versions, et enfin, en les combinant.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 251, "src_audio": "/acl6060/audio/en/251.wav", "src_ref": "The most notable feature of this system is the issue extractor in the upper right corner.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 252, "src_audio": "/acl6060/audio/en/252.wav", "src_ref": "Which must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le système de suivi des problèmes, qui doit être laissé à Jira, ne peut être appliqué qu'aux projets qui utilisent Jira.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 253, "src_audio": "/acl6060/audio/en/253.wav", "src_ref": "In other words, it cannot be used for many projects on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En d'autres mots, il ne peut pas être utilisé pour de nombreux projets sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 254, "src_audio": "/acl6060/audio/en/254.wav", "src_ref": "The second is Glyph, recently announced in twenty twenty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde est Glyphe, récemment annoncée en deux-mille-vingt.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 255, "src_audio": "/acl6060/audio/en/255.wav", "src_ref": "It is available on the internet and can be installed via pip.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Elle est disponible sur internet et peut être installée via pip.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 256, "src_audio": "/acl6060/audio/en/256.wav", "src_ref": "This system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'une des cinq étiquettes telles que les fonctions ou corrections de bugs pour chaque message de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 257, "src_audio": "/acl6060/audio/en/257.wav", "src_ref": "This image is a sample usage that returns a corrective or bug fixes label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette image est un exemple d'utilisation qui renvoie une étiquette de correction ou de débogage.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 258, "src_audio": "/acl6060/audio/en/258.wav", "src_ref": "Glyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données de formation de Glyphe sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 259, "src_audio": "/acl6060/audio/en/259.wav", "src_ref": "The performance of the text classification model is not high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les performances du modèle de classification de texte ne sont pas élevées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 260, "src_audio": "/acl6060/audio/en/260.wav", "src_ref": "I present two related researches, but their problems are limited applicability and scarce data resources.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Je présente deux recherches connexes, mais leurs problèmes sont l'applicabilité limitée et les ressources de données rares.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 261, "src_audio": "/acl6060/audio/en/261.wav", "src_ref": "Our paper solves these two problems and automatically generates high quality release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 262, "src_audio": "/acl6060/audio/en/262.wav", "src_ref": "With a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Avec un problème d'applicabilité limitée, nous proposons une méthode de synthèse par classe de haute qualité en utilisant uniquement des messages de validation comme saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 263, "src_audio": "/acl6060/audio/en/263.wav", "src_ref": "This proposed method can be used for all English repositories.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette méthode proposée peut être utilisée pour tous les référentiels anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 264, "src_audio": "/acl6060/audio/en/264.wav", "src_ref": "For the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour le deuxième problème de ressources de données rares, nous avons construit nos données RNSum composées d'environ quatre-vingt-deux mille éléments de données en collectant des données à partir de référentiels GitHub publics en utilisant l'API GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 265, "src_audio": "/acl6060/audio/en/265.wav", "src_ref": "Next, I'll describe our dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais décrire nos données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 266, "src_audio": "/acl6060/audio/en/266.wav", "src_ref": "Here is an example of data.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici un exemple de données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 267, "src_audio": "/acl6060/audio/en/267.wav", "src_ref": "The left side is a commit message and the right side is the release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le côté gauche est un message de validation et le côté droit représente les notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 268, "src_audio": "/acl6060/audio/en/268.wav", "src_ref": "Release notes are labeled as improvements or fixes, etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les notes de version sont étiquetées comme améliorations ou correctifs, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 269, "src_audio": "/acl6060/audio/en/269.wav", "src_ref": "We have set up a task that takes the commit messages as input and outputs a labeled release notes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons mis en place une tâche qui prend les messages de validation comme saisie et sorties des notes de version étiquetées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 270, "src_audio": "/acl6060/audio/en/270.wav", "src_ref": "This can be regarded as a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela peut être considéré comme une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 271, "src_audio": "/acl6060/audio/en/271.wav", "src_ref": "We have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons prédéfini quatre étiquettes : fonctions, améliorations, corrections de bugs, suppressions de dépréciations et modifications de rupture.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 272, "src_audio": "/acl6060/audio/en/272.wav", "src_ref": "These were set based on previous research and other factors.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Celles-ci ont été établies sur la base de la recherche antérieure et d'autres facteurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 273, "src_audio": "/acl6060/audio/en/273.wav", "src_ref": "The release note on the bottom right is extracted from the release note on the bottom left.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La note de version en bas à droite est extraite de la note de version en bas à gauche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 274, "src_audio": "/acl6060/audio/en/274.wav", "src_ref": "At this time, it is necessary to detect the four labels that have been set up in advance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "À ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été mises en place à l'avance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 275, "src_audio": "/acl6060/audio/en/275.wav", "src_ref": "But the labels are not always consistent with each repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 276, "src_audio": "/acl6060/audio/en/276.wav", "src_ref": "For example, the improvements label includes improvements, enhancements, optimizations, and so on.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par exemple, l'étiquette d'améliorations inclut des améliorations, des perfectionnements, des optimisations, etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 277, "src_audio": "/acl6060/audio/en/277.wav", "src_ref": "We prepared a vocabulary list of about thirty labels for each of these notational variations.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons préparé une liste de vocabulaire d'une trentaine d'étiquettes pour chacune de ces variations de notation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 278, "src_audio": "/acl6060/audio/en/278.wav", "src_ref": "This is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de détecter la classe de note de version et de collecter le texte de la version qui suit en tant que phrase de note de version pour la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 279, "src_audio": "/acl6060/audio/en/279.wav", "src_ref": "Next is a commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il y a un message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 280, "src_audio": "/acl6060/audio/en/280.wav", "src_ref": "Commit messages are not tied to each release.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation ne sont pas liés à chaque version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 281, "src_audio": "/acl6060/audio/en/281.wav", "src_ref": "As shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Comme le montre l'image ci-dessous, si la version actuelle est la version deux point cinq à dix-neuf, nous devons identifier la version deux point cinq à dix-huit antérieure et obtenir un diff.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 282, "src_audio": "/acl6060/audio/en/282.wav", "src_ref": "This is a bit tedious and it is not enough to just get a list of releases and look at the before and after.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "C'est un peu fastidieux et il ne suffit pas d'obtenir une liste de versions et de regarder l'avant et l'après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 283, "src_audio": "/acl6060/audio/en/283.wav", "src_ref": "We created a heuristic matching rule to get the previous and next versions.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons créé une règle de correspondance heuristique pour obtenir les versions antérieures et suivantes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 284, "src_audio": "/acl6060/audio/en/284.wav", "src_ref": "Dataset analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Analyse des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 285, "src_audio": "/acl6060/audio/en/285.wav", "src_ref": "In the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En fin de compte, sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données ont été recueillis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 286, "src_audio": "/acl6060/audio/en/286.wav", "src_ref": "Also, the average number of release notes tokens is sixty three, which is quite high for a summarization task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, le nombre moyen de gages de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 287, "src_audio": "/acl6060/audio/en/287.wav", "src_ref": "Also, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "De même, le nombre de gages uniques est assez grand, s'élevant à huit mille huit cent trente mille.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 288, "src_audio": "/acl6060/audio/en/288.wav", "src_ref": "This is due to the large number of unique class or method names found in the repository.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cela est dû au grand nombre de noms de méthode ou de classe unique trouvés dans le référentiel.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 289, "src_audio": "/acl6060/audio/en/289.wav", "src_ref": "Next, I will explain the proposed method.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, je vais expliquer la méthode proposée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 290, "src_audio": "/acl6060/audio/en/290.wav", "src_ref": "The classwise extractive then abstractive summarization model consists of two neural modules.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le modèle de synthèse extractive puis abstractive par classe se compose de deux modules neuronaux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 291, "src_audio": "/acl6060/audio/en/291.wav", "src_ref": "A classifier using BERT or CodeBERT and a generator using BART.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Un classificateur utilisant BERT ou CodeBERT et un générateur utilisant BART.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 292, "src_audio": "/acl6060/audio/en/292.wav", "src_ref": "First, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Tout d'abord, CEAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, qui utilisent des améliorations, des corrections de bogues, des dépréciations et d'autres.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 293, "src_audio": "/acl6060/audio/en/293.wav", "src_ref": "The commit messages classified as other are discarded.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les messages de validation classés comme autres sont supprimés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 294, "src_audio": "/acl6060/audio/en/294.wav", "src_ref": "Then CEAS applies the generator to the four labeled documents independently and generates release notes for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 295, "src_audio": "/acl6060/audio/en/295.wav", "src_ref": "In this task, the direct correspondences between commit messages and release notes are not known.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 296, "src_audio": "/acl6060/audio/en/296.wav", "src_ref": "Therefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Par conséquent, pour former le classificateur, nous avons réaffecté les enquêtes à chaque message de validation de saisie en utilisant les dix premiers caractères de chaque message de validation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 297, "src_audio": "/acl6060/audio/en/297.wav", "src_ref": "We modeled the classwise abstractive summarization approach by two different methods.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons modélisé l'approche de synthèse abstractive par classe à travers deux méthodes différentes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 298, "src_audio": "/acl6060/audio/en/298.wav", "src_ref": "The first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le premier modèle, que nous appelons CAS unique, se compose d'un réseau de six à six unique et génère un seul texte de note de version donnant un enchaînement de messages de validation de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 299, "src_audio": "/acl6060/audio/en/299.wav", "src_ref": "The output texts can be divided into classwise segments based on special class-specific endpoint symbols.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les textes de sortie peuvent être divisés en segments par classe sur la base de symboles de point d'extrémité particuliers, spécifiques à la classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 300, "src_audio": "/acl6060/audio/en/300.wav", "src_ref": "The second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La seconde méthode, que nous appelons CAS multiple, se compose de quatre réseaux seq2seq différents, dont chacun correspond à l'une des classes de notes de version fixes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 301, "src_audio": "/acl6060/audio/en/301.wav", "src_ref": "Okay, let me explain the experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ok, je vais vous expliquer les expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 302, "src_audio": "/acl6060/audio/en/302.wav", "src_ref": "Five methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cinq méthodes ont été comparées : CEAS, CAS unique, CAS multiple, regroupement et étude antérieure, Glyph.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 303, "src_audio": "/acl6060/audio/en/303.wav", "src_ref": "Regarding evaluation, in some cases, release notes are output in multiple sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 304, "src_audio": "/acl6060/audio/en/304.wav", "src_ref": "Since it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisqu'il est difficile de calculer le nombre de phrases telles qu'elles sont, elles sont combinées avec des espaces et traitées comme une seule phrase longue.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 305, "src_audio": "/acl6060/audio/en/305.wav", "src_ref": "The BLEU is penalized when the system outputs a short sentence.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le BLEU est pénalisé lorsque le système produit une phrase courte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 306, "src_audio": "/acl6060/audio/en/306.wav", "src_ref": "This penalty results in a lower BLEU value in the experiment results described next.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Cette pénalité se traduit par une valeur BLEU inférieure dans les résultats de l'expérience décrits ci-dessous.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 307, "src_audio": "/acl6060/audio/en/307.wav", "src_ref": "Finally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, nous calculons également la spécificité car ROUGE et BLEU ne peuvent pas être calculés si les notes de version sont vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 308, "src_audio": "/acl6060/audio/en/308.wav", "src_ref": "A higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une spécificité plus élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées être vides.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 309, "src_audio": "/acl6060/audio/en/309.wav", "src_ref": "Here are the results.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les résultats sont indiqués ci-après.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 310, "src_audio": "/acl6060/audio/en/310.wav", "src_ref": "Since the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Étant donné que les données contiennent des adresses électroniques, des valeurs hachées, etc., nous avons également évalué les données nettoyées, ce qui les exclut.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 311, "src_audio": "/acl6060/audio/en/311.wav", "src_ref": "CEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS et le CAS ont obtenu des scores de ROUGE-L supérieurs de plus de dix points par rapport aux bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 312, "src_audio": "/acl6060/audio/en/312.wav", "src_ref": "In particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les bases a atteint plus de vingt points.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 313, "src_audio": "/acl6060/audio/en/313.wav", "src_ref": "These results indicate that CEAS and CAS are significantly affected.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ces résultats indiquent que le CEAS et le CAS sont considérablement touchés.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 314, "src_audio": "/acl6060/audio/en/314.wav", "src_ref": "CEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CEAS a obtenu un meilleur score ROUGE-L que le CAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour former le classificateur à l'aide de pseudo-étiquettes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 315, "src_audio": "/acl6060/audio/en/315.wav", "src_ref": "High coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une couverture élevée du CEAS peut être obtenue probablement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 316, "src_audio": "/acl6060/audio/en/316.wav", "src_ref": "CAS-Multi tended to yield higher ROUGE-L than CAS-Single.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le CAS multiple tendait à produire plus de ROUGE-L que le CAS unique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 317, "src_audio": "/acl6060/audio/en/317.wav", "src_ref": "Suggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En suggérant qu'il est également efficace de développer indépendamment et différemment des modèles de abstractive summarization pour chaque classe de note de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 318, "src_audio": "/acl6060/audio/en/318.wav", "src_ref": "Here are an error analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici une analyse d'erreur.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 319, "src_audio": "/acl6060/audio/en/319.wav", "src_ref": "CAS methods tend to output shorter sentences than human reference sentences.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 320, "src_audio": "/acl6060/audio/en/320.wav", "src_ref": "In the figure on the right, the reference sentence has three or four sentences, while CAS has only one.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que le CAS n'en a qu'une.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 321, "src_audio": "/acl6060/audio/en/321.wav", "src_ref": "The reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La raison de la réticence de ce modèle est que dans les données de formation, seulement trente-trois pour cent des phrases sont présentes dans l'étiquette des fonctions et quarante pour cent dans l'étiquette des améliorations.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 322, "src_audio": "/acl6060/audio/en/322.wav", "src_ref": "Furthermore, CAS methods cannot generate accurate release notes without additional information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En outre, les méthodes CAS ne peuvent pas générer des notes de version précises sans informations supplémentaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 323, "src_audio": "/acl6060/audio/en/323.wav", "src_ref": "The top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple en haut à droite est un exemple de message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la progression ou au problème correspondant(e).", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 324, "src_audio": "/acl6060/audio/en/324.wav", "src_ref": "The example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "L'exemple ci-dessous montre que les deux messages de validation dans la saisie sont liés et doivent être combinés en une phrase, mais cela n'est pas fait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 325, "src_audio": "/acl6060/audio/en/325.wav", "src_ref": "Finally, a conclusion.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Enfin, passons à la conclusion.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 326, "src_audio": "/acl6060/audio/en/326.wav", "src_ref": "We have built a new dataset for automatic release note generation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons construit de nouvelles données pour la génération automatique de notes de version.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 327, "src_audio": "/acl6060/audio/en/327.wav", "src_ref": "We have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons également formulé une tâche consistant à saisir des messages de validation et à les résumer afin qu'ils soient applicables à tous les projets écrits en anglais.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 328, "src_audio": "/acl6060/audio/en/328.wav", "src_ref": "Our experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nos expériences montrent que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les bases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 329, "src_audio": "/acl6060/audio/en/329.wav", "src_ref": "Please check out our dataset on GitHub.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Veuillez consulter nos données sur GitHub.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 330, "src_audio": "/acl6060/audio/en/330.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 331, "src_audio": "/acl6060/audio/en/331.wav", "src_ref": "Hello. My name is Asaf Harari.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Bonjour. Je m'appelle Asaf Harari.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 332, "src_audio": "/acl6060/audio/en/332.wav", "src_ref": "And I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et je vais vous présenter notre article : enrichissement de données tabulaires en quelques coups à l'aide d'architectures de transformateurs raffinées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 333, "src_audio": "/acl6060/audio/en/333.wav", "src_ref": "Data scientists analyze data and mainly focus on the manipulating the data's existing features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les scientifiques analysent les données et se concentrent principalement sur la manipulation des fonctions existantes des données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 334, "src_audio": "/acl6060/audio/en/334.wav", "src_ref": "But sometimes, these features are limited.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais parfois, ces fonctions sont limitées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 335, "src_audio": "/acl6060/audio/en/335.wav", "src_ref": "Feature generation using another data source may add substantial information.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La génération de fonctions en utilisant une autre source de données peut ajouter des informations substantielles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 336, "src_audio": "/acl6060/audio/en/336.wav", "src_ref": "Our research goal is automatic tabular data enrichment using external sources' free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des textes libres de sources externes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 337, "src_audio": "/acl6060/audio/en/337.wav", "src_ref": "Assume we have a tabular dataset and a knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Supposons que nous ayons des données tabulaires et une base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 338, "src_audio": "/acl6060/audio/en/338.wav", "src_ref": "We need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles fonctions du texte libre de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 339, "src_audio": "/acl6060/audio/en/339.wav", "src_ref": "Our framework FeSTE is exactly this automatic process.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre cadre FeSTE est exactement ce processus automatique.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 340, "src_audio": "/acl6060/audio/en/340.wav", "src_ref": "So let's see an example in a dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons donc un exemple dans des données introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 341, "src_audio": "/acl6060/audio/en/341.wav", "src_ref": "In this example, the dataset is university dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données sont des données universitaires.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 342, "src_audio": "/acl6060/audio/en/342.wav", "src_ref": "When its goal is to classify universities into low ranking universities and high-ranking universities.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Quand l'objectif est de classer les universités en universités de bas rang et en universités de haut rang.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 343, "src_audio": "/acl6060/audio/en/343.wav", "src_ref": "As knowledge base, we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "En tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 344, "src_audio": "/acl6060/audio/en/344.wav", "src_ref": "The first phase of FeSTE is entity linking.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "La première phase de FeSTE est la liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 345, "src_audio": "/acl6060/audio/en/345.wav", "src_ref": "When each entity, in this example the university name, is linked to an entity within the knowledge base.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 346, "src_audio": "/acl6060/audio/en/346.wav", "src_ref": "And and the text of the entities of the knowledge base is extracted and added to the dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et le texte des entités de la base de connaissances est extrait et ajouté aux données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 347, "src_audio": "/acl6060/audio/en/347.wav", "src_ref": "In this example, the text is the Wikipedia page's abstract.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, le texte est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 348, "src_audio": "/acl6060/audio/en/348.wav", "src_ref": "Now, we need to generate or extract features from the retrieved text.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Maintenant, nous devons générer ou extraire des fonctions à partir du texte extrait.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 349, "src_audio": "/acl6060/audio/en/349.wav", "src_ref": "So, we need to ah feature extraction phase ah which includes text analysis.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous avons donc besoin de la phase d'extraction des fonctions qui comprend l'analyse de texte.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 350, "src_audio": "/acl6060/audio/en/350.wav", "src_ref": "And this is the main novelty of this paper and I will deep dive into it in the next slides.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il s'agit de la principale nouveauté de cet article et je vais mieux l'expliquer dans les prochaines diapositives.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 351, "src_audio": "/acl6060/audio/en/351.wav", "src_ref": "After the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Après la phase d'extraction des fonctions, il y a une phase de génération de fonctions lorsque nous utilisons les fonctions extraites pour générer un petit nombre de nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 352, "src_audio": "/acl6060/audio/en/352.wav", "src_ref": "First generate ah features in the number of classes of the original dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Générez d'abord les fonctions dans le nombre de classes des données d'origine.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 353, "src_audio": "/acl6060/audio/en/353.wav", "src_ref": "In this example, the original dataset has two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, les données d'origine ont deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 354, "src_audio": "/acl6060/audio/en/354.wav", "src_ref": "So, FeSTE generates two new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, FeSTE génère deux nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 355, "src_audio": "/acl6060/audio/en/355.wav", "src_ref": "But if the dataset has five classes, FeSTE generates five new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais si les données ont cinq classes, FeSTE génère cinq nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 356, "src_audio": "/acl6060/audio/en/356.wav", "src_ref": "Each feature represents the likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Chaque fonction représente la probabilité pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 357, "src_audio": "/acl6060/audio/en/357.wav", "src_ref": "To analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour analyser le texte, nous utilisons les analyses de texte de pointe, qui sont des modèles de langue basés sur la conversion comme BERT, GPT, XLNet etc.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 358, "src_audio": "/acl6060/audio/en/358.wav", "src_ref": "It is but it is not likely that we can train language models using the input datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais il est peu probable que nous puissions former des modèles de langue en utilisant les données de saisie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 359, "src_audio": "/acl6060/audio/en/359.wav", "src_ref": "So a naive approach will be ah target task finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ainsi, une approche naïve sera le raffinement de la tâche cible.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 360, "src_audio": "/acl6060/audio/en/360.wav", "src_ref": "So, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans la phase d'extraction des fonctions, nous pouvons télécharger des modèles de langue préformée et raffiner le modèle de langue sur les données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 361, "src_audio": "/acl6060/audio/en/361.wav", "src_ref": "In this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans cet exemple, raffiner le modèle de langue, classer le texte en classes, résumer en classes, basses ou hautes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 362, "src_audio": "/acl6060/audio/en/362.wav", "src_ref": "Receive the language model output, which is the likelihood for each class and use as new features.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Recevoir les résultats du modèle de langue, qui sont la probabilité pour chaque classe et les utiliser comme nouvelles fonctions.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 363, "src_audio": "/acl6060/audio/en/363.wav", "src_ref": "The problem with this approach is datasets may have few distinct entities / texts.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le problème avec cette approche est que les données peuvent avoir peu d'entités / de textes distinct(e)s.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 364, "src_audio": "/acl6060/audio/en/364.wav", "src_ref": "In our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre expérience, près de la moitié des données contiennent moins de quatre cents échantillons et les plus petites données contiennent trente-cinq échantillons dans un ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 365, "src_audio": "/acl6060/audio/en/365.wav", "src_ref": "So to finetune a language model over ah this dataset will be ineffective.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, pour raffiner un modèle de langue sur cela, les données seront inefficaces.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 366, "src_audio": "/acl6060/audio/en/366.wav", "src_ref": "But we can use prior knowledge about pre-analyzed datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais nous pouvons utiliser des connaissances préalables sur des données préanalysées.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 367, "src_audio": "/acl6060/audio/en/367.wav", "src_ref": "Because FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puisque nous appliquons FeSTE sur des données multiples, nous pouvons utiliser les données n moins un pour recueillir des informations sur les données n moins un, et utiliser ces informations lorsque nous analysons les nièmes données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 368, "src_audio": "/acl6060/audio/en/368.wav", "src_ref": "What we, what we suggest is to add, to add another finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ce que nous suggérons, c'est d'ajouter une autre phase de raffinement.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 369, "src_audio": "/acl6060/audio/en/369.wav", "src_ref": "A preliminary multitask finetuning phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Une phase de raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 370, "src_audio": "/acl6060/audio/en/370.wav", "src_ref": "When you finetune the language model over the n minus one datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque vous raffinez le modèle de langue sur les données n moins un.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 371, "src_audio": "/acl6060/audio/en/371.wav", "src_ref": "And, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et ensuite, nous exécutons une autre phase de raffinement qui est un raffinement de tâche cible, lorsque nous raffinons le modèle de langue sur les nièmes données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 372, "src_audio": "/acl6060/audio/en/372.wav", "src_ref": "The state-of-the-art in multitask ah multitask finetuning called MTDNN.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le raffinement multitâche de pointe appelé MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 373, "src_audio": "/acl6060/audio/en/373.wav", "src_ref": "In MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN maintient les têtes dans le nombre de tâches dans l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 374, "src_audio": "/acl6060/audio/en/374.wav", "src_ref": "So, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc, dans cet exemple, il y a quatre tâches dans l'ensemble de formation. Le MTDNN maintient alors quatre têtes comme vous pouvez le voir sur l'image.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 375, "src_audio": "/acl6060/audio/en/375.wav", "src_ref": "And it samples a random batch from ah from the training set.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il échantillonne un lot aléatoire de l'ensemble de formation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 376, "src_audio": "/acl6060/audio/en/376.wav", "src_ref": "And if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si elles appartiennent à un lot aléatoire, par exemple, une seule tâche de classification de phrases, il exécute des chemins d'aller et retour à travers la première tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 377, "src_audio": "/acl6060/audio/en/377.wav", "src_ref": "And if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et si le lot aléatoire appartient à la tâche de classement par paire, il exécute un chemin d'aller et retour dans la dernière tête.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 378, "src_audio": "/acl6060/audio/en/378.wav", "src_ref": "In our scenario, ah tabular datasets vary in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Dans notre scénario, les données tabulaires varient dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 379, "src_audio": "/acl6060/audio/en/379.wav", "src_ref": "So there are many tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il y a donc beaucoup de tâches.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 380, "src_audio": "/acl6060/audio/en/380.wav", "src_ref": "MTDNN maintained number of classes, heads, output layers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Le MTDNN a maintenu le nombre de classes, de têtes et de couches de sortie.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 381, "src_audio": "/acl6060/audio/en/381.wav", "src_ref": "And the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en outre, le MTDNN doit initialiser de nouvelles têtes pour de nouvelles données avec une nouvelle tâche.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 382, "src_audio": "/acl6060/audio/en/382.wav", "src_ref": "Our approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche, appelée raffinement de reformulation de tâche, est dans notre raffinement de reformulation de tâche d'approche. Au lieu de maintenir plusieurs têtes, nous reformulons chaque donnée dans une phrase par problème de classification, étant des tâches de deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 383, "src_audio": "/acl6060/audio/en/383.wav", "src_ref": "So let's see an example.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Prenons donc un exemple.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 384, "src_audio": "/acl6060/audio/en/384.wav", "src_ref": "Here is the our input dataset which consists of entities, features, text and classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici nos données de saisie qui se composent d'entités, de fonctions, de texte et de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 385, "src_audio": "/acl6060/audio/en/385.wav", "src_ref": "And, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et nous reformulons la tâche à partir d'une classification du texte en bas ou haut pour classer le texte, le résumé et la classe en vrai ou faux.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 386, "src_audio": "/acl6060/audio/en/386.wav", "src_ref": "Or in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ou en d'autres mots, nous avons formé le modèle de langue pour classer un résumé et une classe en résumé et classe, si le résumé appartient à la classe ou non.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 387, "src_audio": "/acl6060/audio/en/387.wav", "src_ref": "So the label vector in this case stays always ah which consists always with two classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Donc dans ce cas, le vecteur d'étiquette consiste toujours en deux classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 388, "src_audio": "/acl6060/audio/en/388.wav", "src_ref": "And this is the ah algorithm for our fine, reformulated finetuning approach.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il s'agit de l'algorithme pour notre très bonne approche de raffinement reformulée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 389, "src_audio": "/acl6060/audio/en/389.wav", "src_ref": "So let's see the full framework.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voyons le cadre complet.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 390, "src_audio": "/acl6060/audio/en/390.wav", "src_ref": "Dataset fed into FeSTE.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Les données sont introduites dans FeSTE.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 391, "src_audio": "/acl6060/audio/en/391.wav", "src_ref": "And then ah FeSTE executes entity linking phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Puis FeSTE exécute la phase de liaison d'entités.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 392, "src_audio": "/acl6060/audio/en/392.wav", "src_ref": "It ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 393, "src_audio": "/acl6060/audio/en/393.wav", "src_ref": "Then it reformulated the task into a pairwise sentence classification task.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, il a reformulé la tâche en une tâche de classification des phrases par paire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 394, "src_audio": "/acl6060/audio/en/394.wav", "src_ref": "Applied the language model to the new task and the output likelihood for each class.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il a appliqué le modèle de langue à la nouvelle tâche et la probabilité de sortie pour chaque classe.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 395, "src_audio": "/acl6060/audio/en/395.wav", "src_ref": "And now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et maintenant, le modèle de langue est déjà raffiné sur des données n moins un en utilisant un raffinement multitâche préliminaire.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 396, "src_audio": "/acl6060/audio/en/396.wav", "src_ref": "Then we use the output vector of the language model as a newly generated feature in the number of classes.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous utilisons le vecteur de sortie du modèle de langue comme une fonction nouvellement générée dans le nombre de classes.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 397, "src_audio": "/acl6060/audio/en/397.wav", "src_ref": "To evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour évaluer notre cadre, nous utilisons dix-sept données de classification tabulaires qui varient en taille, fonctions, équilibre, domaine et performance initiale.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 398, "src_audio": "/acl6060/audio/en/398.wav", "src_ref": "And as knowledge base we use Wikipedia.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et en tant que base de connaissances, nous utilisons Wikipédia.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 399, "src_audio": "/acl6060/audio/en/399.wav", "src_ref": "We design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous concevons notre expérience comme une évaluation où nous formons FeSTe sur seize données et l'appliquons à la dix-septième donnée.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 400, "src_audio": "/acl6060/audio/en/400.wav", "src_ref": "We also, we also split each dataset into four folds and apply four folds cross validation.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous divisons également chaque donnée en quatre plis et appliquons une validation croisée de quatre plis.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 401, "src_audio": "/acl6060/audio/en/401.wav", "src_ref": "Then, we generate the new features and evaluate them using five evaluation classifiers.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Ensuite, nous générons les nouvelles fonctions et les évaluons en utilisant cinq classificateurs d'évaluation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 402, "src_audio": "/acl6060/audio/en/402.wav", "src_ref": "We use in our experiments base BERT base architecture.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Nous utilisons dans nos expériences l'architecture de base des Représentations d'encodeurs bidirectionnels à partir de transformateurs.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 403, "src_audio": "/acl6060/audio/en/403.wav", "src_ref": "Here are the results for our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Voici les résultats de nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 404, "src_audio": "/acl6060/audio/en/404.wav", "src_ref": "You can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Vous pouvez voir que nous comparons notre cadre au raffinement des données cibles, au raffinement de la tâche cible et à un raffinement préliminaire MTDNN.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 405, "src_audio": "/acl6060/audio/en/405.wav", "src_ref": "And our reformulated finetuning achieves the best result, the best performance.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et notre raffinement reformulé atteint le meilleur résultat et la meilleure performance.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 406, "src_audio": "/acl6060/audio/en/406.wav", "src_ref": "While MTDNN achieved two percent improvement over the target dataset finetuning.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Alors que le MTDNN a obtenu une amélioration de deux pour cent par rapport au raffinement des données cibles.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 407, "src_audio": "/acl6060/audio/en/407.wav", "src_ref": "Our approach achieved six percent improvement.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Notre approche a obtenu une amélioration de six pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 408, "src_audio": "/acl6060/audio/en/408.wav", "src_ref": "When we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Lorsque nous regardons les petites données, nous pouvons voir que la performance du MTDNN diminue et l'amélioration de la phase de raffinement multitâche préliminaire diminue aussi à un virgule cinq pour cent.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 409, "src_audio": "/acl6060/audio/en/409.wav", "src_ref": "But our performance increased to eleven percent compared to the target task finetuning alone.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais notre performance a augmenté à onze pour cent comparé au raffinement de tâche cible seul.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 410, "src_audio": "/acl6060/audio/en/410.wav", "src_ref": "For summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Pour résumer, FeSTE permet un enrichissement en quelques coups à partir de trente-cinq échantillons dans nos expériences.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 411, "src_audio": "/acl6060/audio/en/411.wav", "src_ref": "It uses one architecture for all tasks and datasets.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il utilise une architecture pour toutes les tâches et données.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 412, "src_audio": "/acl6060/audio/en/412.wav", "src_ref": "And it keeps the head of ah of the model.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Et il garde la tête du modèle.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 413, "src_audio": "/acl6060/audio/en/413.wav", "src_ref": "But it adds reformulation phase.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Mais cela ajoute une phase de reformulation.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 414, "src_audio": "/acl6060/audio/en/414.wav", "src_ref": "It augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Il augmente l'ensemble de formation et a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'introduire dans le modèle de langue et l'utiliser dans le problème de classification par paire de phrases.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 415, "src_audio": "/acl6060/audio/en/415.wav", "src_ref": "Thank you.", "src_lang": "en", "benchmark_metadata": {"context": "short", "dataset_type": "longform", "subset": "eval"}, "tgt_ref": "Merci.", "tgt_lang": "fr"}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.111", "sample_id": 416, "src_audio": "/acl6060/audio/en/416.wav", "src_ref": "Hello. My name is Asaf Harari.\nAnd I will present our paper, Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformers Architectures.\nData scientists analyze data and mainly focus on the manipulating the data's existing features.\nBut sometimes, these features are limited.\nFeature generation using another data source may add substantial information.\nOur research goal is automatic tabular data enrichment using external sources' free text.\nAssume we have a tabular dataset and a knowledge base.\nWe need an automatic process which involves entity linking and text analysis to extract new features from the knowledge base's free text.\nOur framework FeSTE is exactly this automatic process.\nSo let's see an example in a dataset fed into FeSTE.\nIn this example, the dataset is university dataset.\nWhen its goal is to classify universities into low ranking universities and high-ranking universities.\nAs knowledge base, we use Wikipedia.\nThe first phase of FeSTE is entity linking.\nWhen each entity, in this example the university name, is linked to an entity within the knowledge base.\nAnd and the text of the entities of the knowledge base is extracted and added to the dataset.\nIn this example, the text is the Wikipedia page's abstract.\nNow, we need to generate or extract features from the retrieved text.\nSo, we need to ah feature extraction phase ah which includes text analysis.\nAnd this is the main novelty of this paper and I will deep dive into it in the next slides.\nAfter the feature extraction phase, there is a feature generation phase when we use the extracted features to generate a small number of new features.\nFirst generate ah features in the number of classes of the original dataset.\nIn this example, the original dataset has two classes.\nSo, FeSTE generates two new features.\nBut if the dataset has five classes, FeSTE generates five new features.\nEach feature represents the likelihood for each class.\nTo analyze the text, we use the current state-of-the-art of text analysis, which are transformer based language models as BERT, GPT,  XLNet and etc.\nIt is but it is not likely that we can train language models using the input datasets.\nSo a naive approach will be ah target task finetuning.\nSo, in the feature extraction phase, we can download pretrained language models, finetune the language model over the target dataset.\nIn this example to finetune the language model, to classify ah to classify text into classes, abstract into classes, low or high.\nReceive the language model output, which is the likelihood for each class and use as new features.\nThe problem with this approach is datasets may have few distinct entities / texts.\nIn our experiment, almost half of the datasets contain less than four hundred samples and the smallest dataset contain thirty five samples in its, in a training set.\nSo to finetune a language model over ah this dataset will be ineffective.\nBut we can use prior knowledge about pre-analyzed datasets.\nBecause FeSTE, we apply FeSTE over a multiple dataset, we can use the n minus one datasets to gather information about the n minus one datasets, and use this information when we analyze the nth dataset.\nWhat we, what we suggest is to add, to add another finetuning phase.\nA preliminary multitask finetuning phase.\nWhen you finetune the language model over the n minus one datasets.\nAnd, then we execute another finetuning phase which is a target task finetuning, when you fine when we finetune the language model over the nth target dataset.\nThe state-of-the-art in multitask ah multitask finetuning called MTDNN.\nIn MTDNN, MTDNN maintains ah heads in the number of tasks in the training set.\nSo, in this example there are four tasks in the training set, so MTDNN maintain four heads as you can see at the image.\nAnd it samples a random batch from ah from the training set.\nAnd if they random batch belongs to a, for example single sentence classification task, it executes forward and backward paths through the first head.\nAnd if the random batch belongs to pairwise ranking task, it executes forward and backward path through the last head.\nIn our scenario, ah tabular datasets vary in the number of classes.\nSo there are many tasks.\nMTDNN maintained number of classes, heads, output layers.\nAnd the additional, additionally MTDNN needs to initialize new heads for a new dataset with a new task.\nOur approach, called task reformulation finetuning is, in our approach task reformulation finetuning, instead of maintaining multiple heads, we reformulate each dataset into a sentence per classification problem, which is two classes' tasks.\nSo let's see an example.\nHere is the our input dataset which consists of entities, features, text and classes.\nAnd, we reformulate the task from a classifying the text into low or high to classify the text, the abstract and the class into true or false.\nOr in other words, we trained the language model to classify an abstract and class ah to abstract and class ah, if the abstract belongs to the class or not.\nSo the label vector in this case stays always ah which consists always with two classes.\nAnd this is the ah algorithm for our fine, reformulated finetuning approach.\nSo let's see the full framework.\nDataset fed into FeSTE.\nAnd then ah FeSTE executes entity linking phase.\nIt ah it extracts the text from the knowledge base, which in this example is the abstract of the Wikipedia page.\nThen it reformulated the task into a pairwise sentence classification task.\nApplied the language model to the new task and the output likelihood for each class.\nAnd now that the language model is already finetuned over n minus one dataset using a preliminary multitask finetuning.\nThen we use the output vector of the language model as a newly generated feature in the number of classes.\nTo evaluate our framework, we use ah seventeen tabular classification datasets which vary in size, features, balance, domain and initial performance.\nAnd as knowledge base we use Wikipedia.\nWe design our experiment as leave one out ah evaluation where we train FeSTe over sixteen datasets and apply it to the seventeenth dataset.\nWe also, we also split each dataset into four folds and apply four folds cross validation.\nThen, we generate the new features and evaluate them using five evaluation classifiers.\nWe use in our experiments base BERT base architecture.\nHere are the results for our experiments.\nYou can see that we compare our our framework to target dataset finetuning, target task finetuning, and a MTDNN preliminary finetuning.\nAnd our reformulated finetuning achieves the best result, the best performance.\nWhile MTDNN achieved two percent improvement over the target dataset finetuning.\nOur approach achieved six percent improvement.\nWhen we look on the small ah dataset, we can see that the performance of MTDNN decreases and the improvement of the prelim, the preliminary multitask finetuning phase decreases to one point five percent.\nBut our performance increased to eleven percent compared to the target task finetuning alone.\nFor summing, FeSTE enables few shot enrichment from thirty five samples in our experiments.\nIt uses one architecture for all tasks and datasets.\nAnd it keeps the head of ah of the model.\nBut it adds reformulation phase.\nIt augments the train set and it needs a target value with semantic meaning so we can feed it into the language model and use it in the sentence pair classification problem.\nThank you.", "tgt_ref": "Bonjour. Je m'appelle Asaf Harari.\nEt je vais vous présenter notre article : enrichissement de données tabulaires en quelques coups à l'aide d'architectures de transformateurs raffinées.\nLes scientifiques analysent les données et se concentrent principalement sur la manipulation des fonctions existantes des données.\nMais parfois, ces fonctions sont limitées.\nLa génération de fonctions en utilisant une autre source de données peut ajouter des informations substantielles.\nNotre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des textes libres de sources externes.\nSupposons que nous ayons des données tabulaires et une base de connaissances.\nNous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles fonctions du texte libre de la base de connaissances.\nNotre cadre FeSTE est exactement ce processus automatique.\nVoyons donc un exemple dans des données introduites dans FeSTE.\nDans cet exemple, les données sont des données universitaires.\nQuand l'objectif est de classer les universités en universités de bas rang et en universités de haut rang.\nEn tant que base de connaissances, nous utilisons Wikipédia.\nLa première phase de FeSTE est la liaison d'entités.\nLorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances.\nEt le texte des entités de la base de connaissances est extrait et ajouté aux données.\nDans cet exemple, le texte est le résumé de la page Wikipédia.\nMaintenant, nous devons générer ou extraire des fonctions à partir du texte extrait.\nNous avons donc besoin de la phase d'extraction des fonctions qui comprend l'analyse de texte.\nIl s'agit de la principale nouveauté de cet article et je vais mieux l'expliquer dans les prochaines diapositives.\nAprès la phase d'extraction des fonctions, il y a une phase de génération de fonctions lorsque nous utilisons les fonctions extraites pour générer un petit nombre de nouvelles fonctions.\nGénérez d'abord les fonctions dans le nombre de classes des données d'origine.\nDans cet exemple, les données d'origine ont deux classes.\nAinsi, FeSTE génère deux nouvelles fonctions.\nMais si les données ont cinq classes, FeSTE génère cinq nouvelles fonctions.\nChaque fonction représente la probabilité pour chaque classe.\nPour analyser le texte, nous utilisons les analyses de texte de pointe, qui sont des modèles de langue basés sur la conversion comme BERT, GPT, XLNet etc.\nMais il est peu probable que nous puissions former des modèles de langue en utilisant les données de saisie.\nAinsi, une approche naïve sera le raffinement de la tâche cible.\nDans la phase d'extraction des fonctions, nous pouvons télécharger des modèles de langue préformée et raffiner le modèle de langue sur les données cibles.\nDans cet exemple, raffiner le modèle de langue, classer le texte en classes, résumer en classes, basses ou hautes.\nRecevoir les résultats du modèle de langue, qui sont la probabilité pour chaque classe et les utiliser comme nouvelles fonctions.\nLe problème avec cette approche est que les données peuvent avoir peu d'entités / de textes distinct(e)s.\nDans notre expérience, près de la moitié des données contiennent moins de quatre cents échantillons et les plus petites données contiennent trente-cinq échantillons dans un ensemble de formation.\nDonc, pour raffiner un modèle de langue sur cela, les données seront inefficaces.\nMais nous pouvons utiliser des connaissances préalables sur des données préanalysées.\nPuisque nous appliquons FeSTE sur des données multiples, nous pouvons utiliser les données n moins un pour recueillir des informations sur les données n moins un, et utiliser ces informations lorsque nous analysons les nièmes données.\nCe que nous suggérons, c'est d'ajouter une autre phase de raffinement.\nUne phase de raffinement multitâche préliminaire.\nLorsque vous raffinez le modèle de langue sur les données n moins un.\nEt ensuite, nous exécutons une autre phase de raffinement qui est un raffinement de tâche cible, lorsque nous raffinons le modèle de langue sur les nièmes données cibles.\nLe raffinement multitâche de pointe appelé MTDNN.\nLe MTDNN maintient les têtes dans le nombre de tâches dans l'ensemble de formation.\nDonc, dans cet exemple, il y a quatre tâches dans l'ensemble de formation. Le MTDNN maintient alors quatre têtes comme vous pouvez le voir sur l'image.\nEt il échantillonne un lot aléatoire de l'ensemble de formation.\nEt si elles appartiennent à un lot aléatoire, par exemple, une seule tâche de classification de phrases, il exécute des chemins d'aller et retour à travers la première tête.\nEt si le lot aléatoire appartient à la tâche de classement par paire, il exécute un chemin d'aller et retour dans la dernière tête.\nDans notre scénario, les données tabulaires varient dans le nombre de classes.\nIl y a donc beaucoup de tâches.\nLe MTDNN a maintenu le nombre de classes, de têtes et de couches de sortie.\nEt en outre, le MTDNN doit initialiser de nouvelles têtes pour de nouvelles données avec une nouvelle tâche.\nNotre approche, appelée raffinement de reformulation de tâche, est dans notre raffinement de reformulation de tâche d'approche. Au lieu de maintenir plusieurs têtes, nous reformulons chaque donnée dans une phrase par problème de classification, étant des tâches de deux classes.\nPrenons donc un exemple.\nVoici nos données de saisie qui se composent d'entités, de fonctions, de texte et de classes.\nEt nous reformulons la tâche à partir d'une classification du texte en bas ou haut pour classer le texte, le résumé et la classe en vrai ou faux.\nOu en d'autres mots, nous avons formé le modèle de langue pour classer un résumé et une classe en résumé et classe, si le résumé appartient à la classe ou non.\nDonc dans ce cas, le vecteur d'étiquette consiste toujours en deux classes.\nEt il s'agit de l'algorithme pour notre très bonne approche de raffinement reformulée.\nVoyons le cadre complet.\nLes données sont introduites dans FeSTE.\nPuis FeSTE exécute la phase de liaison d'entités.\nIl extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia.\nEnsuite, il a reformulé la tâche en une tâche de classification des phrases par paire.\nIl a appliqué le modèle de langue à la nouvelle tâche et la probabilité de sortie pour chaque classe.\nEt maintenant, le modèle de langue est déjà raffiné sur des données n moins un en utilisant un raffinement multitâche préliminaire.\nEnsuite, nous utilisons le vecteur de sortie du modèle de langue comme une fonction nouvellement générée dans le nombre de classes.\nPour évaluer notre cadre, nous utilisons dix-sept données de classification tabulaires qui varient en taille, fonctions, équilibre, domaine et performance initiale.\nEt en tant que base de connaissances, nous utilisons Wikipédia.\nNous concevons notre expérience comme une évaluation où nous formons FeSTe sur seize données et l'appliquons à la dix-septième donnée.\nNous divisons également chaque donnée en quatre plis et appliquons une validation croisée de quatre plis.\nEnsuite, nous générons les nouvelles fonctions et les évaluons en utilisant cinq classificateurs d'évaluation.\nNous utilisons dans nos expériences l'architecture de base des Représentations d'encodeurs bidirectionnels à partir de transformateurs.\nVoici les résultats de nos expériences.\nVous pouvez voir que nous comparons notre cadre au raffinement des données cibles, au raffinement de la tâche cible et à un raffinement préliminaire MTDNN.\nEt notre raffinement reformulé atteint le meilleur résultat et la meilleure performance.\nAlors que le MTDNN a obtenu une amélioration de deux pour cent par rapport au raffinement des données cibles.\nNotre approche a obtenu une amélioration de six pour cent.\nLorsque nous regardons les petites données, nous pouvons voir que la performance du MTDNN diminue et l'amélioration de la phase de raffinement multitâche préliminaire diminue aussi à un virgule cinq pour cent.\nMais notre performance a augmenté à onze pour cent comparé au raffinement de tâche cible seul.\nPour résumer, FeSTE permet un enrichissement en quelques coups à partir de trente-cinq échantillons dans nos expériences.\nIl utilise une architecture pour toutes les tâches et données.\nEt il garde la tête du modèle.\nMais cela ajoute une phase de reformulation.\nIl augmente l'ensemble de formation et a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'introduire dans le modèle de langue et l'utiliser dans le problème de classification par paire de phrases.\nMerci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.410", "sample_id": 417, "src_audio": "/acl6060/audio/en/417.wav", "src_ref": "Hi everyone. Today I'm going to present our research work Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction.\nI'm Allan from ByteDance AI Lab, and this is a joint work with Jierui Li from the University of Texas at Austin and Wei Lu from SUTD.\nFirst, I'd like to talk about our motivation for reasoning.\nSo here we show an examples where multi-step reasoning is helpful.\nSo this figure is taken from the PaLM paper where they perform prompting to solve the network problem in the few shot learning scenario.\nSo on the left hand side, we can see if we give some examples with just question and answers, we might not be able to obtain the correct answers.\nBut if we give some more reasoning description, the model is able to predict the reasoning description and also make a correct prediction here.\nSo it is good to have interpretable multi-step reasoning as output.\nAnd we also think math word problem is a straightforward application to evaluate such reasoning abilities.\nSo, here in our problem setup, given the questions we need to solve this question and obtain the numerical answers.\nSo in our datasets we are also given the mathematical expression which leads to the ah to this particular answer as well.\nSo, certain assumptions ah also apply as in previous work.\nWe assume the precision of quantities are known.\nAnd we only consider basic operators such as addition, subtractions, multiplication, division, and exponential.\nFurthermore, complicated operators can be actually decomposed into these basic operators.\nSo, previous work in math word problem solving ah actually can ah be categorized into sequence to sequence and sequence to tree model.\nSo, traditional sequence to sequence model convert the expression to a specific sequence for generation.\nAnd it is pretty easy to implement and it can generalize to many different complicated problem.\nBut the drawbacks are the performance is actually generally not better than the structured model and its lack of interpretability for prediction.\nBut actually this direction is still quite popular because of um the transformer model.\nSo, in tree based models, we actually structure these expressions in the tree form and follow a preordered traversal in tree generations.\nSo here we keep generating the operators until we reach the leaves, which are the quantities.\nSo here the good thing is that it actually gives us this binary tree structure, and it is um but actually it is quite counterintuitive because we generate the operator first and then at the end we generate the quantities.\nAnd the second thing is that it also contains some repetitive computations.\nSo here if we look at this expression, eight times three plus three is actually generated twice, but in fact we should reuse the results.\nSo, in our proposed approach we want to solve those problems in a step by step and interpretable manners.\nSo for example, here in the second step, ah we can obtain these divisors which is twenty seven.\nAnd we can also refer back to the original questions to find the relevant contents.\nAnd in these steps we obtain the divisors.\nSo, ah and then at this third step we actually get the quotient.\nAlright. And after these three steps, we can actually reuse the results from the second step, and then get the ah results of the fourth step, and then finally we can obtain the dividends.\nSo, here we actually generate the whole expression directly rather than generating a single operators or quantities.\nSo this makes the process more accurate.\nSo, in our deductive system, we first start with a bunch of quantities presented in the questions and also including some constant as our initial state ah initial state.\nSo, the expression is represented by e i j o p.\nWhere we perform operator from q_i to q_j, and such expression is actually directed.\nSo, we also have subtraction with words here to represent the opposite direction.\nThis is quite similar to relation extraction.\nSo in a formal deductive system, at a time step t, we apply the operator between the q_i and q_j pair, and then we obtain this new expression.\nWe add it to the next state to become a new quantity.\nSo, these slides actually visualize the evolution of the state where we keep adding expression to the current state.\nSo in our model implementations, we first use a pretrained language model which can be BERTs or Robertas and then we encode the sentence and then we obtain these quantity representations.\nSo, once we get the quantity representations, we can start to do inference.\nHere we show an example of q_1 to obtain the representation for q_2 divided by q_2 and then times q_3.\nFirst we get the ah pair representation, which is basically just the concatenation between q_1 and q_2, and then we apply a feedforward network which is parameterized by the operator.\nAnd then finally we obtain the expression representation q_1 divided by q_2.\nBut in fact, in practice, in the inference stage, we might ah be able to get the incorrect expression as well.\nSo, here all the possible expression is equals to three times the number of operators.\nSo the nice thing here is that we can easily add constraints to control this search this search space.\nFor example, if this expression is not allowed, we can simply remove this expression in our search space.\nSo in the second step, we do the same thing, but the only difference is that we ah the only difference is one more quantities.\nSo this quantity come from the previous calculated expression.\nSo finally we can obtain this final expression q_3 times q_4.\nAnd we can also see the number of all the possible ah expression is different from the previous step.\nSo, ah such difference make it hard to apply beam search because the probability distribution between these two steps is unbalanced.\nSo the training procedure is similar to training a sequence to sequence model where we optimize the loss at each time step.\nAnd here we also use this tau to represent when we should terminate this generation process.\nAnd here the space is different from sequence to sequence because the space is different at each time step while in traditional sequence to sequence model this is the number of vocabulary.\nAnd it also allows us to impose certain constraints from prior from prior knowledge.\nSo we conduct experiments on the commonly used math word problem datasets, MAWPS, Math23K,  MathQA and SVAMP.\nAnd here we briefly show the results compared with the previous best approaches.\nSo our best performing variant is Roberta-DeductiveReasoner.\nAnd in fact we do not use beam search, in contrast all previous approaches are using beam search.\nAll right. So, the best approaches are often tree based model.\nSo, overall our reasoner is able to significantl significantly outperform this tree based model.\nBut we can see the absolute numbers on MathQA or SVAMP are not really high.\nSo we further investigate the results on SVAMP.\nAnd this dataset is challenging because the author tried to manually ah adding something to confuse the NLP model like such as adding irrelevant information and extra quantities.\nSo, in our prediction we find some of the intermediate values are actually negatives.\nFor example, um, in these questions we are asking how many apples does Jake have?\nBut we have some extra information like seventeen fewer pictures, and Steven has eight pictures, which is totally irrelevant.\nSo, our model makes some prediction like this which is producing negative values.\nAnd we observe these two expressions actually have similar scores.\nSo, we can actually limit this search space by removing those results that are negatives so that we can make the ah make the answer correct.\nSo um we further find such constraint actually improves quite a lot for some models.\nFor example, for BERT, we improve seven points and then for the Roberta base model we actually improved two points.\nSo better language model has better language understanding abilities so that the number here is higher for Roberta and lower for BERT.\nAnd we also try to analyze the difficulty behind these behind all these datasets.\nWe assume the number of unused quantities can be regarded as irrelevant information here.\nSo ah here we can see that ah,we have the the percentage of samples with unused quantities, and the SVAMP dataset has the largest portion.\nAnd here we also show the overall performance.\nFor those samples without unused quantities, so the overall performance is actually higher than the, the performance is actually higher than the overall performance.\nBut with those samples that with unused quantity is actually way worse than the, worse than the overall performance.\nFor MAWPS, we don't we don't really have ah too many test cases, so I just ignore this part.\nSo, finally we want to show the interpretability through a question perturbation example.\nSo here our model actually makes a wrong prediction at the first step.\nSo, we can actually correlate this expression with the sentence here. Alright.\nSo, we think this sentence might be misleading the model to an incorrect predictions.\nSo here planting another thirty five makes the model makes the model think it should be an addition operator.\nSo we try to revise the sentence to be something like the number of pear trees are thirty five fewer than the apple trees.\nSo, we make it to convey more accurate semantics such that the model is able to make um the prediction correct.\nSo, this study shows how the interpretable predictions help us understand the model behavior.\nSo to conclude our work, so first our model is actually pretty efficient.\nAnd we are able to provide interpretable solving procedure.\nAnd we can easily incorporate some prior knowledge as constraint which can help improve the performance.\nAnd the last thing is that the underlying mechanism does not only apply to network problem solving tasks but also other tasks that involve multi step reasoning.\nWe also have certain limitations.\nAh, if we have a large number of operators or constants, the memory consumption could be pretty high.\nAnd the second thing is that, as mentioned, because the probability distribution is unbalanced between different time steps, so it's also pretty challenging to apply beam search strategy.\nSo this is the end of the talk, and questions are welcomed. Thank you.", "tgt_ref": "Bonjour tout le monde. Aujourd'hui, je vais vous présenter notre travail de recherche « Apprendre à raisonner par déduction » : résolution de problèmes de mots mathématiques comme extraction de relation complexe.\nJe m'appelle Allan du laboratoire d'intelligence artificielle ByteDance, et ceci est un travail en commun avec Jierui Li de l'Université du Texas à Austin et Wei Lu du SUTD.\nTout d'abord, j'aimerais parler de notre motivation pour le raisonnement.\nDonc ici, nous montrons un exemple où le raisonnement en plusieurs étapes est utile.\nCe chiffre est donc tiré de l'article PaLM où ils incitent à résoudre le problème de réseau dans le scénario d'apprentissage de quelques prises de vue.\nDonc, sur le côté gauche, nous pouvons voir que si nous fournissons quelques exemples avec juste des questions et des réponses, nous pourrions ne pas être en mesure d'obtenir les bonnes réponses.\nMais si nous fournissons plus de description du raisonnement, le modèle est capable de prévenir la description du raisonnement, mais aussi d'effectuer ici une prévention correcte.\nIl est alors bon d'avoir comme résultat un raisonnement à plusieurs étapes interprétable.\nEt nous pensons également que les problèmes de mots mathématiques sont une application directe pour évaluer de telles capacités de raisonnement.\nDonc ici, dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques.\nAinsi, dans nos données, nous recevons également l‘expression mathématique qui conduit à cette réponse particulière.\nDonc, certaines hypothèses s'appliquent également comme dans les travaux antérieurs.\nNous supposons que la précision des quantités est connue.\nEt nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle.\nEn outre, les opérateurs compliqués peuvent effectivement être décomposés en ces opérateurs de base.\nAinsi, le travail antérieur dans la résolution de problèmes de mots mathématiques peut en effet être classé dans un modèle séquence à séquence et séquence à arbre.\nLe modèle séquence à séquence traditionnel convertit donc l'expression en une séquence spécifique pour la génération.\nIl est assez facile à mettre en œuvre et peut être généralisé à de nombreux problèmes compliqués différents.\nMais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle structuré et son manque d'interopérabilité pour la prévention.\nMais en réalité, cette direction est encore très populaire en raison du modèle de conversion.\nDonc, dans les modèles à base d'arbre, nous structurons en réalité ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres.\nDonc ici, nous continuons à produire les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités.\nDonc ici, la bonne chose est qu'en fait, cela nous donne cette structure d'arbre binaire. Mais en réalité, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur et ensuite, à la fin, nous générons les quantités.\nEt la deuxième chose est que cela contient également des calculs répétitifs.\nDonc ici, si nous regardons cette expression, huit fois trois plus trois est en fait généré deux fois, mais en réalité, nous devrions réutiliser les résultats.\nAinsi, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable.\nPar exemple, ici dans la deuxième étape, nous pouvons obtenir ces diviseurs qui sont vingt-sept.\nEt nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent.\nEt dans ces étapes, nous obtenons les diviseurs.\nPuis, à cette troisième étape, nous obtenons alors le quotient.\nTrès bien. Et après ces trois étapes, nous pouvons alors réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, et enfin, nous pouvons obtenir les dividendes.\nDonc ici, nous générons en réalité l'expression entière directement plutôt que de produire un seul opérateur ou une seule quantité.\nCela rend le processus plus précis.\nAinsi, dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et incluant également une certaine constante comme notre état initial.\nAinsi, l'expression est représentée par e i j o p.\nOù nous effectuons l'opérateur de q_i à q_j, et cette expression est en fait dirigée.\nDonc, nous avons également ici la soustraction avec des mots pour représenter la direction opposée.\nC'est tout à fait similaire à l'extraction de relation.\nDonc, dans un système déductif formel, à un pas de temps t, on applique l'opérateur entre la paire q_i et q_j, puis on obtient cette nouvelle expression.\nNous l'ajoutons à l'état suivant pour devenir une nouvelle quantité.\nAinsi, ces diapositives visualisent en réalité l'évolution de l'état où nous continuons à ajouter de l'expression à l'état actuel.\nDonc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langue préformée qui peut être BERT ou Roberta, puis nous encodons la phrase et obtenons ensuite ces représentations de quantité.\nDonc, une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire l'inférence.\nNous montrons ici un exemple de q_1 pour obtenir la représentation pour q_2 divisée par q_2 multipliée par q_3.\nTout d'abord, nous obtenons la représentation de paire, qui n'est essentiellement que l'enchaînement entre q_1 et q_2, puis nous appliquons un réseau prédictif qui est paramétré par l'opérateur.\nEt enfin, nous obtenons la représentation de l'expression q_1 divisée par q_2.\nMais en réalité, dans la pratique, au stade de l'inférence, nous pourrions également obtenir une expression incorrecte.\nDonc ici, toute l'expression possible est égale à trois fois le nombre d'opérateurs.\nAlors, ce qui est bien ici, c'est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche.\nPar exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.\nDonc, dans la deuxième étape, nous faisons la même chose, mais la seule différence est que nous avons une quantité de plus.\nCette quantité provient donc de l'expression calculée antérieure.\nFinalement, nous pouvons obtenir cette expression finale q_3 multipliée par q_4.\nEt nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape antérieure.\nAinsi, une telle différence rend difficile l'application de la beam search car la distribution de probabilité entre ces deux étapes est déséquilibrée.\nLa procédure de formation est donc similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps.\nEt ici, nous utilisons également ce tau pour représenter le moment où nous devrions mettre fin à ce processus de génération.\nEt ici, l'espace est différent de séquence à séquence car il est différent à chaque pas de temps, alors que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire.\nEt cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.\nNous menons donc des expériences sur les données de problèmes de mots mathématiques, MAWPS, Math23K, MathQA et SVAMP couramment utilisées.\nEt ici, nous montrons brièvement les résultats comparés aux meilleures approches antérieures.\nDonc, notre variante la plus performante est Roberta-DeuctiveReasoner.\nEt en réalité, nous n'utilisons pas la beam search ; au contraire, toutes les approches antérieures utilisent la beam search.\nTrès bien. Ainsi, les meilleures approches sont souvent un modèle à base d'arbre.\nDonc, dans l'ensemble, notre raisonneur est capable de dépasser significativement ce modèle à base d'arbre.\nMais nous pouvons voir que les nombres absolus sur MathQA ou SVAMP ne sont pas vraiment élevés.\nNous étudions donc plus en détail les résultats sur SVAMP.\nEt ces données sont difficiles parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle TAL traitement automatique du langage naturel comme ajouter des informations non pertinentes et des quantités supplémentaires.\nDonc, dans notre prévention, nous trouvons que certaines des valeurs intermédiaires sont en réalité négatives.\nPar exemple, dans ces questions, nous demandons « combien de pommes a Jake ? »\nMais nous avons des informations supplémentaires comme « dix-sept photos de moins », et « Steven en a huit », ce qui est totalement hors de propos.\nAinsi, notre modèle effectue une prévention comme celle-ci qui produit des valeurs négatives.\nEt nous constatons que ces deux expressions ont en réalité des scores similaires.\nNous pouvons alors limiter cet espace de recherche en supprimant les résultats négatifs afin que nous puissions rendre la réponse correcte.\nNous trouvons donc que cette contrainte améliore en réalité beaucoup pour certains modèles.\nPar exemple, pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous améliorons sept points, puis pour le modèle de base Roberta, nous avons amélioré deux points.\nLe meilleur modèle de langue a donc de meilleures capacités de compréhension de la langue de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs.\nEt nous essayons également d'analyser la difficulté derrière toutes ces données.\nNous supposons que le nombre de quantités inutilisées peut être considéré ici comme une information non pertinente.\nDonc ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et les données SVAMP ont la plus grande partie.\nEt ici, nous montrons également la performance globale.\nPour ces échantillons sans quantités inutilisées, la performance est en réalité supérieure à la performance globale.\nMais avec ces échantillons ayant une quantité inutilisée, c'est en fait bien pire que la performance globale.\nPour MAWPS, nous n'avons pas vraiment trop de cas de test, alors j'ignore simplement cette partie.\nFinalement, nous voulons montrer l'interopérabilité à travers un exemple de perturbation de question.\nDonc ici, notre modèle effectue en réalité une prévention erronée à la première étape.\nAinsi, nous pouvons effectivement corréler cette expression avec la phrase ici. Très bien.\nNous pensons donc que cette phrase pourrait induire le modèle en erreur avec des préventions incorrectes.\nDonc ici, en planter trente-cinq autres fait penser au modèle qu'il devrait être un opérateur d'addition.\nNous essayons alors de réviser la phrase pour que cela soit quelque chose comme « le nombre de poiriers est inférieur de trente-cinq à celui des pommiers ».\nNous faisons en sorte de transmettre une sémantique plus précise afin que le modèle soit capable de rendre la prévention correcte.\nAinsi, cette étude montre comment les préventions interprétables nous aident à comprendre le comportement du modèle.\nDonc, pour conclure notre travail, notre modèle est en réalité assez efficace.\nEt nous sommes en mesure de fournir une procédure de résolution interprétable.\nEt nous pouvons facilement incorporer des connaissances antérieures en tant que contraintes qui peuvent aider à améliorer la performance.\nEt la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution des problèmes de réseau, mais aussi aux autres tâches qui impliquent un raisonnement à plusieurs étapes.\nNous avons également certaines limites.\nSi nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée.\nEt la deuxième chose est que, comme mentionné, puisque la distribution de probabilité est déséquilibrée entre les différentes étapes de temps, il est donc également assez difficile d'appliquer la stratégie de beam search.\nNous arrivons donc à la fin de la discussion, et vos questions sont les bienvenues. Merci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.468", "sample_id": 418, "src_audio": "/acl6060/audio/en/418.wav", "src_ref": "Hi, my name is Antoine and I'm from Maastricht University.\nI will be presenting my joint work with Jerry which is about a New Dataset for Statutory Article Retrieval.\nLegal issues are an integral part of many people's lives.\nBut the majority of citizens have little to know knowledge about their rights and fundamental legal processes.\nAs a result, many vulnerable citizens who cannot afford the costly assistance of a legal expert are left unprotected or, worst, exploited.\nAll work aims to bridge the gap between people and the law by developing an effective retrieval system for statutory articles.\nSuch a system could provide a free professional legal help service for unskilled humans.\nBefore diving into the main contribution of this work, let's first describe the problem of statutory article retrieval.\nGiven a simple question on a legal matter such as, what do I risk if I violate professional confidentiality?\nA model is required to retrieve all relevant statutory articles from a large body of legislation.\nThis information retrieval task comes with its own set of challenges.\nFirst, it deals with two types of language.\nCommon natural language for the questions and complex legal language for the statutes.\nThis difference in language distributions makes it harder for a system to retrieve relevant candidates, as it indirectly requires an inherent interpretation system that can translate a natural question to a legal question that matches the terminology of statutes.\nBesides, statutory law is not a stack of independent articles that can be treated as a complete source of information on their own, unlike news or recipes, for example.\nInstead, it's a structured collection of legal provisions that have a whole meaning only when considered in the overall context, that is, together with the supplementary information from the neighboring articles, the fields and subfields they belong to, and their place in the structure of the law.\nLastly, statutory articles aren't small paragraphs which usually is the typical retrieval unit in most retrieval works.\nHere, there are long documents that may be up to six thousand words.\nThe recent advances in NLP have sparked huge interest in many legal tasks, such as legal judgment prediction or automated contact contract review.\nBut statutory article retrieval has remained mainly untouched due to the lack of large and high quality labeled datasets.\nIn this work, we present a new French native citizen-centric dataset to study whether retrieval models can approximate the efficiency and reliability of a legal expert for the task of statutory article retrieval.\nOur Belgian statutory article retrieval dataset BSARD consists of more than one thousand one hundred legal questions posed by Belgian citizens.\nThese questions cover a wide range of topics from family, housing, money, to work and social security.\nEach of them has been labeled by experienced jurists with references to relevant articles from a corpus of more than twenty-two thousand six hundred legal articles from Belgian codes of law.\nLet's now talk about how we collected this dataset.\nFirst, we started by compiling a large corpus of legal articles.\nWe considered thirty two publicly available Belgian codes and extracted all the articles as well as the corresponding section headings.\nThen we gathered legal questions with references to relevant statutes.\nTo do so, we partner with the Belgian law firm that receives each year around four thousand emails from Belgian citizens who ask for advice on a personal legal issue.\nWe were lucky enough to get access to their websites, where their team of experienced jurists addresses Belgians' most common legal issues.\nWe collected thousands of questions annotated with categories, subcategories and legal references to relevant statutes.\nLastly, we passed the legal references and filtered out the questions whose references were not articles in one of the codes of law we considered.\nThe remaining references were matched and converted to the corresponding article ids from our corpus.\nWe eventually ended up with one thousand one hundred and eight questions, each carefully labeled with the ids of the relevant articles from our large corpus of twenty two thousands and six hundred thirty three statutory articles.\nIn addition, each question comes with the main category and a concatenation of subcategories.\nAnd each articles comes with a concatenation of the subsequence heading in the structure of the law.\nThis extra information is not used in the present work, but might be of interest for future research on legal information retrieval or legal text classification.\nLet's look at some characteristic of our dataset.\nThe questions are between five and forty four words long with a median of fourteen words.\nThe articles are much longer with a median length of seventy seven words, with one hundred and forty two of them exceeding one thousand words.\nThe lengthiest one being up to five thousand seven hundred and ninety words.\nAs previously mentioned, the questions cover a wide range of topics, with around eighty five percent of them being either about family, housing, money or justice.\nWhile the remaining fifteen percent concern either social security, foreigners or work.\nThe article are also very diverse as they come from thirty two different Belgian codes that cover a large number of legal topics.\nHere's the total number of articles collected from each of these Belgian codes.\nOut of the twenty two thousand six hundred and thirty three articles, only one thousand six hundred and twelve are referred to as relevant to at least one question in the dataset.\nAnd around eighty percent of these cited articles come from either the civil code, judicial codes, criminal investigation codes or penal codes.\nMeanwhile, eighteen out of thirty two codes have less than five articles mentioned as relevant to at least one question.\nWhich can be explained by the fact that those codes focused less on individuals and their concerns.\nOverall, the median number of citations for these cited articles is two, and less than twenty-five percent of them are cited more than five times.\nUsing all datasets, we benchmarked several retrieval approaches, including lexical and dense architecture.\nGiven a query and an article, a lexical model assigns a score to the query article pair by computing the sum over the query terms of the weights of each of these terms in that article.\nWe experiment with the standard TF-IDF and BM25 ranking functions.\nThe main problem with these approaches is that they can only retrieve articles that contain keywords present in the query.\nTo overcome this limitation, we experiment with a neural based architecture that can capture semantic relationships between queries and article.\nWe use a bi-encoder model that maps queries and articles into dense vector representations and calculate a relevance score between a query article pair by the similarity of their embeddings.\nThese embeddings typically result from a pooling operation on the output of a word embedding model.\nFirst, we study the effectiveness of Siamese bi-encoders in a zero shot evaluation setup, meaning that pretrained word embedding models are applied out-of-the-box without any additional finetuning.\nWe experiment with context independent text encoder, namely word2vec and fastText, and context dependent embedding models, namely Roberta and more specifically CamemBERT which is a French Roberta model.\nAdditionally, we train our own CamemBERT based model ah bi-encoders on our dataset.\nNote that for training, we experiment with the two flavors of the bi-encoder architecture.\nSiamese, which uses a unique word embedding model that maps the query and article together in a shared dense vector space, and two-tower, which uses two independent word embedding models that encode the query and article separately into different embedding spaces.\nWe experiment with mean, max and CLS pooling as well as product and cosine for computing similarities.\nHere are the result of our baseline on the test sets.\nWith the lexical methods above, the Siamese bi-encoders evaluated in a zero shot setup in the middle, and the finetuned bi-encoders below.\nOverall, the finetuned bi-encoder significantly outperforms all the other baselines.\nThe two-tower model improves over its Siamese variants on recall at one hundred, but performs similarly on the other metrics.\nAlthough BM25 underperformed the trained bi-encoder significantly, its performance indicated that it's still a strong baseline for domain specific retrieval.\nRegarding the zero shot evaluation of Siamese bi-encoder, we find that directly using the embeddings of a pretrained CamemBERT model without optimizing for the information retrieval task gives poor results, which is consistent with previous findings.\nFurthermore, we observe that the word2vec based bi-encoder significantly outperformed the fastText and BERT based models, suggesting that maybe pretrained word level embeddings are more appropriate for the task than character level or subword level embeddings when used out of the box.\nAlthough promising, these results suggest ample opportunity for improvement compared to a skilled legal expert who can eventually retrieve all relevant articles to any question and thus get perfect scores.\nLet's conclude by discussing two limitations of our dataset.\nFirst, the corpus of article is limited to those collected from the thirty two considered Belgian codes, which does not cover the entire Belgian law as articles from decrees, directives and ordinances are missing.\nDuring the dataset construction, all references to these uncollected articles are ignored, which causes some questions to end up with only a fraction of the initial number of relevant articles.\nThis information thus implies that the answer contained in the remaining relevant articles might be incomplete, although it's still completely appropriate.\nSecond, we should note that not all legal questions can be answered with statutes alone.\nFor instance, the question, can I evict my tenants if they make too much noise?\nMight not have a detailed answer within statutory law that quantifies a specific noise threshold at which eviction is allowed.\nInstead, the landlord should probably rely more on case law and find precedents similar to their current situation.\nFor example, the tenants makes two parties a week until two AM.\nHence, some question are better suited than others to the statutory article retrieval task, and the domain of the less suitable ones remains to be determined.\nWe hope that our work sparks interest in developing practical and reliable statutory article retrieval models.\nThat can help improve access to justice for all.\nYou can check out our paper, dataset and code at the following links. Thank you.", "tgt_ref": "Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht.\nJe vais vous présenter mon travail en commun avec Jerry, qui porte sur de nouvelles données pour l'extraction des articles de droit.\nLes questions juridiques font partie intégrante de la vie de nombreuses personnes.\nMais la majorité des citoyens ont peu de connaissances sur leurs droits et leurs processus juridiques fondamentaux.\nEn conséquence, de nombreux citoyens vulnérables qui n'ont pas les moyens de se payer l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités.\nTous les travaux visent à combler le fossé entre les personnes et la loi en développant un système d'extraction efficace pour les articles de droit.\nUn tel système pourrait fournir un service d'aide juridique professionnel gratuit aux personnes non qualifiées.\nAvant de plonger dans la principale contribution de ce travail, nous allons d'abord décrire le problème d'extraction des articles de droit.\nCompte tenu d'une simple question sur une question juridique telle que : « qu'est-ce que je risque si je viole la confidentialité professionnelle ? »\nUn modèle est nécessaire pour extraire tous les articles de droit pertinents d'un grand corpus législatif.\nCette tâche d'extraction des informations comporte son propre ensemble de défis.\nTout d'abord, elle traite de deux types de langue.\nLa langue naturelle commune pour les questions et la langue juridique complexe pour les lois.\nCette différence dans les distributions de langue rend plus difficile pour un système d'extraire les candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des lois.\nEn outre, le droit écrit n'est pas une pile d'articles indépendants qui peuvent être traités comme une source complète d'informations par eux-mêmes, contrairement aux nouvelles ou aux recettes, par exemple.\nAu lieu de cela, il s'agit d'un ensemble structuré de dispositions juridiques qui n'ont un sens entier que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les domaines et sous-domaines auxquels elles appartiennent et leur place dans la structure de la loi.\nEnfin, les articles de droit ne sont pas de petits paragraphes constituant généralement l'unité d'extraction typique dans la plupart des travaux d'extraction.\nIci, il y a de longs documents qui peuvent aller jusqu'à six mille mots.\nLes recent advances en matière de TAL traitement automatique du langage naturel ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prévention du jugement juridique ou l'examen des contrats de contacts automatisés.\nMais l'extraction de l'article de droit est restée essentiellement inchangée en raison du manque de grandes données étiquetées de haute qualité.\nDans ce travail, nous présentons de nouvelles données centrées sur le citoyen natif français pour étudier si l'extraction des modèles peut se rapprocher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche d'extraction de l'article de droit.\nNos données d'extraction de l'article de droit belge BSARD se composent de plus de mille cent questions juridiques posées par des citoyens belges.\nCes questions couvrent un wide range de sujets allant de la famille au logement, à l'argent, au travail et à la sécurité sociale.\nChacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques de codes de droit belge.\nParlons maintenant de la façon dont nous avons collecté ces données.\nTout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques.\nNous avons examiné trente-deux codes belges accessibles au public et extrait tous les articles ainsi que les titres de section correspondants.\nEnsuite, nous avons rassemblé les questions juridiques avec les références aux lois pertinentes.\nPour ce faire, nous nous associons au cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur une question juridique personnelle.\nNous avons eu la chance d'avoir accès à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges.\nNous avons recueilli des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux lois pertinentes.\nEnfin, nous avons fait passer les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avons considérés.\nLes références restantes ont été appariées et converties aux identifiants d'article correspondants de notre corpus.\nNous nous sommes finalement retrouvés avec mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de droit.\nDe plus, chaque question est accompagnée de la catégorie principale et d'un enchaînement de sous-catégories.\nEt chaque article comporte un enchaînement de la rubrique de sous-séquence dans la structure de la loi.\nCes informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour des recherches futures sur l'extraction d'informations juridiques ou la classification de textes juridiques.\nJetons un coup d'œil à certaines caractéristiques de nos données.\nLes questions comportent entre cinq et quarante-quatre mots avec une moyenne de quatorze mots.\nLes articles sont beaucoup plus longs avec une longueur moyenne de soixante-dix-sept mots, avec cent quarante-deux d'entre eux dépassant les mille mots.\nLe plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots.\nComme mentionné précédemment, les questions couvrent un wide range de sujets, dont environ quatre-vingt-cinq pour cent concernent la famille, le logement, l'argent ou la justice.\nAlors que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail.\nL'article est également très diversifié car il provient de trente-deux codes belges différents qui couvrent un grand nombre de sujets juridiques.\nVoici le nombre total d'articles collectés à partir de chacun de ces codes belges.\nSur les vingt-deux mille six cent trente-trois articles, seuls mille six cent douze sont désignés comme pertinents à au moins une question dans les données.\nEt environ quatre-vingts pour cent de ces articles cités proviennent du code civil, des codes judiciaires, des codes d'enquête criminelle ou des codes pénaux.\nPendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question.\nCe qui peut s'expliquer par le fait que ces codes se concentraient moins sur les individus et leurs préoccupations.\nDans l'ensemble, le nombre moyen de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.\nEn utilisant toutes les données, nous avons comparé plusieurs approches d'extraction, y compris l'architecture lexicale et dense.\nÉtant donné une requête et un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de requête des poids de chacun de ces termes dans cet article.\nNous expérimentons avec les fonctions de classement TF-IDF et BM25 standard.\nLe principal problème de ces approches est qu'elles ne peuvent extraire que les articles contenant des mots-clés présents dans la requête.\nPour surmonter cette limitation, nous expérimentons une architecture neuronale qui peut capturer les relations sémantiques entre les requêtes et l'article.\nNous utilisons un modèle bi-encodeur qui cartographie les requêtes et les articles en représentations vecteurs denses et calculons un score de pertinence entre une paire d'articles de requête par la similitude de leurs intégrations.\nCes intégrations résultent typiquement d'une opération de pooling sur la sortie d'un modèle d'intégration de mots.\nTout d'abord, nous étudions l'efficacité des bi-encodeurs siamois dans une configuration d'évaluation « zero shot », ce qui signifie que des modèles d'intégration de mots préformés sont appliqués immédiatement sans aucun raffinement supplémentaire.\nNous expérimentons avec l'encodeur de texte indépendant du contexte, à savoir word2vec et fastText, et des modèles d'intégration dépendants du contexte, à savoir Roberta et plus précisément CamemBERT qui est un modèle Roberta français.\nDe plus, nous formons nos propres bi-encodeurs de modèles basés sur CamemBERT sur nos données.\nNotez que pour la formation, nous expérimentons avec les deux modèles de l'architecture bi-encodeur.\nLe siamois, qui utilise un modèle d'intégration de mots unique cartographiant la requête et l'article ensemble dans un vector space dense partagé, et le modèle à deux tours, qui utilise deux modèles d'intégration de mots indépendants encodant la requête et l'article séparément dans différents espaces d'intégration.\nNous expérimentons la mise en commun moyenne, maximale et CLS, ainsi que le produit et cosinus pour calculer les similitudes.\nVoici le résultat de notre base sur les ensembles de test.\nAvec les méthodes lexicales ci-dessus, les bi-encodeurs siamois ont évalué dans une configuration zero shot au milieu, et les bi-encodeurs raffinés ci-dessous.\nDans l'ensemble, le bi-encodeur raffiné dépasse de manière significative toutes les autres bases.\nLe modèle à deux tours s'améliore par rapport à ses variantes siamoises lors du rappel à cent, mais fonctionne de la même manière sur les autres indicateurs.\nBien que BM25 ait sous-performé le bi-encodeur formé de manière significative, ses performances indiquent qu'il s'agit toujours d'une base solide pour l'extraction spécifique au domaine.\nEn ce qui concerne l'évaluation zero shot du bi-encodeur siamois, nous constatons que l'utilisation directe des intégrations d'un modèle CamemBERT préformé sans optimiser pour la tâche d'extraction d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats antérieurs.\nEn outre, nous observons que le bi-encodeur basé sur word2vec a dépassé de manière significative les modèles basés sur fastText et les Représentations d'encodeurs bidirectionnels à partir de transformateurs, suggérant que peut-être les intégrations de niveau mot préformées sont plus appropriées pour la tâche que les intégrations de niveau caractère ou sous-mot lorsqu'elles sont utilisées immédiatement.\nBien que prometteurs, ces résultats suggèrent de nombreuses possibilités d'amélioration comparé à un expert juridique qualifié qui peut éventuellement extraire tous les articles pertinents à n'importe quelle question, et ainsi obtenir des scores parfaits.\nConcluons en discutant de deux limites de nos données.\nPremièrement, le corpus d'article est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants.\nAu cours de la construction des données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions ne se retrouvent qu'avec une fraction du nombre initial d'articles pertinents.\nCette information implique donc que la réponse contenue dans les autres articles pertinents pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.\nDeuxièmement, il convient de noter qu'on ne peut pas répondre à toutes les questions juridiques uniquement par des lois.\nPar exemple, la question « puis-je expulser mes locataires s'ils font trop de bruit ? »\nElle peut ne pas avoir de réponse détaillée dans le droit écrit qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée.\nAu lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des antécédents similaires à sa situation actuelle.\nPar exemple, les locataires font la fête deux fois par semaine jusqu'à deux heures du matin.\nPar conséquent, certaines questions sont mieux adaptées que d'autres à la tâche d'extraction de l'article de droit, et le domaine des moins adaptées reste à déterminer.\nNous espérons que nos travaux susciteront l'intérêt pour l'élaboration de modèles d'extraction d'article de droit pratiques et fiables.\nCela peut aider à améliorer l'accès à la justice pour tous.\nVous pouvez consulter notre article, nos données et notre code aux liens suivants. Merci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.567", "sample_id": 419, "src_audio": "/acl6060/audio/en/419.wav", "src_ref": "Hello, we are happy to present our work on VALSE; a Task-Independent Benchmark meant for testing vision and language models with specific linguistic phenomena.\nWhy did we do the trouble in setting up this benchmark?\nWell, during the last years, we have seen an explosion of transformer based vision and language models pretrained on large amounts of image text pairs.\nEach one of these models pushes state-of-the-art on vision and language tasks such as visual question answering, visual common sense reasoning, image retrieval, phrase grounding.\nSo we got a message, the accuracies on these tasks and specific benchmarks are increasing steadily.\nBut do we know what the models have actually learned?\nWhat is it that a vision and language transformer understood when assigning a high score for this image and this sentence to match?\nAnd the low score for this one?\nDo vision and language models focus on the right thing?\nOr do they focus on biases as shown by previous work?\nTo shed more light on this aspect, we propose a more task agnostic direction and introduce VALSE that tests the sensitivity of vision and language models to specific linguistic phenomena that affect both the linguistic and the visual modalities.\nWe target existence, plurality, counting, spatial relations, actions and entity coreference.\nBut how do we test whether the vision and language models have captured this phenomena?\nBy foiling a method previously applied for vision and language models only for noun phrases by Ravi Shekhar and collaborators, and on counting by us in previous work.\nFoiling basically means that we take the caption of an image and produce a foil by altering the caption such that it does not describe the image anymore.\nAnd we do these phrase alterations by focusing on six specific pieces such as existence, plurality, counting, spatial relations, actions and entity coreference, where each piece can consist of one or more instruments, in case we found more than one interesting way to create foil instances.\nFor example, in the case of the actions piece, we have two instruments, one in which the action verb is changed with a different action, and one in which actants are swapped.\nCounting and coreference also are pieces that have more than one instrument.\nAnd we create these foils by making sure that they fail to describe the image, that they are grammatical, and otherwise valid sentences.\nThis is not easy to do because a foiled caption may be less likely than the original caption.\nFor example, though it's not impossible, it is statistically less likely for plants to cut a man than a man to cut plants, and large vision and language models could pick up on this.\nTherefore, to obtain valid foils, we must take action.\nFirst, we make use of strong language models to propose foils.\nSecond, we use natural language inference or short NLI to filter out foils that could be still describing the image, since when constructing foils we need to ensure that they fail to describe the image.\nTo test this automatically, we apply natural language inference with the following rationale.\nWe consider an image to be the premise and its caption its entailed hypothesis.\nIn addition, we consider the caption to be the premise, and the foil is its hypothesis.\nIf an NLI model predicts the foil to contradict or to be neutral with respect to the caption, we take this as an indicator of a valid foil.\nIf an NLI predicts the foil to be entailed by the caption, it cannot be a good foil, since by transitivity it will give a truthful description of the image, and we filter these foils out.\nBut this procedure is not perfect, it is just an indicator for valid foils.\nTherefore, as a third measure for generating valid foils, we employ human annotators to validate the data used in VALSE.\nSo, after filtering and human evaluation, we have as many test instances as described in this table.\nNote that VALSE does not deliver any training data but only test data.\nSince it is a zero shot testing benchmark only, it is designed to leverage the existing capabilities of vision and language models after pretraining.\nFinetuning would only enable models to exploit artifacts or statistical biases in the data.\nAnd we all know that these models like to cheat and take shortcuts.\nAnd as we said, we are interested in assessing what capabilities the vision and language models have after pretraining.\nWe experiment with five vision and language models on VALSE, namely with CLIP, LXMert, ViLBERT, ViLBERT twelve in one, and VisualBERT.\nTwo of our most important evaluation metrics are the accuracy of the models in classifying image sentence pairs into captions and foils.\nPerhaps more relevant for this video, we will showcase our more permissive metric, the pairwise accuracy, which measures whether the image sentence alignment score is greater for the correct image text pair than for its foiled pair.\nFor more metrics and results on them, do check out our paper.\nThe results with pairwise accuracy are shown here and they are consistent with the results we got from the other metrics is that the best zero shot performance is achieved by ViLBERT twelve in one, followed by ViLBERT, LXMert, CLIP, and finally VisualBERT.\nIt's notable how instruments centered on the individual objects like existence and noun phrases are almost solved by ViLBERT twelve in one, highlighting that models are capable of identifying named objects and their presence in images.\nHowever, none of the remaining pieces can be reliably solved in our adversarial foiling settings.\nWe see from the plurality and counting instruments that vision and language models have trouble distinguishing references to single versus multiple objects, or counting them in an image.\nThe relation piece shows that they have difficulties in correctly classifying a named spatial relation between objects in an image.\nThey also have trouble distinguishing actions and identifying their participants, even if supported by plausibility biases as we see in the actions piece.\nFrom the coreference piece, we find out that tracing multiple references to the same object in an image by using pronouns is also difficult for vision and language models.\nAs a sanity check, and because it's an interesting experiment, we also benchmark two text only models, GPT one and GPT two, to assess whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and the foiled caption, no image here, and predicting the entry with the lowest perplexity.\nIf the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases.\nAnd it's interesting to see that in some cases, the text only GPT models have captured the plausibility of the world better than the vision and language models.\nSo to sum up, VALSE is a benchmark that uses the lens of linguistic constructs to help the community improve vision and language models by hard testing their visual grounding capabilities.\nOur experiments show that vision and language models identify named objects and their presence in images well, as shown by the existence piece, but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators.\nWe would really like to encourage the community to use VALSE for measuring progress towards language grounding with vision and language models.\nAnd even more, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or finetuning to see whether a dataset helps models improve on any of the aspects tested by VALSE.\nIf you're interested, do check out the VALSE data on GitHub, and if you have any questions do not hesitate to contact us.", "tgt_ref": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE ; un indice de référence indépendant de la tâche destiné à tester les modèles de langue et de vision avec des phénomènes linguistiques spécifiques.\nPourquoi avons-nous pris la peine de mettre en place cet indice de référence ?\nEh bien, au cours des dernières années, nous avons vu une explosion de la vision basée sur le transformateur et des modèles de langue préformés sur de grandes quantités de paires de textes d'images.\nChacun de ces modèles pousse les tâches de pointe en matière de vision et de langue telles que la réponse aux questions visuelles, le raisonnement de bon sens visuel, l'extraction d'images et les bases de phrases.\nNous avons donc reçu un message : les précisions sur ces tâches et les indices de référence spécifiques augmentent régulièrement.\nMais savons-nous ce que les modèles ont réellement appris ?\nQu'est-ce qu'une vision et un transformateur de langue ont compris lors de l'attribution d'un score élevé pour que cette image et cette phrase correspondent ?\nEt le score bas pour celle-ci ?\nEst-ce que les modèles de langue et de vision se concentrent sur la bonne chose ?\nOu se concentrent-ils sur les biais comme le montre le travail antérieur ?\nPour éclairer davantage cet aspect, nous proposons une direction plus indépendante des tâches et introduisons VALSE qui teste la sensibilité des modèles de langue et de vision à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistiques et visuelles.\nNous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités.\nMais comment tester si les modèles de langue et de vision ont capturé ce phénomène ?\nEn pratiquant le foiling sur une méthode précédemment appliquée pour les modèles de langue et de vision seulement pour des phrases nominales de Ravi Shekhar et de ses collaborateurs, et en comptant par nous dans les travaux antérieurs.\nEffectuer un foiling signifie essentiellement que nous prenons la légende d'une image et produisons un foil en modifiant la légende de sorte qu'elle ne décrive plus l'image.\nEt nous apportons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous aurions trouvé plus d'une façon intéressante de créer des instances de foil.\nPar exemple, dans le cas de l'élément d'actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié avec une action différente, et un dans lequel les acteurs sont échangés.\nLe comptage et la coréférence sont également des éléments qui ont plusieurs instruments.\nEt nous créons ces foils en nous assurant qu'ils ne décrivent pas l'image et que ce sont des phrases grammaticales et autrement valides.\nCe n'est pas facile à faire car une légende ayant subi un foil peut être moins probable que la légende originale.\nPar exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que les plantes coupent un homme qu'un homme coupe les plantes, et la grande vision et les modèles de langue pourraient capter cela.\nPar conséquent, pour obtenir des foils valides, nous devons agir.\nTout d'abord, nous utilisons des modèles de langue forts pour proposer des foils.\nDeuxièmement, nous utilisons l'inférence de la langue naturelle ou la NLI courte pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image.\nPour tester cela automatiquement, nous appliquons l'inférence de la langue naturelle avec la justification suivante.\nNous considérons qu'une image est la prémisse et sa légende son hypothèse implicite.\nEn outre, nous considérons la légende comme la prémisse, et le foil est son hypothèse.\nSi un modèle NLI prévient que le foil est en contradiction ou neutre par rapport à la légende, nous considérons cela comme un indicateur d'un foil valide.\nSi une NLI prévient le foil à engendrer par la légende, cela ne peut pas être un bon foil, car par transitivité, cela donnera une description véridique de l'image et nous filtrons ces foils.\nMais cette procédure n'est pas parfaite, il s'agit simplement d'un indicateur pour les foils valides.\nPar conséquent, comme troisième mesure pour produire des foils valides, nous utilisons des annotateurs humains pour valider les données utilisées dans VALSE.\nDonc, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrit dans ce tableau.\nNotez que VALSE ne fournit pas de données de formation mais seulement des données de test.\nComme il s'agit uniquement d'un indicateur de référence de test zero shot, il est conçu pour tirer parti des capacités existantes des modèles de langue et de vision après la préformation.\nLe raffinement permettrait seulement aux modèles d'exploiter des artefacts ou des biais statistiques dans les données.\nEt nous savons tous que ces modèles aiment tricher et prendre des raccourcis.\nEt comme nous l'avons dit, cela nous intéresse d'évaluer les capacités des modèles de langue et de vision après la préformation.\nNous expérimentons avec cinq modèles de langue et de vision sur VALSE, à savoir avec CLIP, LXMert, ViLBERT, ViLBERT douze en un, et VisualBERT.\nDeux de nos indicateurs d'évaluation les plus importants sont la précision des modèles dans la classification de paires de phrases images en légendes et foils.\nPeut-être plus pertinent pour cette vidéo, nous présenterons notre indicateur plus permissif, la précision par paire, qui mesure si le score de sentence alignment d'image est plus élevé pour la bonne paire de texte image que pour sa paire ayant subi un foil.\nPour plus d'indicateurs et de résultats, consultez notre article.\nLes résultats avec une précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres indicateurs. La meilleure performance zero shot est obtenue par ViLBERT douze en un, suivi de ViLBERT, LXMert, CLIP, et enfin VisualBERT.\nIl est remarquable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par ViLBERT douze en un, en soulignant que les modèles sont capables d'identifier des objets nommés et leur présence dans les images.\nCependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de foiling antagonistes.\nNous voyons à partir des instruments de pluralité et de comptage que les modèles de langue et de vision ont du mal à distinguer les références à des objets uniques par rapport à plusieurs, ou à les compter dans une image.\nL'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image.\nIls ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité comme nous le voyons dans l'élément d'actions.\nÀ partir de l'élément de coréférence, nous découvrons que relever plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de langue et de vision.\nPour vérifier la santé mentale, et parce que c'est une expérience intéressante, nous comparons également deux modèles à texte seulement, GPT un et GPT deux, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et ayant subi un foil, sans image ici, et en prévenant l'entrée avec la plus faible perplexité.\nSi la perplexité est plus élevée pour le foil, nous considérons cela comme une indication que la légende ayant subi un foil peut souffrir de biais de plausibilité ou d'autres biais linguistiques.\nEt il est intéressant de voir que dans certains cas, les modèles GPT à texte seulement ont capturé la plausibilité du monde mieux que les modèles de langue et de vision.\nDonc, pour résumer, VALSE est un indicateur de référence qui utilise l'objectif des constructions linguistiques pour aider la communauté à améliorer les modèles de langue et de vision en testant durement leurs capacités de bases visuelles.\nNos expériences montrent que les modèles de langue et de vision identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais luttent pour ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques.\nNous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers les bases de la langue avec des modèles de langue et de vision.\nEt plus encore, VALSE pourrait être utilisé comme une évaluation indirecte des données, car les modèles pourraient être évalués avant et après la formation ou le raffinement pour voir si des données aident les modèles à améliorer l'un des aspects testés par VALSE.\nSi vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
{"dataset_id": "acl_6060", "doc_id": "2022.acl-long.597", "sample_id": 420, "src_audio": "/acl6060/audio/en/420.wav", "src_ref": "Hello, my name is Kamezawa from the University of Tokyo.\nI'll be presenting a paper entitled RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization.\nI'll be explaining in this order.\nFirst, I will introduce automatic release note generation that we are working on in this research.\nA release note is a technical document that summarizes the changes distributed with each release of a software product.\nThe image shows a release note for version two point six point four of the vuejs library.\nRelease notes play an important role in open source development but they're time consuming to prepare manually.\nTherefore, it would be very useful to be able to automatically generate high quality release notes.\nI will defer to two previous researches on automatic release note generation.\nThe first is a system called ARENA released in twenty fourteen.\nIt takes a rule-based approach, for example using the change extractor to extract all differences, library changes and document changes from the differences between releases, and finally combining them.\nThe most notable feature of this system is the issue extractor in the upper right corner.\nWhich must be left to Jira, the issue tracker system, and can only be applied to projects that use Jira.\nIn other words, it cannot be used for many projects on GitHub.\nThe second is Glyph, recently announced in twenty twenty.\nIt is available on the internet and can be installed via pip.\nThis system has a simple learning based text classification model and outputs one of five labels such as features or bug fixes for each input commit message.\nThis image is a sample usage that returns a corrective or bug fixes label.\nGlyph's training data is fairly small, about five thousand, and will be shown in the experiments described below.\nThe performance of the text classification model is not high.\nI present two related researches, but their problems are limited applicability and scarce data resources.\nOur paper solves these two problems and automatically generates high quality release notes.\nWith a limited applicability problem, we propose a high quality classwise summarization method using only commit messages as input.\nThis proposed method can be used for all English repositories.\nFor the second problem of scarce data resources, we built our RNSum dataset consisting of about eighty two thousand pieces of data by collecting data from public GitHub repositories using the GitHub API.\nNext, I'll describe our dataset.\nHere is an example of data.\nThe left side is a commit message and the right side is the release notes.\nRelease notes are labeled as improvements or fixes, etc.\nWe have set up a task that takes the commit messages as input and outputs a labeled release notes.\nThis can be regarded as a summarization task.\nWe have predefined four labels: features, improvements, bug fixes, deprecations removals and breaking changes.\nThese were set based on previous research and other factors.\nThe release note on the bottom right is extracted from the release note on the bottom left.\nAt this time, it is necessary to detect the four labels that have been set up in advance.\nBut the labels are not always consistent with each repository.\nFor example, the improvements label includes improvements, enhancements, optimizations, and so on.\nWe prepared a vocabulary list of about thirty labels for each of these notational variations.\nThis is to detect the release note class, and collects the text of the release that follows as the release note sentence for the class.\nNext is a commit message.\nCommit messages are not tied to each release.\nAs shown in the image below, if the current release is version two point five to nineteen, we need to identify the previous release version two point five to eighteen and get a diff.\nThis is a bit tedious and it is not enough to just get a list of releases and look at the before and after.\nWe created a heuristic matching rule to get the previous and next versions.\nDataset analysis.\nIn the end, seven thousand two hundred repositories and eighty two thousand pieces of data were collected.\nAlso, the average number of release notes tokens is sixty three, which is quite high for a summarization task.\nAlso, the number of unique tokens is quite large at eight thousand eight hundred thirty thousand.\nThis is due to the large number of unique class or method names found in the repository.\nNext, I will explain the proposed method.\nThe classwise extractive then abstractive summarization model consists of two neural modules.\nA classifier using BERT or CodeBERT and a generator using BART.\nFirst, CEAS uses a classifier to classify each commit message into five release notes classes, which use improvements, bug fixes, deprecations, plus an other.\nThe commit messages classified as other are discarded.\nThen CEAS applies the generator to the four labeled documents independently and generates release notes for each class.\nIn this task, the direct correspondences between commit messages and release notes are not known.\nTherefore, to train the classifier, that's why we reassigned surveys to each input commit message using the first ten characters of each commit message.\nWe modeled the classwise abstractive summarization approach by two different methods.\nThe first model, which we call CAS-Single, consists of a single six to six network and generates a single release note text give a concatenation of input commit messages.\nThe output texts can be divided into classwise segments based on special class-specific endpoint symbols.\nThe second method, method, which we call CAS-Multi, consists of four different seq2seq networks, each of which correspond to one of the fixed release note classes.\nOkay, let me explain the experiments.\nFive methods were compared: CEAS, CAS-Single, CAS-Multi, Clustering, and previous study, Glyph.\nRegarding evaluation, in some cases, release notes are output in multiple sentences.\nSince it is difficult to calculate the number of sentences as they are, they are combined with spaces and treated as one long sentence.\nThe BLEU is penalized when the system outputs a short sentence.\nThis penalty results in a lower BLEU value in the experiment results described next.\nFinally, we also calculate the specificity because ROUGE and BLEU cannot be calculated if the release notes are empty.\nA higher specificity means that the model correctly outputs an empty text in cases where the release notes assume empty.\nHere are the results.\nSince the dataset contains e-mail addresses, hashed values, etc, we also evaluated the cleaned dataset, which excludes them.\nCEAS and CAS achieved ROUGE-L scores more than ten points higher than the baselines.\nIn particular, on the clean test set, the score gap between the proposed method and the baselines jumped to more than twenty points.\nThese results indicate that CEAS and CAS are significantly affected.\nCEAS got a better ROUGE-L score than CAS suggesting that combining a classifier and a generator is effective on training the classifier using pseudo labels.\nHigh coverage of CEAS can be achieved probably because the classifier can focus on selecting relevant commit messages for each class.\nCAS-Multi tended to yield higher ROUGE-L than CAS-Single.\nSuggesting that it is also effective to independently develop differently abstractive summarization models for each release note class.\nHere are an error analysis.\nCAS methods tend to output shorter sentences than human reference sentences.\nIn the figure on the right, the reference sentence has three or four sentences, while CAS has only one.\nThe reason for this model's reluctance is that in training data, only thirty three percent of the sentences are present in the features label and forty percent in the improvements label.\nFurthermore, CAS methods cannot generate accurate release notes without additional information.\nThe top example on the right is an example of a very messy commit message, and the complete sentence cannot be generated without reference to the corresponding progress or issue.\nThe example below shows that the two commit messages in the input are related and should be combined into one sentence, but it fails to do so.\nFinally, a conclusion.\nWe have built a new dataset for automatic release note generation.\nWe have also formulated a task of entering commit messages and summarizing them so that it is applicable to all projects written in English.\nOur experiments show that the proposed method generates less noisy release notes at higher coverage than the baselines.\nPlease check out our dataset on GitHub.\nThank you.", "tgt_ref": "Bonjour, je m'appelle Kamezawa de l'Université de Tokyo.\nJe vais vous présenter un article intitulé RNSum : des données à grande échelle pour la génération automatique de note de version via la synthèse des journaux de validation.\nJe vais vous expliquer dans cet ordre.\nTout d'abord, je vais présenter la génération de note de version automatique sur laquelle nous travaillons dans cette recherche.\nUne note de version est un document technique qui résume les modifications distribuées à chaque version d'un produit logiciel.\nL'image montre une note de version pour la version deux point six point quatre de la bibliothèque Vuejs.\nLes notes de version jouent un rôle important dans le développement open source, mais elles mettent du temps à être préparées manuellement.\nPar conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.\nJe m'en remettrai à deux recherches antérieures sur la génération automatique de notes de version.\nLa première est un système appelé ARENA sorti en deux-mille-quatorze.\nIl faut une approche basée sur des règles, par exemple, en utilisant l'extracteur de changement pour extraire toutes les différences, les changements de bibliothèque et les changements de document à partir des différences entre les versions, et enfin, en les combinant.\nLa caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit.\nLe système de suivi des problèmes, qui doit être laissé à Jira, ne peut être appliqué qu'aux projets qui utilisent Jira.\nEn d'autres mots, il ne peut pas être utilisé pour de nombreux projets sur GitHub.\nLa seconde est Glyphe, récemment annoncée en deux-mille-vingt.\nElle est disponible sur internet et peut être installée via pip.\nCe système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'une des cinq étiquettes telles que les fonctions ou corrections de bugs pour chaque message de validation de saisie.\nCette image est un exemple d'utilisation qui renvoie une étiquette de correction ou de débogage.\nLes données de formation de Glyphe sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous.\nLes performances du modèle de classification de texte ne sont pas élevées.\nJe présente deux recherches connexes, mais leurs problèmes sont l'applicabilité limitée et les ressources de données rares.\nNotre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité.\nAvec un problème d'applicabilité limitée, nous proposons une méthode de synthèse par classe de haute qualité en utilisant uniquement des messages de validation comme saisie.\nCette méthode proposée peut être utilisée pour tous les référentiels anglais.\nPour le deuxième problème de ressources de données rares, nous avons construit nos données RNSum composées d'environ quatre-vingt-deux mille éléments de données en collectant des données à partir de référentiels GitHub publics en utilisant l'API GitHub.\nEnsuite, je vais décrire nos données.\nVoici un exemple de données.\nLe côté gauche est un message de validation et le côté droit représente les notes de version.\nLes notes de version sont étiquetées comme améliorations ou correctifs, etc.\nNous avons mis en place une tâche qui prend les messages de validation comme saisie et sorties des notes de version étiquetées.\nCela peut être considéré comme une tâche de synthèse.\nNous avons prédéfini quatre étiquettes : fonctions, améliorations, corrections de bugs, suppressions de dépréciations et modifications de rupture.\nCelles-ci ont été établies sur la base de la recherche antérieure et d'autres facteurs.\nLa note de version en bas à droite est extraite de la note de version en bas à gauche.\nÀ ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été mises en place à l'avance.\nMais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel.\nPar exemple, l'étiquette d'améliorations inclut des améliorations, des perfectionnements, des optimisations, etc.\nNous avons préparé une liste de vocabulaire d'une trentaine d'étiquettes pour chacune de ces variations de notation.\nIl s'agit de détecter la classe de note de version et de collecter le texte de la version qui suit en tant que phrase de note de version pour la classe.\nEnsuite, il y a un message de validation.\nLes messages de validation ne sont pas liés à chaque version.\nComme le montre l'image ci-dessous, si la version actuelle est la version deux point cinq à dix-neuf, nous devons identifier la version deux point cinq à dix-huit antérieure et obtenir un diff.\nC'est un peu fastidieux et il ne suffit pas d'obtenir une liste de versions et de regarder l'avant et l'après.\nNous avons créé une règle de correspondance heuristique pour obtenir les versions antérieures et suivantes.\nAnalyse des données.\nEn fin de compte, sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données ont été recueillis.\nEn outre, le nombre moyen de gages de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse.\nDe même, le nombre de gages uniques est assez grand, s'élevant à huit mille huit cent trente mille.\nCela est dû au grand nombre de noms de méthode ou de classe unique trouvés dans le référentiel.\nEnsuite, je vais expliquer la méthode proposée.\nLe modèle de synthèse extractive puis abstractive par classe se compose de deux modules neuronaux.\nUn classificateur utilisant BERT ou CodeBERT et un générateur utilisant BART.\nTout d'abord, CEAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, qui utilisent des améliorations, des corrections de bogues, des dépréciations et d'autres.\nLes messages de validation classés comme autres sont supprimés.\nEnsuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.\nDans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues.\nPar conséquent, pour former le classificateur, nous avons réaffecté les enquêtes à chaque message de validation de saisie en utilisant les dix premiers caractères de chaque message de validation.\nNous avons modélisé l'approche de synthèse abstractive par classe à travers deux méthodes différentes.\nLe premier modèle, que nous appelons CAS unique, se compose d'un réseau de six à six unique et génère un seul texte de note de version donnant un enchaînement de messages de validation de saisie.\nLes textes de sortie peuvent être divisés en segments par classe sur la base de symboles de point d'extrémité particuliers, spécifiques à la classe.\nLa seconde méthode, que nous appelons CAS multiple, se compose de quatre réseaux seq2seq différents, dont chacun correspond à l'une des classes de notes de version fixes.\nOk, je vais vous expliquer les expériences.\nCinq méthodes ont été comparées : CEAS, CAS unique, CAS multiple, regroupement et étude antérieure, Glyph.\nEn ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases.\nPuisqu'il est difficile de calculer le nombre de phrases telles qu'elles sont, elles sont combinées avec des espaces et traitées comme une seule phrase longue.\nLe BLEU est pénalisé lorsque le système produit une phrase courte.\nCette pénalité se traduit par une valeur BLEU inférieure dans les résultats de l'expérience décrits ci-dessous.\nEnfin, nous calculons également la spécificité car ROUGE et BLEU ne peuvent pas être calculés si les notes de version sont vides.\nUne spécificité plus élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées être vides.\nLes résultats sont indiqués ci-après.\nÉtant donné que les données contiennent des adresses électroniques, des valeurs hachées, etc., nous avons également évalué les données nettoyées, ce qui les exclut.\nLe CEAS et le CAS ont obtenu des scores de ROUGE-L supérieurs de plus de dix points par rapport aux bases.\nEn particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les bases a atteint plus de vingt points.\nCes résultats indiquent que le CEAS et le CAS sont considérablement touchés.\nLe CEAS a obtenu un meilleur score ROUGE-L que le CAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour former le classificateur à l'aide de pseudo-étiquettes.\nUne couverture élevée du CEAS peut être obtenue probablement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.\nLe CAS multiple tendait à produire plus de ROUGE-L que le CAS unique.\nEn suggérant qu'il est également efficace de développer indépendamment et différemment des modèles de abstractive summarization pour chaque classe de note de version.\nVoici une analyse d'erreur.\nLes méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines.\nDans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que le CAS n'en a qu'une.\nLa raison de la réticence de ce modèle est que dans les données de formation, seulement trente-trois pour cent des phrases sont présentes dans l'étiquette des fonctions et quarante pour cent dans l'étiquette des améliorations.\nEn outre, les méthodes CAS ne peuvent pas générer des notes de version précises sans informations supplémentaires.\nL'exemple en haut à droite est un exemple de message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la progression ou au problème correspondant(e).\nL'exemple ci-dessous montre que les deux messages de validation dans la saisie sont liés et doivent être combinés en une phrase, mais cela n'est pas fait.\nEnfin, passons à la conclusion.\nNous avons construit de nouvelles données pour la génération automatique de notes de version.\nNous avons également formulé une tâche consistant à saisir des messages de validation et à les résumer afin qu'ils soient applicables à tous les projets écrits en anglais.\nNos expériences montrent que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les bases.\nVeuillez consulter nos données sur GitHub.\nMerci.", "src_lang": "en", "tgt_lang": "fr", "benchmark_metadata": {"context": "long", "dataset_type": "longform", "subset": "eval"}}
