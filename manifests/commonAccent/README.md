# CommonAccent

## Overview

CommonAccent is subsection of Common Voice (version 11) that has been cleanly labeled with accent information for French, Spanish, English, and Italian. Though originally used for an accent classification task, the dataset contains transcripts of the speech, so it can be used for testing the accent-robustness of ASR and (with reference-free metrics) ST systems. The data are not parallel between langauges, and the number of accent labels used for each language varies. 

```bibtex
@inproceedings{Zuluaga-GomezAV23,
  author       = {Juan Zuluaga{-}Gomez and
                  Sara Ahmed and
                  Danielius Visockas and
                  Cem Subakan},
  editor       = {Naomi Harte and
                  Julie Carson{-}Berndsen and
                  Gareth Jones},
  title        = {CommonAccent: Exploring Large Acoustic Pretrained Models for Accent
                  Classification Based on Common Voice},
  booktitle    = {24th Annual Conference of the International Speech Communication Association,
                  Interspeech 2023, Dublin, Ireland, August 20-24, 2023},
  pages        = {5291--5295},
  publisher    = {{ISCA}},
  year         = {2023},
  url          = {https://doi.org/10.21437/Interspeech.2023-2419},
  doi          = {10.21437/INTERSPEECH.2023-2419},
  timestamp    = {Fri, 14 Jun 2024 23:24:49 +0200},
  biburl       = {https://dblp.org/rec/conf/interspeech/Zuluaga-GomezAV23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
```

## Instructions

Use the following instructions to _reproduce_ the .jsonl files in `manifests/commonAccent` and download the audio samples for use, but note that because of the size of CommonVoice datasets, this is very time consuming. To directly access the audio referred to in the .jsonl files, see below. 

The script `generate.py` is used to access and download the relevant audio files for CommonAccent from CommonVoice v11.0. It also creates the jsonl files to be used as input for inference for each language pair. The langauge pairs from this benchmark are:

- de-en
- es-en
- it-en

- en-es
- en-it
- en-fr
- en-de
- en-pt
- en-nl
- en-zh

The `tgt_ref` field will always be "null" as the dataset has no reference translations. We can therefore generate translations in all possible target langauges. 

Define the path where commonAccent will be stored:

```bash
export H2T_DATADIR=""
```

Run the python script to generate the jsonl files and save the relevant audio files. 

```bash
python3 ./generate.py
```

**To directly use the downloaded audio files:**

Download the data from the relevant github release:

```bash
wget -O audio.zip https://github.com/sarapapi/hearing2translate/releases/download/data-share-commonAccent/audio.zip
```

This will contain the data that would be stored in the `manifests/commonAccent/audio` directory generated by running `generate.py` so you can directly carry out inference. 

## Expected Output

After running the steps above, your directory layout will be:

```
${H2T_DATADIR}/
└─ commonAccent/
   └─ audio/
      └─ en/
      │  ├─ 25076709.wav
      │  ├─ 25076711.wav
      │  └─ ...
      └─ de/
      │  ├─ 19453594.wav
      │  ├─ 19453600.wav
      │  └─ ...
      └─ ...
```

If your generate.py script writes manifests, you should get JSONL files (one per language pair) under your chosen output path (e.g., ./manifests/commonAccent/). A jsonl entry looks like:


```json
{
  "dataset_id": "commonAccent",
  "sample_id": "<string>",
  "src_audio": "/commonAccent/audio/<src_lang>/<audio_file>",
  "src_ref": "<source raw_transcription>",
  "tgt_ref": "<target raw_transcription>",
  "src_lang": "<two-letter ISO 639-1>",
  "tgt_lang": "<two-letter ISO 639-1>",
  "benchmark_metadata": {
    "acc": "<string>",
    "context": "short"
    }
}
```

## License

All datasets are licensed under the Creative Commons license (CC-BY).