<?xml version="1.0" encoding="UTF-8"?>
<mteval>
<srcset setid="iwslt-ACLtest2023" srclang="English">
<doc docid="2022.acl-long.410" genre="presentations">
<talkid>2022.acl-long.410</talkid>
<abstract>Solving math word problems requires deductive reasoning over the quantities in the text. Various recent research efforts mostly relied on sequence-to-sequence or sequence-to-tree models to generate mathematical expressions without explicitly performing relational reasoning between quantities in the given context. While empirically effective, such approaches typically do not provide explanations for the generated expressions. In this work, we view the task as a complex relation extraction problem, proposing a novel approach that presents explainable deductive reasoning steps to iteratively construct target expressions, where each step involves a primitive operation over two quantities defining their relation. Through extensive experiments on four benchmark datasets, we show that the proposed model significantly outperforms existing strong baselines. We further demonstrate that the deductive procedure not only presents more explainable steps but also enables us to make more accurate predictions on questions that require more complex reasoning.</abstract>
<seg id="1">Bonjour tout le monde. Aujourd'hui, je vais vous présenter notre travail de recherche « Apprendre à raisonner par déduction » : résolution de problèmes de mots mathématiques comme extraction de relation complexe.</seg>
<seg id="2">Je m'appelle Allan du laboratoire d'intelligence artificielle ByteDance, et ceci est un travail en commun avec Jierui Li de l'Université du Texas à Austin et Wei Lu du SUTD.</seg>
<seg id="3">Tout d'abord, j'aimerais parler de notre motivation pour le raisonnement.</seg>
<seg id="4">Donc ici, nous montrons un exemple où le raisonnement en plusieurs étapes est utile.</seg>
<seg id="5">Ce chiffre est donc tiré de l'article PaLM où ils incitent à résoudre le problème de réseau dans le scénario d'apprentissage de quelques prises de vue.</seg>
<seg id="6">Donc, sur le côté gauche, nous pouvons voir que si nous fournissons quelques exemples avec juste des questions et des réponses, nous pourrions ne pas être en mesure d'obtenir les bonnes réponses.</seg>
<seg id="7">Mais si nous fournissons plus de description du raisonnement, le modèle est capable de prévenir la description du raisonnement, mais aussi d'effectuer ici une prévention correcte.</seg>
<seg id="8">Il est alors bon d'avoir comme résultat un raisonnement à plusieurs étapes interprétable.</seg>
<seg id="9">Et nous pensons également que les problèmes de mots mathématiques sont une application directe pour évaluer de telles capacités de raisonnement.</seg>
<seg id="10">Donc ici, dans notre configuration de problème, compte tenu des questions, nous devons résoudre cette question et obtenir les réponses numériques.</seg>
<seg id="11">Ainsi, dans nos données, nous recevons également l‘expression mathématique qui conduit à cette réponse particulière.</seg>
<seg id="12">Donc, certaines hypothèses s'appliquent également comme dans les travaux antérieurs.</seg>
<seg id="13">Nous supposons que la précision des quantités est connue.</seg>
<seg id="14">Et nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle.</seg>
<seg id="15">En outre, les opérateurs compliqués peuvent effectivement être décomposés en ces opérateurs de base.</seg>
<seg id="16">Ainsi, le travail antérieur dans la résolution de problèmes de mots mathématiques peut en effet être classé dans un modèle séquence à séquence et séquence à arbre.</seg>
<seg id="17">Le modèle séquence à séquence traditionnel convertit donc l'expression en une séquence spécifique pour la génération.</seg>
<seg id="18">Il est assez facile à mettre en œuvre et peut être généralisé à de nombreux problèmes compliqués différents.</seg>
<seg id="19">Mais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle structuré et son manque d'interopérabilité pour la prévention.</seg>
<seg id="20">Mais en réalité, cette direction est encore très populaire en raison du modèle de conversion.</seg>
<seg id="21">Donc, dans les modèles à base d'arbre, nous structurons en réalité ces expressions sous forme d'arbre et suivons une traversée préordonnée dans les générations d'arbres.</seg>
<seg id="22">Donc ici, nous continuons à produire les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités.</seg>
<seg id="23">Donc ici, la bonne chose est qu'en fait, cela nous donne cette structure d'arbre binaire. Mais en réalité, c'est assez contre-intuitif parce que nous générons d'abord l'opérateur et ensuite, à la fin, nous générons les quantités.</seg>
<seg id="24">Et la deuxième chose est que cela contient également des calculs répétitifs.</seg>
<seg id="25">Donc ici, si nous regardons cette expression, huit fois trois plus trois est en fait généré deux fois, mais en réalité, nous devrions réutiliser les résultats.</seg>
<seg id="26">Ainsi, dans notre approche proposée, nous voulons résoudre ces problèmes étape par étape et de manière interprétable.</seg>
<seg id="27">Par exemple, ici dans la deuxième étape, nous pouvons obtenir ces diviseurs qui sont vingt-sept.</seg>
<seg id="28">Et nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent.</seg>
<seg id="29">Et dans ces étapes, nous obtenons les diviseurs.</seg>
<seg id="30">Puis, à cette troisième étape, nous obtenons alors le quotient.</seg>
<seg id="31">Très bien. Et après ces trois étapes, nous pouvons alors réutiliser les résultats de la deuxième étape, puis obtenir les résultats de la quatrième étape, et enfin, nous pouvons obtenir les dividendes.</seg>
<seg id="32">Donc ici, nous générons en réalité l'expression entière directement plutôt que de produire un seul opérateur ou une seule quantité.</seg>
<seg id="33">Cela rend le processus plus précis.</seg>
<seg id="34">Ainsi, dans notre système déductif, nous commençons d'abord par un tas de quantités présentées dans les questions et incluant également une certaine constante comme notre état initial.</seg>
<seg id="35">Ainsi, l'expression est représentée par e i j o p.</seg>
<seg id="36">Où nous effectuons l'opérateur de q_i à q_j, et cette expression est en fait dirigée.</seg>
<seg id="37">Donc, nous avons également ici la soustraction avec des mots pour représenter la direction opposée.</seg>
<seg id="38">C'est tout à fait similaire à l'extraction de relation.</seg>
<seg id="39">Donc, dans un système déductif formel, à un pas de temps t, on applique l'opérateur entre la paire q_i et q_j, puis on obtient cette nouvelle expression.</seg>
<seg id="40">Nous l'ajoutons à l'état suivant pour devenir une nouvelle quantité.</seg>
<seg id="41">Ainsi, ces diapositives visualisent en réalité l'évolution de l'état où nous continuons à ajouter de l'expression à l'état actuel.</seg>
<seg id="42">Donc, dans nos implémentations de modèle, nous utilisons d'abord un modèle de langue préformée qui peut être BERT ou Roberta, puis nous encodons la phrase et obtenons ensuite ces représentations de quantité.</seg>
<seg id="43">Donc, une fois que nous obtenons les représentations de quantité, nous pouvons commencer à faire l'inférence.</seg>
<seg id="44">Nous montrons ici un exemple de q_1 pour obtenir la représentation pour q_2 divisée par q_2 multipliée par q_3.</seg>
<seg id="45">Tout d'abord, nous obtenons la représentation de paire, qui n'est essentiellement que l'enchaînement entre q_1 et q_2, puis nous appliquons un réseau prédictif qui est paramétré par l'opérateur.</seg>
<seg id="46">Et enfin, nous obtenons la représentation de l'expression q_1 divisée par q_2.</seg>
<seg id="47">Mais en réalité, dans la pratique, au stade de l'inférence, nous pourrions également obtenir une expression incorrecte.</seg>
<seg id="48">Donc ici, toute l'expression possible est égale à trois fois le nombre d'opérateurs.</seg>
<seg id="49">Alors, ce qui est bien ici, c'est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche.</seg>
<seg id="50">Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche.</seg>
<seg id="51">Donc, dans la deuxième étape, nous faisons la même chose, mais la seule différence est que nous avons une quantité de plus.</seg>
<seg id="52">Cette quantité provient donc de l'expression calculée antérieure.</seg>
<seg id="53">Finalement, nous pouvons obtenir cette expression finale q_3 multipliée par q_4.</seg>
<seg id="54">Et nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape antérieure.</seg>
<seg id="55">Ainsi, une telle différence rend difficile l'application de la beam search car la distribution de probabilité entre ces deux étapes est déséquilibrée.</seg>
<seg id="56">La procédure de formation est donc similaire à la formation d'un modèle séquence à séquence où nous optimisons la perte à chaque pas de temps.</seg>
<seg id="57">Et ici, nous utilisons également ce tau pour représenter le moment où nous devrions mettre fin à ce processus de génération.</seg>
<seg id="58">Et ici, l'espace est différent de séquence à séquence car il est différent à chaque pas de temps, alors que dans le modèle séquence à séquence traditionnel, c'est le nombre de vocabulaire.</seg>
<seg id="59">Et cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures.</seg>
<seg id="60">Nous menons donc des expériences sur les données de problèmes de mots mathématiques, MAWPS, Math23K, MathQA et SVAMP couramment utilisées.</seg>
<seg id="61">Et ici, nous montrons brièvement les résultats comparés aux meilleures approches antérieures.</seg>
<seg id="62">Donc, notre variante la plus performante est Roberta-DeuctiveReasoner.</seg>
<seg id="63">Et en réalité, nous n'utilisons pas la beam search ; au contraire, toutes les approches antérieures utilisent la beam search.</seg>
<seg id="64">Très bien. Ainsi, les meilleures approches sont souvent un modèle à base d'arbre.</seg>
<seg id="65">Donc, dans l'ensemble, notre raisonneur est capable de dépasser significativement ce modèle à base d'arbre.</seg>
<seg id="66">Mais nous pouvons voir que les nombres absolus sur MathQA ou SVAMP ne sont pas vraiment élevés.</seg>
<seg id="67">Nous étudions donc plus en détail les résultats sur SVAMP.</seg>
<seg id="68">Et ces données sont difficiles parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle TAL traitement automatique du langage naturel comme ajouter des informations non pertinentes et des quantités supplémentaires.</seg>
<seg id="69">Donc, dans notre prévention, nous trouvons que certaines des valeurs intermédiaires sont en réalité négatives.</seg>
<seg id="70">Par exemple, dans ces questions, nous demandons « combien de pommes a Jake ? »</seg>
<seg id="71">Mais nous avons des informations supplémentaires comme « dix-sept photos de moins », et « Steven en a huit », ce qui est totalement hors de propos.</seg>
<seg id="72">Ainsi, notre modèle effectue une prévention comme celle-ci qui produit des valeurs négatives.</seg>
<seg id="73">Et nous constatons que ces deux expressions ont en réalité des scores similaires.</seg>
<seg id="74">Nous pouvons alors limiter cet espace de recherche en supprimant les résultats négatifs afin que nous puissions rendre la réponse correcte.</seg>
<seg id="75">Nous trouvons donc que cette contrainte améliore en réalité beaucoup pour certains modèles.</seg>
<seg id="76">Par exemple, pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous améliorons sept points, puis pour le modèle de base Roberta, nous avons amélioré deux points.</seg>
<seg id="77">Le meilleur modèle de langue a donc de meilleures capacités de compréhension de la langue de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Représentations d'encodeurs bidirectionnels à partir de transformateurs.</seg>
<seg id="78">Et nous essayons également d'analyser la difficulté derrière toutes ces données.</seg>
<seg id="79">Nous supposons que le nombre de quantités inutilisées peut être considéré ici comme une information non pertinente.</seg>
<seg id="80">Donc ici, nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités inutilisées, et les données SVAMP ont la plus grande partie.</seg>
<seg id="81">Et ici, nous montrons également la performance globale.</seg>
<seg id="82">Pour ces échantillons sans quantités inutilisées, la performance est en réalité supérieure à la performance globale.</seg>
<seg id="83">Mais avec ces échantillons ayant une quantité inutilisée, c'est en fait bien pire que la performance globale.</seg>
<seg id="84">Pour MAWPS, nous n'avons pas vraiment trop de cas de test, alors j'ignore simplement cette partie.</seg>
<seg id="85">Finalement, nous voulons montrer l'interopérabilité à travers un exemple de perturbation de question.</seg>
<seg id="86">Donc ici, notre modèle effectue en réalité une prévention erronée à la première étape.</seg>
<seg id="87">Ainsi, nous pouvons effectivement corréler cette expression avec la phrase ici. Très bien.</seg>
<seg id="88">Nous pensons donc que cette phrase pourrait induire le modèle en erreur avec des préventions incorrectes.</seg>
<seg id="89">Donc ici, en planter trente-cinq autres fait penser au modèle qu'il devrait être un opérateur d'addition.</seg>
<seg id="90">Nous essayons alors de réviser la phrase pour que cela soit quelque chose comme « le nombre de poiriers est inférieur de trente-cinq à celui des pommiers ».</seg>
<seg id="91">Nous faisons en sorte de transmettre une sémantique plus précise afin que le modèle soit capable de rendre la prévention correcte.</seg>
<seg id="92">Ainsi, cette étude montre comment les préventions interprétables nous aident à comprendre le comportement du modèle.</seg>
<seg id="93">Donc, pour conclure notre travail, notre modèle est en réalité assez efficace.</seg>
<seg id="94">Et nous sommes en mesure de fournir une procédure de résolution interprétable.</seg>
<seg id="95">Et nous pouvons facilement incorporer des connaissances antérieures en tant que contraintes qui peuvent aider à améliorer la performance.</seg>
<seg id="96">Et la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution des problèmes de réseau, mais aussi aux autres tâches qui impliquent un raisonnement à plusieurs étapes.</seg>
<seg id="97">Nous avons également certaines limites.</seg>
<seg id="98">Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée.</seg>
<seg id="99">Et la deuxième chose est que, comme mentionné, puisque la distribution de probabilité est déséquilibrée entre les différentes étapes de temps, il est donc également assez difficile d'appliquer la stratégie de beam search.</seg>
<seg id="100">Nous arrivons donc à la fin de la discussion, et vos questions sont les bienvenues. Merci.</seg>
</doc>
<doc docid="2022.acl-long.468" genre="presentations">
<talkid>2022.acl-long.468</talkid>
<abstract>Statutory article retrieval is the task of automatically retrieving law articles relevant to a legal question. While recent advances in natural language processing have sparked considerable interest in many legal tasks, statutory article retrieval remains primarily untouched due to the scarcity of large-scale and high-quality annotated datasets. To address this bottleneck, we introduce the Belgian Statutory Article Retrieval Dataset (BSARD), which consists of 1,100+ French native legal questions labeled by experienced jurists with relevant articles from a corpus of 22,600+ Belgian law articles. Using BSARD, we benchmark several state-of-the-art retrieval approaches, including lexical and dense architectures, both in zero-shot and supervised setups. We find that fine-tuned dense retrieval models significantly outperform other systems. Our best performing baseline achieves 74.8% R@100, which is promising for the feasibility of the task and indicates there is still room for improvement. By the specificity of the domain and addressed task, BSARD presents a unique challenge problem for future research on legal information retrieval. Our dataset and source code are publicly available.</abstract>
<seg id="101">Bonjour, je m'appelle Antoine et je suis de l'Université de Maastricht.</seg>
<seg id="102">Je vais vous présenter mon travail en commun avec Jerry, qui porte sur de nouvelles données pour l'extraction des articles de droit.</seg>
<seg id="103">Les questions juridiques font partie intégrante de la vie de nombreuses personnes.</seg>
<seg id="104">Mais la majorité des citoyens ont peu de connaissances sur leurs droits et leurs processus juridiques fondamentaux.</seg>
<seg id="105">En conséquence, de nombreux citoyens vulnérables qui n'ont pas les moyens de se payer l'aide coûteuse d'un expert juridique sont laissés sans protection ou, pire encore, exploités.</seg>
<seg id="106">Tous les travaux visent à combler le fossé entre les personnes et la loi en développant un système d'extraction efficace pour les articles de droit.</seg>
<seg id="107">Un tel système pourrait fournir un service d'aide juridique professionnel gratuit aux personnes non qualifiées.</seg>
<seg id="108">Avant de plonger dans la principale contribution de ce travail, nous allons d'abord décrire le problème d'extraction des articles de droit.</seg>
<seg id="109">Compte tenu d'une simple question sur une question juridique telle que : « qu'est-ce que je risque si je viole la confidentialité professionnelle ? »</seg>
<seg id="110">Un modèle est nécessaire pour extraire tous les articles de droit pertinents d'un grand corpus législatif.</seg>
<seg id="111">Cette tâche d'extraction des informations comporte son propre ensemble de défis.</seg>
<seg id="112">Tout d'abord, elle traite de deux types de langue.</seg>
<seg id="113">La langue naturelle commune pour les questions et la langue juridique complexe pour les lois.</seg>
<seg id="114">Cette différence dans les distributions de langue rend plus difficile pour un système d'extraire les candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des lois.</seg>
<seg id="115">En outre, le droit écrit n'est pas une pile d'articles indépendants qui peuvent être traités comme une source complète d'informations par eux-mêmes, contrairement aux nouvelles ou aux recettes, par exemple.</seg>
<seg id="116">Au lieu de cela, il s'agit d'un ensemble structuré de dispositions juridiques qui n'ont un sens entier que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les domaines et sous-domaines auxquels elles appartiennent et leur place dans la structure de la loi.</seg>
<seg id="117">Enfin, les articles de droit ne sont pas de petits paragraphes constituant généralement l'unité d'extraction typique dans la plupart des travaux d'extraction.</seg>
<seg id="118">Ici, il y a de longs documents qui peuvent aller jusqu'à six mille mots.</seg>
<seg id="119">Les recent advances en matière de TAL traitement automatique du langage naturel ont suscité un vif intérêt pour de nombreuses tâches juridiques, telles que la prévention du jugement juridique ou l'examen des contrats de contacts automatisés.</seg>
<seg id="120">Mais l'extraction de l'article de droit est restée essentiellement inchangée en raison du manque de grandes données étiquetées de haute qualité.</seg>
<seg id="121">Dans ce travail, nous présentons de nouvelles données centrées sur le citoyen natif français pour étudier si l'extraction des modèles peut se rapprocher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche d'extraction de l'article de droit.</seg>
<seg id="122">Nos données d'extraction de l'article de droit belge BSARD se composent de plus de mille cent questions juridiques posées par des citoyens belges.</seg>
<seg id="123">Ces questions couvrent un wide range de sujets allant de la famille au logement, à l'argent, au travail et à la sécurité sociale.</seg>
<seg id="124">Chacune d'entre elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques de codes de droit belge.</seg>
<seg id="125">Parlons maintenant de la façon dont nous avons collecté ces données.</seg>
<seg id="126">Tout d'abord, nous avons commencé par compiler un grand corpus d'articles juridiques.</seg>
<seg id="127">Nous avons examiné trente-deux codes belges accessibles au public et extrait tous les articles ainsi que les titres de section correspondants.</seg>
<seg id="128">Ensuite, nous avons rassemblé les questions juridiques avec les références aux lois pertinentes.</seg>
<seg id="129">Pour ce faire, nous nous associons au cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur une question juridique personnelle.</seg>
<seg id="130">Nous avons eu la chance d'avoir accès à leurs sites web, où leur équipe de juristes expérimentés aborde les questions juridiques les plus courantes des Belges.</seg>
<seg id="131">Nous avons recueilli des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux lois pertinentes.</seg>
<seg id="132">Enfin, nous avons fait passer les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avons considérés.</seg>
<seg id="133">Les références restantes ont été appariées et converties aux identifiants d'article correspondants de notre corpus.</seg>
<seg id="134">Nous nous sommes finalement retrouvés avec mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de droit.</seg>
<seg id="135">De plus, chaque question est accompagnée de la catégorie principale et d'un enchaînement de sous-catégories.</seg>
<seg id="136">Et chaque article comporte un enchaînement de la rubrique de sous-séquence dans la structure de la loi.</seg>
<seg id="137">Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour des recherches futures sur l'extraction d'informations juridiques ou la classification de textes juridiques.</seg>
<seg id="138">Jetons un coup d'œil à certaines caractéristiques de nos données.</seg>
<seg id="139">Les questions comportent entre cinq et quarante-quatre mots avec une moyenne de quatorze mots.</seg>
<seg id="140">Les articles sont beaucoup plus longs avec une longueur moyenne de soixante-dix-sept mots, avec cent quarante-deux d'entre eux dépassant les mille mots.</seg>
<seg id="141">Le plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots.</seg>
<seg id="142">Comme mentionné précédemment, les questions couvrent un wide range de sujets, dont environ quatre-vingt-cinq pour cent concernent la famille, le logement, l'argent ou la justice.</seg>
<seg id="143">Alors que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail.</seg>
<seg id="144">L'article est également très diversifié car il provient de trente-deux codes belges différents qui couvrent un grand nombre de sujets juridiques.</seg>
<seg id="145">Voici le nombre total d'articles collectés à partir de chacun de ces codes belges.</seg>
<seg id="146">Sur les vingt-deux mille six cent trente-trois articles, seuls mille six cent douze sont désignés comme pertinents à au moins une question dans les données.</seg>
<seg id="147">Et environ quatre-vingts pour cent de ces articles cités proviennent du code civil, des codes judiciaires, des codes d'enquête criminelle ou des codes pénaux.</seg>
<seg id="148">Pendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question.</seg>
<seg id="149">Ce qui peut s'expliquer par le fait que ces codes se concentraient moins sur les individus et leurs préoccupations.</seg>
<seg id="150">Dans l'ensemble, le nombre moyen de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.</seg>
<seg id="151">En utilisant toutes les données, nous avons comparé plusieurs approches d'extraction, y compris l'architecture lexicale et dense.</seg>
<seg id="152">Étant donné une requête et un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de requête des poids de chacun de ces termes dans cet article.</seg>
<seg id="153">Nous expérimentons avec les fonctions de classement TF-IDF et BM25 standard.</seg>
<seg id="154">Le principal problème de ces approches est qu'elles ne peuvent extraire que les articles contenant des mots-clés présents dans la requête.</seg>
<seg id="155">Pour surmonter cette limitation, nous expérimentons une architecture neuronale qui peut capturer les relations sémantiques entre les requêtes et l'article.</seg>
<seg id="156">Nous utilisons un modèle bi-encodeur qui cartographie les requêtes et les articles en représentations vecteurs denses et calculons un score de pertinence entre une paire d'articles de requête par la similitude de leurs intégrations.</seg>
<seg id="157">Ces intégrations résultent typiquement d'une opération de pooling sur la sortie d'un modèle d'intégration de mots.</seg>
<seg id="158">Tout d'abord, nous étudions l'efficacité des bi-encodeurs siamois dans une configuration d'évaluation « zero shot », ce qui signifie que des modèles d'intégration de mots préformés sont appliqués immédiatement sans aucun raffinement supplémentaire.</seg>
<seg id="159">Nous expérimentons avec l'encodeur de texte indépendant du contexte, à savoir word2vec et fastText, et des modèles d'intégration dépendants du contexte, à savoir Roberta et plus précisément CamemBERT qui est un modèle Roberta français.</seg>
<seg id="160">De plus, nous formons nos propres bi-encodeurs de modèles basés sur CamemBERT sur nos données.</seg>
<seg id="161">Notez que pour la formation, nous expérimentons avec les deux modèles de l'architecture bi-encodeur.</seg>
<seg id="162">Le siamois, qui utilise un modèle d'intégration de mots unique cartographiant la requête et l'article ensemble dans un vector space dense partagé, et le modèle à deux tours, qui utilise deux modèles d'intégration de mots indépendants encodant la requête et l'article séparément dans différents espaces d'intégration.</seg>
<seg id="163">Nous expérimentons la mise en commun moyenne, maximale et CLS, ainsi que le produit et cosinus pour calculer les similitudes.</seg>
<seg id="164">Voici le résultat de notre base sur les ensembles de test.</seg>
<seg id="165">Avec les méthodes lexicales ci-dessus, les bi-encodeurs siamois ont évalué dans une configuration zero shot au milieu, et les bi-encodeurs raffinés ci-dessous.</seg>
<seg id="166">Dans l'ensemble, le bi-encodeur raffiné dépasse de manière significative toutes les autres bases.</seg>
<seg id="167">Le modèle à deux tours s'améliore par rapport à ses variantes siamoises lors du rappel à cent, mais fonctionne de la même manière sur les autres indicateurs.</seg>
<seg id="168">Bien que BM25 ait sous-performé le bi-encodeur formé de manière significative, ses performances indiquent qu'il s'agit toujours d'une base solide pour l'extraction spécifique au domaine.</seg>
<seg id="169">En ce qui concerne l'évaluation zero shot du bi-encodeur siamois, nous constatons que l'utilisation directe des intégrations d'un modèle CamemBERT préformé sans optimiser pour la tâche d'extraction d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats antérieurs.</seg>
<seg id="170">En outre, nous observons que le bi-encodeur basé sur word2vec a dépassé de manière significative les modèles basés sur fastText et les Représentations d'encodeurs bidirectionnels à partir de transformateurs, suggérant que peut-être les intégrations de niveau mot préformées sont plus appropriées pour la tâche que les intégrations de niveau caractère ou sous-mot lorsqu'elles sont utilisées immédiatement.</seg>
<seg id="171">Bien que prometteurs, ces résultats suggèrent de nombreuses possibilités d'amélioration comparé à un expert juridique qualifié qui peut éventuellement extraire tous les articles pertinents à n'importe quelle question, et ainsi obtenir des scores parfaits.</seg>
<seg id="172">Concluons en discutant de deux limites de nos données.</seg>
<seg id="173">Premièrement, le corpus d'article est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants.</seg>
<seg id="174">Au cours de la construction des données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions ne se retrouvent qu'avec une fraction du nombre initial d'articles pertinents.</seg>
<seg id="175">Cette information implique donc que la réponse contenue dans les autres articles pertinents pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.</seg>
<seg id="176">Deuxièmement, il convient de noter qu'on ne peut pas répondre à toutes les questions juridiques uniquement par des lois.</seg>
<seg id="177">Par exemple, la question « puis-je expulser mes locataires s'ils font trop de bruit ? »</seg>
<seg id="178">Elle peut ne pas avoir de réponse détaillée dans le droit écrit qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée.</seg>
<seg id="179">Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des antécédents similaires à sa situation actuelle.</seg>
<seg id="180">Par exemple, les locataires font la fête deux fois par semaine jusqu'à deux heures du matin.</seg>
<seg id="181">Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche d'extraction de l'article de droit, et le domaine des moins adaptées reste à déterminer.</seg>
<seg id="182">Nous espérons que nos travaux susciteront l'intérêt pour l'élaboration de modèles d'extraction d'article de droit pratiques et fiables.</seg>
<seg id="183">Cela peut aider à améliorer l'accès à la justice pour tous.</seg>
<seg id="184">Vous pouvez consulter notre article, nos données et notre code aux liens suivants. Merci.</seg>
</doc>
<doc docid="2022.acl-long.567" genre="presentations">
<talkid>2022.acl-long.567</talkid>
<abstract>We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V&L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&L models from a linguistic perspective, complementing the canonical task-centred V&L evaluations.</abstract>
<seg id="185">Bonjour, nous sommes heureux de vous présenter notre travail sur VALSE ; un indice de référence indépendant de la tâche destiné à tester les modèles de langue et de vision avec des phénomènes linguistiques spécifiques.</seg>
<seg id="186">Pourquoi avons-nous pris la peine de mettre en place cet indice de référence ?</seg>
<seg id="187">Eh bien, au cours des dernières années, nous avons vu une explosion de la vision basée sur le transformateur et des modèles de langue préformés sur de grandes quantités de paires de textes d'images.</seg>
<seg id="188">Chacun de ces modèles pousse les tâches de pointe en matière de vision et de langue telles que la réponse aux questions visuelles, le raisonnement de bon sens visuel, l'extraction d'images et les bases de phrases.</seg>
<seg id="189">Nous avons donc reçu un message : les précisions sur ces tâches et les indices de référence spécifiques augmentent régulièrement.</seg>
<seg id="190">Mais savons-nous ce que les modèles ont réellement appris ?</seg>
<seg id="191">Qu'est-ce qu'une vision et un transformateur de langue ont compris lors de l'attribution d'un score élevé pour que cette image et cette phrase correspondent ?</seg>
<seg id="192">Et le score bas pour celle-ci ?</seg>
<seg id="193">Est-ce que les modèles de langue et de vision se concentrent sur la bonne chose ?</seg>
<seg id="194">Ou se concentrent-ils sur les biais comme le montre le travail antérieur ?</seg>
<seg id="195">Pour éclairer davantage cet aspect, nous proposons une direction plus indépendante des tâches et introduisons VALSE qui teste la sensibilité des modèles de langue et de vision à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistiques et visuelles.</seg>
<seg id="196">Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités.</seg>
<seg id="197">Mais comment tester si les modèles de langue et de vision ont capturé ce phénomène ?</seg>
<seg id="198">En pratiquant le foiling sur une méthode précédemment appliquée pour les modèles de langue et de vision seulement pour des phrases nominales de Ravi Shekhar et de ses collaborateurs, et en comptant par nous dans les travaux antérieurs.</seg>
<seg id="199">Effectuer un foiling signifie essentiellement que nous prenons la légende d'une image et produisons un foil en modifiant la légende de sorte qu'elle ne décrive plus l'image.</seg>
<seg id="200">Et nous apportons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence sur les entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous aurions trouvé plus d'une façon intéressante de créer des instances de foil.</seg>
<seg id="201">Par exemple, dans le cas de l'élément d'actions, nous avons deux instruments, un dans lequel le verbe d'action est modifié avec une action différente, et un dans lequel les acteurs sont échangés.</seg>
<seg id="202">Le comptage et la coréférence sont également des éléments qui ont plusieurs instruments.</seg>
<seg id="203">Et nous créons ces foils en nous assurant qu'ils ne décrivent pas l'image et que ce sont des phrases grammaticales et autrement valides.</seg>
<seg id="204">Ce n'est pas facile à faire car une légende ayant subi un foil peut être moins probable que la légende originale.</seg>
<seg id="205">Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que les plantes coupent un homme qu'un homme coupe les plantes, et la grande vision et les modèles de langue pourraient capter cela.</seg>
<seg id="206">Par conséquent, pour obtenir des foils valides, nous devons agir.</seg>
<seg id="207">Tout d'abord, nous utilisons des modèles de langue forts pour proposer des foils.</seg>
<seg id="208">Deuxièmement, nous utilisons l'inférence de la langue naturelle ou la NLI courte pour filtrer les foils qui pourraient encore décrire l'image, car lors de la construction des foils, nous devons nous assurer qu'ils ne décrivent pas l'image.</seg>
<seg id="209">Pour tester cela automatiquement, nous appliquons l'inférence de la langue naturelle avec la justification suivante.</seg>
<seg id="210">Nous considérons qu'une image est la prémisse et sa légende son hypothèse implicite.</seg>
<seg id="211">En outre, nous considérons la légende comme la prémisse, et le foil est son hypothèse.</seg>
<seg id="212">Si un modèle NLI prévient que le foil est en contradiction ou neutre par rapport à la légende, nous considérons cela comme un indicateur d'un foil valide.</seg>
<seg id="213">Si une NLI prévient le foil à engendrer par la légende, cela ne peut pas être un bon foil, car par transitivité, cela donnera une description véridique de l'image et nous filtrons ces foils.</seg>
<seg id="214">Mais cette procédure n'est pas parfaite, il s'agit simplement d'un indicateur pour les foils valides.</seg>
<seg id="215">Par conséquent, comme troisième mesure pour produire des foils valides, nous utilisons des annotateurs humains pour valider les données utilisées dans VALSE.</seg>
<seg id="216">Donc, après filtrage et évaluation humaine, nous avons autant d'instances de test que décrit dans ce tableau.</seg>
<seg id="217">Notez que VALSE ne fournit pas de données de formation mais seulement des données de test.</seg>
<seg id="218">Comme il s'agit uniquement d'un indicateur de référence de test zero shot, il est conçu pour tirer parti des capacités existantes des modèles de langue et de vision après la préformation.</seg>
<seg id="219">Le raffinement permettrait seulement aux modèles d'exploiter des artefacts ou des biais statistiques dans les données.</seg>
<seg id="220">Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis.</seg>
<seg id="221">Et comme nous l'avons dit, cela nous intéresse d'évaluer les capacités des modèles de langue et de vision après la préformation.</seg>
<seg id="222">Nous expérimentons avec cinq modèles de langue et de vision sur VALSE, à savoir avec CLIP, LXMert, ViLBERT, ViLBERT douze en un, et VisualBERT.</seg>
<seg id="223">Deux de nos indicateurs d'évaluation les plus importants sont la précision des modèles dans la classification de paires de phrases images en légendes et foils.</seg>
<seg id="224">Peut-être plus pertinent pour cette vidéo, nous présenterons notre indicateur plus permissif, la précision par paire, qui mesure si le score de sentence alignment d'image est plus élevé pour la bonne paire de texte image que pour sa paire ayant subi un foil.</seg>
<seg id="225">Pour plus d'indicateurs et de résultats, consultez notre article.</seg>
<seg id="226">Les résultats avec une précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus des autres indicateurs. La meilleure performance zero shot est obtenue par ViLBERT douze en un, suivi de ViLBERT, LXMert, CLIP, et enfin VisualBERT.</seg>
<seg id="227">Il est remarquable de voir comment les instruments centrés sur les objets individuels comme l'existence et les phrases nominales sont presque résolus par ViLBERT douze en un, en soulignant que les modèles sont capables d'identifier des objets nommés et leur présence dans les images.</seg>
<seg id="228">Cependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de foiling antagonistes.</seg>
<seg id="229">Nous voyons à partir des instruments de pluralité et de comptage que les modèles de langue et de vision ont du mal à distinguer les références à des objets uniques par rapport à plusieurs, ou à les compter dans une image.</seg>
<seg id="230">L'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image.</seg>
<seg id="231">Ils ont également du mal à distinguer les actions et à identifier leurs participants, même s'ils sont soutenus par des biais de plausibilité comme nous le voyons dans l'élément d'actions.</seg>
<seg id="232">À partir de l'élément de coréférence, nous découvrons que relever plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de langue et de vision.</seg>
<seg id="233">Pour vérifier la santé mentale, et parce que c'est une expérience intéressante, nous comparons également deux modèles à texte seulement, GPT un et GPT deux, pour évaluer si VALSE peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et ayant subi un foil, sans image ici, et en prévenant l'entrée avec la plus faible perplexité.</seg>
<seg id="234">Si la perplexité est plus élevée pour le foil, nous considérons cela comme une indication que la légende ayant subi un foil peut souffrir de biais de plausibilité ou d'autres biais linguistiques.</seg>
<seg id="235">Et il est intéressant de voir que dans certains cas, les modèles GPT à texte seulement ont capturé la plausibilité du monde mieux que les modèles de langue et de vision.</seg>
<seg id="236">Donc, pour résumer, VALSE est un indicateur de référence qui utilise l'objectif des constructions linguistiques pour aider la communauté à améliorer les modèles de langue et de vision en testant durement leurs capacités de bases visuelles.</seg>
<seg id="237">Nos expériences montrent que les modèles de langue et de vision identifient bien les objets nommés et leur présence dans les images, comme le montre l'élément d'existence, mais luttent pour ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques.</seg>
<seg id="238">Nous aimerions vraiment encourager la communauté à utiliser VALSE pour mesurer les progrès vers les bases de la langue avec des modèles de langue et de vision.</seg>
<seg id="239">Et plus encore, VALSE pourrait être utilisé comme une évaluation indirecte des données, car les modèles pourraient être évalués avant et après la formation ou le raffinement pour voir si des données aident les modèles à améliorer l'un des aspects testés par VALSE.</seg>
<seg id="240">Si vous êtes intéressé, consultez les données VALSE sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter.</seg>
</doc>
<doc docid="2022.acl-long.597" genre="presentations">
<talkid>2022.acl-long.597</talkid>
<abstract>A release note is a technical document that describes the latest changes to a software product and is crucial in open source software development. However, it still remains challenging to generate release notes automatically. In this paper, we present a new dataset called RNSum, which contains approximately 82,000 English release notes and the associated commit messages derived from the online repositories in GitHub. Then, we propose classwise extractive-then-abstractive/abstractive summarization approaches to this task, which can employ a modern transformer-based seq2seq network like BART and can be applied to various repositories without specific constraints. The experimental results on the RNSum dataset show that the proposed methods can generate less noisy release notes at higher coverage than the baselines. We also observe that there is a significant gap in the coverage of essential information when compared to human references. Our dataset and the code are publicly available.</abstract>
<seg id="241">Bonjour, je m'appelle Kamezawa de l'Université de Tokyo.</seg>
<seg id="242">Je vais vous présenter un article intitulé RNSum : des données à grande échelle pour la génération automatique de note de version via la synthèse des journaux de validation.</seg>
<seg id="243">Je vais vous expliquer dans cet ordre.</seg>
<seg id="244">Tout d'abord, je vais présenter la génération de note de version automatique sur laquelle nous travaillons dans cette recherche.</seg>
<seg id="245">Une note de version est un document technique qui résume les modifications distribuées à chaque version d'un produit logiciel.</seg>
<seg id="246">L'image montre une note de version pour la version deux point six point quatre de la bibliothèque Vuejs.</seg>
<seg id="247">Les notes de version jouent un rôle important dans le développement open source, mais elles mettent du temps à être préparées manuellement.</seg>
<seg id="248">Par conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité.</seg>
<seg id="249">Je m'en remettrai à deux recherches antérieures sur la génération automatique de notes de version.</seg>
<seg id="250">La première est un système appelé ARENA sorti en deux-mille-quatorze.</seg>
<seg id="251">Il faut une approche basée sur des règles, par exemple, en utilisant l'extracteur de changement pour extraire toutes les différences, les changements de bibliothèque et les changements de document à partir des différences entre les versions, et enfin, en les combinant.</seg>
<seg id="252">La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit.</seg>
<seg id="253">Le système de suivi des problèmes, qui doit être laissé à Jira, ne peut être appliqué qu'aux projets qui utilisent Jira.</seg>
<seg id="254">En d'autres mots, il ne peut pas être utilisé pour de nombreux projets sur GitHub.</seg>
<seg id="255">La seconde est Glyphe, récemment annoncée en deux-mille-vingt.</seg>
<seg id="256">Elle est disponible sur internet et peut être installée via pip.</seg>
<seg id="257">Ce système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'une des cinq étiquettes telles que les fonctions ou corrections de bugs pour chaque message de validation de saisie.</seg>
<seg id="258">Cette image est un exemple d'utilisation qui renvoie une étiquette de correction ou de débogage.</seg>
<seg id="259">Les données de formation de Glyphe sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous.</seg>
<seg id="260">Les performances du modèle de classification de texte ne sont pas élevées.</seg>
<seg id="261">Je présente deux recherches connexes, mais leurs problèmes sont l'applicabilité limitée et les ressources de données rares.</seg>
<seg id="262">Notre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité.</seg>
<seg id="263">Avec un problème d'applicabilité limitée, nous proposons une méthode de synthèse par classe de haute qualité en utilisant uniquement des messages de validation comme saisie.</seg>
<seg id="264">Cette méthode proposée peut être utilisée pour tous les référentiels anglais.</seg>
<seg id="265">Pour le deuxième problème de ressources de données rares, nous avons construit nos données RNSum composées d'environ quatre-vingt-deux mille éléments de données en collectant des données à partir de référentiels GitHub publics en utilisant l'API GitHub.</seg>
<seg id="266">Ensuite, je vais décrire nos données.</seg>
<seg id="267">Voici un exemple de données.</seg>
<seg id="268">Le côté gauche est un message de validation et le côté droit représente les notes de version.</seg>
<seg id="269">Les notes de version sont étiquetées comme améliorations ou correctifs, etc.</seg>
<seg id="270">Nous avons mis en place une tâche qui prend les messages de validation comme saisie et sorties des notes de version étiquetées.</seg>
<seg id="271">Cela peut être considéré comme une tâche de synthèse.</seg>
<seg id="272">Nous avons prédéfini quatre étiquettes : fonctions, améliorations, corrections de bugs, suppressions de dépréciations et modifications de rupture.</seg>
<seg id="273">Celles-ci ont été établies sur la base de la recherche antérieure et d'autres facteurs.</seg>
<seg id="274">La note de version en bas à droite est extraite de la note de version en bas à gauche.</seg>
<seg id="275">À ce stade, il est nécessaire de détecter les quatre étiquettes qui ont été mises en place à l'avance.</seg>
<seg id="276">Mais les étiquettes ne sont pas toujours cohérentes avec chaque référentiel.</seg>
<seg id="277">Par exemple, l'étiquette d'améliorations inclut des améliorations, des perfectionnements, des optimisations, etc.</seg>
<seg id="278">Nous avons préparé une liste de vocabulaire d'une trentaine d'étiquettes pour chacune de ces variations de notation.</seg>
<seg id="279">Il s'agit de détecter la classe de note de version et de collecter le texte de la version qui suit en tant que phrase de note de version pour la classe.</seg>
<seg id="280">Ensuite, il y a un message de validation.</seg>
<seg id="281">Les messages de validation ne sont pas liés à chaque version.</seg>
<seg id="282">Comme le montre l'image ci-dessous, si la version actuelle est la version deux point cinq à dix-neuf, nous devons identifier la version deux point cinq à dix-huit antérieure et obtenir un diff.</seg>
<seg id="283">C'est un peu fastidieux et il ne suffit pas d'obtenir une liste de versions et de regarder l'avant et l'après.</seg>
<seg id="284">Nous avons créé une règle de correspondance heuristique pour obtenir les versions antérieures et suivantes.</seg>
<seg id="285">Analyse des données.</seg>
<seg id="286">En fin de compte, sept mille deux cents dépôts et quatre-vingt-deux mille éléments de données ont été recueillis.</seg>
<seg id="287">En outre, le nombre moyen de gages de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse.</seg>
<seg id="288">De même, le nombre de gages uniques est assez grand, s'élevant à huit mille huit cent trente mille.</seg>
<seg id="289">Cela est dû au grand nombre de noms de méthode ou de classe unique trouvés dans le référentiel.</seg>
<seg id="290">Ensuite, je vais expliquer la méthode proposée.</seg>
<seg id="291">Le modèle de synthèse extractive puis abstractive par classe se compose de deux modules neuronaux.</seg>
<seg id="292">Un classificateur utilisant BERT ou CodeBERT et un générateur utilisant BART.</seg>
<seg id="293">Tout d'abord, CEAS utilise un classificateur pour classer chaque message de validation en cinq classes de notes de version, qui utilisent des améliorations, des corrections de bogues, des dépréciations et d'autres.</seg>
<seg id="294">Les messages de validation classés comme autres sont supprimés.</seg>
<seg id="295">Ensuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.</seg>
<seg id="296">Dans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues.</seg>
<seg id="297">Par conséquent, pour former le classificateur, nous avons réaffecté les enquêtes à chaque message de validation de saisie en utilisant les dix premiers caractères de chaque message de validation.</seg>
<seg id="298">Nous avons modélisé l'approche de synthèse abstractive par classe à travers deux méthodes différentes.</seg>
<seg id="299">Le premier modèle, que nous appelons CAS unique, se compose d'un réseau de six à six unique et génère un seul texte de note de version donnant un enchaînement de messages de validation de saisie.</seg>
<seg id="300">Les textes de sortie peuvent être divisés en segments par classe sur la base de symboles de point d'extrémité particuliers, spécifiques à la classe.</seg>
<seg id="301">La seconde méthode, que nous appelons CAS multiple, se compose de quatre réseaux seq2seq différents, dont chacun correspond à l'une des classes de notes de version fixes.</seg>
<seg id="302">Ok, je vais vous expliquer les expériences.</seg>
<seg id="303">Cinq méthodes ont été comparées : CEAS, CAS unique, CAS multiple, regroupement et étude antérieure, Glyph.</seg>
<seg id="304">En ce qui concerne l'évaluation, dans certains cas, les notes de version sont produites en plusieurs phrases.</seg>
<seg id="305">Puisqu'il est difficile de calculer le nombre de phrases telles qu'elles sont, elles sont combinées avec des espaces et traitées comme une seule phrase longue.</seg>
<seg id="306">Le BLEU est pénalisé lorsque le système produit une phrase courte.</seg>
<seg id="307">Cette pénalité se traduit par une valeur BLEU inférieure dans les résultats de l'expérience décrits ci-dessous.</seg>
<seg id="308">Enfin, nous calculons également la spécificité car ROUGE et BLEU ne peuvent pas être calculés si les notes de version sont vides.</seg>
<seg id="309">Une spécificité plus élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version sont supposées être vides.</seg>
<seg id="310">Les résultats sont indiqués ci-après.</seg>
<seg id="311">Étant donné que les données contiennent des adresses électroniques, des valeurs hachées, etc., nous avons également évalué les données nettoyées, ce qui les exclut.</seg>
<seg id="312">Le CEAS et le CAS ont obtenu des scores de ROUGE-L supérieurs de plus de dix points par rapport aux bases.</seg>
<seg id="313">En particulier, sur l'ensemble de test propre, l'écart de score entre la méthode proposée et les bases a atteint plus de vingt points.</seg>
<seg id="314">Ces résultats indiquent que le CEAS et le CAS sont considérablement touchés.</seg>
<seg id="315">Le CEAS a obtenu un meilleur score ROUGE-L que le CAS, ce qui suggère que la combinaison d'un classificateur et d'un générateur est efficace pour former le classificateur à l'aide de pseudo-étiquettes.</seg>
<seg id="316">Une couverture élevée du CEAS peut être obtenue probablement car le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.</seg>
<seg id="317">Le CAS multiple tendait à produire plus de ROUGE-L que le CAS unique.</seg>
<seg id="318">En suggérant qu'il est également efficace de développer indépendamment et différemment des modèles de abstractive summarization pour chaque classe de note de version.</seg>
<seg id="319">Voici une analyse d'erreur.</seg>
<seg id="320">Les méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines.</seg>
<seg id="321">Dans la figure de droite, la phrase de référence a trois ou quatre phrases, tandis que le CAS n'en a qu'une.</seg>
<seg id="322">La raison de la réticence de ce modèle est que dans les données de formation, seulement trente-trois pour cent des phrases sont présentes dans l'étiquette des fonctions et quarante pour cent dans l'étiquette des améliorations.</seg>
<seg id="323">En outre, les méthodes CAS ne peuvent pas générer des notes de version précises sans informations supplémentaires.</seg>
<seg id="324">L'exemple en haut à droite est un exemple de message de validation très désordonné, et la phrase complète ne peut pas être générée sans référence à la progression ou au problème correspondant(e).</seg>
<seg id="325">L'exemple ci-dessous montre que les deux messages de validation dans la saisie sont liés et doivent être combinés en une phrase, mais cela n'est pas fait.</seg>
<seg id="326">Enfin, passons à la conclusion.</seg>
<seg id="327">Nous avons construit de nouvelles données pour la génération automatique de notes de version.</seg>
<seg id="328">Nous avons également formulé une tâche consistant à saisir des messages de validation et à les résumer afin qu'ils soient applicables à tous les projets écrits en anglais.</seg>
<seg id="329">Nos expériences montrent que la méthode proposée génère des notes de version moins bruyantes à une couverture plus élevée que les bases.</seg>
<seg id="330">Veuillez consulter nos données sur GitHub.</seg>
<seg id="331">Merci.</seg>
</doc>
<doc docid="2022.acl-long.111" genre="presentations">
<talkid>2022.acl-long.111</talkid>
<abstract>The enrichment of tabular datasets using external sources has gained significant attention in recent years. Existing solutions, however, either ignore external unstructured data completely or devise dataset-specific solutions. In this study we proposed Few-Shot Transformer based Enrichment (FeSTE), a generic and robust framework for the enrichment of tabular datasets using unstructured data. By training over multiple datasets, our approach is able to develop generic models that can be applied to additional datasets with minimal training (i.e., few-shot). Our approach is based on an adaptation of BERT, for which we present a novel fine-tuning approach that reformulates the tuples of the datasets as sentences. Our evaluation, conducted on 17 datasets, shows that FeSTE is able to generate high quality features and significantly outperform existing fine-tuning solutions.</abstract>
<seg id="332">Bonjour. Je m'appelle Asaf Harari.</seg>
<seg id="333">Et je vais vous présenter notre article : enrichissement de données tabulaires en quelques coups à l'aide d'architectures de transformateurs raffinées.</seg>
<seg id="334">Les scientifiques analysent les données et se concentrent principalement sur la manipulation des fonctions existantes des données.</seg>
<seg id="335">Mais parfois, ces fonctions sont limitées.</seg>
<seg id="336">La génération de fonctions en utilisant une autre source de données peut ajouter des informations substantielles.</seg>
<seg id="337">Notre objectif de recherche est l'enrichissement automatique de données tabulaires en utilisant des textes libres de sources externes.</seg>
<seg id="338">Supposons que nous ayons des données tabulaires et une base de connaissances.</seg>
<seg id="339">Nous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles fonctions du texte libre de la base de connaissances.</seg>
<seg id="340">Notre cadre FeSTE est exactement ce processus automatique.</seg>
<seg id="341">Voyons donc un exemple dans des données introduites dans FeSTE.</seg>
<seg id="342">Dans cet exemple, les données sont des données universitaires.</seg>
<seg id="343">Quand l'objectif est de classer les universités en universités de bas rang et en universités de haut rang.</seg>
<seg id="344">En tant que base de connaissances, nous utilisons Wikipédia.</seg>
<seg id="345">La première phase de FeSTE est la liaison d'entités.</seg>
<seg id="346">Lorsque chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances.</seg>
<seg id="347">Et le texte des entités de la base de connaissances est extrait et ajouté aux données.</seg>
<seg id="348">Dans cet exemple, le texte est le résumé de la page Wikipédia.</seg>
<seg id="349">Maintenant, nous devons générer ou extraire des fonctions à partir du texte extrait.</seg>
<seg id="350">Nous avons donc besoin de la phase d'extraction des fonctions qui comprend l'analyse de texte.</seg>
<seg id="351">Il s'agit de la principale nouveauté de cet article et je vais mieux l'expliquer dans les prochaines diapositives.</seg>
<seg id="352">Après la phase d'extraction des fonctions, il y a une phase de génération de fonctions lorsque nous utilisons les fonctions extraites pour générer un petit nombre de nouvelles fonctions.</seg>
<seg id="353">Générez d'abord les fonctions dans le nombre de classes des données d'origine.</seg>
<seg id="354">Dans cet exemple, les données d'origine ont deux classes.</seg>
<seg id="355">Ainsi, FeSTE génère deux nouvelles fonctions.</seg>
<seg id="356">Mais si les données ont cinq classes, FeSTE génère cinq nouvelles fonctions.</seg>
<seg id="357">Chaque fonction représente la probabilité pour chaque classe.</seg>
<seg id="358">Pour analyser le texte, nous utilisons les analyses de texte de pointe, qui sont des modèles de langue basés sur la conversion comme BERT, GPT, XLNet etc.</seg>
<seg id="359">Mais il est peu probable que nous puissions former des modèles de langue en utilisant les données de saisie.</seg>
<seg id="360">Ainsi, une approche naïve sera le raffinement de la tâche cible.</seg>
<seg id="361">Dans la phase d'extraction des fonctions, nous pouvons télécharger des modèles de langue préformée et raffiner le modèle de langue sur les données cibles.</seg>
<seg id="362">Dans cet exemple, raffiner le modèle de langue, classer le texte en classes, résumer en classes, basses ou hautes.</seg>
<seg id="363">Recevoir les résultats du modèle de langue, qui sont la probabilité pour chaque classe et les utiliser comme nouvelles fonctions.</seg>
<seg id="364">Le problème avec cette approche est que les données peuvent avoir peu d'entités / de textes distinct(e)s.</seg>
<seg id="365">Dans notre expérience, près de la moitié des données contiennent moins de quatre cents échantillons et les plus petites données contiennent trente-cinq échantillons dans un ensemble de formation.</seg>
<seg id="366">Donc, pour raffiner un modèle de langue sur cela, les données seront inefficaces.</seg>
<seg id="367">Mais nous pouvons utiliser des connaissances préalables sur des données préanalysées.</seg>
<seg id="368">Puisque nous appliquons FeSTE sur des données multiples, nous pouvons utiliser les données n moins un pour recueillir des informations sur les données n moins un, et utiliser ces informations lorsque nous analysons les nièmes données.</seg>
<seg id="369">Ce que nous suggérons, c'est d'ajouter une autre phase de raffinement.</seg>
<seg id="370">Une phase de raffinement multitâche préliminaire.</seg>
<seg id="371">Lorsque vous raffinez le modèle de langue sur les données n moins un.</seg>
<seg id="372">Et ensuite, nous exécutons une autre phase de raffinement qui est un raffinement de tâche cible, lorsque nous raffinons le modèle de langue sur les nièmes données cibles.</seg>
<seg id="373">Le raffinement multitâche de pointe appelé MTDNN.</seg>
<seg id="374">Le MTDNN maintient les têtes dans le nombre de tâches dans l'ensemble de formation.</seg>
<seg id="375">Donc, dans cet exemple, il y a quatre tâches dans l'ensemble de formation. Le MTDNN maintient alors quatre têtes comme vous pouvez le voir sur l'image.</seg>
<seg id="376">Et il échantillonne un lot aléatoire de l'ensemble de formation.</seg>
<seg id="377">Et si elles appartiennent à un lot aléatoire, par exemple, une seule tâche de classification de phrases, il exécute des chemins d'aller et retour à travers la première tête.</seg>
<seg id="378">Et si le lot aléatoire appartient à la tâche de classement par paire, il exécute un chemin d'aller et retour dans la dernière tête.</seg>
<seg id="379">Dans notre scénario, les données tabulaires varient dans le nombre de classes.</seg>
<seg id="380">Il y a donc beaucoup de tâches.</seg>
<seg id="381">Le MTDNN a maintenu le nombre de classes, de têtes et de couches de sortie.</seg>
<seg id="382">Et en outre, le MTDNN doit initialiser de nouvelles têtes pour de nouvelles données avec une nouvelle tâche.</seg>
<seg id="383">Notre approche, appelée raffinement de reformulation de tâche, est dans notre raffinement de reformulation de tâche d'approche. Au lieu de maintenir plusieurs têtes, nous reformulons chaque donnée dans une phrase par problème de classification, étant des tâches de deux classes.</seg>
<seg id="384">Prenons donc un exemple.</seg>
<seg id="385">Voici nos données de saisie qui se composent d'entités, de fonctions, de texte et de classes.</seg>
<seg id="386">Et nous reformulons la tâche à partir d'une classification du texte en bas ou haut pour classer le texte, le résumé et la classe en vrai ou faux.</seg>
<seg id="387">Ou en d'autres mots, nous avons formé le modèle de langue pour classer un résumé et une classe en résumé et classe, si le résumé appartient à la classe ou non.</seg>
<seg id="388">Donc dans ce cas, le vecteur d'étiquette consiste toujours en deux classes.</seg>
<seg id="389">Et il s'agit de l'algorithme pour notre très bonne approche de raffinement reformulée.</seg>
<seg id="390">Voyons le cadre complet.</seg>
<seg id="391">Les données sont introduites dans FeSTE.</seg>
<seg id="392">Puis FeSTE exécute la phase de liaison d'entités.</seg>
<seg id="393">Il extrait le texte de la base de connaissances, qui dans cet exemple est le résumé de la page Wikipédia.</seg>
<seg id="394">Ensuite, il a reformulé la tâche en une tâche de classification des phrases par paire.</seg>
<seg id="395">Il a appliqué le modèle de langue à la nouvelle tâche et la probabilité de sortie pour chaque classe.</seg>
<seg id="396">Et maintenant, le modèle de langue est déjà raffiné sur des données n moins un en utilisant un raffinement multitâche préliminaire.</seg>
<seg id="397">Ensuite, nous utilisons le vecteur de sortie du modèle de langue comme une fonction nouvellement générée dans le nombre de classes.</seg>
<seg id="398">Pour évaluer notre cadre, nous utilisons dix-sept données de classification tabulaires qui varient en taille, fonctions, équilibre, domaine et performance initiale.</seg>
<seg id="399">Et en tant que base de connaissances, nous utilisons Wikipédia.</seg>
<seg id="400">Nous concevons notre expérience comme une évaluation où nous formons FeSTe sur seize données et l'appliquons à la dix-septième donnée.</seg>
<seg id="401">Nous divisons également chaque donnée en quatre plis et appliquons une validation croisée de quatre plis.</seg>
<seg id="402">Ensuite, nous générons les nouvelles fonctions et les évaluons en utilisant cinq classificateurs d'évaluation.</seg>
<seg id="403">Nous utilisons dans nos expériences l'architecture de base des Représentations d'encodeurs bidirectionnels à partir de transformateurs.</seg>
<seg id="404">Voici les résultats de nos expériences.</seg>
<seg id="405">Vous pouvez voir que nous comparons notre cadre au raffinement des données cibles, au raffinement de la tâche cible et à un raffinement préliminaire MTDNN.</seg>
<seg id="406">Et notre raffinement reformulé atteint le meilleur résultat et la meilleure performance.</seg>
<seg id="407">Alors que le MTDNN a obtenu une amélioration de deux pour cent par rapport au raffinement des données cibles.</seg>
<seg id="408">Notre approche a obtenu une amélioration de six pour cent.</seg>
<seg id="409">Lorsque nous regardons les petites données, nous pouvons voir que la performance du MTDNN diminue et l'amélioration de la phase de raffinement multitâche préliminaire diminue aussi à un virgule cinq pour cent.</seg>
<seg id="410">Mais notre performance a augmenté à onze pour cent comparé au raffinement de tâche cible seul.</seg>
<seg id="411">Pour résumer, FeSTE permet un enrichissement en quelques coups à partir de trente-cinq échantillons dans nos expériences.</seg>
<seg id="412">Il utilise une architecture pour toutes les tâches et données.</seg>
<seg id="413">Et il garde la tête du modèle.</seg>
<seg id="414">Mais cela ajoute une phase de reformulation.</seg>
<seg id="415">Il augmente l'ensemble de formation et a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'introduire dans le modèle de langue et l'utiliser dans le problème de classification par paire de phrases.</seg>
<seg id="416">Merci.</seg>
</doc>
</srcset>
</mteval>
